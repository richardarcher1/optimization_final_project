{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.08231652821810587,
  "eval_steps": 42519,
  "global_step": 17500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.7038016124631924e-06,
      "grad_norm": 3.8041722774505615,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.7375,
      "step": 1
    },
    {
      "epoch": 9.407603224926385e-06,
      "grad_norm": 4.354788303375244,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.5438,
      "step": 2
    },
    {
      "epoch": 1.4111404837389578e-05,
      "grad_norm": 6.505493640899658,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6008,
      "step": 3
    },
    {
      "epoch": 1.881520644985277e-05,
      "grad_norm": 4.024658203125,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.5406,
      "step": 4
    },
    {
      "epoch": 2.3519008062315965e-05,
      "grad_norm": 0.6205708980560303,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0346,
      "step": 5
    },
    {
      "epoch": 2.8222809674779156e-05,
      "grad_norm": 2.1557469367980957,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.2234,
      "step": 6
    },
    {
      "epoch": 3.292661128724235e-05,
      "grad_norm": 2.4203062057495117,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.1888,
      "step": 7
    },
    {
      "epoch": 3.763041289970554e-05,
      "grad_norm": 4.038275718688965,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.5306,
      "step": 8
    },
    {
      "epoch": 4.2334214512168735e-05,
      "grad_norm": 5.567150592803955,
      "learning_rate": 3.6e-06,
      "loss": 0.8904,
      "step": 9
    },
    {
      "epoch": 4.703801612463193e-05,
      "grad_norm": 2.0147883892059326,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.2581,
      "step": 10
    },
    {
      "epoch": 5.174181773709512e-05,
      "grad_norm": 2.8323683738708496,
      "learning_rate": 4.4e-06,
      "loss": 0.1862,
      "step": 11
    },
    {
      "epoch": 5.644561934955831e-05,
      "grad_norm": 1.6947802305221558,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.1079,
      "step": 12
    },
    {
      "epoch": 6.11494209620215e-05,
      "grad_norm": 4.809198379516602,
      "learning_rate": 5.2e-06,
      "loss": 0.6683,
      "step": 13
    },
    {
      "epoch": 6.58532225744847e-05,
      "grad_norm": 5.638553142547607,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.8748,
      "step": 14
    },
    {
      "epoch": 7.055702418694789e-05,
      "grad_norm": 1.4174034595489502,
      "learning_rate": 6e-06,
      "loss": 0.1869,
      "step": 15
    },
    {
      "epoch": 7.526082579941108e-05,
      "grad_norm": 7.593025207519531,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.7924,
      "step": 16
    },
    {
      "epoch": 7.996462741187428e-05,
      "grad_norm": 7.029094696044922,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.8385,
      "step": 17
    },
    {
      "epoch": 8.466842902433747e-05,
      "grad_norm": 4.548736095428467,
      "learning_rate": 7.2e-06,
      "loss": 0.7518,
      "step": 18
    },
    {
      "epoch": 8.937223063680066e-05,
      "grad_norm": 2.9964427947998047,
      "learning_rate": 7.6e-06,
      "loss": 0.6038,
      "step": 19
    },
    {
      "epoch": 9.407603224926386e-05,
      "grad_norm": 7.868130207061768,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.625,
      "step": 20
    },
    {
      "epoch": 9.877983386172705e-05,
      "grad_norm": 2.2138407230377197,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.1034,
      "step": 21
    },
    {
      "epoch": 0.00010348363547419024,
      "grad_norm": 2.5939178466796875,
      "learning_rate": 8.8e-06,
      "loss": 0.3425,
      "step": 22
    },
    {
      "epoch": 0.00010818743708665344,
      "grad_norm": 1.3374449014663696,
      "learning_rate": 9.2e-06,
      "loss": 0.076,
      "step": 23
    },
    {
      "epoch": 0.00011289123869911663,
      "grad_norm": 2.0057127475738525,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.3457,
      "step": 24
    },
    {
      "epoch": 0.00011759504031157981,
      "grad_norm": 2.428433418273926,
      "learning_rate": 1e-05,
      "loss": 0.2852,
      "step": 25
    },
    {
      "epoch": 0.000122298841924043,
      "grad_norm": 4.798130512237549,
      "learning_rate": 1.04e-05,
      "loss": 0.55,
      "step": 26
    },
    {
      "epoch": 0.0001270026435365062,
      "grad_norm": 1.618207335472107,
      "learning_rate": 1.08e-05,
      "loss": 0.219,
      "step": 27
    },
    {
      "epoch": 0.0001317064451489694,
      "grad_norm": 1.1659034490585327,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.053,
      "step": 28
    },
    {
      "epoch": 0.00013641024676143258,
      "grad_norm": 2.4309582710266113,
      "learning_rate": 1.16e-05,
      "loss": 0.346,
      "step": 29
    },
    {
      "epoch": 0.00014111404837389578,
      "grad_norm": 0.9599677324295044,
      "learning_rate": 1.2e-05,
      "loss": 0.0486,
      "step": 30
    },
    {
      "epoch": 0.00014581784998635898,
      "grad_norm": 4.665159225463867,
      "learning_rate": 1.24e-05,
      "loss": 0.4113,
      "step": 31
    },
    {
      "epoch": 0.00015052165159882216,
      "grad_norm": 5.331118106842041,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.9317,
      "step": 32
    },
    {
      "epoch": 0.00015522545321128536,
      "grad_norm": 1.710689663887024,
      "learning_rate": 1.32e-05,
      "loss": 0.2711,
      "step": 33
    },
    {
      "epoch": 0.00015992925482374856,
      "grad_norm": 2.14216947555542,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.1654,
      "step": 34
    },
    {
      "epoch": 0.00016463305643621174,
      "grad_norm": 4.485231399536133,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5813,
      "step": 35
    },
    {
      "epoch": 0.00016933685804867494,
      "grad_norm": 1.9404183626174927,
      "learning_rate": 1.44e-05,
      "loss": 0.3459,
      "step": 36
    },
    {
      "epoch": 0.00017404065966113814,
      "grad_norm": 1.6494685411453247,
      "learning_rate": 1.48e-05,
      "loss": 0.2261,
      "step": 37
    },
    {
      "epoch": 0.00017874446127360131,
      "grad_norm": 2.5063393115997314,
      "learning_rate": 1.52e-05,
      "loss": 0.2159,
      "step": 38
    },
    {
      "epoch": 0.00018344826288606452,
      "grad_norm": 1.0466309785842896,
      "learning_rate": 1.56e-05,
      "loss": 0.0309,
      "step": 39
    },
    {
      "epoch": 0.00018815206449852772,
      "grad_norm": 1.0847481489181519,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1703,
      "step": 40
    },
    {
      "epoch": 0.0001928558661109909,
      "grad_norm": 3.547720193862915,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.119,
      "step": 41
    },
    {
      "epoch": 0.0001975596677234541,
      "grad_norm": 1.8616689443588257,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2958,
      "step": 42
    },
    {
      "epoch": 0.0002022634693359173,
      "grad_norm": 5.395574569702148,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.3685,
      "step": 43
    },
    {
      "epoch": 0.00020696727094838047,
      "grad_norm": 2.6937150955200195,
      "learning_rate": 1.76e-05,
      "loss": 0.1679,
      "step": 44
    },
    {
      "epoch": 0.00021167107256084367,
      "grad_norm": 2.65310001373291,
      "learning_rate": 1.8e-05,
      "loss": 0.1311,
      "step": 45
    },
    {
      "epoch": 0.00021637487417330687,
      "grad_norm": 3.083963632583618,
      "learning_rate": 1.84e-05,
      "loss": 0.3188,
      "step": 46
    },
    {
      "epoch": 0.00022107867578577005,
      "grad_norm": 2.4717369079589844,
      "learning_rate": 1.88e-05,
      "loss": 0.075,
      "step": 47
    },
    {
      "epoch": 0.00022578247739823325,
      "grad_norm": 2.2474513053894043,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.2237,
      "step": 48
    },
    {
      "epoch": 0.00023048627901069645,
      "grad_norm": 2.5590081214904785,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.1322,
      "step": 49
    },
    {
      "epoch": 0.00023519008062315963,
      "grad_norm": 3.6861019134521484,
      "learning_rate": 2e-05,
      "loss": 0.253,
      "step": 50
    },
    {
      "epoch": 0.00023989388223562283,
      "grad_norm": 5.394125461578369,
      "learning_rate": 2.04e-05,
      "loss": 0.1632,
      "step": 51
    },
    {
      "epoch": 0.000244597683848086,
      "grad_norm": 2.0115935802459717,
      "learning_rate": 2.08e-05,
      "loss": 0.1024,
      "step": 52
    },
    {
      "epoch": 0.0002493014854605492,
      "grad_norm": 3.8001034259796143,
      "learning_rate": 2.12e-05,
      "loss": 0.3323,
      "step": 53
    },
    {
      "epoch": 0.0002540052870730124,
      "grad_norm": 5.794704914093018,
      "learning_rate": 2.16e-05,
      "loss": 0.1992,
      "step": 54
    },
    {
      "epoch": 0.0002587090886854756,
      "grad_norm": 10.996417999267578,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.933,
      "step": 55
    },
    {
      "epoch": 0.0002634128902979388,
      "grad_norm": 10.299786567687988,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.9505,
      "step": 56
    },
    {
      "epoch": 0.000268116691910402,
      "grad_norm": 12.242238998413086,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.0156,
      "step": 57
    },
    {
      "epoch": 0.00027282049352286516,
      "grad_norm": 2.7500998973846436,
      "learning_rate": 2.32e-05,
      "loss": 0.271,
      "step": 58
    },
    {
      "epoch": 0.00027752429513532836,
      "grad_norm": 2.10526442527771,
      "learning_rate": 2.36e-05,
      "loss": 0.089,
      "step": 59
    },
    {
      "epoch": 0.00028222809674779156,
      "grad_norm": 12.716087341308594,
      "learning_rate": 2.4e-05,
      "loss": 0.8743,
      "step": 60
    },
    {
      "epoch": 0.00028693189836025477,
      "grad_norm": 9.265603065490723,
      "learning_rate": 2.44e-05,
      "loss": 0.5664,
      "step": 61
    },
    {
      "epoch": 0.00029163569997271797,
      "grad_norm": 13.476297378540039,
      "learning_rate": 2.48e-05,
      "loss": 0.6459,
      "step": 62
    },
    {
      "epoch": 0.00029633950158518117,
      "grad_norm": 2.3783257007598877,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0892,
      "step": 63
    },
    {
      "epoch": 0.0003010433031976443,
      "grad_norm": 12.59670639038086,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.4919,
      "step": 64
    },
    {
      "epoch": 0.0003057471048101075,
      "grad_norm": 9.37648868560791,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4956,
      "step": 65
    },
    {
      "epoch": 0.0003104509064225707,
      "grad_norm": 3.158296585083008,
      "learning_rate": 2.64e-05,
      "loss": 0.1308,
      "step": 66
    },
    {
      "epoch": 0.0003151547080350339,
      "grad_norm": 8.29719352722168,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.2386,
      "step": 67
    },
    {
      "epoch": 0.0003198585096474971,
      "grad_norm": 4.998426914215088,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.2414,
      "step": 68
    },
    {
      "epoch": 0.0003245623112599603,
      "grad_norm": 3.8499088287353516,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.1199,
      "step": 69
    },
    {
      "epoch": 0.00032926611287242347,
      "grad_norm": 7.454540729522705,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.451,
      "step": 70
    },
    {
      "epoch": 0.0003339699144848867,
      "grad_norm": 4.821732997894287,
      "learning_rate": 2.84e-05,
      "loss": 0.2166,
      "step": 71
    },
    {
      "epoch": 0.0003386737160973499,
      "grad_norm": 2.726874589920044,
      "learning_rate": 2.88e-05,
      "loss": 0.0991,
      "step": 72
    },
    {
      "epoch": 0.0003433775177098131,
      "grad_norm": 13.291203498840332,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.9011,
      "step": 73
    },
    {
      "epoch": 0.0003480813193222763,
      "grad_norm": 8.086989402770996,
      "learning_rate": 2.96e-05,
      "loss": 0.3664,
      "step": 74
    },
    {
      "epoch": 0.0003527851209347395,
      "grad_norm": 14.738718032836914,
      "learning_rate": 3e-05,
      "loss": 0.6584,
      "step": 75
    },
    {
      "epoch": 0.00035748892254720263,
      "grad_norm": 10.834789276123047,
      "learning_rate": 3.04e-05,
      "loss": 0.5116,
      "step": 76
    },
    {
      "epoch": 0.00036219272415966583,
      "grad_norm": 2.7025794982910156,
      "learning_rate": 3.08e-05,
      "loss": 0.1983,
      "step": 77
    },
    {
      "epoch": 0.00036689652577212903,
      "grad_norm": 2.4473915100097656,
      "learning_rate": 3.12e-05,
      "loss": 0.1744,
      "step": 78
    },
    {
      "epoch": 0.00037160032738459223,
      "grad_norm": 6.345713138580322,
      "learning_rate": 3.16e-05,
      "loss": 0.229,
      "step": 79
    },
    {
      "epoch": 0.00037630412899705544,
      "grad_norm": 10.160054206848145,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4571,
      "step": 80
    },
    {
      "epoch": 0.00038100793060951864,
      "grad_norm": 2.898986339569092,
      "learning_rate": 3.24e-05,
      "loss": 0.0716,
      "step": 81
    },
    {
      "epoch": 0.0003857117322219818,
      "grad_norm": 5.519388198852539,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.1854,
      "step": 82
    },
    {
      "epoch": 0.000390415533834445,
      "grad_norm": 4.479554653167725,
      "learning_rate": 3.32e-05,
      "loss": 0.2935,
      "step": 83
    },
    {
      "epoch": 0.0003951193354469082,
      "grad_norm": 10.21839714050293,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.4533,
      "step": 84
    },
    {
      "epoch": 0.0003998231370593714,
      "grad_norm": 1.8370931148529053,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0839,
      "step": 85
    },
    {
      "epoch": 0.0004045269386718346,
      "grad_norm": 6.385246276855469,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.306,
      "step": 86
    },
    {
      "epoch": 0.0004092307402842978,
      "grad_norm": 9.136309623718262,
      "learning_rate": 3.48e-05,
      "loss": 0.3864,
      "step": 87
    },
    {
      "epoch": 0.00041393454189676094,
      "grad_norm": 6.78175163269043,
      "learning_rate": 3.52e-05,
      "loss": 0.2843,
      "step": 88
    },
    {
      "epoch": 0.00041863834350922414,
      "grad_norm": 1.9543720483779907,
      "learning_rate": 3.56e-05,
      "loss": 0.0363,
      "step": 89
    },
    {
      "epoch": 0.00042334214512168735,
      "grad_norm": 4.420919895172119,
      "learning_rate": 3.6e-05,
      "loss": 0.2107,
      "step": 90
    },
    {
      "epoch": 0.00042804594673415055,
      "grad_norm": 5.062385559082031,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.1204,
      "step": 91
    },
    {
      "epoch": 0.00043274974834661375,
      "grad_norm": 4.584104537963867,
      "learning_rate": 3.68e-05,
      "loss": 0.5167,
      "step": 92
    },
    {
      "epoch": 0.00043745354995907695,
      "grad_norm": 11.308633804321289,
      "learning_rate": 3.72e-05,
      "loss": 0.5346,
      "step": 93
    },
    {
      "epoch": 0.0004421573515715401,
      "grad_norm": 9.862894058227539,
      "learning_rate": 3.76e-05,
      "loss": 0.7457,
      "step": 94
    },
    {
      "epoch": 0.0004468611531840033,
      "grad_norm": 33.0572509765625,
      "learning_rate": 3.8e-05,
      "loss": 0.7491,
      "step": 95
    },
    {
      "epoch": 0.0004515649547964665,
      "grad_norm": 2.078057289123535,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0662,
      "step": 96
    },
    {
      "epoch": 0.0004562687564089297,
      "grad_norm": 9.548422813415527,
      "learning_rate": 3.88e-05,
      "loss": 1.217,
      "step": 97
    },
    {
      "epoch": 0.0004609725580213929,
      "grad_norm": 3.2684428691864014,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.1829,
      "step": 98
    },
    {
      "epoch": 0.0004656763596338561,
      "grad_norm": 13.482787132263184,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.3653,
      "step": 99
    },
    {
      "epoch": 0.00047038016124631925,
      "grad_norm": 2.9813427925109863,
      "learning_rate": 4e-05,
      "loss": 0.1295,
      "step": 100
    },
    {
      "epoch": 0.00047508396285878246,
      "grad_norm": 12.643174171447754,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.6676,
      "step": 101
    },
    {
      "epoch": 0.00047978776447124566,
      "grad_norm": 18.31464195251465,
      "learning_rate": 4.08e-05,
      "loss": 1.6749,
      "step": 102
    },
    {
      "epoch": 0.00048449156608370886,
      "grad_norm": 5.860870361328125,
      "learning_rate": 4.12e-05,
      "loss": 0.1404,
      "step": 103
    },
    {
      "epoch": 0.000489195367696172,
      "grad_norm": 7.620203495025635,
      "learning_rate": 4.16e-05,
      "loss": 0.3323,
      "step": 104
    },
    {
      "epoch": 0.0004938991693086353,
      "grad_norm": 4.882566928863525,
      "learning_rate": 4.2e-05,
      "loss": 0.1472,
      "step": 105
    },
    {
      "epoch": 0.0004986029709210984,
      "grad_norm": 7.276484489440918,
      "learning_rate": 4.24e-05,
      "loss": 0.1996,
      "step": 106
    },
    {
      "epoch": 0.0005033067725335617,
      "grad_norm": 9.876720428466797,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.7294,
      "step": 107
    },
    {
      "epoch": 0.0005080105741460248,
      "grad_norm": 8.190587043762207,
      "learning_rate": 4.32e-05,
      "loss": 0.4564,
      "step": 108
    },
    {
      "epoch": 0.000512714375758488,
      "grad_norm": 12.742801666259766,
      "learning_rate": 4.36e-05,
      "loss": 1.1462,
      "step": 109
    },
    {
      "epoch": 0.0005174181773709512,
      "grad_norm": 4.173835277557373,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.1501,
      "step": 110
    },
    {
      "epoch": 0.0005221219789834144,
      "grad_norm": 2.1551454067230225,
      "learning_rate": 4.44e-05,
      "loss": 0.0968,
      "step": 111
    },
    {
      "epoch": 0.0005268257805958776,
      "grad_norm": 9.060416221618652,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.9367,
      "step": 112
    },
    {
      "epoch": 0.0005315295822083408,
      "grad_norm": 1.3614718914031982,
      "learning_rate": 4.52e-05,
      "loss": 0.0317,
      "step": 113
    },
    {
      "epoch": 0.000536233383820804,
      "grad_norm": 7.594012260437012,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.8011,
      "step": 114
    },
    {
      "epoch": 0.0005409371854332672,
      "grad_norm": 8.204193115234375,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.4185,
      "step": 115
    },
    {
      "epoch": 0.0005456409870457303,
      "grad_norm": 1.4568963050842285,
      "learning_rate": 4.64e-05,
      "loss": 0.0542,
      "step": 116
    },
    {
      "epoch": 0.0005503447886581936,
      "grad_norm": 9.194090843200684,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.469,
      "step": 117
    },
    {
      "epoch": 0.0005550485902706567,
      "grad_norm": 2.9352447986602783,
      "learning_rate": 4.72e-05,
      "loss": 0.2225,
      "step": 118
    },
    {
      "epoch": 0.00055975239188312,
      "grad_norm": 18.89095115661621,
      "learning_rate": 4.76e-05,
      "loss": 0.8045,
      "step": 119
    },
    {
      "epoch": 0.0005644561934955831,
      "grad_norm": 1.3745579719543457,
      "learning_rate": 4.8e-05,
      "loss": 0.0519,
      "step": 120
    },
    {
      "epoch": 0.0005691599951080463,
      "grad_norm": 6.6944475173950195,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.4802,
      "step": 121
    },
    {
      "epoch": 0.0005738637967205095,
      "grad_norm": 3.0428519248962402,
      "learning_rate": 4.88e-05,
      "loss": 0.1497,
      "step": 122
    },
    {
      "epoch": 0.0005785675983329727,
      "grad_norm": 5.592650413513184,
      "learning_rate": 4.92e-05,
      "loss": 0.5897,
      "step": 123
    },
    {
      "epoch": 0.0005832713999454359,
      "grad_norm": 2.0244908332824707,
      "learning_rate": 4.96e-05,
      "loss": 0.0951,
      "step": 124
    },
    {
      "epoch": 0.0005879752015578991,
      "grad_norm": 4.579395771026611,
      "learning_rate": 5e-05,
      "loss": 0.2466,
      "step": 125
    },
    {
      "epoch": 0.0005926790031703623,
      "grad_norm": 7.484107971191406,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 0.4343,
      "step": 126
    },
    {
      "epoch": 0.0005973828047828255,
      "grad_norm": 3.6764602661132812,
      "learning_rate": 5.08e-05,
      "loss": 0.2355,
      "step": 127
    },
    {
      "epoch": 0.0006020866063952886,
      "grad_norm": 2.7435081005096436,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 0.11,
      "step": 128
    },
    {
      "epoch": 0.0006067904080077519,
      "grad_norm": 2.7252235412597656,
      "learning_rate": 5.16e-05,
      "loss": 0.1106,
      "step": 129
    },
    {
      "epoch": 0.000611494209620215,
      "grad_norm": 5.371652126312256,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.262,
      "step": 130
    },
    {
      "epoch": 0.0006161980112326783,
      "grad_norm": 5.665862560272217,
      "learning_rate": 5.2400000000000007e-05,
      "loss": 0.4071,
      "step": 131
    },
    {
      "epoch": 0.0006209018128451414,
      "grad_norm": 6.609068870544434,
      "learning_rate": 5.28e-05,
      "loss": 0.5057,
      "step": 132
    },
    {
      "epoch": 0.0006256056144576046,
      "grad_norm": 1.3313148021697998,
      "learning_rate": 5.3200000000000006e-05,
      "loss": 0.0579,
      "step": 133
    },
    {
      "epoch": 0.0006303094160700678,
      "grad_norm": 3.316894769668579,
      "learning_rate": 5.360000000000001e-05,
      "loss": 0.1622,
      "step": 134
    },
    {
      "epoch": 0.000635013217682531,
      "grad_norm": 4.961429595947266,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.3672,
      "step": 135
    },
    {
      "epoch": 0.0006397170192949942,
      "grad_norm": 5.017652988433838,
      "learning_rate": 5.440000000000001e-05,
      "loss": 0.3129,
      "step": 136
    },
    {
      "epoch": 0.0006444208209074574,
      "grad_norm": 8.691753387451172,
      "learning_rate": 5.4800000000000004e-05,
      "loss": 0.5734,
      "step": 137
    },
    {
      "epoch": 0.0006491246225199207,
      "grad_norm": 0.6221335530281067,
      "learning_rate": 5.520000000000001e-05,
      "loss": 0.0149,
      "step": 138
    },
    {
      "epoch": 0.0006538284241323838,
      "grad_norm": 1.8843748569488525,
      "learning_rate": 5.560000000000001e-05,
      "loss": 0.1999,
      "step": 139
    },
    {
      "epoch": 0.0006585322257448469,
      "grad_norm": 9.793206214904785,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.6915,
      "step": 140
    },
    {
      "epoch": 0.0006632360273573102,
      "grad_norm": 9.097029685974121,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 0.8844,
      "step": 141
    },
    {
      "epoch": 0.0006679398289697733,
      "grad_norm": 5.145073413848877,
      "learning_rate": 5.68e-05,
      "loss": 0.3095,
      "step": 142
    },
    {
      "epoch": 0.0006726436305822366,
      "grad_norm": 5.8812785148620605,
      "learning_rate": 5.72e-05,
      "loss": 0.4756,
      "step": 143
    },
    {
      "epoch": 0.0006773474321946998,
      "grad_norm": 7.443398952484131,
      "learning_rate": 5.76e-05,
      "loss": 0.4334,
      "step": 144
    },
    {
      "epoch": 0.0006820512338071629,
      "grad_norm": 11.967938423156738,
      "learning_rate": 5.8e-05,
      "loss": 0.7469,
      "step": 145
    },
    {
      "epoch": 0.0006867550354196262,
      "grad_norm": 1.9351905584335327,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 0.0628,
      "step": 146
    },
    {
      "epoch": 0.0006914588370320893,
      "grad_norm": 3.3493361473083496,
      "learning_rate": 5.88e-05,
      "loss": 0.1065,
      "step": 147
    },
    {
      "epoch": 0.0006961626386445526,
      "grad_norm": 6.269193172454834,
      "learning_rate": 5.92e-05,
      "loss": 0.4698,
      "step": 148
    },
    {
      "epoch": 0.0007008664402570157,
      "grad_norm": 0.9900681972503662,
      "learning_rate": 5.96e-05,
      "loss": 0.0214,
      "step": 149
    },
    {
      "epoch": 0.000705570241869479,
      "grad_norm": 3.5989089012145996,
      "learning_rate": 6e-05,
      "loss": 0.3623,
      "step": 150
    },
    {
      "epoch": 0.0007102740434819421,
      "grad_norm": 1.4215503931045532,
      "learning_rate": 6.04e-05,
      "loss": 0.1001,
      "step": 151
    },
    {
      "epoch": 0.0007149778450944053,
      "grad_norm": 2.1982524394989014,
      "learning_rate": 6.08e-05,
      "loss": 0.0498,
      "step": 152
    },
    {
      "epoch": 0.0007196816467068685,
      "grad_norm": 4.4305267333984375,
      "learning_rate": 6.12e-05,
      "loss": 0.3794,
      "step": 153
    },
    {
      "epoch": 0.0007243854483193317,
      "grad_norm": 1.8252030611038208,
      "learning_rate": 6.16e-05,
      "loss": 0.1198,
      "step": 154
    },
    {
      "epoch": 0.0007290892499317949,
      "grad_norm": 6.528279781341553,
      "learning_rate": 6.2e-05,
      "loss": 0.2826,
      "step": 155
    },
    {
      "epoch": 0.0007337930515442581,
      "grad_norm": 3.325836658477783,
      "learning_rate": 6.24e-05,
      "loss": 0.0713,
      "step": 156
    },
    {
      "epoch": 0.0007384968531567212,
      "grad_norm": 2.212897539138794,
      "learning_rate": 6.280000000000001e-05,
      "loss": 0.1107,
      "step": 157
    },
    {
      "epoch": 0.0007432006547691845,
      "grad_norm": 12.83935260772705,
      "learning_rate": 6.32e-05,
      "loss": 0.7381,
      "step": 158
    },
    {
      "epoch": 0.0007479044563816476,
      "grad_norm": 6.565043926239014,
      "learning_rate": 6.36e-05,
      "loss": 0.28,
      "step": 159
    },
    {
      "epoch": 0.0007526082579941109,
      "grad_norm": 7.796690940856934,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.957,
      "step": 160
    },
    {
      "epoch": 0.000757312059606574,
      "grad_norm": 1.417212963104248,
      "learning_rate": 6.440000000000001e-05,
      "loss": 0.0339,
      "step": 161
    },
    {
      "epoch": 0.0007620158612190373,
      "grad_norm": 8.943127632141113,
      "learning_rate": 6.48e-05,
      "loss": 0.8647,
      "step": 162
    },
    {
      "epoch": 0.0007667196628315004,
      "grad_norm": 4.862338542938232,
      "learning_rate": 6.52e-05,
      "loss": 0.2068,
      "step": 163
    },
    {
      "epoch": 0.0007714234644439636,
      "grad_norm": 11.924006462097168,
      "learning_rate": 6.560000000000001e-05,
      "loss": 0.9752,
      "step": 164
    },
    {
      "epoch": 0.0007761272660564268,
      "grad_norm": 9.445218086242676,
      "learning_rate": 6.6e-05,
      "loss": 0.7182,
      "step": 165
    },
    {
      "epoch": 0.00078083106766889,
      "grad_norm": 13.994558334350586,
      "learning_rate": 6.64e-05,
      "loss": 0.857,
      "step": 166
    },
    {
      "epoch": 0.0007855348692813532,
      "grad_norm": 0.8699049949645996,
      "learning_rate": 6.680000000000001e-05,
      "loss": 0.0215,
      "step": 167
    },
    {
      "epoch": 0.0007902386708938164,
      "grad_norm": 1.8705347776412964,
      "learning_rate": 6.720000000000001e-05,
      "loss": 0.0448,
      "step": 168
    },
    {
      "epoch": 0.0007949424725062795,
      "grad_norm": 0.3770587742328644,
      "learning_rate": 6.76e-05,
      "loss": 0.009,
      "step": 169
    },
    {
      "epoch": 0.0007996462741187428,
      "grad_norm": 0.5122890472412109,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0142,
      "step": 170
    },
    {
      "epoch": 0.0008043500757312059,
      "grad_norm": 0.4145972430706024,
      "learning_rate": 6.840000000000001e-05,
      "loss": 0.0102,
      "step": 171
    },
    {
      "epoch": 0.0008090538773436692,
      "grad_norm": 3.248856782913208,
      "learning_rate": 6.879999999999999e-05,
      "loss": 0.1887,
      "step": 172
    },
    {
      "epoch": 0.0008137576789561323,
      "grad_norm": 4.414053440093994,
      "learning_rate": 6.92e-05,
      "loss": 0.4014,
      "step": 173
    },
    {
      "epoch": 0.0008184614805685956,
      "grad_norm": 5.474277019500732,
      "learning_rate": 6.96e-05,
      "loss": 0.418,
      "step": 174
    },
    {
      "epoch": 0.0008231652821810587,
      "grad_norm": 4.56879997253418,
      "learning_rate": 7e-05,
      "loss": 0.3402,
      "step": 175
    },
    {
      "epoch": 0.0008278690837935219,
      "grad_norm": 9.654291152954102,
      "learning_rate": 7.04e-05,
      "loss": 1.128,
      "step": 176
    },
    {
      "epoch": 0.0008325728854059851,
      "grad_norm": 5.972589015960693,
      "learning_rate": 7.08e-05,
      "loss": 0.4881,
      "step": 177
    },
    {
      "epoch": 0.0008372766870184483,
      "grad_norm": 10.788414001464844,
      "learning_rate": 7.12e-05,
      "loss": 0.3418,
      "step": 178
    },
    {
      "epoch": 0.0008419804886309115,
      "grad_norm": 6.574192523956299,
      "learning_rate": 7.16e-05,
      "loss": 0.7221,
      "step": 179
    },
    {
      "epoch": 0.0008466842902433747,
      "grad_norm": 9.723762512207031,
      "learning_rate": 7.2e-05,
      "loss": 0.9245,
      "step": 180
    },
    {
      "epoch": 0.0008513880918558378,
      "grad_norm": 1.9738357067108154,
      "learning_rate": 7.24e-05,
      "loss": 0.1339,
      "step": 181
    },
    {
      "epoch": 0.0008560918934683011,
      "grad_norm": 6.301769733428955,
      "learning_rate": 7.280000000000001e-05,
      "loss": 0.377,
      "step": 182
    },
    {
      "epoch": 0.0008607956950807642,
      "grad_norm": 2.496238946914673,
      "learning_rate": 7.32e-05,
      "loss": 0.2578,
      "step": 183
    },
    {
      "epoch": 0.0008654994966932275,
      "grad_norm": 0.6077069044113159,
      "learning_rate": 7.36e-05,
      "loss": 0.0142,
      "step": 184
    },
    {
      "epoch": 0.0008702032983056906,
      "grad_norm": 5.066396236419678,
      "learning_rate": 7.4e-05,
      "loss": 0.5662,
      "step": 185
    },
    {
      "epoch": 0.0008749070999181539,
      "grad_norm": 5.250504016876221,
      "learning_rate": 7.44e-05,
      "loss": 0.3519,
      "step": 186
    },
    {
      "epoch": 0.000879610901530617,
      "grad_norm": 1.6067606210708618,
      "learning_rate": 7.48e-05,
      "loss": 0.2353,
      "step": 187
    },
    {
      "epoch": 0.0008843147031430802,
      "grad_norm": 4.391499042510986,
      "learning_rate": 7.52e-05,
      "loss": 0.2789,
      "step": 188
    },
    {
      "epoch": 0.0008890185047555435,
      "grad_norm": 4.714088439941406,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.3386,
      "step": 189
    },
    {
      "epoch": 0.0008937223063680066,
      "grad_norm": 1.2048958539962769,
      "learning_rate": 7.6e-05,
      "loss": 0.0683,
      "step": 190
    },
    {
      "epoch": 0.0008984261079804699,
      "grad_norm": 2.281769275665283,
      "learning_rate": 7.64e-05,
      "loss": 0.2103,
      "step": 191
    },
    {
      "epoch": 0.000903129909592933,
      "grad_norm": 2.5437893867492676,
      "learning_rate": 7.680000000000001e-05,
      "loss": 0.3243,
      "step": 192
    },
    {
      "epoch": 0.0009078337112053962,
      "grad_norm": 4.540826320648193,
      "learning_rate": 7.72e-05,
      "loss": 0.3501,
      "step": 193
    },
    {
      "epoch": 0.0009125375128178594,
      "grad_norm": 5.794620037078857,
      "learning_rate": 7.76e-05,
      "loss": 0.469,
      "step": 194
    },
    {
      "epoch": 0.0009172413144303226,
      "grad_norm": 4.361606121063232,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.4278,
      "step": 195
    },
    {
      "epoch": 0.0009219451160427858,
      "grad_norm": 4.752676963806152,
      "learning_rate": 7.840000000000001e-05,
      "loss": 0.2079,
      "step": 196
    },
    {
      "epoch": 0.000926648917655249,
      "grad_norm": 6.17271614074707,
      "learning_rate": 7.88e-05,
      "loss": 0.2917,
      "step": 197
    },
    {
      "epoch": 0.0009313527192677122,
      "grad_norm": 9.199727058410645,
      "learning_rate": 7.920000000000001e-05,
      "loss": 0.5121,
      "step": 198
    },
    {
      "epoch": 0.0009360565208801754,
      "grad_norm": 7.618838787078857,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.4281,
      "step": 199
    },
    {
      "epoch": 0.0009407603224926385,
      "grad_norm": 3.7657148838043213,
      "learning_rate": 8e-05,
      "loss": 0.1815,
      "step": 200
    },
    {
      "epoch": 0.0009454641241051018,
      "grad_norm": 6.839115142822266,
      "learning_rate": 8.04e-05,
      "loss": 0.3815,
      "step": 201
    },
    {
      "epoch": 0.0009501679257175649,
      "grad_norm": 14.295804023742676,
      "learning_rate": 8.080000000000001e-05,
      "loss": 0.2907,
      "step": 202
    },
    {
      "epoch": 0.0009548717273300282,
      "grad_norm": 3.368785858154297,
      "learning_rate": 8.120000000000001e-05,
      "loss": 0.2574,
      "step": 203
    },
    {
      "epoch": 0.0009595755289424913,
      "grad_norm": 2.79160475730896,
      "learning_rate": 8.16e-05,
      "loss": 0.1702,
      "step": 204
    },
    {
      "epoch": 0.0009642793305549545,
      "grad_norm": 6.467050552368164,
      "learning_rate": 8.2e-05,
      "loss": 0.3644,
      "step": 205
    },
    {
      "epoch": 0.0009689831321674177,
      "grad_norm": 2.370896339416504,
      "learning_rate": 8.24e-05,
      "loss": 0.2172,
      "step": 206
    },
    {
      "epoch": 0.0009736869337798809,
      "grad_norm": 3.2137718200683594,
      "learning_rate": 8.28e-05,
      "loss": 0.1011,
      "step": 207
    },
    {
      "epoch": 0.000978390735392344,
      "grad_norm": 1.7555370330810547,
      "learning_rate": 8.32e-05,
      "loss": 0.1656,
      "step": 208
    },
    {
      "epoch": 0.0009830945370048073,
      "grad_norm": 2.7426974773406982,
      "learning_rate": 8.36e-05,
      "loss": 0.273,
      "step": 209
    },
    {
      "epoch": 0.0009877983386172705,
      "grad_norm": 2.8553502559661865,
      "learning_rate": 8.4e-05,
      "loss": 0.1907,
      "step": 210
    },
    {
      "epoch": 0.0009925021402297336,
      "grad_norm": 1.90424382686615,
      "learning_rate": 8.44e-05,
      "loss": 0.107,
      "step": 211
    },
    {
      "epoch": 0.0009972059418421968,
      "grad_norm": 5.73706579208374,
      "learning_rate": 8.48e-05,
      "loss": 0.3738,
      "step": 212
    },
    {
      "epoch": 0.00100190974345466,
      "grad_norm": 2.8048489093780518,
      "learning_rate": 8.52e-05,
      "loss": 0.4472,
      "step": 213
    },
    {
      "epoch": 0.0010066135450671233,
      "grad_norm": 1.6752967834472656,
      "learning_rate": 8.560000000000001e-05,
      "loss": 0.0435,
      "step": 214
    },
    {
      "epoch": 0.0010113173466795864,
      "grad_norm": 2.529334306716919,
      "learning_rate": 8.6e-05,
      "loss": 0.0836,
      "step": 215
    },
    {
      "epoch": 0.0010160211482920496,
      "grad_norm": 4.091884613037109,
      "learning_rate": 8.64e-05,
      "loss": 0.4066,
      "step": 216
    },
    {
      "epoch": 0.0010207249499045129,
      "grad_norm": 2.7089121341705322,
      "learning_rate": 8.680000000000001e-05,
      "loss": 0.3219,
      "step": 217
    },
    {
      "epoch": 0.001025428751516976,
      "grad_norm": 1.5982224941253662,
      "learning_rate": 8.72e-05,
      "loss": 0.0786,
      "step": 218
    },
    {
      "epoch": 0.0010301325531294392,
      "grad_norm": 4.552698135375977,
      "learning_rate": 8.76e-05,
      "loss": 0.3143,
      "step": 219
    },
    {
      "epoch": 0.0010348363547419024,
      "grad_norm": 2.8155717849731445,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.1612,
      "step": 220
    },
    {
      "epoch": 0.0010395401563543657,
      "grad_norm": 1.075063943862915,
      "learning_rate": 8.840000000000001e-05,
      "loss": 0.0651,
      "step": 221
    },
    {
      "epoch": 0.0010442439579668287,
      "grad_norm": 5.97920036315918,
      "learning_rate": 8.88e-05,
      "loss": 0.3641,
      "step": 222
    },
    {
      "epoch": 0.001048947759579292,
      "grad_norm": 3.6123642921447754,
      "learning_rate": 8.92e-05,
      "loss": 0.1178,
      "step": 223
    },
    {
      "epoch": 0.0010536515611917552,
      "grad_norm": 5.538098335266113,
      "learning_rate": 8.960000000000001e-05,
      "loss": 0.2042,
      "step": 224
    },
    {
      "epoch": 0.0010583553628042183,
      "grad_norm": 6.724918842315674,
      "learning_rate": 9e-05,
      "loss": 0.5351,
      "step": 225
    },
    {
      "epoch": 0.0010630591644166815,
      "grad_norm": 2.9325978755950928,
      "learning_rate": 9.04e-05,
      "loss": 0.105,
      "step": 226
    },
    {
      "epoch": 0.0010677629660291448,
      "grad_norm": 9.571220397949219,
      "learning_rate": 9.080000000000001e-05,
      "loss": 0.8588,
      "step": 227
    },
    {
      "epoch": 0.001072466767641608,
      "grad_norm": 6.101480484008789,
      "learning_rate": 9.120000000000001e-05,
      "loss": 0.6106,
      "step": 228
    },
    {
      "epoch": 0.001077170569254071,
      "grad_norm": 10.276091575622559,
      "learning_rate": 9.16e-05,
      "loss": 0.7576,
      "step": 229
    },
    {
      "epoch": 0.0010818743708665343,
      "grad_norm": 1.127669095993042,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.0297,
      "step": 230
    },
    {
      "epoch": 0.0010865781724789976,
      "grad_norm": 2.7628986835479736,
      "learning_rate": 9.240000000000001e-05,
      "loss": 0.1571,
      "step": 231
    },
    {
      "epoch": 0.0010912819740914606,
      "grad_norm": 14.119653701782227,
      "learning_rate": 9.28e-05,
      "loss": 1.9333,
      "step": 232
    },
    {
      "epoch": 0.001095985775703924,
      "grad_norm": 13.460287094116211,
      "learning_rate": 9.320000000000002e-05,
      "loss": 1.212,
      "step": 233
    },
    {
      "epoch": 0.0011006895773163872,
      "grad_norm": 4.873778343200684,
      "learning_rate": 9.360000000000001e-05,
      "loss": 0.3513,
      "step": 234
    },
    {
      "epoch": 0.0011053933789288502,
      "grad_norm": 3.878368616104126,
      "learning_rate": 9.4e-05,
      "loss": 0.2724,
      "step": 235
    },
    {
      "epoch": 0.0011100971805413134,
      "grad_norm": 3.217156410217285,
      "learning_rate": 9.44e-05,
      "loss": 0.2127,
      "step": 236
    },
    {
      "epoch": 0.0011148009821537767,
      "grad_norm": 6.07342004776001,
      "learning_rate": 9.48e-05,
      "loss": 0.2077,
      "step": 237
    },
    {
      "epoch": 0.00111950478376624,
      "grad_norm": 1.9611207246780396,
      "learning_rate": 9.52e-05,
      "loss": 0.1384,
      "step": 238
    },
    {
      "epoch": 0.001124208585378703,
      "grad_norm": 3.120346784591675,
      "learning_rate": 9.56e-05,
      "loss": 0.3784,
      "step": 239
    },
    {
      "epoch": 0.0011289123869911663,
      "grad_norm": 5.3007493019104,
      "learning_rate": 9.6e-05,
      "loss": 0.8169,
      "step": 240
    },
    {
      "epoch": 0.0011336161886036295,
      "grad_norm": 0.5454070568084717,
      "learning_rate": 9.64e-05,
      "loss": 0.0179,
      "step": 241
    },
    {
      "epoch": 0.0011383199902160925,
      "grad_norm": 13.921612739562988,
      "learning_rate": 9.680000000000001e-05,
      "loss": 0.5207,
      "step": 242
    },
    {
      "epoch": 0.0011430237918285558,
      "grad_norm": 3.9617912769317627,
      "learning_rate": 9.72e-05,
      "loss": 0.1699,
      "step": 243
    },
    {
      "epoch": 0.001147727593441019,
      "grad_norm": 2.149620532989502,
      "learning_rate": 9.76e-05,
      "loss": 0.1292,
      "step": 244
    },
    {
      "epoch": 0.0011524313950534823,
      "grad_norm": 5.69052791595459,
      "learning_rate": 9.8e-05,
      "loss": 0.5643,
      "step": 245
    },
    {
      "epoch": 0.0011571351966659454,
      "grad_norm": 2.9579975605010986,
      "learning_rate": 9.84e-05,
      "loss": 0.2219,
      "step": 246
    },
    {
      "epoch": 0.0011618389982784086,
      "grad_norm": 2.4756321907043457,
      "learning_rate": 9.88e-05,
      "loss": 0.2636,
      "step": 247
    },
    {
      "epoch": 0.0011665427998908719,
      "grad_norm": 3.988203287124634,
      "learning_rate": 9.92e-05,
      "loss": 0.491,
      "step": 248
    },
    {
      "epoch": 0.001171246601503335,
      "grad_norm": 1.099124789237976,
      "learning_rate": 9.960000000000001e-05,
      "loss": 0.0622,
      "step": 249
    },
    {
      "epoch": 0.0011759504031157982,
      "grad_norm": 4.605346202850342,
      "learning_rate": 0.0001,
      "loss": 0.2227,
      "step": 250
    },
    {
      "epoch": 0.0011806542047282614,
      "grad_norm": 0.6628114581108093,
      "learning_rate": 0.0001004,
      "loss": 0.0529,
      "step": 251
    },
    {
      "epoch": 0.0011853580063407247,
      "grad_norm": 4.205711364746094,
      "learning_rate": 0.00010080000000000001,
      "loss": 0.2924,
      "step": 252
    },
    {
      "epoch": 0.0011900618079531877,
      "grad_norm": 3.8498294353485107,
      "learning_rate": 0.00010120000000000001,
      "loss": 0.2712,
      "step": 253
    },
    {
      "epoch": 0.001194765609565651,
      "grad_norm": 9.447155952453613,
      "learning_rate": 0.0001016,
      "loss": 0.9995,
      "step": 254
    },
    {
      "epoch": 0.0011994694111781142,
      "grad_norm": 3.0022261142730713,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.3952,
      "step": 255
    },
    {
      "epoch": 0.0012041732127905773,
      "grad_norm": 6.012879371643066,
      "learning_rate": 0.00010240000000000001,
      "loss": 1.2967,
      "step": 256
    },
    {
      "epoch": 0.0012088770144030405,
      "grad_norm": 2.265723466873169,
      "learning_rate": 0.0001028,
      "loss": 0.1677,
      "step": 257
    },
    {
      "epoch": 0.0012135808160155038,
      "grad_norm": 1.6405222415924072,
      "learning_rate": 0.0001032,
      "loss": 0.2574,
      "step": 258
    },
    {
      "epoch": 0.0012182846176279668,
      "grad_norm": 2.443418025970459,
      "learning_rate": 0.00010360000000000001,
      "loss": 0.1905,
      "step": 259
    },
    {
      "epoch": 0.00122298841924043,
      "grad_norm": 4.350813865661621,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.267,
      "step": 260
    },
    {
      "epoch": 0.0012276922208528933,
      "grad_norm": 1.9887791872024536,
      "learning_rate": 0.0001044,
      "loss": 0.1648,
      "step": 261
    },
    {
      "epoch": 0.0012323960224653566,
      "grad_norm": 1.3817839622497559,
      "learning_rate": 0.00010480000000000001,
      "loss": 0.0726,
      "step": 262
    },
    {
      "epoch": 0.0012370998240778196,
      "grad_norm": 3.970262050628662,
      "learning_rate": 0.00010520000000000001,
      "loss": 0.2769,
      "step": 263
    },
    {
      "epoch": 0.0012418036256902829,
      "grad_norm": 13.086316108703613,
      "learning_rate": 0.0001056,
      "loss": 1.1528,
      "step": 264
    },
    {
      "epoch": 0.0012465074273027461,
      "grad_norm": 8.955090522766113,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.5816,
      "step": 265
    },
    {
      "epoch": 0.0012512112289152092,
      "grad_norm": 7.970674991607666,
      "learning_rate": 0.00010640000000000001,
      "loss": 0.371,
      "step": 266
    },
    {
      "epoch": 0.0012559150305276724,
      "grad_norm": 5.113158226013184,
      "learning_rate": 0.00010680000000000001,
      "loss": 0.6101,
      "step": 267
    },
    {
      "epoch": 0.0012606188321401357,
      "grad_norm": 6.568826675415039,
      "learning_rate": 0.00010720000000000002,
      "loss": 0.3781,
      "step": 268
    },
    {
      "epoch": 0.001265322633752599,
      "grad_norm": 6.245065689086914,
      "learning_rate": 0.00010760000000000001,
      "loss": 0.3579,
      "step": 269
    },
    {
      "epoch": 0.001270026435365062,
      "grad_norm": 2.730668306350708,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.1458,
      "step": 270
    },
    {
      "epoch": 0.0012747302369775252,
      "grad_norm": 2.522618532180786,
      "learning_rate": 0.00010840000000000002,
      "loss": 0.2655,
      "step": 271
    },
    {
      "epoch": 0.0012794340385899885,
      "grad_norm": 3.7141852378845215,
      "learning_rate": 0.00010880000000000002,
      "loss": 0.4087,
      "step": 272
    },
    {
      "epoch": 0.0012841378402024515,
      "grad_norm": 5.851397514343262,
      "learning_rate": 0.00010920000000000001,
      "loss": 0.363,
      "step": 273
    },
    {
      "epoch": 0.0012888416418149148,
      "grad_norm": 4.247869491577148,
      "learning_rate": 0.00010960000000000001,
      "loss": 0.2285,
      "step": 274
    },
    {
      "epoch": 0.001293545443427378,
      "grad_norm": 2.001089572906494,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.1478,
      "step": 275
    },
    {
      "epoch": 0.0012982492450398413,
      "grad_norm": 0.5621914267539978,
      "learning_rate": 0.00011040000000000001,
      "loss": 0.0185,
      "step": 276
    },
    {
      "epoch": 0.0013029530466523043,
      "grad_norm": 2.1327598094940186,
      "learning_rate": 0.00011080000000000001,
      "loss": 0.1601,
      "step": 277
    },
    {
      "epoch": 0.0013076568482647676,
      "grad_norm": 3.787323236465454,
      "learning_rate": 0.00011120000000000002,
      "loss": 0.2316,
      "step": 278
    },
    {
      "epoch": 0.0013123606498772309,
      "grad_norm": 7.50827169418335,
      "learning_rate": 0.00011160000000000002,
      "loss": 0.7375,
      "step": 279
    },
    {
      "epoch": 0.0013170644514896939,
      "grad_norm": 2.4351136684417725,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.1164,
      "step": 280
    },
    {
      "epoch": 0.0013217682531021571,
      "grad_norm": 6.235156536102295,
      "learning_rate": 0.00011240000000000002,
      "loss": 0.6748,
      "step": 281
    },
    {
      "epoch": 0.0013264720547146204,
      "grad_norm": 6.498373985290527,
      "learning_rate": 0.00011279999999999999,
      "loss": 0.3911,
      "step": 282
    },
    {
      "epoch": 0.0013311758563270834,
      "grad_norm": 1.9898738861083984,
      "learning_rate": 0.0001132,
      "loss": 0.1857,
      "step": 283
    },
    {
      "epoch": 0.0013358796579395467,
      "grad_norm": 5.861527442932129,
      "learning_rate": 0.0001136,
      "loss": 0.7763,
      "step": 284
    },
    {
      "epoch": 0.00134058345955201,
      "grad_norm": 4.748760223388672,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.483,
      "step": 285
    },
    {
      "epoch": 0.0013452872611644732,
      "grad_norm": 0.20194639265537262,
      "learning_rate": 0.0001144,
      "loss": 0.0063,
      "step": 286
    },
    {
      "epoch": 0.0013499910627769362,
      "grad_norm": 5.861070156097412,
      "learning_rate": 0.0001148,
      "loss": 0.8052,
      "step": 287
    },
    {
      "epoch": 0.0013546948643893995,
      "grad_norm": 4.721028804779053,
      "learning_rate": 0.0001152,
      "loss": 0.7506,
      "step": 288
    },
    {
      "epoch": 0.0013593986660018628,
      "grad_norm": 0.6236014366149902,
      "learning_rate": 0.00011559999999999999,
      "loss": 0.0229,
      "step": 289
    },
    {
      "epoch": 0.0013641024676143258,
      "grad_norm": 4.265346050262451,
      "learning_rate": 0.000116,
      "loss": 0.4207,
      "step": 290
    },
    {
      "epoch": 0.001368806269226789,
      "grad_norm": 1.9097594022750854,
      "learning_rate": 0.0001164,
      "loss": 0.1345,
      "step": 291
    },
    {
      "epoch": 0.0013735100708392523,
      "grad_norm": 7.239140510559082,
      "learning_rate": 0.00011679999999999999,
      "loss": 0.6882,
      "step": 292
    },
    {
      "epoch": 0.0013782138724517156,
      "grad_norm": 3.469158887863159,
      "learning_rate": 0.0001172,
      "loss": 0.537,
      "step": 293
    },
    {
      "epoch": 0.0013829176740641786,
      "grad_norm": 6.196928024291992,
      "learning_rate": 0.0001176,
      "loss": 0.652,
      "step": 294
    },
    {
      "epoch": 0.0013876214756766419,
      "grad_norm": 5.817218780517578,
      "learning_rate": 0.000118,
      "loss": 0.5883,
      "step": 295
    },
    {
      "epoch": 0.0013923252772891051,
      "grad_norm": 0.9719970226287842,
      "learning_rate": 0.0001184,
      "loss": 0.0995,
      "step": 296
    },
    {
      "epoch": 0.0013970290789015682,
      "grad_norm": 1.451691746711731,
      "learning_rate": 0.0001188,
      "loss": 0.0822,
      "step": 297
    },
    {
      "epoch": 0.0014017328805140314,
      "grad_norm": 4.7092790603637695,
      "learning_rate": 0.0001192,
      "loss": 0.6913,
      "step": 298
    },
    {
      "epoch": 0.0014064366821264947,
      "grad_norm": 2.3695647716522217,
      "learning_rate": 0.00011960000000000001,
      "loss": 0.1064,
      "step": 299
    },
    {
      "epoch": 0.001411140483738958,
      "grad_norm": 1.1952271461486816,
      "learning_rate": 0.00012,
      "loss": 0.2535,
      "step": 300
    },
    {
      "epoch": 0.001415844285351421,
      "grad_norm": 5.761165142059326,
      "learning_rate": 0.0001204,
      "loss": 0.4578,
      "step": 301
    },
    {
      "epoch": 0.0014205480869638842,
      "grad_norm": 9.61013126373291,
      "learning_rate": 0.0001208,
      "loss": 0.5669,
      "step": 302
    },
    {
      "epoch": 0.0014252518885763475,
      "grad_norm": 4.267979621887207,
      "learning_rate": 0.0001212,
      "loss": 0.4776,
      "step": 303
    },
    {
      "epoch": 0.0014299556901888105,
      "grad_norm": 3.4100570678710938,
      "learning_rate": 0.0001216,
      "loss": 0.3825,
      "step": 304
    },
    {
      "epoch": 0.0014346594918012738,
      "grad_norm": 2.896878719329834,
      "learning_rate": 0.000122,
      "loss": 0.479,
      "step": 305
    },
    {
      "epoch": 0.001439363293413737,
      "grad_norm": 7.535650730133057,
      "learning_rate": 0.0001224,
      "loss": 0.988,
      "step": 306
    },
    {
      "epoch": 0.0014440670950262,
      "grad_norm": 7.952488899230957,
      "learning_rate": 0.0001228,
      "loss": 1.0579,
      "step": 307
    },
    {
      "epoch": 0.0014487708966386633,
      "grad_norm": 1.957253336906433,
      "learning_rate": 0.0001232,
      "loss": 0.2634,
      "step": 308
    },
    {
      "epoch": 0.0014534746982511266,
      "grad_norm": 4.205195426940918,
      "learning_rate": 0.0001236,
      "loss": 0.5535,
      "step": 309
    },
    {
      "epoch": 0.0014581784998635898,
      "grad_norm": 2.924506425857544,
      "learning_rate": 0.000124,
      "loss": 0.4655,
      "step": 310
    },
    {
      "epoch": 0.0014628823014760529,
      "grad_norm": 3.6749260425567627,
      "learning_rate": 0.00012440000000000002,
      "loss": 0.5132,
      "step": 311
    },
    {
      "epoch": 0.0014675861030885161,
      "grad_norm": 2.4244179725646973,
      "learning_rate": 0.0001248,
      "loss": 0.4429,
      "step": 312
    },
    {
      "epoch": 0.0014722899047009794,
      "grad_norm": 2.774231195449829,
      "learning_rate": 0.0001252,
      "loss": 0.3405,
      "step": 313
    },
    {
      "epoch": 0.0014769937063134424,
      "grad_norm": 2.4018137454986572,
      "learning_rate": 0.00012560000000000002,
      "loss": 0.2759,
      "step": 314
    },
    {
      "epoch": 0.0014816975079259057,
      "grad_norm": 2.1334757804870605,
      "learning_rate": 0.000126,
      "loss": 0.2498,
      "step": 315
    },
    {
      "epoch": 0.001486401309538369,
      "grad_norm": 2.5756373405456543,
      "learning_rate": 0.0001264,
      "loss": 0.3827,
      "step": 316
    },
    {
      "epoch": 0.0014911051111508322,
      "grad_norm": 2.4243853092193604,
      "learning_rate": 0.00012680000000000002,
      "loss": 0.3527,
      "step": 317
    },
    {
      "epoch": 0.0014958089127632952,
      "grad_norm": 3.0257978439331055,
      "learning_rate": 0.0001272,
      "loss": 0.2736,
      "step": 318
    },
    {
      "epoch": 0.0015005127143757585,
      "grad_norm": 2.501581907272339,
      "learning_rate": 0.0001276,
      "loss": 0.5115,
      "step": 319
    },
    {
      "epoch": 0.0015052165159882217,
      "grad_norm": 2.4450430870056152,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.149,
      "step": 320
    },
    {
      "epoch": 0.0015099203176006848,
      "grad_norm": 4.12658166885376,
      "learning_rate": 0.0001284,
      "loss": 0.4058,
      "step": 321
    },
    {
      "epoch": 0.001514624119213148,
      "grad_norm": 5.315903186798096,
      "learning_rate": 0.00012880000000000001,
      "loss": 0.5214,
      "step": 322
    },
    {
      "epoch": 0.0015193279208256113,
      "grad_norm": 1.2464120388031006,
      "learning_rate": 0.00012920000000000002,
      "loss": 0.1062,
      "step": 323
    },
    {
      "epoch": 0.0015240317224380746,
      "grad_norm": 1.9358088970184326,
      "learning_rate": 0.0001296,
      "loss": 0.1757,
      "step": 324
    },
    {
      "epoch": 0.0015287355240505376,
      "grad_norm": 1.993677020072937,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.1378,
      "step": 325
    },
    {
      "epoch": 0.0015334393256630008,
      "grad_norm": 2.1281518936157227,
      "learning_rate": 0.0001304,
      "loss": 0.148,
      "step": 326
    },
    {
      "epoch": 0.001538143127275464,
      "grad_norm": 4.741973876953125,
      "learning_rate": 0.0001308,
      "loss": 0.3754,
      "step": 327
    },
    {
      "epoch": 0.0015428469288879271,
      "grad_norm": 1.0350943803787231,
      "learning_rate": 0.00013120000000000002,
      "loss": 0.0534,
      "step": 328
    },
    {
      "epoch": 0.0015475507305003904,
      "grad_norm": 7.16654109954834,
      "learning_rate": 0.0001316,
      "loss": 0.3577,
      "step": 329
    },
    {
      "epoch": 0.0015522545321128537,
      "grad_norm": 3.2450430393218994,
      "learning_rate": 0.000132,
      "loss": 0.1855,
      "step": 330
    },
    {
      "epoch": 0.001556958333725317,
      "grad_norm": 1.9723613262176514,
      "learning_rate": 0.00013240000000000002,
      "loss": 0.0674,
      "step": 331
    },
    {
      "epoch": 0.00156166213533778,
      "grad_norm": 2.4443912506103516,
      "learning_rate": 0.0001328,
      "loss": 0.0759,
      "step": 332
    },
    {
      "epoch": 0.0015663659369502432,
      "grad_norm": 2.283569097518921,
      "learning_rate": 0.0001332,
      "loss": 0.0385,
      "step": 333
    },
    {
      "epoch": 0.0015710697385627065,
      "grad_norm": 1.6785517930984497,
      "learning_rate": 0.00013360000000000002,
      "loss": 0.1018,
      "step": 334
    },
    {
      "epoch": 0.0015757735401751695,
      "grad_norm": 2.3101062774658203,
      "learning_rate": 0.000134,
      "loss": 0.0711,
      "step": 335
    },
    {
      "epoch": 0.0015804773417876328,
      "grad_norm": 8.354108810424805,
      "learning_rate": 0.00013440000000000001,
      "loss": 0.2996,
      "step": 336
    },
    {
      "epoch": 0.001585181143400096,
      "grad_norm": 0.7124830484390259,
      "learning_rate": 0.00013480000000000002,
      "loss": 0.0205,
      "step": 337
    },
    {
      "epoch": 0.001589884945012559,
      "grad_norm": 13.39250373840332,
      "learning_rate": 0.0001352,
      "loss": 1.2915,
      "step": 338
    },
    {
      "epoch": 0.0015945887466250223,
      "grad_norm": 4.381448268890381,
      "learning_rate": 0.00013560000000000002,
      "loss": 0.1355,
      "step": 339
    },
    {
      "epoch": 0.0015992925482374856,
      "grad_norm": 3.804953098297119,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.134,
      "step": 340
    },
    {
      "epoch": 0.0016039963498499488,
      "grad_norm": 17.82666015625,
      "learning_rate": 0.0001364,
      "loss": 0.8447,
      "step": 341
    },
    {
      "epoch": 0.0016087001514624119,
      "grad_norm": 9.356554985046387,
      "learning_rate": 0.00013680000000000002,
      "loss": 0.7243,
      "step": 342
    },
    {
      "epoch": 0.0016134039530748751,
      "grad_norm": 12.969133377075195,
      "learning_rate": 0.00013720000000000003,
      "loss": 0.9061,
      "step": 343
    },
    {
      "epoch": 0.0016181077546873384,
      "grad_norm": 6.46461296081543,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.3836,
      "step": 344
    },
    {
      "epoch": 0.0016228115562998014,
      "grad_norm": 14.767903327941895,
      "learning_rate": 0.000138,
      "loss": 0.9385,
      "step": 345
    },
    {
      "epoch": 0.0016275153579122647,
      "grad_norm": 4.248624324798584,
      "learning_rate": 0.0001384,
      "loss": 0.1006,
      "step": 346
    },
    {
      "epoch": 0.001632219159524728,
      "grad_norm": 5.127457141876221,
      "learning_rate": 0.00013879999999999999,
      "loss": 0.1415,
      "step": 347
    },
    {
      "epoch": 0.0016369229611371912,
      "grad_norm": 2.027552843093872,
      "learning_rate": 0.0001392,
      "loss": 0.0487,
      "step": 348
    },
    {
      "epoch": 0.0016416267627496542,
      "grad_norm": 0.6455308794975281,
      "learning_rate": 0.0001396,
      "loss": 0.0231,
      "step": 349
    },
    {
      "epoch": 0.0016463305643621175,
      "grad_norm": 4.503232955932617,
      "learning_rate": 0.00014,
      "loss": 0.3631,
      "step": 350
    },
    {
      "epoch": 0.0016510343659745807,
      "grad_norm": 5.003231525421143,
      "learning_rate": 0.0001404,
      "loss": 0.1904,
      "step": 351
    },
    {
      "epoch": 0.0016557381675870438,
      "grad_norm": 7.7387375831604,
      "learning_rate": 0.0001408,
      "loss": 0.3851,
      "step": 352
    },
    {
      "epoch": 0.001660441969199507,
      "grad_norm": 7.251846790313721,
      "learning_rate": 0.0001412,
      "loss": 0.3054,
      "step": 353
    },
    {
      "epoch": 0.0016651457708119703,
      "grad_norm": 0.8463186025619507,
      "learning_rate": 0.0001416,
      "loss": 0.0314,
      "step": 354
    },
    {
      "epoch": 0.0016698495724244335,
      "grad_norm": 3.3773341178894043,
      "learning_rate": 0.000142,
      "loss": 0.2139,
      "step": 355
    },
    {
      "epoch": 0.0016745533740368966,
      "grad_norm": 0.49571314454078674,
      "learning_rate": 0.0001424,
      "loss": 0.0212,
      "step": 356
    },
    {
      "epoch": 0.0016792571756493598,
      "grad_norm": 6.931609630584717,
      "learning_rate": 0.0001428,
      "loss": 0.5459,
      "step": 357
    },
    {
      "epoch": 0.001683960977261823,
      "grad_norm": 7.149381637573242,
      "learning_rate": 0.0001432,
      "loss": 0.5258,
      "step": 358
    },
    {
      "epoch": 0.0016886647788742861,
      "grad_norm": 10.123090744018555,
      "learning_rate": 0.0001436,
      "loss": 1.2159,
      "step": 359
    },
    {
      "epoch": 0.0016933685804867494,
      "grad_norm": 3.2398242950439453,
      "learning_rate": 0.000144,
      "loss": 0.1301,
      "step": 360
    },
    {
      "epoch": 0.0016980723820992126,
      "grad_norm": 1.6010240316390991,
      "learning_rate": 0.0001444,
      "loss": 0.0628,
      "step": 361
    },
    {
      "epoch": 0.0017027761837116757,
      "grad_norm": 0.8089118599891663,
      "learning_rate": 0.0001448,
      "loss": 0.0255,
      "step": 362
    },
    {
      "epoch": 0.001707479985324139,
      "grad_norm": 2.7672698497772217,
      "learning_rate": 0.0001452,
      "loss": 0.101,
      "step": 363
    },
    {
      "epoch": 0.0017121837869366022,
      "grad_norm": 4.092283248901367,
      "learning_rate": 0.00014560000000000002,
      "loss": 0.2196,
      "step": 364
    },
    {
      "epoch": 0.0017168875885490654,
      "grad_norm": 6.312417030334473,
      "learning_rate": 0.000146,
      "loss": 0.4104,
      "step": 365
    },
    {
      "epoch": 0.0017215913901615285,
      "grad_norm": 6.616907119750977,
      "learning_rate": 0.0001464,
      "loss": 0.3651,
      "step": 366
    },
    {
      "epoch": 0.0017262951917739917,
      "grad_norm": 4.744364261627197,
      "learning_rate": 0.00014680000000000002,
      "loss": 0.2126,
      "step": 367
    },
    {
      "epoch": 0.001730998993386455,
      "grad_norm": 1.456031084060669,
      "learning_rate": 0.0001472,
      "loss": 0.047,
      "step": 368
    },
    {
      "epoch": 0.001735702794998918,
      "grad_norm": 6.134387969970703,
      "learning_rate": 0.0001476,
      "loss": 0.5088,
      "step": 369
    },
    {
      "epoch": 0.0017404065966113813,
      "grad_norm": 8.216341018676758,
      "learning_rate": 0.000148,
      "loss": 0.6108,
      "step": 370
    },
    {
      "epoch": 0.0017451103982238445,
      "grad_norm": 3.7597427368164062,
      "learning_rate": 0.0001484,
      "loss": 0.149,
      "step": 371
    },
    {
      "epoch": 0.0017498141998363078,
      "grad_norm": 9.81719970703125,
      "learning_rate": 0.0001488,
      "loss": 0.8091,
      "step": 372
    },
    {
      "epoch": 0.0017545180014487708,
      "grad_norm": 9.361310005187988,
      "learning_rate": 0.0001492,
      "loss": 0.8549,
      "step": 373
    },
    {
      "epoch": 0.001759221803061234,
      "grad_norm": 9.573917388916016,
      "learning_rate": 0.0001496,
      "loss": 0.8137,
      "step": 374
    },
    {
      "epoch": 0.0017639256046736974,
      "grad_norm": 9.143319129943848,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1135,
      "step": 375
    },
    {
      "epoch": 0.0017686294062861604,
      "grad_norm": 3.718782663345337,
      "learning_rate": 0.0001504,
      "loss": 0.2149,
      "step": 376
    },
    {
      "epoch": 0.0017733332078986236,
      "grad_norm": 5.175637245178223,
      "learning_rate": 0.0001508,
      "loss": 0.3678,
      "step": 377
    },
    {
      "epoch": 0.001778037009511087,
      "grad_norm": 1.4497367143630981,
      "learning_rate": 0.00015120000000000002,
      "loss": 0.052,
      "step": 378
    },
    {
      "epoch": 0.0017827408111235502,
      "grad_norm": 6.6607794761657715,
      "learning_rate": 0.0001516,
      "loss": 0.5786,
      "step": 379
    },
    {
      "epoch": 0.0017874446127360132,
      "grad_norm": 1.5905323028564453,
      "learning_rate": 0.000152,
      "loss": 0.0694,
      "step": 380
    },
    {
      "epoch": 0.0017921484143484765,
      "grad_norm": 2.8540422916412354,
      "learning_rate": 0.00015240000000000002,
      "loss": 0.1495,
      "step": 381
    },
    {
      "epoch": 0.0017968522159609397,
      "grad_norm": 2.023268461227417,
      "learning_rate": 0.0001528,
      "loss": 0.0823,
      "step": 382
    },
    {
      "epoch": 0.0018015560175734027,
      "grad_norm": 0.8683632612228394,
      "learning_rate": 0.0001532,
      "loss": 0.0576,
      "step": 383
    },
    {
      "epoch": 0.001806259819185866,
      "grad_norm": 1.2248761653900146,
      "learning_rate": 0.00015360000000000002,
      "loss": 0.0439,
      "step": 384
    },
    {
      "epoch": 0.0018109636207983293,
      "grad_norm": 5.612423896789551,
      "learning_rate": 0.000154,
      "loss": 0.3471,
      "step": 385
    },
    {
      "epoch": 0.0018156674224107923,
      "grad_norm": 2.4408791065216064,
      "learning_rate": 0.0001544,
      "loss": 0.0914,
      "step": 386
    },
    {
      "epoch": 0.0018203712240232556,
      "grad_norm": 0.32082241773605347,
      "learning_rate": 0.00015480000000000002,
      "loss": 0.0091,
      "step": 387
    },
    {
      "epoch": 0.0018250750256357188,
      "grad_norm": 10.327925682067871,
      "learning_rate": 0.0001552,
      "loss": 0.571,
      "step": 388
    },
    {
      "epoch": 0.001829778827248182,
      "grad_norm": 0.12793880701065063,
      "learning_rate": 0.00015560000000000001,
      "loss": 0.0019,
      "step": 389
    },
    {
      "epoch": 0.001834482628860645,
      "grad_norm": 0.835273265838623,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0245,
      "step": 390
    },
    {
      "epoch": 0.0018391864304731084,
      "grad_norm": 0.30255475640296936,
      "learning_rate": 0.0001564,
      "loss": 0.0073,
      "step": 391
    },
    {
      "epoch": 0.0018438902320855716,
      "grad_norm": 8.770179748535156,
      "learning_rate": 0.00015680000000000002,
      "loss": 0.6341,
      "step": 392
    },
    {
      "epoch": 0.0018485940336980347,
      "grad_norm": 0.06474225968122482,
      "learning_rate": 0.00015720000000000003,
      "loss": 0.0012,
      "step": 393
    },
    {
      "epoch": 0.001853297835310498,
      "grad_norm": 12.610589027404785,
      "learning_rate": 0.0001576,
      "loss": 1.891,
      "step": 394
    },
    {
      "epoch": 0.0018580016369229612,
      "grad_norm": 7.723695278167725,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.8129,
      "step": 395
    },
    {
      "epoch": 0.0018627054385354244,
      "grad_norm": 7.2633280754089355,
      "learning_rate": 0.00015840000000000003,
      "loss": 0.6139,
      "step": 396
    },
    {
      "epoch": 0.0018674092401478875,
      "grad_norm": 0.46716001629829407,
      "learning_rate": 0.0001588,
      "loss": 0.0099,
      "step": 397
    },
    {
      "epoch": 0.0018721130417603507,
      "grad_norm": 7.197908401489258,
      "learning_rate": 0.00015920000000000002,
      "loss": 0.6898,
      "step": 398
    },
    {
      "epoch": 0.001876816843372814,
      "grad_norm": 0.5740136504173279,
      "learning_rate": 0.0001596,
      "loss": 0.0117,
      "step": 399
    },
    {
      "epoch": 0.001881520644985277,
      "grad_norm": 7.378072738647461,
      "learning_rate": 0.00016,
      "loss": 0.3782,
      "step": 400
    },
    {
      "epoch": 0.0018862244465977403,
      "grad_norm": 1.9648194313049316,
      "learning_rate": 0.00016040000000000002,
      "loss": 0.0921,
      "step": 401
    },
    {
      "epoch": 0.0018909282482102035,
      "grad_norm": 2.49945068359375,
      "learning_rate": 0.0001608,
      "loss": 0.1016,
      "step": 402
    },
    {
      "epoch": 0.0018956320498226668,
      "grad_norm": 8.174307823181152,
      "learning_rate": 0.00016120000000000002,
      "loss": 0.9645,
      "step": 403
    },
    {
      "epoch": 0.0019003358514351298,
      "grad_norm": 0.46510857343673706,
      "learning_rate": 0.00016160000000000002,
      "loss": 0.0127,
      "step": 404
    },
    {
      "epoch": 0.001905039653047593,
      "grad_norm": 5.25247049331665,
      "learning_rate": 0.000162,
      "loss": 0.6457,
      "step": 405
    },
    {
      "epoch": 0.0019097434546600563,
      "grad_norm": 5.902981758117676,
      "learning_rate": 0.00016240000000000002,
      "loss": 0.268,
      "step": 406
    },
    {
      "epoch": 0.0019144472562725194,
      "grad_norm": 8.578977584838867,
      "learning_rate": 0.0001628,
      "loss": 1.5509,
      "step": 407
    },
    {
      "epoch": 0.0019191510578849826,
      "grad_norm": 1.3066927194595337,
      "learning_rate": 0.0001632,
      "loss": 0.1135,
      "step": 408
    },
    {
      "epoch": 0.0019238548594974459,
      "grad_norm": 1.2204082012176514,
      "learning_rate": 0.0001636,
      "loss": 0.1076,
      "step": 409
    },
    {
      "epoch": 0.001928558661109909,
      "grad_norm": 3.1726009845733643,
      "learning_rate": 0.000164,
      "loss": 0.1829,
      "step": 410
    },
    {
      "epoch": 0.0019332624627223722,
      "grad_norm": 10.586771011352539,
      "learning_rate": 0.0001644,
      "loss": 0.4915,
      "step": 411
    },
    {
      "epoch": 0.0019379662643348354,
      "grad_norm": 5.281284809112549,
      "learning_rate": 0.0001648,
      "loss": 0.5445,
      "step": 412
    },
    {
      "epoch": 0.0019426700659472987,
      "grad_norm": 7.744019985198975,
      "learning_rate": 0.0001652,
      "loss": 0.4866,
      "step": 413
    },
    {
      "epoch": 0.0019473738675597617,
      "grad_norm": 2.30348801612854,
      "learning_rate": 0.0001656,
      "loss": 0.1509,
      "step": 414
    },
    {
      "epoch": 0.001952077669172225,
      "grad_norm": 5.621163368225098,
      "learning_rate": 0.000166,
      "loss": 0.4085,
      "step": 415
    },
    {
      "epoch": 0.001956781470784688,
      "grad_norm": 2.718182325363159,
      "learning_rate": 0.0001664,
      "loss": 0.1989,
      "step": 416
    },
    {
      "epoch": 0.0019614852723971515,
      "grad_norm": 5.092797756195068,
      "learning_rate": 0.0001668,
      "loss": 0.6662,
      "step": 417
    },
    {
      "epoch": 0.0019661890740096145,
      "grad_norm": 6.925167083740234,
      "learning_rate": 0.0001672,
      "loss": 0.5721,
      "step": 418
    },
    {
      "epoch": 0.0019708928756220776,
      "grad_norm": 1.394405722618103,
      "learning_rate": 0.0001676,
      "loss": 0.0616,
      "step": 419
    },
    {
      "epoch": 0.001975596677234541,
      "grad_norm": 3.7499024868011475,
      "learning_rate": 0.000168,
      "loss": 0.2968,
      "step": 420
    },
    {
      "epoch": 0.001980300478847004,
      "grad_norm": 2.0088789463043213,
      "learning_rate": 0.0001684,
      "loss": 0.1763,
      "step": 421
    },
    {
      "epoch": 0.001985004280459467,
      "grad_norm": 3.8349266052246094,
      "learning_rate": 0.0001688,
      "loss": 0.3431,
      "step": 422
    },
    {
      "epoch": 0.0019897080820719306,
      "grad_norm": 1.7392799854278564,
      "learning_rate": 0.0001692,
      "loss": 0.164,
      "step": 423
    },
    {
      "epoch": 0.0019944118836843936,
      "grad_norm": 2.1135196685791016,
      "learning_rate": 0.0001696,
      "loss": 0.188,
      "step": 424
    },
    {
      "epoch": 0.001999115685296857,
      "grad_norm": 3.892944812774658,
      "learning_rate": 0.00017,
      "loss": 0.2384,
      "step": 425
    },
    {
      "epoch": 0.00200381948690932,
      "grad_norm": 0.41945555806159973,
      "learning_rate": 0.0001704,
      "loss": 0.0252,
      "step": 426
    },
    {
      "epoch": 0.002008523288521783,
      "grad_norm": 2.866692066192627,
      "learning_rate": 0.0001708,
      "loss": 0.3043,
      "step": 427
    },
    {
      "epoch": 0.0020132270901342467,
      "grad_norm": 3.9674036502838135,
      "learning_rate": 0.00017120000000000001,
      "loss": 0.45,
      "step": 428
    },
    {
      "epoch": 0.0020179308917467097,
      "grad_norm": 2.7942330837249756,
      "learning_rate": 0.0001716,
      "loss": 0.2072,
      "step": 429
    },
    {
      "epoch": 0.0020226346933591727,
      "grad_norm": 1.9509483575820923,
      "learning_rate": 0.000172,
      "loss": 0.1296,
      "step": 430
    },
    {
      "epoch": 0.0020273384949716362,
      "grad_norm": 6.762782573699951,
      "learning_rate": 0.00017240000000000002,
      "loss": 0.5934,
      "step": 431
    },
    {
      "epoch": 0.0020320422965840993,
      "grad_norm": 6.404508113861084,
      "learning_rate": 0.0001728,
      "loss": 0.6801,
      "step": 432
    },
    {
      "epoch": 0.0020367460981965623,
      "grad_norm": 1.2534981966018677,
      "learning_rate": 0.0001732,
      "loss": 0.1217,
      "step": 433
    },
    {
      "epoch": 0.0020414498998090258,
      "grad_norm": 1.5184433460235596,
      "learning_rate": 0.00017360000000000002,
      "loss": 0.0942,
      "step": 434
    },
    {
      "epoch": 0.002046153701421489,
      "grad_norm": 5.89815092086792,
      "learning_rate": 0.000174,
      "loss": 0.654,
      "step": 435
    },
    {
      "epoch": 0.002050857503033952,
      "grad_norm": 3.3252062797546387,
      "learning_rate": 0.0001744,
      "loss": 0.2026,
      "step": 436
    },
    {
      "epoch": 0.0020555613046464153,
      "grad_norm": 0.7325847744941711,
      "learning_rate": 0.00017480000000000002,
      "loss": 0.0299,
      "step": 437
    },
    {
      "epoch": 0.0020602651062588784,
      "grad_norm": 2.149791955947876,
      "learning_rate": 0.0001752,
      "loss": 0.1405,
      "step": 438
    },
    {
      "epoch": 0.002064968907871342,
      "grad_norm": 4.564706802368164,
      "learning_rate": 0.0001756,
      "loss": 0.593,
      "step": 439
    },
    {
      "epoch": 0.002069672709483805,
      "grad_norm": 0.5548848509788513,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0314,
      "step": 440
    },
    {
      "epoch": 0.002074376511096268,
      "grad_norm": 0.5786711573600769,
      "learning_rate": 0.0001764,
      "loss": 0.0227,
      "step": 441
    },
    {
      "epoch": 0.0020790803127087314,
      "grad_norm": 7.1305131912231445,
      "learning_rate": 0.00017680000000000001,
      "loss": 1.4823,
      "step": 442
    },
    {
      "epoch": 0.0020837841143211944,
      "grad_norm": 0.2096025049686432,
      "learning_rate": 0.0001772,
      "loss": 0.0069,
      "step": 443
    },
    {
      "epoch": 0.0020884879159336575,
      "grad_norm": 4.333567142486572,
      "learning_rate": 0.0001776,
      "loss": 0.2295,
      "step": 444
    },
    {
      "epoch": 0.002093191717546121,
      "grad_norm": 3.485567331314087,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.3651,
      "step": 445
    },
    {
      "epoch": 0.002097895519158584,
      "grad_norm": 5.096225261688232,
      "learning_rate": 0.0001784,
      "loss": 0.6821,
      "step": 446
    },
    {
      "epoch": 0.002102599320771047,
      "grad_norm": 5.824506759643555,
      "learning_rate": 0.0001788,
      "loss": 0.7786,
      "step": 447
    },
    {
      "epoch": 0.0021073031223835105,
      "grad_norm": 0.23080496490001678,
      "learning_rate": 0.00017920000000000002,
      "loss": 0.0075,
      "step": 448
    },
    {
      "epoch": 0.0021120069239959735,
      "grad_norm": 1.3946672677993774,
      "learning_rate": 0.0001796,
      "loss": 0.0798,
      "step": 449
    },
    {
      "epoch": 0.0021167107256084366,
      "grad_norm": 4.821137428283691,
      "learning_rate": 0.00018,
      "loss": 0.5363,
      "step": 450
    },
    {
      "epoch": 0.0021214145272209,
      "grad_norm": 1.5111775398254395,
      "learning_rate": 0.00018040000000000002,
      "loss": 0.1087,
      "step": 451
    },
    {
      "epoch": 0.002126118328833363,
      "grad_norm": 6.121540069580078,
      "learning_rate": 0.0001808,
      "loss": 0.2188,
      "step": 452
    },
    {
      "epoch": 0.002130822130445826,
      "grad_norm": 6.667111873626709,
      "learning_rate": 0.0001812,
      "loss": 0.8351,
      "step": 453
    },
    {
      "epoch": 0.0021355259320582896,
      "grad_norm": 1.9470659494400024,
      "learning_rate": 0.00018160000000000002,
      "loss": 0.0979,
      "step": 454
    },
    {
      "epoch": 0.0021402297336707526,
      "grad_norm": 1.9844422340393066,
      "learning_rate": 0.000182,
      "loss": 0.115,
      "step": 455
    },
    {
      "epoch": 0.002144933535283216,
      "grad_norm": 3.2512948513031006,
      "learning_rate": 0.00018240000000000002,
      "loss": 0.1841,
      "step": 456
    },
    {
      "epoch": 0.002149637336895679,
      "grad_norm": 1.5105171203613281,
      "learning_rate": 0.00018280000000000003,
      "loss": 0.1943,
      "step": 457
    },
    {
      "epoch": 0.002154341138508142,
      "grad_norm": 1.6252412796020508,
      "learning_rate": 0.0001832,
      "loss": 0.0982,
      "step": 458
    },
    {
      "epoch": 0.0021590449401206056,
      "grad_norm": 2.7867486476898193,
      "learning_rate": 0.00018360000000000002,
      "loss": 0.2819,
      "step": 459
    },
    {
      "epoch": 0.0021637487417330687,
      "grad_norm": 3.677786350250244,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3459,
      "step": 460
    },
    {
      "epoch": 0.0021684525433455317,
      "grad_norm": 1.1252609491348267,
      "learning_rate": 0.0001844,
      "loss": 0.1023,
      "step": 461
    },
    {
      "epoch": 0.002173156344957995,
      "grad_norm": 0.34616151452064514,
      "learning_rate": 0.00018480000000000002,
      "loss": 0.019,
      "step": 462
    },
    {
      "epoch": 0.0021778601465704582,
      "grad_norm": 1.7373374700546265,
      "learning_rate": 0.00018520000000000003,
      "loss": 0.2261,
      "step": 463
    },
    {
      "epoch": 0.0021825639481829213,
      "grad_norm": 0.8922981023788452,
      "learning_rate": 0.0001856,
      "loss": 0.0461,
      "step": 464
    },
    {
      "epoch": 0.0021872677497953848,
      "grad_norm": 4.738465785980225,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.2765,
      "step": 465
    },
    {
      "epoch": 0.002191971551407848,
      "grad_norm": 4.691183567047119,
      "learning_rate": 0.00018640000000000003,
      "loss": 0.992,
      "step": 466
    },
    {
      "epoch": 0.002196675353020311,
      "grad_norm": 3.280848503112793,
      "learning_rate": 0.00018680000000000001,
      "loss": 0.1805,
      "step": 467
    },
    {
      "epoch": 0.0022013791546327743,
      "grad_norm": 2.4245572090148926,
      "learning_rate": 0.00018720000000000002,
      "loss": 0.2653,
      "step": 468
    },
    {
      "epoch": 0.0022060829562452373,
      "grad_norm": 4.130688667297363,
      "learning_rate": 0.0001876,
      "loss": 0.2599,
      "step": 469
    },
    {
      "epoch": 0.0022107867578577004,
      "grad_norm": 0.6237450242042542,
      "learning_rate": 0.000188,
      "loss": 0.021,
      "step": 470
    },
    {
      "epoch": 0.002215490559470164,
      "grad_norm": 0.15407976508140564,
      "learning_rate": 0.0001884,
      "loss": 0.0045,
      "step": 471
    },
    {
      "epoch": 0.002220194361082627,
      "grad_norm": 5.603780746459961,
      "learning_rate": 0.0001888,
      "loss": 0.5043,
      "step": 472
    },
    {
      "epoch": 0.0022248981626950904,
      "grad_norm": 9.7479829788208,
      "learning_rate": 0.0001892,
      "loss": 0.9334,
      "step": 473
    },
    {
      "epoch": 0.0022296019643075534,
      "grad_norm": 6.137670993804932,
      "learning_rate": 0.0001896,
      "loss": 0.3281,
      "step": 474
    },
    {
      "epoch": 0.0022343057659200164,
      "grad_norm": 5.173526287078857,
      "learning_rate": 0.00019,
      "loss": 0.4695,
      "step": 475
    },
    {
      "epoch": 0.00223900956753248,
      "grad_norm": 3.5330491065979004,
      "learning_rate": 0.0001904,
      "loss": 0.3578,
      "step": 476
    },
    {
      "epoch": 0.002243713369144943,
      "grad_norm": 3.5067591667175293,
      "learning_rate": 0.0001908,
      "loss": 0.2513,
      "step": 477
    },
    {
      "epoch": 0.002248417170757406,
      "grad_norm": 2.5952534675598145,
      "learning_rate": 0.0001912,
      "loss": 0.1811,
      "step": 478
    },
    {
      "epoch": 0.0022531209723698695,
      "grad_norm": 7.664407730102539,
      "learning_rate": 0.0001916,
      "loss": 0.55,
      "step": 479
    },
    {
      "epoch": 0.0022578247739823325,
      "grad_norm": 12.325654029846191,
      "learning_rate": 0.000192,
      "loss": 0.419,
      "step": 480
    },
    {
      "epoch": 0.0022625285755947955,
      "grad_norm": 5.907459259033203,
      "learning_rate": 0.00019240000000000001,
      "loss": 0.6165,
      "step": 481
    },
    {
      "epoch": 0.002267232377207259,
      "grad_norm": 2.6723532676696777,
      "learning_rate": 0.0001928,
      "loss": 0.2301,
      "step": 482
    },
    {
      "epoch": 0.002271936178819722,
      "grad_norm": 2.586239814758301,
      "learning_rate": 0.0001932,
      "loss": 0.2677,
      "step": 483
    },
    {
      "epoch": 0.002276639980432185,
      "grad_norm": 4.0440144538879395,
      "learning_rate": 0.00019360000000000002,
      "loss": 0.5411,
      "step": 484
    },
    {
      "epoch": 0.0022813437820446486,
      "grad_norm": 3.957775354385376,
      "learning_rate": 0.000194,
      "loss": 0.3404,
      "step": 485
    },
    {
      "epoch": 0.0022860475836571116,
      "grad_norm": 1.59304678440094,
      "learning_rate": 0.0001944,
      "loss": 0.085,
      "step": 486
    },
    {
      "epoch": 0.002290751385269575,
      "grad_norm": 2.6465861797332764,
      "learning_rate": 0.0001948,
      "loss": 0.1939,
      "step": 487
    },
    {
      "epoch": 0.002295455186882038,
      "grad_norm": 1.9595757722854614,
      "learning_rate": 0.0001952,
      "loss": 0.1307,
      "step": 488
    },
    {
      "epoch": 0.002300158988494501,
      "grad_norm": 4.419256210327148,
      "learning_rate": 0.0001956,
      "loss": 0.495,
      "step": 489
    },
    {
      "epoch": 0.0023048627901069646,
      "grad_norm": 3.186998128890991,
      "learning_rate": 0.000196,
      "loss": 0.2656,
      "step": 490
    },
    {
      "epoch": 0.0023095665917194277,
      "grad_norm": 3.9447336196899414,
      "learning_rate": 0.0001964,
      "loss": 0.5269,
      "step": 491
    },
    {
      "epoch": 0.0023142703933318907,
      "grad_norm": 1.285272479057312,
      "learning_rate": 0.0001968,
      "loss": 0.0928,
      "step": 492
    },
    {
      "epoch": 0.002318974194944354,
      "grad_norm": 5.694546222686768,
      "learning_rate": 0.0001972,
      "loss": 0.9207,
      "step": 493
    },
    {
      "epoch": 0.0023236779965568172,
      "grad_norm": 7.832240581512451,
      "learning_rate": 0.0001976,
      "loss": 0.8962,
      "step": 494
    },
    {
      "epoch": 0.0023283817981692803,
      "grad_norm": 6.968967914581299,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.1535,
      "step": 495
    },
    {
      "epoch": 0.0023330855997817437,
      "grad_norm": 3.456744432449341,
      "learning_rate": 0.0001984,
      "loss": 0.4575,
      "step": 496
    },
    {
      "epoch": 0.0023377894013942068,
      "grad_norm": 3.8342556953430176,
      "learning_rate": 0.0001988,
      "loss": 0.6583,
      "step": 497
    },
    {
      "epoch": 0.00234249320300667,
      "grad_norm": 3.798800230026245,
      "learning_rate": 0.00019920000000000002,
      "loss": 0.6581,
      "step": 498
    },
    {
      "epoch": 0.0023471970046191333,
      "grad_norm": 1.041553020477295,
      "learning_rate": 0.0001996,
      "loss": 0.1214,
      "step": 499
    },
    {
      "epoch": 0.0023519008062315963,
      "grad_norm": 1.7373206615447998,
      "learning_rate": 0.0002,
      "loss": 0.1671,
      "step": 500
    },
    {
      "epoch": 0.0023566046078440594,
      "grad_norm": 2.733170509338379,
      "learning_rate": 0.00019999905702188653,
      "loss": 0.2806,
      "step": 501
    },
    {
      "epoch": 0.002361308409456523,
      "grad_norm": 1.8406935930252075,
      "learning_rate": 0.00019999811404377305,
      "loss": 0.1807,
      "step": 502
    },
    {
      "epoch": 0.002366012211068986,
      "grad_norm": 0.8960445523262024,
      "learning_rate": 0.0001999971710656596,
      "loss": 0.0481,
      "step": 503
    },
    {
      "epoch": 0.0023707160126814493,
      "grad_norm": 2.177539110183716,
      "learning_rate": 0.0001999962280875461,
      "loss": 0.2986,
      "step": 504
    },
    {
      "epoch": 0.0023754198142939124,
      "grad_norm": 3.447396755218506,
      "learning_rate": 0.00019999528510943263,
      "loss": 0.3535,
      "step": 505
    },
    {
      "epoch": 0.0023801236159063754,
      "grad_norm": 1.0203230381011963,
      "learning_rate": 0.00019999434213131912,
      "loss": 0.0744,
      "step": 506
    },
    {
      "epoch": 0.002384827417518839,
      "grad_norm": 2.2024595737457275,
      "learning_rate": 0.00019999339915320567,
      "loss": 0.2709,
      "step": 507
    },
    {
      "epoch": 0.002389531219131302,
      "grad_norm": 2.625650405883789,
      "learning_rate": 0.00019999245617509219,
      "loss": 0.1525,
      "step": 508
    },
    {
      "epoch": 0.002394235020743765,
      "grad_norm": 1.6832960844039917,
      "learning_rate": 0.0001999915131969787,
      "loss": 0.1382,
      "step": 509
    },
    {
      "epoch": 0.0023989388223562285,
      "grad_norm": 4.251504898071289,
      "learning_rate": 0.00019999057021886522,
      "loss": 0.2626,
      "step": 510
    },
    {
      "epoch": 0.0024036426239686915,
      "grad_norm": 3.4471030235290527,
      "learning_rate": 0.00019998962724075174,
      "loss": 0.2386,
      "step": 511
    },
    {
      "epoch": 0.0024083464255811545,
      "grad_norm": 4.806637287139893,
      "learning_rate": 0.0001999886842626383,
      "loss": 0.3893,
      "step": 512
    },
    {
      "epoch": 0.002413050227193618,
      "grad_norm": 6.059075832366943,
      "learning_rate": 0.0001999877412845248,
      "loss": 0.3727,
      "step": 513
    },
    {
      "epoch": 0.002417754028806081,
      "grad_norm": 2.395587921142578,
      "learning_rate": 0.00019998679830641132,
      "loss": 0.208,
      "step": 514
    },
    {
      "epoch": 0.002422457830418544,
      "grad_norm": 3.651764392852783,
      "learning_rate": 0.00019998585532829784,
      "loss": 0.2421,
      "step": 515
    },
    {
      "epoch": 0.0024271616320310076,
      "grad_norm": 6.9652910232543945,
      "learning_rate": 0.00019998491235018436,
      "loss": 0.7684,
      "step": 516
    },
    {
      "epoch": 0.0024318654336434706,
      "grad_norm": 6.124037265777588,
      "learning_rate": 0.00019998396937207088,
      "loss": 0.5321,
      "step": 517
    },
    {
      "epoch": 0.0024365692352559336,
      "grad_norm": 4.278610706329346,
      "learning_rate": 0.0001999830263939574,
      "loss": 0.292,
      "step": 518
    },
    {
      "epoch": 0.002441273036868397,
      "grad_norm": 1.1434571743011475,
      "learning_rate": 0.00019998208341584392,
      "loss": 0.0476,
      "step": 519
    },
    {
      "epoch": 0.00244597683848086,
      "grad_norm": 0.5602289438247681,
      "learning_rate": 0.00019998114043773044,
      "loss": 0.0371,
      "step": 520
    },
    {
      "epoch": 0.0024506806400933236,
      "grad_norm": 5.589516639709473,
      "learning_rate": 0.00019998019745961698,
      "loss": 0.9811,
      "step": 521
    },
    {
      "epoch": 0.0024553844417057867,
      "grad_norm": 7.275086402893066,
      "learning_rate": 0.0001999792544815035,
      "loss": 0.8024,
      "step": 522
    },
    {
      "epoch": 0.0024600882433182497,
      "grad_norm": 3.6631767749786377,
      "learning_rate": 0.00019997831150339002,
      "loss": 0.2967,
      "step": 523
    },
    {
      "epoch": 0.002464792044930713,
      "grad_norm": 1.0775015354156494,
      "learning_rate": 0.00019997736852527654,
      "loss": 0.0552,
      "step": 524
    },
    {
      "epoch": 0.002469495846543176,
      "grad_norm": 1.2534099817276,
      "learning_rate": 0.00019997642554716308,
      "loss": 0.0823,
      "step": 525
    },
    {
      "epoch": 0.0024741996481556392,
      "grad_norm": 1.1758276224136353,
      "learning_rate": 0.00019997548256904958,
      "loss": 0.0907,
      "step": 526
    },
    {
      "epoch": 0.0024789034497681027,
      "grad_norm": 1.0112347602844238,
      "learning_rate": 0.0001999745395909361,
      "loss": 0.0559,
      "step": 527
    },
    {
      "epoch": 0.0024836072513805658,
      "grad_norm": 4.944222450256348,
      "learning_rate": 0.0001999735966128226,
      "loss": 0.5837,
      "step": 528
    },
    {
      "epoch": 0.002488311052993029,
      "grad_norm": 1.8065460920333862,
      "learning_rate": 0.00019997265363470913,
      "loss": 0.1347,
      "step": 529
    },
    {
      "epoch": 0.0024930148546054923,
      "grad_norm": 0.922940731048584,
      "learning_rate": 0.00019997171065659568,
      "loss": 0.0388,
      "step": 530
    },
    {
      "epoch": 0.0024977186562179553,
      "grad_norm": 3.3904435634613037,
      "learning_rate": 0.0001999707676784822,
      "loss": 0.3006,
      "step": 531
    },
    {
      "epoch": 0.0025024224578304183,
      "grad_norm": 4.31658411026001,
      "learning_rate": 0.00019996982470036871,
      "loss": 0.4903,
      "step": 532
    },
    {
      "epoch": 0.002507126259442882,
      "grad_norm": 5.422018527984619,
      "learning_rate": 0.00019996888172225523,
      "loss": 0.5437,
      "step": 533
    },
    {
      "epoch": 0.002511830061055345,
      "grad_norm": 2.776301383972168,
      "learning_rate": 0.00019996793874414175,
      "loss": 0.168,
      "step": 534
    },
    {
      "epoch": 0.0025165338626678083,
      "grad_norm": 2.6906278133392334,
      "learning_rate": 0.0001999669957660283,
      "loss": 0.1889,
      "step": 535
    },
    {
      "epoch": 0.0025212376642802714,
      "grad_norm": 10.224071502685547,
      "learning_rate": 0.00019996605278791482,
      "loss": 1.355,
      "step": 536
    },
    {
      "epoch": 0.0025259414658927344,
      "grad_norm": 10.28413200378418,
      "learning_rate": 0.0001999651098098013,
      "loss": 1.1856,
      "step": 537
    },
    {
      "epoch": 0.002530645267505198,
      "grad_norm": 1.4848287105560303,
      "learning_rate": 0.00019996416683168783,
      "loss": 0.0659,
      "step": 538
    },
    {
      "epoch": 0.002535349069117661,
      "grad_norm": 2.585472583770752,
      "learning_rate": 0.00019996322385357437,
      "loss": 0.2186,
      "step": 539
    },
    {
      "epoch": 0.002540052870730124,
      "grad_norm": 1.6322215795516968,
      "learning_rate": 0.0001999622808754609,
      "loss": 0.0678,
      "step": 540
    },
    {
      "epoch": 0.0025447566723425874,
      "grad_norm": 1.3138689994812012,
      "learning_rate": 0.0001999613378973474,
      "loss": 0.0577,
      "step": 541
    },
    {
      "epoch": 0.0025494604739550505,
      "grad_norm": 5.464565753936768,
      "learning_rate": 0.00019996039491923393,
      "loss": 0.7226,
      "step": 542
    },
    {
      "epoch": 0.0025541642755675135,
      "grad_norm": 2.3187978267669678,
      "learning_rate": 0.00019995945194112045,
      "loss": 0.1611,
      "step": 543
    },
    {
      "epoch": 0.002558868077179977,
      "grad_norm": 5.241946220397949,
      "learning_rate": 0.000199958508963007,
      "loss": 0.7109,
      "step": 544
    },
    {
      "epoch": 0.00256357187879244,
      "grad_norm": 7.148946285247803,
      "learning_rate": 0.0001999575659848935,
      "loss": 0.9148,
      "step": 545
    },
    {
      "epoch": 0.002568275680404903,
      "grad_norm": 4.584968566894531,
      "learning_rate": 0.00019995662300678003,
      "loss": 0.8222,
      "step": 546
    },
    {
      "epoch": 0.0025729794820173665,
      "grad_norm": 1.0163313150405884,
      "learning_rate": 0.00019995568002866655,
      "loss": 0.1344,
      "step": 547
    },
    {
      "epoch": 0.0025776832836298296,
      "grad_norm": 2.384119987487793,
      "learning_rate": 0.00019995473705055307,
      "loss": 0.4091,
      "step": 548
    },
    {
      "epoch": 0.0025823870852422926,
      "grad_norm": 1.4941052198410034,
      "learning_rate": 0.00019995379407243959,
      "loss": 0.2154,
      "step": 549
    },
    {
      "epoch": 0.002587090886854756,
      "grad_norm": 1.1270636320114136,
      "learning_rate": 0.0001999528510943261,
      "loss": 0.0647,
      "step": 550
    },
    {
      "epoch": 0.002591794688467219,
      "grad_norm": 1.4387873411178589,
      "learning_rate": 0.00019995190811621262,
      "loss": 0.1066,
      "step": 551
    },
    {
      "epoch": 0.0025964984900796826,
      "grad_norm": 7.61419677734375,
      "learning_rate": 0.00019995096513809914,
      "loss": 0.9263,
      "step": 552
    },
    {
      "epoch": 0.0026012022916921456,
      "grad_norm": 2.3461380004882812,
      "learning_rate": 0.0001999500221599857,
      "loss": 0.1271,
      "step": 553
    },
    {
      "epoch": 0.0026059060933046087,
      "grad_norm": 3.3745126724243164,
      "learning_rate": 0.0001999490791818722,
      "loss": 0.3913,
      "step": 554
    },
    {
      "epoch": 0.002610609894917072,
      "grad_norm": 1.1886646747589111,
      "learning_rate": 0.00019994813620375872,
      "loss": 0.1279,
      "step": 555
    },
    {
      "epoch": 0.002615313696529535,
      "grad_norm": 5.635507106781006,
      "learning_rate": 0.00019994719322564524,
      "loss": 0.6348,
      "step": 556
    },
    {
      "epoch": 0.0026200174981419982,
      "grad_norm": 3.9375510215759277,
      "learning_rate": 0.00019994625024753176,
      "loss": 0.2696,
      "step": 557
    },
    {
      "epoch": 0.0026247212997544617,
      "grad_norm": 6.265040874481201,
      "learning_rate": 0.00019994530726941828,
      "loss": 0.8545,
      "step": 558
    },
    {
      "epoch": 0.0026294251013669247,
      "grad_norm": 5.659762859344482,
      "learning_rate": 0.0001999443642913048,
      "loss": 0.5455,
      "step": 559
    },
    {
      "epoch": 0.0026341289029793878,
      "grad_norm": 1.0451126098632812,
      "learning_rate": 0.00019994342131319132,
      "loss": 0.0981,
      "step": 560
    },
    {
      "epoch": 0.0026388327045918513,
      "grad_norm": 2.7365615367889404,
      "learning_rate": 0.00019994247833507784,
      "loss": 0.1385,
      "step": 561
    },
    {
      "epoch": 0.0026435365062043143,
      "grad_norm": 3.3486101627349854,
      "learning_rate": 0.00019994153535696438,
      "loss": 0.3833,
      "step": 562
    },
    {
      "epoch": 0.0026482403078167773,
      "grad_norm": 1.8383309841156006,
      "learning_rate": 0.0001999405923788509,
      "loss": 0.0886,
      "step": 563
    },
    {
      "epoch": 0.002652944109429241,
      "grad_norm": 5.769720077514648,
      "learning_rate": 0.00019993964940073742,
      "loss": 0.4087,
      "step": 564
    },
    {
      "epoch": 0.002657647911041704,
      "grad_norm": 3.2684357166290283,
      "learning_rate": 0.00019993870642262394,
      "loss": 0.3887,
      "step": 565
    },
    {
      "epoch": 0.002662351712654167,
      "grad_norm": 5.807337760925293,
      "learning_rate": 0.00019993776344451048,
      "loss": 0.6165,
      "step": 566
    },
    {
      "epoch": 0.0026670555142666304,
      "grad_norm": 5.6973958015441895,
      "learning_rate": 0.000199936820466397,
      "loss": 0.2833,
      "step": 567
    },
    {
      "epoch": 0.0026717593158790934,
      "grad_norm": 10.67391300201416,
      "learning_rate": 0.0001999358774882835,
      "loss": 0.8161,
      "step": 568
    },
    {
      "epoch": 0.002676463117491557,
      "grad_norm": 3.006187677383423,
      "learning_rate": 0.00019993493451017,
      "loss": 0.1708,
      "step": 569
    },
    {
      "epoch": 0.00268116691910402,
      "grad_norm": 3.5090417861938477,
      "learning_rate": 0.00019993399153205653,
      "loss": 0.2324,
      "step": 570
    },
    {
      "epoch": 0.002685870720716483,
      "grad_norm": 1.9104583263397217,
      "learning_rate": 0.00019993304855394308,
      "loss": 0.1244,
      "step": 571
    },
    {
      "epoch": 0.0026905745223289464,
      "grad_norm": 4.58314323425293,
      "learning_rate": 0.0001999321055758296,
      "loss": 0.6577,
      "step": 572
    },
    {
      "epoch": 0.0026952783239414095,
      "grad_norm": 3.729210615158081,
      "learning_rate": 0.00019993116259771611,
      "loss": 0.2797,
      "step": 573
    },
    {
      "epoch": 0.0026999821255538725,
      "grad_norm": 0.3151964545249939,
      "learning_rate": 0.00019993021961960263,
      "loss": 0.0111,
      "step": 574
    },
    {
      "epoch": 0.002704685927166336,
      "grad_norm": 3.0578484535217285,
      "learning_rate": 0.00019992927664148918,
      "loss": 0.2106,
      "step": 575
    },
    {
      "epoch": 0.002709389728778799,
      "grad_norm": 0.36270835995674133,
      "learning_rate": 0.0001999283336633757,
      "loss": 0.0131,
      "step": 576
    },
    {
      "epoch": 0.002714093530391262,
      "grad_norm": 4.578805446624756,
      "learning_rate": 0.00019992739068526222,
      "loss": 0.6182,
      "step": 577
    },
    {
      "epoch": 0.0027187973320037255,
      "grad_norm": 1.4137585163116455,
      "learning_rate": 0.00019992644770714873,
      "loss": 0.1828,
      "step": 578
    },
    {
      "epoch": 0.0027235011336161886,
      "grad_norm": 3.771151304244995,
      "learning_rate": 0.00019992550472903525,
      "loss": 0.5986,
      "step": 579
    },
    {
      "epoch": 0.0027282049352286516,
      "grad_norm": 5.3465495109558105,
      "learning_rate": 0.00019992456175092177,
      "loss": 0.598,
      "step": 580
    },
    {
      "epoch": 0.002732908736841115,
      "grad_norm": 2.330317497253418,
      "learning_rate": 0.0001999236187728083,
      "loss": 0.2394,
      "step": 581
    },
    {
      "epoch": 0.002737612538453578,
      "grad_norm": 2.4833929538726807,
      "learning_rate": 0.0001999226757946948,
      "loss": 0.1781,
      "step": 582
    },
    {
      "epoch": 0.0027423163400660416,
      "grad_norm": 1.0978729724884033,
      "learning_rate": 0.00019992173281658133,
      "loss": 0.0827,
      "step": 583
    },
    {
      "epoch": 0.0027470201416785046,
      "grad_norm": 2.2897653579711914,
      "learning_rate": 0.00019992078983846785,
      "loss": 0.1497,
      "step": 584
    },
    {
      "epoch": 0.0027517239432909677,
      "grad_norm": 3.143242359161377,
      "learning_rate": 0.0001999198468603544,
      "loss": 0.4842,
      "step": 585
    },
    {
      "epoch": 0.002756427744903431,
      "grad_norm": 2.6832809448242188,
      "learning_rate": 0.0001999189038822409,
      "loss": 0.2921,
      "step": 586
    },
    {
      "epoch": 0.002761131546515894,
      "grad_norm": 5.166665554046631,
      "learning_rate": 0.00019991796090412743,
      "loss": 0.5207,
      "step": 587
    },
    {
      "epoch": 0.002765835348128357,
      "grad_norm": 0.8477105498313904,
      "learning_rate": 0.00019991701792601395,
      "loss": 0.0472,
      "step": 588
    },
    {
      "epoch": 0.0027705391497408207,
      "grad_norm": 1.8871163129806519,
      "learning_rate": 0.00019991607494790047,
      "loss": 0.1533,
      "step": 589
    },
    {
      "epoch": 0.0027752429513532837,
      "grad_norm": 1.741644024848938,
      "learning_rate": 0.00019991513196978699,
      "loss": 0.1795,
      "step": 590
    },
    {
      "epoch": 0.0027799467529657468,
      "grad_norm": 1.1360101699829102,
      "learning_rate": 0.0001999141889916735,
      "loss": 0.1051,
      "step": 591
    },
    {
      "epoch": 0.0027846505545782102,
      "grad_norm": 2.9317915439605713,
      "learning_rate": 0.00019991324601356002,
      "loss": 0.2745,
      "step": 592
    },
    {
      "epoch": 0.0027893543561906733,
      "grad_norm": 8.545475959777832,
      "learning_rate": 0.00019991230303544654,
      "loss": 0.2086,
      "step": 593
    },
    {
      "epoch": 0.0027940581578031363,
      "grad_norm": 1.1745413541793823,
      "learning_rate": 0.0001999113600573331,
      "loss": 0.0503,
      "step": 594
    },
    {
      "epoch": 0.0027987619594156,
      "grad_norm": 1.6579742431640625,
      "learning_rate": 0.0001999104170792196,
      "loss": 0.2281,
      "step": 595
    },
    {
      "epoch": 0.002803465761028063,
      "grad_norm": 4.4159626960754395,
      "learning_rate": 0.00019990947410110612,
      "loss": 0.257,
      "step": 596
    },
    {
      "epoch": 0.002808169562640526,
      "grad_norm": 1.1932029724121094,
      "learning_rate": 0.00019990853112299264,
      "loss": 0.0608,
      "step": 597
    },
    {
      "epoch": 0.0028128733642529893,
      "grad_norm": 1.2683508396148682,
      "learning_rate": 0.0001999075881448792,
      "loss": 0.0994,
      "step": 598
    },
    {
      "epoch": 0.0028175771658654524,
      "grad_norm": 6.023769378662109,
      "learning_rate": 0.00019990664516676568,
      "loss": 0.9381,
      "step": 599
    },
    {
      "epoch": 0.002822280967477916,
      "grad_norm": 6.486326217651367,
      "learning_rate": 0.0001999057021886522,
      "loss": 0.7768,
      "step": 600
    },
    {
      "epoch": 0.002826984769090379,
      "grad_norm": 5.9107489585876465,
      "learning_rate": 0.00019990475921053872,
      "loss": 0.3047,
      "step": 601
    },
    {
      "epoch": 0.002831688570702842,
      "grad_norm": 1.7816431522369385,
      "learning_rate": 0.00019990381623242524,
      "loss": 0.1483,
      "step": 602
    },
    {
      "epoch": 0.0028363923723153054,
      "grad_norm": 5.979990482330322,
      "learning_rate": 0.00019990287325431178,
      "loss": 0.308,
      "step": 603
    },
    {
      "epoch": 0.0028410961739277684,
      "grad_norm": 2.1106746196746826,
      "learning_rate": 0.0001999019302761983,
      "loss": 0.0951,
      "step": 604
    },
    {
      "epoch": 0.0028457999755402315,
      "grad_norm": 5.376126766204834,
      "learning_rate": 0.00019990098729808482,
      "loss": 0.2781,
      "step": 605
    },
    {
      "epoch": 0.002850503777152695,
      "grad_norm": 3.920464277267456,
      "learning_rate": 0.00019990004431997134,
      "loss": 0.5376,
      "step": 606
    },
    {
      "epoch": 0.002855207578765158,
      "grad_norm": 1.4465391635894775,
      "learning_rate": 0.00019989910134185788,
      "loss": 0.1126,
      "step": 607
    },
    {
      "epoch": 0.002859911380377621,
      "grad_norm": 3.4608986377716064,
      "learning_rate": 0.0001998981583637444,
      "loss": 0.3299,
      "step": 608
    },
    {
      "epoch": 0.0028646151819900845,
      "grad_norm": 6.820397853851318,
      "learning_rate": 0.00019989721538563092,
      "loss": 0.5168,
      "step": 609
    },
    {
      "epoch": 0.0028693189836025475,
      "grad_norm": 7.750339031219482,
      "learning_rate": 0.00019989627240751744,
      "loss": 1.1041,
      "step": 610
    },
    {
      "epoch": 0.0028740227852150106,
      "grad_norm": 3.512814521789551,
      "learning_rate": 0.00019989532942940393,
      "loss": 0.4075,
      "step": 611
    },
    {
      "epoch": 0.002878726586827474,
      "grad_norm": 3.448805809020996,
      "learning_rate": 0.00019989438645129048,
      "loss": 0.3298,
      "step": 612
    },
    {
      "epoch": 0.002883430388439937,
      "grad_norm": 0.8876003623008728,
      "learning_rate": 0.000199893443473177,
      "loss": 0.0687,
      "step": 613
    },
    {
      "epoch": 0.0028881341900524,
      "grad_norm": 2.7814791202545166,
      "learning_rate": 0.00019989250049506351,
      "loss": 0.2559,
      "step": 614
    },
    {
      "epoch": 0.0028928379916648636,
      "grad_norm": 4.265305042266846,
      "learning_rate": 0.00019989155751695003,
      "loss": 0.4086,
      "step": 615
    },
    {
      "epoch": 0.0028975417932773266,
      "grad_norm": 2.155289888381958,
      "learning_rate": 0.00019989061453883658,
      "loss": 0.1587,
      "step": 616
    },
    {
      "epoch": 0.00290224559488979,
      "grad_norm": 2.114879846572876,
      "learning_rate": 0.0001998896715607231,
      "loss": 0.1158,
      "step": 617
    },
    {
      "epoch": 0.002906949396502253,
      "grad_norm": 4.858074188232422,
      "learning_rate": 0.00019988872858260962,
      "loss": 0.6083,
      "step": 618
    },
    {
      "epoch": 0.002911653198114716,
      "grad_norm": 1.7556989192962646,
      "learning_rate": 0.00019988778560449613,
      "loss": 0.1904,
      "step": 619
    },
    {
      "epoch": 0.0029163569997271797,
      "grad_norm": 5.425992012023926,
      "learning_rate": 0.00019988684262638265,
      "loss": 0.2776,
      "step": 620
    },
    {
      "epoch": 0.0029210608013396427,
      "grad_norm": 2.372183322906494,
      "learning_rate": 0.00019988589964826917,
      "loss": 0.1843,
      "step": 621
    },
    {
      "epoch": 0.0029257646029521057,
      "grad_norm": 4.964325428009033,
      "learning_rate": 0.0001998849566701557,
      "loss": 0.3778,
      "step": 622
    },
    {
      "epoch": 0.0029304684045645692,
      "grad_norm": 2.822535991668701,
      "learning_rate": 0.0001998840136920422,
      "loss": 0.2228,
      "step": 623
    },
    {
      "epoch": 0.0029351722061770323,
      "grad_norm": 5.761855602264404,
      "learning_rate": 0.00019988307071392873,
      "loss": 1.0201,
      "step": 624
    },
    {
      "epoch": 0.0029398760077894953,
      "grad_norm": 4.019589424133301,
      "learning_rate": 0.00019988212773581527,
      "loss": 0.3764,
      "step": 625
    },
    {
      "epoch": 0.0029445798094019588,
      "grad_norm": 1.0415745973587036,
      "learning_rate": 0.0001998811847577018,
      "loss": 0.0586,
      "step": 626
    },
    {
      "epoch": 0.002949283611014422,
      "grad_norm": 4.722923278808594,
      "learning_rate": 0.0001998802417795883,
      "loss": 0.3379,
      "step": 627
    },
    {
      "epoch": 0.002953987412626885,
      "grad_norm": 5.970535755157471,
      "learning_rate": 0.00019987929880147483,
      "loss": 0.7326,
      "step": 628
    },
    {
      "epoch": 0.0029586912142393483,
      "grad_norm": 2.435281276702881,
      "learning_rate": 0.00019987835582336135,
      "loss": 0.3266,
      "step": 629
    },
    {
      "epoch": 0.0029633950158518114,
      "grad_norm": 1.466345191001892,
      "learning_rate": 0.00019987741284524787,
      "loss": 0.0475,
      "step": 630
    },
    {
      "epoch": 0.002968098817464275,
      "grad_norm": 0.9785760641098022,
      "learning_rate": 0.00019987646986713439,
      "loss": 0.0651,
      "step": 631
    },
    {
      "epoch": 0.002972802619076738,
      "grad_norm": 2.968933343887329,
      "learning_rate": 0.0001998755268890209,
      "loss": 0.3145,
      "step": 632
    },
    {
      "epoch": 0.002977506420689201,
      "grad_norm": 0.6597616672515869,
      "learning_rate": 0.00019987458391090742,
      "loss": 0.0372,
      "step": 633
    },
    {
      "epoch": 0.0029822102223016644,
      "grad_norm": 1.2770494222640991,
      "learning_rate": 0.00019987364093279394,
      "loss": 0.0838,
      "step": 634
    },
    {
      "epoch": 0.0029869140239141274,
      "grad_norm": 3.5290069580078125,
      "learning_rate": 0.0001998726979546805,
      "loss": 0.3791,
      "step": 635
    },
    {
      "epoch": 0.0029916178255265905,
      "grad_norm": 5.321798324584961,
      "learning_rate": 0.000199871754976567,
      "loss": 0.6737,
      "step": 636
    },
    {
      "epoch": 0.002996321627139054,
      "grad_norm": 7.353355407714844,
      "learning_rate": 0.00019987081199845352,
      "loss": 0.9599,
      "step": 637
    },
    {
      "epoch": 0.003001025428751517,
      "grad_norm": 4.455419063568115,
      "learning_rate": 0.00019986986902034004,
      "loss": 0.4745,
      "step": 638
    },
    {
      "epoch": 0.00300572923036398,
      "grad_norm": 5.390873908996582,
      "learning_rate": 0.0001998689260422266,
      "loss": 0.7398,
      "step": 639
    },
    {
      "epoch": 0.0030104330319764435,
      "grad_norm": 5.923178195953369,
      "learning_rate": 0.0001998679830641131,
      "loss": 0.697,
      "step": 640
    },
    {
      "epoch": 0.0030151368335889065,
      "grad_norm": 0.9895789623260498,
      "learning_rate": 0.0001998670400859996,
      "loss": 0.0672,
      "step": 641
    },
    {
      "epoch": 0.0030198406352013696,
      "grad_norm": 4.6699981689453125,
      "learning_rate": 0.00019986609710788612,
      "loss": 0.3894,
      "step": 642
    },
    {
      "epoch": 0.003024544436813833,
      "grad_norm": 2.9827492237091064,
      "learning_rate": 0.00019986515412977264,
      "loss": 0.2429,
      "step": 643
    },
    {
      "epoch": 0.003029248238426296,
      "grad_norm": 4.89498233795166,
      "learning_rate": 0.00019986421115165918,
      "loss": 0.4751,
      "step": 644
    },
    {
      "epoch": 0.003033952040038759,
      "grad_norm": 0.9322036504745483,
      "learning_rate": 0.0001998632681735457,
      "loss": 0.0484,
      "step": 645
    },
    {
      "epoch": 0.0030386558416512226,
      "grad_norm": 1.255699872970581,
      "learning_rate": 0.00019986232519543222,
      "loss": 0.1231,
      "step": 646
    },
    {
      "epoch": 0.0030433596432636856,
      "grad_norm": 4.447022438049316,
      "learning_rate": 0.00019986138221731874,
      "loss": 0.755,
      "step": 647
    },
    {
      "epoch": 0.003048063444876149,
      "grad_norm": 2.8242523670196533,
      "learning_rate": 0.00019986043923920528,
      "loss": 0.1157,
      "step": 648
    },
    {
      "epoch": 0.003052767246488612,
      "grad_norm": 2.3320181369781494,
      "learning_rate": 0.0001998594962610918,
      "loss": 0.2693,
      "step": 649
    },
    {
      "epoch": 0.003057471048101075,
      "grad_norm": 1.925260066986084,
      "learning_rate": 0.00019985855328297832,
      "loss": 0.1919,
      "step": 650
    },
    {
      "epoch": 0.0030621748497135387,
      "grad_norm": 1.8796541690826416,
      "learning_rate": 0.00019985761030486484,
      "loss": 0.117,
      "step": 651
    },
    {
      "epoch": 0.0030668786513260017,
      "grad_norm": 1.1869040727615356,
      "learning_rate": 0.00019985666732675136,
      "loss": 0.0477,
      "step": 652
    },
    {
      "epoch": 0.0030715824529384647,
      "grad_norm": 5.200232982635498,
      "learning_rate": 0.00019985572434863788,
      "loss": 0.1342,
      "step": 653
    },
    {
      "epoch": 0.003076286254550928,
      "grad_norm": 4.148637294769287,
      "learning_rate": 0.0001998547813705244,
      "loss": 0.2333,
      "step": 654
    },
    {
      "epoch": 0.0030809900561633912,
      "grad_norm": 2.3496146202087402,
      "learning_rate": 0.00019985383839241091,
      "loss": 0.1796,
      "step": 655
    },
    {
      "epoch": 0.0030856938577758543,
      "grad_norm": 4.071491241455078,
      "learning_rate": 0.00019985289541429743,
      "loss": 0.5679,
      "step": 656
    },
    {
      "epoch": 0.0030903976593883178,
      "grad_norm": 4.623934745788574,
      "learning_rate": 0.00019985195243618398,
      "loss": 0.5293,
      "step": 657
    },
    {
      "epoch": 0.003095101461000781,
      "grad_norm": 3.2329652309417725,
      "learning_rate": 0.0001998510094580705,
      "loss": 0.3043,
      "step": 658
    },
    {
      "epoch": 0.003099805262613244,
      "grad_norm": 5.119171142578125,
      "learning_rate": 0.00019985006647995702,
      "loss": 0.3622,
      "step": 659
    },
    {
      "epoch": 0.0031045090642257073,
      "grad_norm": 2.5936732292175293,
      "learning_rate": 0.00019984912350184353,
      "loss": 0.1239,
      "step": 660
    },
    {
      "epoch": 0.0031092128658381703,
      "grad_norm": 1.5400404930114746,
      "learning_rate": 0.00019984818052373005,
      "loss": 0.2147,
      "step": 661
    },
    {
      "epoch": 0.003113916667450634,
      "grad_norm": 4.7632222175598145,
      "learning_rate": 0.00019984723754561657,
      "loss": 0.7331,
      "step": 662
    },
    {
      "epoch": 0.003118620469063097,
      "grad_norm": 3.956862688064575,
      "learning_rate": 0.0001998462945675031,
      "loss": 0.2979,
      "step": 663
    },
    {
      "epoch": 0.00312332427067556,
      "grad_norm": 3.5224850177764893,
      "learning_rate": 0.0001998453515893896,
      "loss": 0.3799,
      "step": 664
    },
    {
      "epoch": 0.0031280280722880234,
      "grad_norm": 2.1835744380950928,
      "learning_rate": 0.00019984440861127613,
      "loss": 0.2335,
      "step": 665
    },
    {
      "epoch": 0.0031327318739004864,
      "grad_norm": 3.460556745529175,
      "learning_rate": 0.00019984346563316267,
      "loss": 0.3413,
      "step": 666
    },
    {
      "epoch": 0.0031374356755129494,
      "grad_norm": 2.0403270721435547,
      "learning_rate": 0.0001998425226550492,
      "loss": 0.1253,
      "step": 667
    },
    {
      "epoch": 0.003142139477125413,
      "grad_norm": 1.8948084115982056,
      "learning_rate": 0.0001998415796769357,
      "loss": 0.0892,
      "step": 668
    },
    {
      "epoch": 0.003146843278737876,
      "grad_norm": 4.522554874420166,
      "learning_rate": 0.00019984063669882223,
      "loss": 0.3471,
      "step": 669
    },
    {
      "epoch": 0.003151547080350339,
      "grad_norm": 3.44358229637146,
      "learning_rate": 0.00019983969372070875,
      "loss": 0.296,
      "step": 670
    },
    {
      "epoch": 0.0031562508819628025,
      "grad_norm": 2.3977081775665283,
      "learning_rate": 0.0001998387507425953,
      "loss": 0.2188,
      "step": 671
    },
    {
      "epoch": 0.0031609546835752655,
      "grad_norm": 2.8824875354766846,
      "learning_rate": 0.00019983780776448179,
      "loss": 0.275,
      "step": 672
    },
    {
      "epoch": 0.0031656584851877285,
      "grad_norm": 1.496519684791565,
      "learning_rate": 0.0001998368647863683,
      "loss": 0.0925,
      "step": 673
    },
    {
      "epoch": 0.003170362286800192,
      "grad_norm": 2.536027193069458,
      "learning_rate": 0.00019983592180825482,
      "loss": 0.4141,
      "step": 674
    },
    {
      "epoch": 0.003175066088412655,
      "grad_norm": 4.470987796783447,
      "learning_rate": 0.00019983497883014137,
      "loss": 0.7461,
      "step": 675
    },
    {
      "epoch": 0.003179769890025118,
      "grad_norm": 7.664361476898193,
      "learning_rate": 0.0001998340358520279,
      "loss": 0.6813,
      "step": 676
    },
    {
      "epoch": 0.0031844736916375816,
      "grad_norm": 0.46496066451072693,
      "learning_rate": 0.0001998330928739144,
      "loss": 0.0201,
      "step": 677
    },
    {
      "epoch": 0.0031891774932500446,
      "grad_norm": 2.6213464736938477,
      "learning_rate": 0.00019983214989580092,
      "loss": 0.3449,
      "step": 678
    },
    {
      "epoch": 0.003193881294862508,
      "grad_norm": 2.133852481842041,
      "learning_rate": 0.00019983120691768744,
      "loss": 0.2775,
      "step": 679
    },
    {
      "epoch": 0.003198585096474971,
      "grad_norm": 2.3365325927734375,
      "learning_rate": 0.000199830263939574,
      "loss": 0.3445,
      "step": 680
    },
    {
      "epoch": 0.003203288898087434,
      "grad_norm": 3.1866402626037598,
      "learning_rate": 0.0001998293209614605,
      "loss": 0.4403,
      "step": 681
    },
    {
      "epoch": 0.0032079926996998976,
      "grad_norm": 0.8377223014831543,
      "learning_rate": 0.00019982837798334703,
      "loss": 0.0395,
      "step": 682
    },
    {
      "epoch": 0.0032126965013123607,
      "grad_norm": 3.9217641353607178,
      "learning_rate": 0.00019982743500523355,
      "loss": 0.2284,
      "step": 683
    },
    {
      "epoch": 0.0032174003029248237,
      "grad_norm": 2.0801703929901123,
      "learning_rate": 0.00019982649202712004,
      "loss": 0.2378,
      "step": 684
    },
    {
      "epoch": 0.003222104104537287,
      "grad_norm": 1.5071691274642944,
      "learning_rate": 0.00019982554904900658,
      "loss": 0.1527,
      "step": 685
    },
    {
      "epoch": 0.0032268079061497502,
      "grad_norm": 1.7807644605636597,
      "learning_rate": 0.0001998246060708931,
      "loss": 0.1528,
      "step": 686
    },
    {
      "epoch": 0.0032315117077622133,
      "grad_norm": 0.8965314030647278,
      "learning_rate": 0.00019982366309277962,
      "loss": 0.042,
      "step": 687
    },
    {
      "epoch": 0.0032362155093746767,
      "grad_norm": 1.6664773225784302,
      "learning_rate": 0.00019982272011466614,
      "loss": 0.1817,
      "step": 688
    },
    {
      "epoch": 0.0032409193109871398,
      "grad_norm": 2.259044885635376,
      "learning_rate": 0.00019982177713655268,
      "loss": 0.1059,
      "step": 689
    },
    {
      "epoch": 0.003245623112599603,
      "grad_norm": 3.732649326324463,
      "learning_rate": 0.0001998208341584392,
      "loss": 0.3466,
      "step": 690
    },
    {
      "epoch": 0.0032503269142120663,
      "grad_norm": 0.6865069270133972,
      "learning_rate": 0.00019981989118032572,
      "loss": 0.0369,
      "step": 691
    },
    {
      "epoch": 0.0032550307158245293,
      "grad_norm": 2.923830032348633,
      "learning_rate": 0.00019981894820221224,
      "loss": 0.3983,
      "step": 692
    },
    {
      "epoch": 0.0032597345174369924,
      "grad_norm": 1.0155783891677856,
      "learning_rate": 0.00019981800522409876,
      "loss": 0.1191,
      "step": 693
    },
    {
      "epoch": 0.003264438319049456,
      "grad_norm": 4.116394996643066,
      "learning_rate": 0.00019981706224598528,
      "loss": 0.7991,
      "step": 694
    },
    {
      "epoch": 0.003269142120661919,
      "grad_norm": 0.26660841703414917,
      "learning_rate": 0.0001998161192678718,
      "loss": 0.0093,
      "step": 695
    },
    {
      "epoch": 0.0032738459222743824,
      "grad_norm": 0.049822740256786346,
      "learning_rate": 0.00019981517628975831,
      "loss": 0.0009,
      "step": 696
    },
    {
      "epoch": 0.0032785497238868454,
      "grad_norm": 0.35425907373428345,
      "learning_rate": 0.00019981423331164483,
      "loss": 0.0152,
      "step": 697
    },
    {
      "epoch": 0.0032832535254993084,
      "grad_norm": 4.4763031005859375,
      "learning_rate": 0.00019981329033353138,
      "loss": 0.5249,
      "step": 698
    },
    {
      "epoch": 0.003287957327111772,
      "grad_norm": 1.7317616939544678,
      "learning_rate": 0.0001998123473554179,
      "loss": 0.1375,
      "step": 699
    },
    {
      "epoch": 0.003292661128724235,
      "grad_norm": 2.0036051273345947,
      "learning_rate": 0.00019981140437730442,
      "loss": 0.1594,
      "step": 700
    },
    {
      "epoch": 0.003297364930336698,
      "grad_norm": 6.914712429046631,
      "learning_rate": 0.00019981046139919093,
      "loss": 0.4164,
      "step": 701
    },
    {
      "epoch": 0.0033020687319491615,
      "grad_norm": 7.351229667663574,
      "learning_rate": 0.00019980951842107745,
      "loss": 0.3932,
      "step": 702
    },
    {
      "epoch": 0.0033067725335616245,
      "grad_norm": 1.9157869815826416,
      "learning_rate": 0.00019980857544296397,
      "loss": 0.0914,
      "step": 703
    },
    {
      "epoch": 0.0033114763351740875,
      "grad_norm": 4.803820610046387,
      "learning_rate": 0.0001998076324648505,
      "loss": 0.3791,
      "step": 704
    },
    {
      "epoch": 0.003316180136786551,
      "grad_norm": 4.092446804046631,
      "learning_rate": 0.000199806689486737,
      "loss": 0.3111,
      "step": 705
    },
    {
      "epoch": 0.003320883938399014,
      "grad_norm": 8.289037704467773,
      "learning_rate": 0.00019980574650862353,
      "loss": 0.414,
      "step": 706
    },
    {
      "epoch": 0.003325587740011477,
      "grad_norm": 1.6593165397644043,
      "learning_rate": 0.00019980480353051007,
      "loss": 0.1817,
      "step": 707
    },
    {
      "epoch": 0.0033302915416239406,
      "grad_norm": 1.3535640239715576,
      "learning_rate": 0.0001998038605523966,
      "loss": 0.0798,
      "step": 708
    },
    {
      "epoch": 0.0033349953432364036,
      "grad_norm": 3.2224557399749756,
      "learning_rate": 0.0001998029175742831,
      "loss": 0.2774,
      "step": 709
    },
    {
      "epoch": 0.003339699144848867,
      "grad_norm": 1.0758998394012451,
      "learning_rate": 0.00019980197459616963,
      "loss": 0.0917,
      "step": 710
    },
    {
      "epoch": 0.00334440294646133,
      "grad_norm": 7.0475568771362305,
      "learning_rate": 0.00019980103161805615,
      "loss": 0.5593,
      "step": 711
    },
    {
      "epoch": 0.003349106748073793,
      "grad_norm": 0.8616170287132263,
      "learning_rate": 0.0001998000886399427,
      "loss": 0.0488,
      "step": 712
    },
    {
      "epoch": 0.0033538105496862566,
      "grad_norm": 0.9882147908210754,
      "learning_rate": 0.0001997991456618292,
      "loss": 0.0289,
      "step": 713
    },
    {
      "epoch": 0.0033585143512987197,
      "grad_norm": 0.3512348532676697,
      "learning_rate": 0.00019979820268371573,
      "loss": 0.0171,
      "step": 714
    },
    {
      "epoch": 0.0033632181529111827,
      "grad_norm": 7.051764011383057,
      "learning_rate": 0.00019979725970560222,
      "loss": 1.9375,
      "step": 715
    },
    {
      "epoch": 0.003367921954523646,
      "grad_norm": 1.6180366277694702,
      "learning_rate": 0.00019979631672748877,
      "loss": 0.0407,
      "step": 716
    },
    {
      "epoch": 0.003372625756136109,
      "grad_norm": 2.079805612564087,
      "learning_rate": 0.0001997953737493753,
      "loss": 0.0991,
      "step": 717
    },
    {
      "epoch": 0.0033773295577485722,
      "grad_norm": 2.244856119155884,
      "learning_rate": 0.0001997944307712618,
      "loss": 0.1052,
      "step": 718
    },
    {
      "epoch": 0.0033820333593610357,
      "grad_norm": 5.23406982421875,
      "learning_rate": 0.00019979348779314832,
      "loss": 0.5572,
      "step": 719
    },
    {
      "epoch": 0.0033867371609734988,
      "grad_norm": 4.127285480499268,
      "learning_rate": 0.00019979254481503484,
      "loss": 0.5623,
      "step": 720
    },
    {
      "epoch": 0.003391440962585962,
      "grad_norm": 2.6614346504211426,
      "learning_rate": 0.0001997916018369214,
      "loss": 0.1108,
      "step": 721
    },
    {
      "epoch": 0.0033961447641984253,
      "grad_norm": 6.999116897583008,
      "learning_rate": 0.0001997906588588079,
      "loss": 0.3679,
      "step": 722
    },
    {
      "epoch": 0.0034008485658108883,
      "grad_norm": 5.0894389152526855,
      "learning_rate": 0.00019978971588069443,
      "loss": 0.7167,
      "step": 723
    },
    {
      "epoch": 0.0034055523674233514,
      "grad_norm": 0.3045954704284668,
      "learning_rate": 0.00019978877290258094,
      "loss": 0.0132,
      "step": 724
    },
    {
      "epoch": 0.003410256169035815,
      "grad_norm": 3.3110711574554443,
      "learning_rate": 0.00019978782992446746,
      "loss": 0.2442,
      "step": 725
    },
    {
      "epoch": 0.003414959970648278,
      "grad_norm": 7.648102760314941,
      "learning_rate": 0.00019978688694635398,
      "loss": 0.6009,
      "step": 726
    },
    {
      "epoch": 0.0034196637722607413,
      "grad_norm": 7.214248180389404,
      "learning_rate": 0.0001997859439682405,
      "loss": 0.524,
      "step": 727
    },
    {
      "epoch": 0.0034243675738732044,
      "grad_norm": 1.3795311450958252,
      "learning_rate": 0.00019978500099012702,
      "loss": 0.1033,
      "step": 728
    },
    {
      "epoch": 0.0034290713754856674,
      "grad_norm": 4.430289268493652,
      "learning_rate": 0.00019978405801201354,
      "loss": 0.2421,
      "step": 729
    },
    {
      "epoch": 0.003433775177098131,
      "grad_norm": 0.9835808277130127,
      "learning_rate": 0.00019978311503390008,
      "loss": 0.058,
      "step": 730
    },
    {
      "epoch": 0.003438478978710594,
      "grad_norm": 4.867995262145996,
      "learning_rate": 0.0001997821720557866,
      "loss": 0.4837,
      "step": 731
    },
    {
      "epoch": 0.003443182780323057,
      "grad_norm": 1.2088003158569336,
      "learning_rate": 0.00019978122907767312,
      "loss": 0.0817,
      "step": 732
    },
    {
      "epoch": 0.0034478865819355204,
      "grad_norm": 3.145982027053833,
      "learning_rate": 0.00019978028609955964,
      "loss": 0.2513,
      "step": 733
    },
    {
      "epoch": 0.0034525903835479835,
      "grad_norm": 1.0700922012329102,
      "learning_rate": 0.00019977934312144616,
      "loss": 0.0782,
      "step": 734
    },
    {
      "epoch": 0.0034572941851604465,
      "grad_norm": 4.819093704223633,
      "learning_rate": 0.00019977840014333268,
      "loss": 0.4716,
      "step": 735
    },
    {
      "epoch": 0.00346199798677291,
      "grad_norm": 1.0987012386322021,
      "learning_rate": 0.0001997774571652192,
      "loss": 0.0838,
      "step": 736
    },
    {
      "epoch": 0.003466701788385373,
      "grad_norm": 4.498098373413086,
      "learning_rate": 0.00019977651418710571,
      "loss": 0.813,
      "step": 737
    },
    {
      "epoch": 0.003471405589997836,
      "grad_norm": 2.007070541381836,
      "learning_rate": 0.00019977557120899223,
      "loss": 0.1707,
      "step": 738
    },
    {
      "epoch": 0.0034761093916102995,
      "grad_norm": 1.4923171997070312,
      "learning_rate": 0.00019977462823087878,
      "loss": 0.0989,
      "step": 739
    },
    {
      "epoch": 0.0034808131932227626,
      "grad_norm": 2.7298333644866943,
      "learning_rate": 0.0001997736852527653,
      "loss": 0.4036,
      "step": 740
    },
    {
      "epoch": 0.0034855169948352256,
      "grad_norm": 3.2051470279693604,
      "learning_rate": 0.00019977274227465182,
      "loss": 0.2748,
      "step": 741
    },
    {
      "epoch": 0.003490220796447689,
      "grad_norm": 2.9899046421051025,
      "learning_rate": 0.00019977179929653833,
      "loss": 0.2817,
      "step": 742
    },
    {
      "epoch": 0.003494924598060152,
      "grad_norm": 2.592867851257324,
      "learning_rate": 0.00019977085631842485,
      "loss": 0.4408,
      "step": 743
    },
    {
      "epoch": 0.0034996283996726156,
      "grad_norm": 2.420393466949463,
      "learning_rate": 0.0001997699133403114,
      "loss": 0.3379,
      "step": 744
    },
    {
      "epoch": 0.0035043322012850786,
      "grad_norm": 2.5421671867370605,
      "learning_rate": 0.00019976897036219792,
      "loss": 0.3067,
      "step": 745
    },
    {
      "epoch": 0.0035090360028975417,
      "grad_norm": 1.3961797952651978,
      "learning_rate": 0.0001997680273840844,
      "loss": 0.0816,
      "step": 746
    },
    {
      "epoch": 0.003513739804510005,
      "grad_norm": 1.6264028549194336,
      "learning_rate": 0.00019976708440597093,
      "loss": 0.2849,
      "step": 747
    },
    {
      "epoch": 0.003518443606122468,
      "grad_norm": 4.887629985809326,
      "learning_rate": 0.00019976614142785747,
      "loss": 0.5643,
      "step": 748
    },
    {
      "epoch": 0.0035231474077349312,
      "grad_norm": 1.7852598428726196,
      "learning_rate": 0.000199765198449744,
      "loss": 0.1302,
      "step": 749
    },
    {
      "epoch": 0.0035278512093473947,
      "grad_norm": 3.496859550476074,
      "learning_rate": 0.0001997642554716305,
      "loss": 0.4507,
      "step": 750
    },
    {
      "epoch": 0.0035325550109598577,
      "grad_norm": 1.9377378225326538,
      "learning_rate": 0.00019976331249351703,
      "loss": 0.1132,
      "step": 751
    },
    {
      "epoch": 0.003537258812572321,
      "grad_norm": 3.2283592224121094,
      "learning_rate": 0.00019976236951540355,
      "loss": 0.1464,
      "step": 752
    },
    {
      "epoch": 0.0035419626141847843,
      "grad_norm": 1.0126895904541016,
      "learning_rate": 0.0001997614265372901,
      "loss": 0.0642,
      "step": 753
    },
    {
      "epoch": 0.0035466664157972473,
      "grad_norm": 2.15046763420105,
      "learning_rate": 0.0001997604835591766,
      "loss": 0.1565,
      "step": 754
    },
    {
      "epoch": 0.0035513702174097103,
      "grad_norm": 4.371125221252441,
      "learning_rate": 0.00019975954058106313,
      "loss": 0.3923,
      "step": 755
    },
    {
      "epoch": 0.003556074019022174,
      "grad_norm": 0.9291052222251892,
      "learning_rate": 0.00019975859760294965,
      "loss": 0.057,
      "step": 756
    },
    {
      "epoch": 0.003560777820634637,
      "grad_norm": 4.891977787017822,
      "learning_rate": 0.00019975765462483617,
      "loss": 0.4021,
      "step": 757
    },
    {
      "epoch": 0.0035654816222471003,
      "grad_norm": 6.337736129760742,
      "learning_rate": 0.0001997567116467227,
      "loss": 0.5628,
      "step": 758
    },
    {
      "epoch": 0.0035701854238595634,
      "grad_norm": 5.258932590484619,
      "learning_rate": 0.0001997557686686092,
      "loss": 0.4926,
      "step": 759
    },
    {
      "epoch": 0.0035748892254720264,
      "grad_norm": 8.956976890563965,
      "learning_rate": 0.00019975482569049572,
      "loss": 0.4917,
      "step": 760
    },
    {
      "epoch": 0.00357959302708449,
      "grad_norm": 0.9098829030990601,
      "learning_rate": 0.00019975388271238224,
      "loss": 0.0507,
      "step": 761
    },
    {
      "epoch": 0.003584296828696953,
      "grad_norm": 9.902016639709473,
      "learning_rate": 0.0001997529397342688,
      "loss": 1.2121,
      "step": 762
    },
    {
      "epoch": 0.003589000630309416,
      "grad_norm": 13.188253402709961,
      "learning_rate": 0.0001997519967561553,
      "loss": 0.5873,
      "step": 763
    },
    {
      "epoch": 0.0035937044319218794,
      "grad_norm": 5.708322525024414,
      "learning_rate": 0.00019975105377804183,
      "loss": 0.847,
      "step": 764
    },
    {
      "epoch": 0.0035984082335343425,
      "grad_norm": 1.9623807668685913,
      "learning_rate": 0.00019975011079992834,
      "loss": 0.123,
      "step": 765
    },
    {
      "epoch": 0.0036031120351468055,
      "grad_norm": 3.1812825202941895,
      "learning_rate": 0.00019974916782181486,
      "loss": 0.1545,
      "step": 766
    },
    {
      "epoch": 0.003607815836759269,
      "grad_norm": 2.3966028690338135,
      "learning_rate": 0.00019974822484370138,
      "loss": 0.1213,
      "step": 767
    },
    {
      "epoch": 0.003612519638371732,
      "grad_norm": 6.531218528747559,
      "learning_rate": 0.0001997472818655879,
      "loss": 0.5429,
      "step": 768
    },
    {
      "epoch": 0.003617223439984195,
      "grad_norm": 2.932382106781006,
      "learning_rate": 0.00019974633888747442,
      "loss": 0.468,
      "step": 769
    },
    {
      "epoch": 0.0036219272415966585,
      "grad_norm": 7.266598224639893,
      "learning_rate": 0.00019974539590936094,
      "loss": 0.6348,
      "step": 770
    },
    {
      "epoch": 0.0036266310432091216,
      "grad_norm": 10.07787799835205,
      "learning_rate": 0.00019974445293124748,
      "loss": 0.245,
      "step": 771
    },
    {
      "epoch": 0.0036313348448215846,
      "grad_norm": 5.017037391662598,
      "learning_rate": 0.000199743509953134,
      "loss": 0.6583,
      "step": 772
    },
    {
      "epoch": 0.003636038646434048,
      "grad_norm": 4.218452453613281,
      "learning_rate": 0.00019974256697502052,
      "loss": 0.6163,
      "step": 773
    },
    {
      "epoch": 0.003640742448046511,
      "grad_norm": 0.5723045468330383,
      "learning_rate": 0.00019974162399690704,
      "loss": 0.0312,
      "step": 774
    },
    {
      "epoch": 0.0036454462496589746,
      "grad_norm": 1.5858299732208252,
      "learning_rate": 0.00019974068101879359,
      "loss": 0.1595,
      "step": 775
    },
    {
      "epoch": 0.0036501500512714376,
      "grad_norm": 1.7094080448150635,
      "learning_rate": 0.0001997397380406801,
      "loss": 0.1687,
      "step": 776
    },
    {
      "epoch": 0.0036548538528839007,
      "grad_norm": 4.929214954376221,
      "learning_rate": 0.0001997387950625666,
      "loss": 0.7163,
      "step": 777
    },
    {
      "epoch": 0.003659557654496364,
      "grad_norm": 0.8485893607139587,
      "learning_rate": 0.00019973785208445311,
      "loss": 0.0713,
      "step": 778
    },
    {
      "epoch": 0.003664261456108827,
      "grad_norm": 2.429117202758789,
      "learning_rate": 0.00019973690910633963,
      "loss": 0.1789,
      "step": 779
    },
    {
      "epoch": 0.00366896525772129,
      "grad_norm": 6.601913928985596,
      "learning_rate": 0.00019973596612822618,
      "loss": 0.6066,
      "step": 780
    },
    {
      "epoch": 0.0036736690593337537,
      "grad_norm": 0.6980249285697937,
      "learning_rate": 0.0001997350231501127,
      "loss": 0.0355,
      "step": 781
    },
    {
      "epoch": 0.0036783728609462167,
      "grad_norm": 3.3183889389038086,
      "learning_rate": 0.00019973408017199922,
      "loss": 0.2565,
      "step": 782
    },
    {
      "epoch": 0.0036830766625586798,
      "grad_norm": 1.7383477687835693,
      "learning_rate": 0.00019973313719388573,
      "loss": 0.1905,
      "step": 783
    },
    {
      "epoch": 0.0036877804641711432,
      "grad_norm": 1.0041919946670532,
      "learning_rate": 0.00019973219421577228,
      "loss": 0.0642,
      "step": 784
    },
    {
      "epoch": 0.0036924842657836063,
      "grad_norm": 6.271389484405518,
      "learning_rate": 0.0001997312512376588,
      "loss": 0.7233,
      "step": 785
    },
    {
      "epoch": 0.0036971880673960693,
      "grad_norm": 1.2432334423065186,
      "learning_rate": 0.00019973030825954532,
      "loss": 0.0708,
      "step": 786
    },
    {
      "epoch": 0.003701891869008533,
      "grad_norm": 0.7254047393798828,
      "learning_rate": 0.00019972936528143184,
      "loss": 0.035,
      "step": 787
    },
    {
      "epoch": 0.003706595670620996,
      "grad_norm": 0.41578084230422974,
      "learning_rate": 0.00019972842230331833,
      "loss": 0.0147,
      "step": 788
    },
    {
      "epoch": 0.003711299472233459,
      "grad_norm": 7.647470474243164,
      "learning_rate": 0.00019972747932520487,
      "loss": 0.9813,
      "step": 789
    },
    {
      "epoch": 0.0037160032738459223,
      "grad_norm": 9.043394088745117,
      "learning_rate": 0.0001997265363470914,
      "loss": 0.5683,
      "step": 790
    },
    {
      "epoch": 0.0037207070754583854,
      "grad_norm": 4.077976703643799,
      "learning_rate": 0.0001997255933689779,
      "loss": 0.2939,
      "step": 791
    },
    {
      "epoch": 0.003725410877070849,
      "grad_norm": 4.999049663543701,
      "learning_rate": 0.00019972465039086443,
      "loss": 0.3417,
      "step": 792
    },
    {
      "epoch": 0.003730114678683312,
      "grad_norm": 2.734470844268799,
      "learning_rate": 0.00019972370741275095,
      "loss": 0.2965,
      "step": 793
    },
    {
      "epoch": 0.003734818480295775,
      "grad_norm": 4.27565336227417,
      "learning_rate": 0.0001997227644346375,
      "loss": 0.4357,
      "step": 794
    },
    {
      "epoch": 0.0037395222819082384,
      "grad_norm": 1.8730779886245728,
      "learning_rate": 0.000199721821456524,
      "loss": 0.2044,
      "step": 795
    },
    {
      "epoch": 0.0037442260835207014,
      "grad_norm": 6.089479923248291,
      "learning_rate": 0.00019972087847841053,
      "loss": 0.6735,
      "step": 796
    },
    {
      "epoch": 0.0037489298851331645,
      "grad_norm": 4.855020999908447,
      "learning_rate": 0.00019971993550029705,
      "loss": 0.319,
      "step": 797
    },
    {
      "epoch": 0.003753633686745628,
      "grad_norm": 5.966873645782471,
      "learning_rate": 0.00019971899252218357,
      "loss": 0.6209,
      "step": 798
    },
    {
      "epoch": 0.003758337488358091,
      "grad_norm": 2.4595608711242676,
      "learning_rate": 0.0001997180495440701,
      "loss": 0.2008,
      "step": 799
    },
    {
      "epoch": 0.003763041289970554,
      "grad_norm": 3.0964159965515137,
      "learning_rate": 0.0001997171065659566,
      "loss": 0.3928,
      "step": 800
    },
    {
      "epoch": 0.0037677450915830175,
      "grad_norm": 3.017940044403076,
      "learning_rate": 0.00019971616358784312,
      "loss": 0.1625,
      "step": 801
    },
    {
      "epoch": 0.0037724488931954805,
      "grad_norm": 3.077155828475952,
      "learning_rate": 0.00019971522060972964,
      "loss": 0.1405,
      "step": 802
    },
    {
      "epoch": 0.0037771526948079436,
      "grad_norm": 3.839731454849243,
      "learning_rate": 0.0001997142776316162,
      "loss": 0.1925,
      "step": 803
    },
    {
      "epoch": 0.003781856496420407,
      "grad_norm": 3.9447035789489746,
      "learning_rate": 0.0001997133346535027,
      "loss": 0.3221,
      "step": 804
    },
    {
      "epoch": 0.00378656029803287,
      "grad_norm": 3.661208152770996,
      "learning_rate": 0.00019971239167538923,
      "loss": 0.3085,
      "step": 805
    },
    {
      "epoch": 0.0037912640996453336,
      "grad_norm": 3.171854019165039,
      "learning_rate": 0.00019971144869727574,
      "loss": 0.2067,
      "step": 806
    },
    {
      "epoch": 0.0037959679012577966,
      "grad_norm": 4.156179428100586,
      "learning_rate": 0.0001997105057191623,
      "loss": 0.3249,
      "step": 807
    },
    {
      "epoch": 0.0038006717028702596,
      "grad_norm": 0.7393237352371216,
      "learning_rate": 0.00019970956274104878,
      "loss": 0.0492,
      "step": 808
    },
    {
      "epoch": 0.003805375504482723,
      "grad_norm": 1.9175344705581665,
      "learning_rate": 0.0001997086197629353,
      "loss": 0.1697,
      "step": 809
    },
    {
      "epoch": 0.003810079306095186,
      "grad_norm": 0.8247316479682922,
      "learning_rate": 0.00019970767678482182,
      "loss": 0.0854,
      "step": 810
    },
    {
      "epoch": 0.003814783107707649,
      "grad_norm": 4.238018035888672,
      "learning_rate": 0.00019970673380670834,
      "loss": 0.5455,
      "step": 811
    },
    {
      "epoch": 0.0038194869093201127,
      "grad_norm": 2.976881504058838,
      "learning_rate": 0.00019970579082859488,
      "loss": 0.3302,
      "step": 812
    },
    {
      "epoch": 0.0038241907109325757,
      "grad_norm": 1.8273224830627441,
      "learning_rate": 0.0001997048478504814,
      "loss": 0.1593,
      "step": 813
    },
    {
      "epoch": 0.0038288945125450388,
      "grad_norm": 0.7396630048751831,
      "learning_rate": 0.00019970390487236792,
      "loss": 0.0598,
      "step": 814
    },
    {
      "epoch": 0.0038335983141575022,
      "grad_norm": 1.375240683555603,
      "learning_rate": 0.00019970296189425444,
      "loss": 0.0856,
      "step": 815
    },
    {
      "epoch": 0.0038383021157699653,
      "grad_norm": 2.4276986122131348,
      "learning_rate": 0.00019970201891614099,
      "loss": 0.1118,
      "step": 816
    },
    {
      "epoch": 0.0038430059173824283,
      "grad_norm": 3.337308406829834,
      "learning_rate": 0.0001997010759380275,
      "loss": 0.1846,
      "step": 817
    },
    {
      "epoch": 0.0038477097189948918,
      "grad_norm": 2.7963943481445312,
      "learning_rate": 0.00019970013295991402,
      "loss": 0.2228,
      "step": 818
    },
    {
      "epoch": 0.003852413520607355,
      "grad_norm": 3.234858274459839,
      "learning_rate": 0.00019969918998180051,
      "loss": 0.2644,
      "step": 819
    },
    {
      "epoch": 0.003857117322219818,
      "grad_norm": 3.1895601749420166,
      "learning_rate": 0.00019969824700368703,
      "loss": 0.5279,
      "step": 820
    },
    {
      "epoch": 0.0038618211238322813,
      "grad_norm": 2.9223039150238037,
      "learning_rate": 0.00019969730402557358,
      "loss": 0.4018,
      "step": 821
    },
    {
      "epoch": 0.0038665249254447444,
      "grad_norm": 4.152519226074219,
      "learning_rate": 0.0001996963610474601,
      "loss": 0.417,
      "step": 822
    },
    {
      "epoch": 0.003871228727057208,
      "grad_norm": 3.0384833812713623,
      "learning_rate": 0.00019969541806934662,
      "loss": 0.2001,
      "step": 823
    },
    {
      "epoch": 0.003875932528669671,
      "grad_norm": 2.0597362518310547,
      "learning_rate": 0.00019969447509123313,
      "loss": 0.1356,
      "step": 824
    },
    {
      "epoch": 0.003880636330282134,
      "grad_norm": 1.1083074808120728,
      "learning_rate": 0.00019969353211311968,
      "loss": 0.112,
      "step": 825
    },
    {
      "epoch": 0.0038853401318945974,
      "grad_norm": 3.516921043395996,
      "learning_rate": 0.0001996925891350062,
      "loss": 0.3874,
      "step": 826
    },
    {
      "epoch": 0.0038900439335070604,
      "grad_norm": 0.8500960469245911,
      "learning_rate": 0.00019969164615689272,
      "loss": 0.0675,
      "step": 827
    },
    {
      "epoch": 0.0038947477351195235,
      "grad_norm": 0.9188404679298401,
      "learning_rate": 0.00019969070317877924,
      "loss": 0.0569,
      "step": 828
    },
    {
      "epoch": 0.003899451536731987,
      "grad_norm": 1.9219945669174194,
      "learning_rate": 0.00019968976020066576,
      "loss": 0.1342,
      "step": 829
    },
    {
      "epoch": 0.00390415533834445,
      "grad_norm": 3.1163229942321777,
      "learning_rate": 0.00019968881722255227,
      "loss": 0.2113,
      "step": 830
    },
    {
      "epoch": 0.003908859139956913,
      "grad_norm": 0.3294054865837097,
      "learning_rate": 0.0001996878742444388,
      "loss": 0.0127,
      "step": 831
    },
    {
      "epoch": 0.003913562941569376,
      "grad_norm": 3.5172455310821533,
      "learning_rate": 0.0001996869312663253,
      "loss": 0.2742,
      "step": 832
    },
    {
      "epoch": 0.00391826674318184,
      "grad_norm": 1.927416205406189,
      "learning_rate": 0.00019968598828821183,
      "loss": 0.1041,
      "step": 833
    },
    {
      "epoch": 0.003922970544794303,
      "grad_norm": 1.5283513069152832,
      "learning_rate": 0.00019968504531009838,
      "loss": 0.0608,
      "step": 834
    },
    {
      "epoch": 0.003927674346406766,
      "grad_norm": 4.355809211730957,
      "learning_rate": 0.0001996841023319849,
      "loss": 0.6392,
      "step": 835
    },
    {
      "epoch": 0.003932378148019229,
      "grad_norm": 7.324599266052246,
      "learning_rate": 0.0001996831593538714,
      "loss": 0.9752,
      "step": 836
    },
    {
      "epoch": 0.003937081949631692,
      "grad_norm": 1.6721248626708984,
      "learning_rate": 0.00019968221637575793,
      "loss": 0.0868,
      "step": 837
    },
    {
      "epoch": 0.003941785751244155,
      "grad_norm": 3.6601593494415283,
      "learning_rate": 0.00019968127339764445,
      "loss": 0.1553,
      "step": 838
    },
    {
      "epoch": 0.003946489552856619,
      "grad_norm": 3.891674757003784,
      "learning_rate": 0.00019968033041953097,
      "loss": 0.4995,
      "step": 839
    },
    {
      "epoch": 0.003951193354469082,
      "grad_norm": 5.520595073699951,
      "learning_rate": 0.0001996793874414175,
      "loss": 0.342,
      "step": 840
    },
    {
      "epoch": 0.003955897156081545,
      "grad_norm": 5.791471004486084,
      "learning_rate": 0.000199678444463304,
      "loss": 0.6729,
      "step": 841
    },
    {
      "epoch": 0.003960600957694008,
      "grad_norm": 0.6360845565795898,
      "learning_rate": 0.00019967750148519052,
      "loss": 0.0391,
      "step": 842
    },
    {
      "epoch": 0.003965304759306471,
      "grad_norm": 4.245542526245117,
      "learning_rate": 0.00019967655850707704,
      "loss": 0.5969,
      "step": 843
    },
    {
      "epoch": 0.003970008560918934,
      "grad_norm": 0.9177573323249817,
      "learning_rate": 0.0001996756155289636,
      "loss": 0.0385,
      "step": 844
    },
    {
      "epoch": 0.003974712362531398,
      "grad_norm": 0.5353983044624329,
      "learning_rate": 0.0001996746725508501,
      "loss": 0.0204,
      "step": 845
    },
    {
      "epoch": 0.003979416164143861,
      "grad_norm": 3.2036917209625244,
      "learning_rate": 0.00019967372957273663,
      "loss": 0.2564,
      "step": 846
    },
    {
      "epoch": 0.003984119965756324,
      "grad_norm": 5.347578048706055,
      "learning_rate": 0.00019967278659462314,
      "loss": 0.7149,
      "step": 847
    },
    {
      "epoch": 0.003988823767368787,
      "grad_norm": 3.8765556812286377,
      "learning_rate": 0.0001996718436165097,
      "loss": 0.4441,
      "step": 848
    },
    {
      "epoch": 0.00399352756898125,
      "grad_norm": 3.894300699234009,
      "learning_rate": 0.0001996709006383962,
      "loss": 0.4679,
      "step": 849
    },
    {
      "epoch": 0.003998231370593714,
      "grad_norm": 2.2992677688598633,
      "learning_rate": 0.0001996699576602827,
      "loss": 0.3826,
      "step": 850
    },
    {
      "epoch": 0.004002935172206177,
      "grad_norm": 5.999057292938232,
      "learning_rate": 0.00019966901468216922,
      "loss": 0.3428,
      "step": 851
    },
    {
      "epoch": 0.00400763897381864,
      "grad_norm": 2.2287259101867676,
      "learning_rate": 0.00019966807170405574,
      "loss": 0.1011,
      "step": 852
    },
    {
      "epoch": 0.004012342775431103,
      "grad_norm": 2.062133312225342,
      "learning_rate": 0.00019966712872594228,
      "loss": 0.1273,
      "step": 853
    },
    {
      "epoch": 0.004017046577043566,
      "grad_norm": 5.200307369232178,
      "learning_rate": 0.0001996661857478288,
      "loss": 1.0722,
      "step": 854
    },
    {
      "epoch": 0.004021750378656029,
      "grad_norm": 1.7610176801681519,
      "learning_rate": 0.00019966524276971532,
      "loss": 0.1183,
      "step": 855
    },
    {
      "epoch": 0.004026454180268493,
      "grad_norm": 3.9438118934631348,
      "learning_rate": 0.00019966429979160184,
      "loss": 0.576,
      "step": 856
    },
    {
      "epoch": 0.004031157981880956,
      "grad_norm": 2.1197304725646973,
      "learning_rate": 0.00019966335681348839,
      "loss": 0.2238,
      "step": 857
    },
    {
      "epoch": 0.004035861783493419,
      "grad_norm": 6.491802215576172,
      "learning_rate": 0.0001996624138353749,
      "loss": 0.7893,
      "step": 858
    },
    {
      "epoch": 0.0040405655851058824,
      "grad_norm": 6.291945934295654,
      "learning_rate": 0.00019966147085726142,
      "loss": 1.132,
      "step": 859
    },
    {
      "epoch": 0.0040452693867183455,
      "grad_norm": 5.788924217224121,
      "learning_rate": 0.00019966052787914794,
      "loss": 0.8024,
      "step": 860
    },
    {
      "epoch": 0.0040499731883308085,
      "grad_norm": 4.322174549102783,
      "learning_rate": 0.00019965958490103446,
      "loss": 0.6629,
      "step": 861
    },
    {
      "epoch": 0.0040546769899432724,
      "grad_norm": 2.0006439685821533,
      "learning_rate": 0.00019965864192292098,
      "loss": 0.151,
      "step": 862
    },
    {
      "epoch": 0.0040593807915557355,
      "grad_norm": 5.606760501861572,
      "learning_rate": 0.0001996576989448075,
      "loss": 0.8002,
      "step": 863
    },
    {
      "epoch": 0.0040640845931681985,
      "grad_norm": 1.7605232000350952,
      "learning_rate": 0.00019965675596669402,
      "loss": 0.1832,
      "step": 864
    },
    {
      "epoch": 0.0040687883947806616,
      "grad_norm": 1.3694320917129517,
      "learning_rate": 0.00019965581298858053,
      "loss": 0.3688,
      "step": 865
    },
    {
      "epoch": 0.004073492196393125,
      "grad_norm": 1.5993189811706543,
      "learning_rate": 0.00019965487001046708,
      "loss": 0.2427,
      "step": 866
    },
    {
      "epoch": 0.0040781959980055885,
      "grad_norm": 1.4998372793197632,
      "learning_rate": 0.0001996539270323536,
      "loss": 0.2507,
      "step": 867
    },
    {
      "epoch": 0.0040828997996180515,
      "grad_norm": 1.5021344423294067,
      "learning_rate": 0.00019965298405424012,
      "loss": 0.1601,
      "step": 868
    },
    {
      "epoch": 0.004087603601230515,
      "grad_norm": 0.8515642285346985,
      "learning_rate": 0.00019965204107612664,
      "loss": 0.1471,
      "step": 869
    },
    {
      "epoch": 0.004092307402842978,
      "grad_norm": 0.9317629337310791,
      "learning_rate": 0.00019965109809801316,
      "loss": 0.1031,
      "step": 870
    },
    {
      "epoch": 0.004097011204455441,
      "grad_norm": 2.0518393516540527,
      "learning_rate": 0.00019965015511989967,
      "loss": 0.2911,
      "step": 871
    },
    {
      "epoch": 0.004101715006067904,
      "grad_norm": 2.4519801139831543,
      "learning_rate": 0.0001996492121417862,
      "loss": 0.2398,
      "step": 872
    },
    {
      "epoch": 0.004106418807680368,
      "grad_norm": 1.7986570596694946,
      "learning_rate": 0.0001996482691636727,
      "loss": 0.3043,
      "step": 873
    },
    {
      "epoch": 0.004111122609292831,
      "grad_norm": 2.651280403137207,
      "learning_rate": 0.00019964732618555923,
      "loss": 0.2834,
      "step": 874
    },
    {
      "epoch": 0.004115826410905294,
      "grad_norm": 0.057060301303863525,
      "learning_rate": 0.00019964638320744578,
      "loss": 0.0026,
      "step": 875
    },
    {
      "epoch": 0.004120530212517757,
      "grad_norm": 1.8324172496795654,
      "learning_rate": 0.0001996454402293323,
      "loss": 0.1715,
      "step": 876
    },
    {
      "epoch": 0.00412523401413022,
      "grad_norm": 2.8559815883636475,
      "learning_rate": 0.0001996444972512188,
      "loss": 0.1747,
      "step": 877
    },
    {
      "epoch": 0.004129937815742684,
      "grad_norm": 4.312530994415283,
      "learning_rate": 0.00019964355427310533,
      "loss": 0.4679,
      "step": 878
    },
    {
      "epoch": 0.004134641617355147,
      "grad_norm": 3.6600635051727295,
      "learning_rate": 0.00019964261129499185,
      "loss": 0.3443,
      "step": 879
    },
    {
      "epoch": 0.00413934541896761,
      "grad_norm": 7.24200963973999,
      "learning_rate": 0.0001996416683168784,
      "loss": 1.1127,
      "step": 880
    },
    {
      "epoch": 0.004144049220580073,
      "grad_norm": 5.192946910858154,
      "learning_rate": 0.0001996407253387649,
      "loss": 0.2734,
      "step": 881
    },
    {
      "epoch": 0.004148753022192536,
      "grad_norm": 2.2150843143463135,
      "learning_rate": 0.0001996397823606514,
      "loss": 0.1784,
      "step": 882
    },
    {
      "epoch": 0.004153456823804999,
      "grad_norm": 3.5504543781280518,
      "learning_rate": 0.00019963883938253792,
      "loss": 0.4418,
      "step": 883
    },
    {
      "epoch": 0.004158160625417463,
      "grad_norm": 0.7357102036476135,
      "learning_rate": 0.00019963789640442447,
      "loss": 0.0306,
      "step": 884
    },
    {
      "epoch": 0.004162864427029926,
      "grad_norm": 4.456344127655029,
      "learning_rate": 0.000199636953426311,
      "loss": 0.2673,
      "step": 885
    },
    {
      "epoch": 0.004167568228642389,
      "grad_norm": 4.1521501541137695,
      "learning_rate": 0.0001996360104481975,
      "loss": 0.4662,
      "step": 886
    },
    {
      "epoch": 0.004172272030254852,
      "grad_norm": 1.6022080183029175,
      "learning_rate": 0.00019963506747008403,
      "loss": 0.0802,
      "step": 887
    },
    {
      "epoch": 0.004176975831867315,
      "grad_norm": 2.3389244079589844,
      "learning_rate": 0.00019963412449197054,
      "loss": 0.1645,
      "step": 888
    },
    {
      "epoch": 0.004181679633479778,
      "grad_norm": 2.216244697570801,
      "learning_rate": 0.0001996331815138571,
      "loss": 0.32,
      "step": 889
    },
    {
      "epoch": 0.004186383435092242,
      "grad_norm": 1.5978516340255737,
      "learning_rate": 0.0001996322385357436,
      "loss": 0.267,
      "step": 890
    },
    {
      "epoch": 0.004191087236704705,
      "grad_norm": 1.8133282661437988,
      "learning_rate": 0.00019963129555763013,
      "loss": 0.2402,
      "step": 891
    },
    {
      "epoch": 0.004195791038317168,
      "grad_norm": 2.2511255741119385,
      "learning_rate": 0.00019963035257951662,
      "loss": 0.1581,
      "step": 892
    },
    {
      "epoch": 0.004200494839929631,
      "grad_norm": 2.2536075115203857,
      "learning_rate": 0.00019962940960140314,
      "loss": 0.2668,
      "step": 893
    },
    {
      "epoch": 0.004205198641542094,
      "grad_norm": 3.0754106044769287,
      "learning_rate": 0.00019962846662328968,
      "loss": 0.3943,
      "step": 894
    },
    {
      "epoch": 0.004209902443154558,
      "grad_norm": 4.3067121505737305,
      "learning_rate": 0.0001996275236451762,
      "loss": 0.7683,
      "step": 895
    },
    {
      "epoch": 0.004214606244767021,
      "grad_norm": 1.6551918983459473,
      "learning_rate": 0.00019962658066706272,
      "loss": 0.1083,
      "step": 896
    },
    {
      "epoch": 0.004219310046379484,
      "grad_norm": 1.1116377115249634,
      "learning_rate": 0.00019962563768894924,
      "loss": 0.0797,
      "step": 897
    },
    {
      "epoch": 0.004224013847991947,
      "grad_norm": 5.60377311706543,
      "learning_rate": 0.00019962469471083579,
      "loss": 0.6621,
      "step": 898
    },
    {
      "epoch": 0.00422871764960441,
      "grad_norm": 2.269144296646118,
      "learning_rate": 0.0001996237517327223,
      "loss": 0.1391,
      "step": 899
    },
    {
      "epoch": 0.004233421451216873,
      "grad_norm": 4.111459732055664,
      "learning_rate": 0.00019962280875460882,
      "loss": 0.5594,
      "step": 900
    },
    {
      "epoch": 0.004238125252829337,
      "grad_norm": 12.483025550842285,
      "learning_rate": 0.00019962186577649534,
      "loss": 0.6212,
      "step": 901
    },
    {
      "epoch": 0.0042428290544418,
      "grad_norm": 4.360960483551025,
      "learning_rate": 0.00019962092279838186,
      "loss": 0.4917,
      "step": 902
    },
    {
      "epoch": 0.004247532856054263,
      "grad_norm": 5.507785797119141,
      "learning_rate": 0.00019961997982026838,
      "loss": 0.4741,
      "step": 903
    },
    {
      "epoch": 0.004252236657666726,
      "grad_norm": 2.8661887645721436,
      "learning_rate": 0.0001996190368421549,
      "loss": 0.3985,
      "step": 904
    },
    {
      "epoch": 0.004256940459279189,
      "grad_norm": 5.462536811828613,
      "learning_rate": 0.00019961809386404142,
      "loss": 1.1777,
      "step": 905
    },
    {
      "epoch": 0.004261644260891652,
      "grad_norm": 2.271923780441284,
      "learning_rate": 0.00019961715088592793,
      "loss": 0.3083,
      "step": 906
    },
    {
      "epoch": 0.004266348062504116,
      "grad_norm": 3.3140461444854736,
      "learning_rate": 0.00019961620790781448,
      "loss": 0.4028,
      "step": 907
    },
    {
      "epoch": 0.004271051864116579,
      "grad_norm": 1.091633915901184,
      "learning_rate": 0.000199615264929701,
      "loss": 0.0983,
      "step": 908
    },
    {
      "epoch": 0.004275755665729042,
      "grad_norm": 1.4230220317840576,
      "learning_rate": 0.00019961432195158752,
      "loss": 0.1459,
      "step": 909
    },
    {
      "epoch": 0.004280459467341505,
      "grad_norm": 2.245314121246338,
      "learning_rate": 0.00019961337897347404,
      "loss": 0.3139,
      "step": 910
    },
    {
      "epoch": 0.004285163268953968,
      "grad_norm": 2.5795958042144775,
      "learning_rate": 0.00019961243599536056,
      "loss": 0.3275,
      "step": 911
    },
    {
      "epoch": 0.004289867070566432,
      "grad_norm": 3.3103761672973633,
      "learning_rate": 0.00019961149301724707,
      "loss": 0.384,
      "step": 912
    },
    {
      "epoch": 0.004294570872178895,
      "grad_norm": 1.2406344413757324,
      "learning_rate": 0.0001996105500391336,
      "loss": 0.1954,
      "step": 913
    },
    {
      "epoch": 0.004299274673791358,
      "grad_norm": 1.357040524482727,
      "learning_rate": 0.0001996096070610201,
      "loss": 0.1226,
      "step": 914
    },
    {
      "epoch": 0.004303978475403821,
      "grad_norm": 3.5683462619781494,
      "learning_rate": 0.00019960866408290663,
      "loss": 0.5051,
      "step": 915
    },
    {
      "epoch": 0.004308682277016284,
      "grad_norm": 3.2784461975097656,
      "learning_rate": 0.00019960772110479318,
      "loss": 0.4417,
      "step": 916
    },
    {
      "epoch": 0.004313386078628747,
      "grad_norm": 2.0401432514190674,
      "learning_rate": 0.0001996067781266797,
      "loss": 0.1924,
      "step": 917
    },
    {
      "epoch": 0.004318089880241211,
      "grad_norm": 2.8115358352661133,
      "learning_rate": 0.0001996058351485662,
      "loss": 0.517,
      "step": 918
    },
    {
      "epoch": 0.004322793681853674,
      "grad_norm": 1.46018385887146,
      "learning_rate": 0.00019960489217045273,
      "loss": 0.1038,
      "step": 919
    },
    {
      "epoch": 0.004327497483466137,
      "grad_norm": 0.4845624268054962,
      "learning_rate": 0.00019960394919233925,
      "loss": 0.0317,
      "step": 920
    },
    {
      "epoch": 0.0043322012850786,
      "grad_norm": 1.2271971702575684,
      "learning_rate": 0.0001996030062142258,
      "loss": 0.1474,
      "step": 921
    },
    {
      "epoch": 0.0043369050866910635,
      "grad_norm": 2.8424367904663086,
      "learning_rate": 0.00019960206323611231,
      "loss": 0.4868,
      "step": 922
    },
    {
      "epoch": 0.0043416088883035265,
      "grad_norm": 1.5377953052520752,
      "learning_rate": 0.0001996011202579988,
      "loss": 0.1404,
      "step": 923
    },
    {
      "epoch": 0.00434631268991599,
      "grad_norm": 1.2843941450119019,
      "learning_rate": 0.00019960017727988532,
      "loss": 0.1417,
      "step": 924
    },
    {
      "epoch": 0.0043510164915284534,
      "grad_norm": 1.3842206001281738,
      "learning_rate": 0.00019959923430177187,
      "loss": 0.083,
      "step": 925
    },
    {
      "epoch": 0.0043557202931409165,
      "grad_norm": 1.7149360179901123,
      "learning_rate": 0.0001995982913236584,
      "loss": 0.1455,
      "step": 926
    },
    {
      "epoch": 0.0043604240947533795,
      "grad_norm": 4.338540077209473,
      "learning_rate": 0.0001995973483455449,
      "loss": 0.7917,
      "step": 927
    },
    {
      "epoch": 0.0043651278963658426,
      "grad_norm": 1.5320348739624023,
      "learning_rate": 0.00019959640536743143,
      "loss": 0.0967,
      "step": 928
    },
    {
      "epoch": 0.0043698316979783065,
      "grad_norm": 2.3303372859954834,
      "learning_rate": 0.00019959546238931794,
      "loss": 0.1819,
      "step": 929
    },
    {
      "epoch": 0.0043745354995907695,
      "grad_norm": 1.3356075286865234,
      "learning_rate": 0.0001995945194112045,
      "loss": 0.1309,
      "step": 930
    },
    {
      "epoch": 0.0043792393012032325,
      "grad_norm": 3.839742660522461,
      "learning_rate": 0.000199593576433091,
      "loss": 0.5089,
      "step": 931
    },
    {
      "epoch": 0.004383943102815696,
      "grad_norm": 8.025230407714844,
      "learning_rate": 0.00019959263345497753,
      "loss": 1.1945,
      "step": 932
    },
    {
      "epoch": 0.004388646904428159,
      "grad_norm": 1.2798118591308594,
      "learning_rate": 0.00019959169047686405,
      "loss": 0.058,
      "step": 933
    },
    {
      "epoch": 0.004393350706040622,
      "grad_norm": 4.224756717681885,
      "learning_rate": 0.00019959074749875057,
      "loss": 0.3425,
      "step": 934
    },
    {
      "epoch": 0.004398054507653086,
      "grad_norm": 3.9899611473083496,
      "learning_rate": 0.00019958980452063708,
      "loss": 0.2297,
      "step": 935
    },
    {
      "epoch": 0.004402758309265549,
      "grad_norm": 4.7733869552612305,
      "learning_rate": 0.0001995888615425236,
      "loss": 0.5158,
      "step": 936
    },
    {
      "epoch": 0.004407462110878012,
      "grad_norm": 1.0257493257522583,
      "learning_rate": 0.00019958791856441012,
      "loss": 0.0502,
      "step": 937
    },
    {
      "epoch": 0.004412165912490475,
      "grad_norm": 2.2008612155914307,
      "learning_rate": 0.00019958697558629664,
      "loss": 0.2069,
      "step": 938
    },
    {
      "epoch": 0.004416869714102938,
      "grad_norm": 0.045743364840745926,
      "learning_rate": 0.00019958603260818319,
      "loss": 0.0019,
      "step": 939
    },
    {
      "epoch": 0.004421573515715401,
      "grad_norm": 2.251311779022217,
      "learning_rate": 0.0001995850896300697,
      "loss": 0.1402,
      "step": 940
    },
    {
      "epoch": 0.004426277317327865,
      "grad_norm": 6.030086517333984,
      "learning_rate": 0.00019958414665195622,
      "loss": 0.7199,
      "step": 941
    },
    {
      "epoch": 0.004430981118940328,
      "grad_norm": 4.009695529937744,
      "learning_rate": 0.00019958320367384274,
      "loss": 0.4455,
      "step": 942
    },
    {
      "epoch": 0.004435684920552791,
      "grad_norm": 5.446501731872559,
      "learning_rate": 0.00019958226069572926,
      "loss": 1.0093,
      "step": 943
    },
    {
      "epoch": 0.004440388722165254,
      "grad_norm": 3.7866311073303223,
      "learning_rate": 0.00019958131771761578,
      "loss": 0.9183,
      "step": 944
    },
    {
      "epoch": 0.004445092523777717,
      "grad_norm": 1.9830983877182007,
      "learning_rate": 0.0001995803747395023,
      "loss": 0.103,
      "step": 945
    },
    {
      "epoch": 0.004449796325390181,
      "grad_norm": 0.4683980345726013,
      "learning_rate": 0.00019957943176138882,
      "loss": 0.0283,
      "step": 946
    },
    {
      "epoch": 0.004454500127002644,
      "grad_norm": 0.39663249254226685,
      "learning_rate": 0.00019957848878327533,
      "loss": 0.0194,
      "step": 947
    },
    {
      "epoch": 0.004459203928615107,
      "grad_norm": 1.2187803983688354,
      "learning_rate": 0.00019957754580516188,
      "loss": 0.1114,
      "step": 948
    },
    {
      "epoch": 0.00446390773022757,
      "grad_norm": 1.4147366285324097,
      "learning_rate": 0.0001995766028270484,
      "loss": 0.1365,
      "step": 949
    },
    {
      "epoch": 0.004468611531840033,
      "grad_norm": 1.4079573154449463,
      "learning_rate": 0.00019957565984893492,
      "loss": 0.0555,
      "step": 950
    },
    {
      "epoch": 0.004473315333452496,
      "grad_norm": 5.449100494384766,
      "learning_rate": 0.00019957471687082144,
      "loss": 0.8513,
      "step": 951
    },
    {
      "epoch": 0.00447801913506496,
      "grad_norm": 3.629546642303467,
      "learning_rate": 0.00019957377389270795,
      "loss": 0.3315,
      "step": 952
    },
    {
      "epoch": 0.004482722936677423,
      "grad_norm": 2.6535229682922363,
      "learning_rate": 0.0001995728309145945,
      "loss": 0.195,
      "step": 953
    },
    {
      "epoch": 0.004487426738289886,
      "grad_norm": 3.9355952739715576,
      "learning_rate": 0.000199571887936481,
      "loss": 0.3604,
      "step": 954
    },
    {
      "epoch": 0.004492130539902349,
      "grad_norm": 8.626532554626465,
      "learning_rate": 0.0001995709449583675,
      "loss": 0.8112,
      "step": 955
    },
    {
      "epoch": 0.004496834341514812,
      "grad_norm": 0.9100933074951172,
      "learning_rate": 0.00019957000198025403,
      "loss": 0.1116,
      "step": 956
    },
    {
      "epoch": 0.004501538143127276,
      "grad_norm": 0.6352298855781555,
      "learning_rate": 0.00019956905900214058,
      "loss": 0.0467,
      "step": 957
    },
    {
      "epoch": 0.004506241944739739,
      "grad_norm": 1.5351358652114868,
      "learning_rate": 0.0001995681160240271,
      "loss": 0.1405,
      "step": 958
    },
    {
      "epoch": 0.004510945746352202,
      "grad_norm": 3.540846824645996,
      "learning_rate": 0.0001995671730459136,
      "loss": 0.4124,
      "step": 959
    },
    {
      "epoch": 0.004515649547964665,
      "grad_norm": 4.930948734283447,
      "learning_rate": 0.00019956623006780013,
      "loss": 0.693,
      "step": 960
    },
    {
      "epoch": 0.004520353349577128,
      "grad_norm": 5.22108793258667,
      "learning_rate": 0.00019956528708968665,
      "loss": 0.9958,
      "step": 961
    },
    {
      "epoch": 0.004525057151189591,
      "grad_norm": 1.3187599182128906,
      "learning_rate": 0.0001995643441115732,
      "loss": 0.088,
      "step": 962
    },
    {
      "epoch": 0.004529760952802055,
      "grad_norm": 2.4778757095336914,
      "learning_rate": 0.00019956340113345971,
      "loss": 0.2594,
      "step": 963
    },
    {
      "epoch": 0.004534464754414518,
      "grad_norm": 3.469446897506714,
      "learning_rate": 0.00019956245815534623,
      "loss": 0.1832,
      "step": 964
    },
    {
      "epoch": 0.004539168556026981,
      "grad_norm": 3.6709208488464355,
      "learning_rate": 0.00019956151517723275,
      "loss": 0.459,
      "step": 965
    },
    {
      "epoch": 0.004543872357639444,
      "grad_norm": 4.824512958526611,
      "learning_rate": 0.00019956057219911927,
      "loss": 0.3857,
      "step": 966
    },
    {
      "epoch": 0.004548576159251907,
      "grad_norm": 2.7064156532287598,
      "learning_rate": 0.0001995596292210058,
      "loss": 0.2129,
      "step": 967
    },
    {
      "epoch": 0.00455327996086437,
      "grad_norm": 3.5702197551727295,
      "learning_rate": 0.0001995586862428923,
      "loss": 0.3311,
      "step": 968
    },
    {
      "epoch": 0.004557983762476834,
      "grad_norm": 3.2822742462158203,
      "learning_rate": 0.00019955774326477883,
      "loss": 0.3924,
      "step": 969
    },
    {
      "epoch": 0.004562687564089297,
      "grad_norm": 0.755229115486145,
      "learning_rate": 0.00019955680028666534,
      "loss": 0.0562,
      "step": 970
    },
    {
      "epoch": 0.00456739136570176,
      "grad_norm": 2.853001594543457,
      "learning_rate": 0.0001995558573085519,
      "loss": 0.2717,
      "step": 971
    },
    {
      "epoch": 0.004572095167314223,
      "grad_norm": 1.9549243450164795,
      "learning_rate": 0.0001995549143304384,
      "loss": 0.2454,
      "step": 972
    },
    {
      "epoch": 0.004576798968926686,
      "grad_norm": 1.1374268531799316,
      "learning_rate": 0.00019955397135232493,
      "loss": 0.1016,
      "step": 973
    },
    {
      "epoch": 0.00458150277053915,
      "grad_norm": 2.5307137966156006,
      "learning_rate": 0.00019955302837421145,
      "loss": 0.2085,
      "step": 974
    },
    {
      "epoch": 0.004586206572151613,
      "grad_norm": 0.48829177021980286,
      "learning_rate": 0.00019955208539609797,
      "loss": 0.0417,
      "step": 975
    },
    {
      "epoch": 0.004590910373764076,
      "grad_norm": 3.1844186782836914,
      "learning_rate": 0.00019955114241798448,
      "loss": 0.3265,
      "step": 976
    },
    {
      "epoch": 0.004595614175376539,
      "grad_norm": 3.6221959590911865,
      "learning_rate": 0.000199550199439871,
      "loss": 0.3608,
      "step": 977
    },
    {
      "epoch": 0.004600317976989002,
      "grad_norm": 2.9251344203948975,
      "learning_rate": 0.00019954925646175752,
      "loss": 0.2564,
      "step": 978
    },
    {
      "epoch": 0.004605021778601465,
      "grad_norm": 1.3449184894561768,
      "learning_rate": 0.00019954831348364404,
      "loss": 0.108,
      "step": 979
    },
    {
      "epoch": 0.004609725580213929,
      "grad_norm": 1.962941288948059,
      "learning_rate": 0.00019954737050553059,
      "loss": 0.1655,
      "step": 980
    },
    {
      "epoch": 0.004614429381826392,
      "grad_norm": 2.5324764251708984,
      "learning_rate": 0.0001995464275274171,
      "loss": 0.3142,
      "step": 981
    },
    {
      "epoch": 0.004619133183438855,
      "grad_norm": 11.012393951416016,
      "learning_rate": 0.00019954548454930362,
      "loss": 1.0578,
      "step": 982
    },
    {
      "epoch": 0.004623836985051318,
      "grad_norm": 0.5263782143592834,
      "learning_rate": 0.00019954454157119014,
      "loss": 0.0407,
      "step": 983
    },
    {
      "epoch": 0.004628540786663781,
      "grad_norm": 3.5418176651000977,
      "learning_rate": 0.0001995435985930767,
      "loss": 0.4474,
      "step": 984
    },
    {
      "epoch": 0.0046332445882762445,
      "grad_norm": 5.036700248718262,
      "learning_rate": 0.00019954265561496318,
      "loss": 0.6779,
      "step": 985
    },
    {
      "epoch": 0.004637948389888708,
      "grad_norm": 2.3286685943603516,
      "learning_rate": 0.0001995417126368497,
      "loss": 0.1594,
      "step": 986
    },
    {
      "epoch": 0.004642652191501171,
      "grad_norm": 5.382603168487549,
      "learning_rate": 0.00019954076965873622,
      "loss": 0.7271,
      "step": 987
    },
    {
      "epoch": 0.0046473559931136344,
      "grad_norm": 4.186121463775635,
      "learning_rate": 0.00019953982668062273,
      "loss": 0.5224,
      "step": 988
    },
    {
      "epoch": 0.0046520597947260975,
      "grad_norm": 1.1043283939361572,
      "learning_rate": 0.00019953888370250928,
      "loss": 0.1904,
      "step": 989
    },
    {
      "epoch": 0.0046567635963385605,
      "grad_norm": 3.55419921875,
      "learning_rate": 0.0001995379407243958,
      "loss": 0.5333,
      "step": 990
    },
    {
      "epoch": 0.004661467397951024,
      "grad_norm": 0.3639831840991974,
      "learning_rate": 0.00019953699774628232,
      "loss": 0.0229,
      "step": 991
    },
    {
      "epoch": 0.0046661711995634875,
      "grad_norm": 4.613901138305664,
      "learning_rate": 0.00019953605476816884,
      "loss": 0.4689,
      "step": 992
    },
    {
      "epoch": 0.0046708750011759505,
      "grad_norm": 2.176645517349243,
      "learning_rate": 0.00019953511179005538,
      "loss": 0.4317,
      "step": 993
    },
    {
      "epoch": 0.0046755788027884135,
      "grad_norm": 0.9780870079994202,
      "learning_rate": 0.0001995341688119419,
      "loss": 0.0627,
      "step": 994
    },
    {
      "epoch": 0.004680282604400877,
      "grad_norm": 3.4286422729492188,
      "learning_rate": 0.00019953322583382842,
      "loss": 0.5877,
      "step": 995
    },
    {
      "epoch": 0.00468498640601334,
      "grad_norm": 0.7196195721626282,
      "learning_rate": 0.00019953228285571494,
      "loss": 0.0755,
      "step": 996
    },
    {
      "epoch": 0.0046896902076258035,
      "grad_norm": 3.8133199214935303,
      "learning_rate": 0.00019953133987760143,
      "loss": 0.7722,
      "step": 997
    },
    {
      "epoch": 0.004694394009238267,
      "grad_norm": 0.9880680441856384,
      "learning_rate": 0.00019953039689948798,
      "loss": 0.1169,
      "step": 998
    },
    {
      "epoch": 0.00469909781085073,
      "grad_norm": 2.7337920665740967,
      "learning_rate": 0.0001995294539213745,
      "loss": 0.6155,
      "step": 999
    },
    {
      "epoch": 0.004703801612463193,
      "grad_norm": 1.417164921760559,
      "learning_rate": 0.000199528510943261,
      "loss": 0.3805,
      "step": 1000
    },
    {
      "epoch": 0.004708505414075656,
      "grad_norm": 2.4144654273986816,
      "learning_rate": 0.00019952756796514753,
      "loss": 0.3085,
      "step": 1001
    },
    {
      "epoch": 0.004713209215688119,
      "grad_norm": 3.371823310852051,
      "learning_rate": 0.00019952662498703405,
      "loss": 0.2914,
      "step": 1002
    },
    {
      "epoch": 0.004717913017300583,
      "grad_norm": 3.0819480419158936,
      "learning_rate": 0.0001995256820089206,
      "loss": 0.4458,
      "step": 1003
    },
    {
      "epoch": 0.004722616818913046,
      "grad_norm": 0.5641683340072632,
      "learning_rate": 0.00019952473903080711,
      "loss": 0.0371,
      "step": 1004
    },
    {
      "epoch": 0.004727320620525509,
      "grad_norm": 1.409862756729126,
      "learning_rate": 0.00019952379605269363,
      "loss": 0.2671,
      "step": 1005
    },
    {
      "epoch": 0.004732024422137972,
      "grad_norm": 0.7071142196655273,
      "learning_rate": 0.00019952285307458015,
      "loss": 0.0589,
      "step": 1006
    },
    {
      "epoch": 0.004736728223750435,
      "grad_norm": 1.2442493438720703,
      "learning_rate": 0.00019952191009646667,
      "loss": 0.0891,
      "step": 1007
    },
    {
      "epoch": 0.004741432025362899,
      "grad_norm": 1.0347414016723633,
      "learning_rate": 0.0001995209671183532,
      "loss": 0.0956,
      "step": 1008
    },
    {
      "epoch": 0.004746135826975362,
      "grad_norm": 1.0161340236663818,
      "learning_rate": 0.0001995200241402397,
      "loss": 0.0977,
      "step": 1009
    },
    {
      "epoch": 0.004750839628587825,
      "grad_norm": 3.2515039443969727,
      "learning_rate": 0.00019951908116212623,
      "loss": 0.3356,
      "step": 1010
    },
    {
      "epoch": 0.004755543430200288,
      "grad_norm": 1.6685450077056885,
      "learning_rate": 0.00019951813818401274,
      "loss": 0.1069,
      "step": 1011
    },
    {
      "epoch": 0.004760247231812751,
      "grad_norm": 0.5254742503166199,
      "learning_rate": 0.0001995171952058993,
      "loss": 0.0391,
      "step": 1012
    },
    {
      "epoch": 0.004764951033425214,
      "grad_norm": 4.024691581726074,
      "learning_rate": 0.0001995162522277858,
      "loss": 0.3176,
      "step": 1013
    },
    {
      "epoch": 0.004769654835037678,
      "grad_norm": 2.1956052780151367,
      "learning_rate": 0.00019951530924967233,
      "loss": 0.2312,
      "step": 1014
    },
    {
      "epoch": 0.004774358636650141,
      "grad_norm": 0.5061267018318176,
      "learning_rate": 0.00019951436627155885,
      "loss": 0.0244,
      "step": 1015
    },
    {
      "epoch": 0.004779062438262604,
      "grad_norm": 2.4792463779449463,
      "learning_rate": 0.00019951342329344537,
      "loss": 0.1097,
      "step": 1016
    },
    {
      "epoch": 0.004783766239875067,
      "grad_norm": 1.4886369705200195,
      "learning_rate": 0.00019951248031533188,
      "loss": 0.083,
      "step": 1017
    },
    {
      "epoch": 0.00478847004148753,
      "grad_norm": 1.2577072381973267,
      "learning_rate": 0.0001995115373372184,
      "loss": 0.0507,
      "step": 1018
    },
    {
      "epoch": 0.004793173843099993,
      "grad_norm": 3.470728635787964,
      "learning_rate": 0.00019951059435910492,
      "loss": 0.4907,
      "step": 1019
    },
    {
      "epoch": 0.004797877644712457,
      "grad_norm": 3.469719648361206,
      "learning_rate": 0.00019950965138099144,
      "loss": 0.1951,
      "step": 1020
    },
    {
      "epoch": 0.00480258144632492,
      "grad_norm": 5.453260898590088,
      "learning_rate": 0.00019950870840287799,
      "loss": 0.3029,
      "step": 1021
    },
    {
      "epoch": 0.004807285247937383,
      "grad_norm": 5.863182544708252,
      "learning_rate": 0.0001995077654247645,
      "loss": 0.5312,
      "step": 1022
    },
    {
      "epoch": 0.004811989049549846,
      "grad_norm": 0.14239713549613953,
      "learning_rate": 0.00019950682244665102,
      "loss": 0.0063,
      "step": 1023
    },
    {
      "epoch": 0.004816692851162309,
      "grad_norm": 7.697413921356201,
      "learning_rate": 0.00019950587946853754,
      "loss": 0.8534,
      "step": 1024
    },
    {
      "epoch": 0.004821396652774773,
      "grad_norm": 0.8317485451698303,
      "learning_rate": 0.0001995049364904241,
      "loss": 0.0385,
      "step": 1025
    },
    {
      "epoch": 0.004826100454387236,
      "grad_norm": 1.4528212547302246,
      "learning_rate": 0.0001995039935123106,
      "loss": 0.0811,
      "step": 1026
    },
    {
      "epoch": 0.004830804255999699,
      "grad_norm": 3.97784423828125,
      "learning_rate": 0.00019950305053419712,
      "loss": 0.5784,
      "step": 1027
    },
    {
      "epoch": 0.004835508057612162,
      "grad_norm": 5.4048662185668945,
      "learning_rate": 0.00019950210755608362,
      "loss": 0.927,
      "step": 1028
    },
    {
      "epoch": 0.004840211859224625,
      "grad_norm": 0.3229825794696808,
      "learning_rate": 0.00019950116457797013,
      "loss": 0.0164,
      "step": 1029
    },
    {
      "epoch": 0.004844915660837088,
      "grad_norm": 2.857028007507324,
      "learning_rate": 0.00019950022159985668,
      "loss": 0.1135,
      "step": 1030
    },
    {
      "epoch": 0.004849619462449552,
      "grad_norm": 4.3562822341918945,
      "learning_rate": 0.0001994992786217432,
      "loss": 0.1896,
      "step": 1031
    },
    {
      "epoch": 0.004854323264062015,
      "grad_norm": 2.629474639892578,
      "learning_rate": 0.00019949833564362972,
      "loss": 0.1226,
      "step": 1032
    },
    {
      "epoch": 0.004859027065674478,
      "grad_norm": 1.9294987916946411,
      "learning_rate": 0.00019949739266551624,
      "loss": 0.1256,
      "step": 1033
    },
    {
      "epoch": 0.004863730867286941,
      "grad_norm": 3.276109457015991,
      "learning_rate": 0.00019949644968740278,
      "loss": 0.2731,
      "step": 1034
    },
    {
      "epoch": 0.004868434668899404,
      "grad_norm": 3.0557732582092285,
      "learning_rate": 0.0001994955067092893,
      "loss": 0.2279,
      "step": 1035
    },
    {
      "epoch": 0.004873138470511867,
      "grad_norm": 3.415196418762207,
      "learning_rate": 0.00019949456373117582,
      "loss": 0.2323,
      "step": 1036
    },
    {
      "epoch": 0.004877842272124331,
      "grad_norm": 4.686676502227783,
      "learning_rate": 0.00019949362075306234,
      "loss": 0.3226,
      "step": 1037
    },
    {
      "epoch": 0.004882546073736794,
      "grad_norm": 1.4375680685043335,
      "learning_rate": 0.00019949267777494886,
      "loss": 0.1089,
      "step": 1038
    },
    {
      "epoch": 0.004887249875349257,
      "grad_norm": 0.5946130752563477,
      "learning_rate": 0.00019949173479683538,
      "loss": 0.0292,
      "step": 1039
    },
    {
      "epoch": 0.00489195367696172,
      "grad_norm": 7.800351142883301,
      "learning_rate": 0.0001994907918187219,
      "loss": 0.381,
      "step": 1040
    },
    {
      "epoch": 0.004896657478574183,
      "grad_norm": 0.6906222105026245,
      "learning_rate": 0.0001994898488406084,
      "loss": 0.0345,
      "step": 1041
    },
    {
      "epoch": 0.004901361280186647,
      "grad_norm": 2.8716204166412354,
      "learning_rate": 0.00019948890586249493,
      "loss": 0.3979,
      "step": 1042
    },
    {
      "epoch": 0.00490606508179911,
      "grad_norm": 2.7221360206604004,
      "learning_rate": 0.00019948796288438148,
      "loss": 0.2312,
      "step": 1043
    },
    {
      "epoch": 0.004910768883411573,
      "grad_norm": 1.5596845149993896,
      "learning_rate": 0.000199487019906268,
      "loss": 0.1057,
      "step": 1044
    },
    {
      "epoch": 0.004915472685024036,
      "grad_norm": 5.640894889831543,
      "learning_rate": 0.00019948607692815451,
      "loss": 0.4494,
      "step": 1045
    },
    {
      "epoch": 0.004920176486636499,
      "grad_norm": 0.8089331984519958,
      "learning_rate": 0.00019948513395004103,
      "loss": 0.0661,
      "step": 1046
    },
    {
      "epoch": 0.004924880288248962,
      "grad_norm": 0.11882041394710541,
      "learning_rate": 0.00019948419097192755,
      "loss": 0.0048,
      "step": 1047
    },
    {
      "epoch": 0.004929584089861426,
      "grad_norm": 6.32000732421875,
      "learning_rate": 0.00019948324799381407,
      "loss": 0.8707,
      "step": 1048
    },
    {
      "epoch": 0.004934287891473889,
      "grad_norm": 4.88183069229126,
      "learning_rate": 0.0001994823050157006,
      "loss": 0.8122,
      "step": 1049
    },
    {
      "epoch": 0.004938991693086352,
      "grad_norm": 3.576549530029297,
      "learning_rate": 0.0001994813620375871,
      "loss": 0.2541,
      "step": 1050
    },
    {
      "epoch": 0.0049436954946988155,
      "grad_norm": 3.2755815982818604,
      "learning_rate": 0.00019948041905947363,
      "loss": 0.0851,
      "step": 1051
    },
    {
      "epoch": 0.0049483992963112785,
      "grad_norm": 6.1543660163879395,
      "learning_rate": 0.00019947947608136014,
      "loss": 0.6002,
      "step": 1052
    },
    {
      "epoch": 0.004953103097923742,
      "grad_norm": 4.3052659034729,
      "learning_rate": 0.0001994785331032467,
      "loss": 0.1786,
      "step": 1053
    },
    {
      "epoch": 0.0049578068995362054,
      "grad_norm": 7.985748291015625,
      "learning_rate": 0.0001994775901251332,
      "loss": 0.3858,
      "step": 1054
    },
    {
      "epoch": 0.0049625107011486685,
      "grad_norm": 4.39776086807251,
      "learning_rate": 0.00019947664714701973,
      "loss": 0.5021,
      "step": 1055
    },
    {
      "epoch": 0.0049672145027611315,
      "grad_norm": 6.377254486083984,
      "learning_rate": 0.00019947570416890625,
      "loss": 0.5788,
      "step": 1056
    },
    {
      "epoch": 0.0049719183043735946,
      "grad_norm": 4.167307376861572,
      "learning_rate": 0.0001994747611907928,
      "loss": 0.212,
      "step": 1057
    },
    {
      "epoch": 0.004976622105986058,
      "grad_norm": 1.1526685953140259,
      "learning_rate": 0.0001994738182126793,
      "loss": 0.0561,
      "step": 1058
    },
    {
      "epoch": 0.0049813259075985215,
      "grad_norm": 5.158437728881836,
      "learning_rate": 0.0001994728752345658,
      "loss": 0.2687,
      "step": 1059
    },
    {
      "epoch": 0.0049860297092109845,
      "grad_norm": 1.498111367225647,
      "learning_rate": 0.00019947193225645232,
      "loss": 0.1288,
      "step": 1060
    },
    {
      "epoch": 0.004990733510823448,
      "grad_norm": 1.9581708908081055,
      "learning_rate": 0.00019947098927833884,
      "loss": 0.1301,
      "step": 1061
    },
    {
      "epoch": 0.004995437312435911,
      "grad_norm": 3.626494884490967,
      "learning_rate": 0.00019947004630022539,
      "loss": 0.2672,
      "step": 1062
    },
    {
      "epoch": 0.005000141114048374,
      "grad_norm": 3.713090658187866,
      "learning_rate": 0.0001994691033221119,
      "loss": 0.262,
      "step": 1063
    },
    {
      "epoch": 0.005004844915660837,
      "grad_norm": 4.511417865753174,
      "learning_rate": 0.00019946816034399842,
      "loss": 0.3901,
      "step": 1064
    },
    {
      "epoch": 0.005009548717273301,
      "grad_norm": 3.0603978633880615,
      "learning_rate": 0.00019946721736588494,
      "loss": 0.4096,
      "step": 1065
    },
    {
      "epoch": 0.005014252518885764,
      "grad_norm": 8.997435569763184,
      "learning_rate": 0.0001994662743877715,
      "loss": 1.0569,
      "step": 1066
    },
    {
      "epoch": 0.005018956320498227,
      "grad_norm": 2.2681939601898193,
      "learning_rate": 0.000199465331409658,
      "loss": 0.2313,
      "step": 1067
    },
    {
      "epoch": 0.00502366012211069,
      "grad_norm": 4.060677528381348,
      "learning_rate": 0.00019946438843154452,
      "loss": 0.1254,
      "step": 1068
    },
    {
      "epoch": 0.005028363923723153,
      "grad_norm": 2.0920708179473877,
      "learning_rate": 0.00019946344545343104,
      "loss": 0.3237,
      "step": 1069
    },
    {
      "epoch": 0.005033067725335617,
      "grad_norm": 1.4260982275009155,
      "learning_rate": 0.00019946250247531753,
      "loss": 0.1982,
      "step": 1070
    },
    {
      "epoch": 0.00503777152694808,
      "grad_norm": 3.120993137359619,
      "learning_rate": 0.00019946155949720408,
      "loss": 0.4726,
      "step": 1071
    },
    {
      "epoch": 0.005042475328560543,
      "grad_norm": 0.5227658748626709,
      "learning_rate": 0.0001994606165190906,
      "loss": 0.0406,
      "step": 1072
    },
    {
      "epoch": 0.005047179130173006,
      "grad_norm": 1.3785842657089233,
      "learning_rate": 0.00019945967354097712,
      "loss": 0.0958,
      "step": 1073
    },
    {
      "epoch": 0.005051882931785469,
      "grad_norm": 0.9395413994789124,
      "learning_rate": 0.00019945873056286364,
      "loss": 0.0694,
      "step": 1074
    },
    {
      "epoch": 0.005056586733397932,
      "grad_norm": 2.5200278759002686,
      "learning_rate": 0.00019945778758475018,
      "loss": 0.2412,
      "step": 1075
    },
    {
      "epoch": 0.005061290535010396,
      "grad_norm": 1.4473766088485718,
      "learning_rate": 0.0001994568446066367,
      "loss": 0.2174,
      "step": 1076
    },
    {
      "epoch": 0.005065994336622859,
      "grad_norm": 0.6396288871765137,
      "learning_rate": 0.00019945590162852322,
      "loss": 0.0368,
      "step": 1077
    },
    {
      "epoch": 0.005070698138235322,
      "grad_norm": 6.36503267288208,
      "learning_rate": 0.00019945495865040974,
      "loss": 0.9595,
      "step": 1078
    },
    {
      "epoch": 0.005075401939847785,
      "grad_norm": 1.5787138938903809,
      "learning_rate": 0.00019945401567229626,
      "loss": 0.114,
      "step": 1079
    },
    {
      "epoch": 0.005080105741460248,
      "grad_norm": 1.5769119262695312,
      "learning_rate": 0.00019945307269418278,
      "loss": 0.1512,
      "step": 1080
    },
    {
      "epoch": 0.005084809543072711,
      "grad_norm": 0.9310958981513977,
      "learning_rate": 0.0001994521297160693,
      "loss": 0.0541,
      "step": 1081
    },
    {
      "epoch": 0.005089513344685175,
      "grad_norm": 0.862682044506073,
      "learning_rate": 0.0001994511867379558,
      "loss": 0.0366,
      "step": 1082
    },
    {
      "epoch": 0.005094217146297638,
      "grad_norm": 4.943583965301514,
      "learning_rate": 0.00019945024375984233,
      "loss": 0.7797,
      "step": 1083
    },
    {
      "epoch": 0.005098920947910101,
      "grad_norm": 5.206130027770996,
      "learning_rate": 0.00019944930078172888,
      "loss": 0.5981,
      "step": 1084
    },
    {
      "epoch": 0.005103624749522564,
      "grad_norm": 0.11009436100721359,
      "learning_rate": 0.0001994483578036154,
      "loss": 0.0057,
      "step": 1085
    },
    {
      "epoch": 0.005108328551135027,
      "grad_norm": 0.22992177307605743,
      "learning_rate": 0.00019944741482550191,
      "loss": 0.0108,
      "step": 1086
    },
    {
      "epoch": 0.005113032352747491,
      "grad_norm": 5.002813339233398,
      "learning_rate": 0.00019944647184738843,
      "loss": 0.6821,
      "step": 1087
    },
    {
      "epoch": 0.005117736154359954,
      "grad_norm": 2.4107606410980225,
      "learning_rate": 0.00019944552886927495,
      "loss": 0.2198,
      "step": 1088
    },
    {
      "epoch": 0.005122439955972417,
      "grad_norm": 3.9342703819274902,
      "learning_rate": 0.0001994445858911615,
      "loss": 0.3272,
      "step": 1089
    },
    {
      "epoch": 0.00512714375758488,
      "grad_norm": 2.632293224334717,
      "learning_rate": 0.000199443642913048,
      "loss": 0.1643,
      "step": 1090
    },
    {
      "epoch": 0.005131847559197343,
      "grad_norm": 0.6136506199836731,
      "learning_rate": 0.0001994426999349345,
      "loss": 0.0426,
      "step": 1091
    },
    {
      "epoch": 0.005136551360809806,
      "grad_norm": 5.331970691680908,
      "learning_rate": 0.00019944175695682103,
      "loss": 0.6165,
      "step": 1092
    },
    {
      "epoch": 0.00514125516242227,
      "grad_norm": 4.332890510559082,
      "learning_rate": 0.00019944081397870757,
      "loss": 0.5747,
      "step": 1093
    },
    {
      "epoch": 0.005145958964034733,
      "grad_norm": 0.876818835735321,
      "learning_rate": 0.0001994398710005941,
      "loss": 0.0445,
      "step": 1094
    },
    {
      "epoch": 0.005150662765647196,
      "grad_norm": 0.6209219098091125,
      "learning_rate": 0.0001994389280224806,
      "loss": 0.0415,
      "step": 1095
    },
    {
      "epoch": 0.005155366567259659,
      "grad_norm": 0.705991268157959,
      "learning_rate": 0.00019943798504436713,
      "loss": 0.0656,
      "step": 1096
    },
    {
      "epoch": 0.005160070368872122,
      "grad_norm": 2.3394858837127686,
      "learning_rate": 0.00019943704206625365,
      "loss": 0.228,
      "step": 1097
    },
    {
      "epoch": 0.005164774170484585,
      "grad_norm": 2.318864583969116,
      "learning_rate": 0.0001994360990881402,
      "loss": 0.2776,
      "step": 1098
    },
    {
      "epoch": 0.005169477972097049,
      "grad_norm": 2.3617846965789795,
      "learning_rate": 0.0001994351561100267,
      "loss": 0.1797,
      "step": 1099
    },
    {
      "epoch": 0.005174181773709512,
      "grad_norm": 2.4583628177642822,
      "learning_rate": 0.00019943421313191323,
      "loss": 0.3105,
      "step": 1100
    },
    {
      "epoch": 0.005178885575321975,
      "grad_norm": 4.5207133293151855,
      "learning_rate": 0.00019943327015379972,
      "loss": 0.3363,
      "step": 1101
    },
    {
      "epoch": 0.005183589376934438,
      "grad_norm": 2.1288506984710693,
      "learning_rate": 0.00019943232717568624,
      "loss": 0.0836,
      "step": 1102
    },
    {
      "epoch": 0.005188293178546901,
      "grad_norm": 0.9160679578781128,
      "learning_rate": 0.00019943138419757279,
      "loss": 0.0305,
      "step": 1103
    },
    {
      "epoch": 0.005192996980159365,
      "grad_norm": 6.8340582847595215,
      "learning_rate": 0.0001994304412194593,
      "loss": 0.2274,
      "step": 1104
    },
    {
      "epoch": 0.005197700781771828,
      "grad_norm": 5.055383682250977,
      "learning_rate": 0.00019942949824134582,
      "loss": 0.2578,
      "step": 1105
    },
    {
      "epoch": 0.005202404583384291,
      "grad_norm": 0.8925439119338989,
      "learning_rate": 0.00019942855526323234,
      "loss": 0.0442,
      "step": 1106
    },
    {
      "epoch": 0.005207108384996754,
      "grad_norm": 1.331179141998291,
      "learning_rate": 0.0001994276122851189,
      "loss": 0.0796,
      "step": 1107
    },
    {
      "epoch": 0.005211812186609217,
      "grad_norm": 2.715717315673828,
      "learning_rate": 0.0001994266693070054,
      "loss": 0.1744,
      "step": 1108
    },
    {
      "epoch": 0.00521651598822168,
      "grad_norm": 1.4115400314331055,
      "learning_rate": 0.00019942572632889192,
      "loss": 0.0591,
      "step": 1109
    },
    {
      "epoch": 0.005221219789834144,
      "grad_norm": 1.4880256652832031,
      "learning_rate": 0.00019942478335077844,
      "loss": 0.0843,
      "step": 1110
    },
    {
      "epoch": 0.005225923591446607,
      "grad_norm": 8.558088302612305,
      "learning_rate": 0.00019942384037266496,
      "loss": 1.2488,
      "step": 1111
    },
    {
      "epoch": 0.00523062739305907,
      "grad_norm": 7.5143046379089355,
      "learning_rate": 0.00019942289739455148,
      "loss": 0.9331,
      "step": 1112
    },
    {
      "epoch": 0.005235331194671533,
      "grad_norm": 6.310933589935303,
      "learning_rate": 0.000199421954416438,
      "loss": 0.589,
      "step": 1113
    },
    {
      "epoch": 0.0052400349962839965,
      "grad_norm": 4.477171421051025,
      "learning_rate": 0.00019942101143832452,
      "loss": 0.1327,
      "step": 1114
    },
    {
      "epoch": 0.0052447387978964595,
      "grad_norm": 5.244927883148193,
      "learning_rate": 0.00019942006846021104,
      "loss": 0.608,
      "step": 1115
    },
    {
      "epoch": 0.005249442599508923,
      "grad_norm": 4.488310813903809,
      "learning_rate": 0.00019941912548209758,
      "loss": 0.4816,
      "step": 1116
    },
    {
      "epoch": 0.0052541464011213864,
      "grad_norm": 5.4850335121154785,
      "learning_rate": 0.0001994181825039841,
      "loss": 0.3794,
      "step": 1117
    },
    {
      "epoch": 0.0052588502027338495,
      "grad_norm": 2.1099581718444824,
      "learning_rate": 0.00019941723952587062,
      "loss": 0.2771,
      "step": 1118
    },
    {
      "epoch": 0.0052635540043463125,
      "grad_norm": 4.633598327636719,
      "learning_rate": 0.00019941629654775714,
      "loss": 0.7191,
      "step": 1119
    },
    {
      "epoch": 0.0052682578059587756,
      "grad_norm": 3.6017260551452637,
      "learning_rate": 0.00019941535356964366,
      "loss": 0.2731,
      "step": 1120
    },
    {
      "epoch": 0.0052729616075712395,
      "grad_norm": 3.1258151531219482,
      "learning_rate": 0.00019941441059153018,
      "loss": 0.2204,
      "step": 1121
    },
    {
      "epoch": 0.0052776654091837025,
      "grad_norm": 6.469143867492676,
      "learning_rate": 0.0001994134676134167,
      "loss": 0.8975,
      "step": 1122
    },
    {
      "epoch": 0.0052823692107961655,
      "grad_norm": 0.25350576639175415,
      "learning_rate": 0.0001994125246353032,
      "loss": 0.0129,
      "step": 1123
    },
    {
      "epoch": 0.005287073012408629,
      "grad_norm": 4.364136695861816,
      "learning_rate": 0.00019941158165718973,
      "loss": 0.4651,
      "step": 1124
    },
    {
      "epoch": 0.005291776814021092,
      "grad_norm": 5.914736270904541,
      "learning_rate": 0.00019941063867907628,
      "loss": 0.7046,
      "step": 1125
    },
    {
      "epoch": 0.005296480615633555,
      "grad_norm": 3.7020633220672607,
      "learning_rate": 0.0001994096957009628,
      "loss": 0.1473,
      "step": 1126
    },
    {
      "epoch": 0.005301184417246019,
      "grad_norm": 0.35071516036987305,
      "learning_rate": 0.00019940875272284931,
      "loss": 0.0183,
      "step": 1127
    },
    {
      "epoch": 0.005305888218858482,
      "grad_norm": 4.434957981109619,
      "learning_rate": 0.00019940780974473583,
      "loss": 0.4541,
      "step": 1128
    },
    {
      "epoch": 0.005310592020470945,
      "grad_norm": 1.1684153079986572,
      "learning_rate": 0.00019940686676662235,
      "loss": 0.0652,
      "step": 1129
    },
    {
      "epoch": 0.005315295822083408,
      "grad_norm": 0.5167628526687622,
      "learning_rate": 0.0001994059237885089,
      "loss": 0.0307,
      "step": 1130
    },
    {
      "epoch": 0.005319999623695871,
      "grad_norm": 1.133687973022461,
      "learning_rate": 0.00019940498081039542,
      "loss": 0.0737,
      "step": 1131
    },
    {
      "epoch": 0.005324703425308334,
      "grad_norm": 3.4795587062835693,
      "learning_rate": 0.0001994040378322819,
      "loss": 0.193,
      "step": 1132
    },
    {
      "epoch": 0.005329407226920798,
      "grad_norm": 2.3188705444335938,
      "learning_rate": 0.00019940309485416843,
      "loss": 0.2018,
      "step": 1133
    },
    {
      "epoch": 0.005334111028533261,
      "grad_norm": 2.1599762439727783,
      "learning_rate": 0.00019940215187605497,
      "loss": 0.1705,
      "step": 1134
    },
    {
      "epoch": 0.005338814830145724,
      "grad_norm": 2.8607177734375,
      "learning_rate": 0.0001994012088979415,
      "loss": 0.1146,
      "step": 1135
    },
    {
      "epoch": 0.005343518631758187,
      "grad_norm": 4.604135513305664,
      "learning_rate": 0.000199400265919828,
      "loss": 0.4745,
      "step": 1136
    },
    {
      "epoch": 0.00534822243337065,
      "grad_norm": 0.7763581871986389,
      "learning_rate": 0.00019939932294171453,
      "loss": 0.0385,
      "step": 1137
    },
    {
      "epoch": 0.005352926234983114,
      "grad_norm": 6.071297645568848,
      "learning_rate": 0.00019939837996360105,
      "loss": 0.4392,
      "step": 1138
    },
    {
      "epoch": 0.005357630036595577,
      "grad_norm": 7.172021389007568,
      "learning_rate": 0.0001993974369854876,
      "loss": 0.5017,
      "step": 1139
    },
    {
      "epoch": 0.00536233383820804,
      "grad_norm": 2.903217077255249,
      "learning_rate": 0.0001993964940073741,
      "loss": 0.1705,
      "step": 1140
    },
    {
      "epoch": 0.005367037639820503,
      "grad_norm": 3.9746034145355225,
      "learning_rate": 0.00019939555102926063,
      "loss": 0.3467,
      "step": 1141
    },
    {
      "epoch": 0.005371741441432966,
      "grad_norm": 0.8437036275863647,
      "learning_rate": 0.00019939460805114715,
      "loss": 0.0665,
      "step": 1142
    },
    {
      "epoch": 0.005376445243045429,
      "grad_norm": 1.61798095703125,
      "learning_rate": 0.00019939366507303367,
      "loss": 0.0955,
      "step": 1143
    },
    {
      "epoch": 0.005381149044657893,
      "grad_norm": 4.404269218444824,
      "learning_rate": 0.00019939272209492019,
      "loss": 0.6919,
      "step": 1144
    },
    {
      "epoch": 0.005385852846270356,
      "grad_norm": 1.2087243795394897,
      "learning_rate": 0.0001993917791168067,
      "loss": 0.0666,
      "step": 1145
    },
    {
      "epoch": 0.005390556647882819,
      "grad_norm": 4.665010929107666,
      "learning_rate": 0.00019939083613869322,
      "loss": 0.3521,
      "step": 1146
    },
    {
      "epoch": 0.005395260449495282,
      "grad_norm": 1.894173264503479,
      "learning_rate": 0.00019938989316057974,
      "loss": 0.1211,
      "step": 1147
    },
    {
      "epoch": 0.005399964251107745,
      "grad_norm": 0.22987908124923706,
      "learning_rate": 0.0001993889501824663,
      "loss": 0.0077,
      "step": 1148
    },
    {
      "epoch": 0.005404668052720209,
      "grad_norm": 0.8262093663215637,
      "learning_rate": 0.0001993880072043528,
      "loss": 0.0554,
      "step": 1149
    },
    {
      "epoch": 0.005409371854332672,
      "grad_norm": 2.2996907234191895,
      "learning_rate": 0.00019938706422623932,
      "loss": 0.1415,
      "step": 1150
    },
    {
      "epoch": 0.005414075655945135,
      "grad_norm": 9.867459297180176,
      "learning_rate": 0.00019938612124812584,
      "loss": 0.5008,
      "step": 1151
    },
    {
      "epoch": 0.005418779457557598,
      "grad_norm": 2.409198045730591,
      "learning_rate": 0.00019938517827001236,
      "loss": 0.0591,
      "step": 1152
    },
    {
      "epoch": 0.005423483259170061,
      "grad_norm": 4.635859489440918,
      "learning_rate": 0.00019938423529189888,
      "loss": 0.5211,
      "step": 1153
    },
    {
      "epoch": 0.005428187060782524,
      "grad_norm": 5.254377841949463,
      "learning_rate": 0.0001993832923137854,
      "loss": 0.7986,
      "step": 1154
    },
    {
      "epoch": 0.005432890862394988,
      "grad_norm": 6.370785236358643,
      "learning_rate": 0.00019938234933567192,
      "loss": 0.5416,
      "step": 1155
    },
    {
      "epoch": 0.005437594664007451,
      "grad_norm": 5.404613494873047,
      "learning_rate": 0.00019938140635755844,
      "loss": 0.8931,
      "step": 1156
    },
    {
      "epoch": 0.005442298465619914,
      "grad_norm": 3.238684892654419,
      "learning_rate": 0.00019938046337944498,
      "loss": 0.1448,
      "step": 1157
    },
    {
      "epoch": 0.005447002267232377,
      "grad_norm": 10.10220718383789,
      "learning_rate": 0.0001993795204013315,
      "loss": 1.402,
      "step": 1158
    },
    {
      "epoch": 0.00545170606884484,
      "grad_norm": 8.26309871673584,
      "learning_rate": 0.00019937857742321802,
      "loss": 0.6499,
      "step": 1159
    },
    {
      "epoch": 0.005456409870457303,
      "grad_norm": 2.1420462131500244,
      "learning_rate": 0.00019937763444510454,
      "loss": 0.1453,
      "step": 1160
    },
    {
      "epoch": 0.005461113672069767,
      "grad_norm": 1.3605626821517944,
      "learning_rate": 0.00019937669146699106,
      "loss": 0.0672,
      "step": 1161
    },
    {
      "epoch": 0.00546581747368223,
      "grad_norm": 2.3025505542755127,
      "learning_rate": 0.0001993757484888776,
      "loss": 0.1737,
      "step": 1162
    },
    {
      "epoch": 0.005470521275294693,
      "grad_norm": 6.32482385635376,
      "learning_rate": 0.0001993748055107641,
      "loss": 0.9749,
      "step": 1163
    },
    {
      "epoch": 0.005475225076907156,
      "grad_norm": 3.37217378616333,
      "learning_rate": 0.0001993738625326506,
      "loss": 0.2535,
      "step": 1164
    },
    {
      "epoch": 0.005479928878519619,
      "grad_norm": 1.3887039422988892,
      "learning_rate": 0.00019937291955453713,
      "loss": 0.228,
      "step": 1165
    },
    {
      "epoch": 0.005484632680132083,
      "grad_norm": 0.38081851601600647,
      "learning_rate": 0.00019937197657642368,
      "loss": 0.0268,
      "step": 1166
    },
    {
      "epoch": 0.005489336481744546,
      "grad_norm": 0.858305811882019,
      "learning_rate": 0.0001993710335983102,
      "loss": 0.0476,
      "step": 1167
    },
    {
      "epoch": 0.005494040283357009,
      "grad_norm": 1.484155535697937,
      "learning_rate": 0.00019937009062019671,
      "loss": 0.0967,
      "step": 1168
    },
    {
      "epoch": 0.005498744084969472,
      "grad_norm": 3.1054580211639404,
      "learning_rate": 0.00019936914764208323,
      "loss": 0.2803,
      "step": 1169
    },
    {
      "epoch": 0.005503447886581935,
      "grad_norm": 2.6280763149261475,
      "learning_rate": 0.00019936820466396975,
      "loss": 0.2317,
      "step": 1170
    },
    {
      "epoch": 0.005508151688194398,
      "grad_norm": 2.470693826675415,
      "learning_rate": 0.0001993672616858563,
      "loss": 0.1874,
      "step": 1171
    },
    {
      "epoch": 0.005512855489806862,
      "grad_norm": 2.865025520324707,
      "learning_rate": 0.00019936631870774282,
      "loss": 0.2834,
      "step": 1172
    },
    {
      "epoch": 0.005517559291419325,
      "grad_norm": 0.11109992861747742,
      "learning_rate": 0.00019936537572962933,
      "loss": 0.0048,
      "step": 1173
    },
    {
      "epoch": 0.005522263093031788,
      "grad_norm": 6.851002216339111,
      "learning_rate": 0.00019936443275151583,
      "loss": 0.2993,
      "step": 1174
    },
    {
      "epoch": 0.005526966894644251,
      "grad_norm": 3.9289348125457764,
      "learning_rate": 0.00019936348977340237,
      "loss": 0.3913,
      "step": 1175
    },
    {
      "epoch": 0.005531670696256714,
      "grad_norm": 2.923616886138916,
      "learning_rate": 0.0001993625467952889,
      "loss": 0.3884,
      "step": 1176
    },
    {
      "epoch": 0.0055363744978691775,
      "grad_norm": 4.819845199584961,
      "learning_rate": 0.0001993616038171754,
      "loss": 0.9621,
      "step": 1177
    },
    {
      "epoch": 0.005541078299481641,
      "grad_norm": 2.939687967300415,
      "learning_rate": 0.00019936066083906193,
      "loss": 0.2993,
      "step": 1178
    },
    {
      "epoch": 0.005545782101094104,
      "grad_norm": 1.674376368522644,
      "learning_rate": 0.00019935971786094845,
      "loss": 0.1028,
      "step": 1179
    },
    {
      "epoch": 0.0055504859027065674,
      "grad_norm": 6.404157638549805,
      "learning_rate": 0.000199358774882835,
      "loss": 0.3606,
      "step": 1180
    },
    {
      "epoch": 0.0055551897043190305,
      "grad_norm": 2.5653977394104004,
      "learning_rate": 0.0001993578319047215,
      "loss": 0.2268,
      "step": 1181
    },
    {
      "epoch": 0.0055598935059314935,
      "grad_norm": 2.8671603202819824,
      "learning_rate": 0.00019935688892660803,
      "loss": 0.5163,
      "step": 1182
    },
    {
      "epoch": 0.0055645973075439574,
      "grad_norm": 6.628057479858398,
      "learning_rate": 0.00019935594594849455,
      "loss": 0.9266,
      "step": 1183
    },
    {
      "epoch": 0.0055693011091564205,
      "grad_norm": 4.409815311431885,
      "learning_rate": 0.00019935500297038107,
      "loss": 0.6785,
      "step": 1184
    },
    {
      "epoch": 0.0055740049107688835,
      "grad_norm": 4.213709354400635,
      "learning_rate": 0.00019935405999226759,
      "loss": 0.37,
      "step": 1185
    },
    {
      "epoch": 0.0055787087123813466,
      "grad_norm": 1.1696022748947144,
      "learning_rate": 0.0001993531170141541,
      "loss": 0.1483,
      "step": 1186
    },
    {
      "epoch": 0.00558341251399381,
      "grad_norm": 5.5416741371154785,
      "learning_rate": 0.00019935217403604062,
      "loss": 0.3633,
      "step": 1187
    },
    {
      "epoch": 0.005588116315606273,
      "grad_norm": 2.0847277641296387,
      "learning_rate": 0.00019935123105792714,
      "loss": 0.1944,
      "step": 1188
    },
    {
      "epoch": 0.0055928201172187365,
      "grad_norm": 0.5164781808853149,
      "learning_rate": 0.0001993502880798137,
      "loss": 0.0423,
      "step": 1189
    },
    {
      "epoch": 0.0055975239188312,
      "grad_norm": 5.6198811531066895,
      "learning_rate": 0.0001993493451017002,
      "loss": 0.7773,
      "step": 1190
    },
    {
      "epoch": 0.005602227720443663,
      "grad_norm": 3.4439780712127686,
      "learning_rate": 0.00019934840212358672,
      "loss": 0.7319,
      "step": 1191
    },
    {
      "epoch": 0.005606931522056126,
      "grad_norm": 0.892350435256958,
      "learning_rate": 0.00019934745914547324,
      "loss": 0.0901,
      "step": 1192
    },
    {
      "epoch": 0.005611635323668589,
      "grad_norm": 4.1819000244140625,
      "learning_rate": 0.0001993465161673598,
      "loss": 0.5658,
      "step": 1193
    },
    {
      "epoch": 0.005616339125281052,
      "grad_norm": 3.774714708328247,
      "learning_rate": 0.00019934557318924628,
      "loss": 0.3712,
      "step": 1194
    },
    {
      "epoch": 0.005621042926893516,
      "grad_norm": 2.5496082305908203,
      "learning_rate": 0.0001993446302111328,
      "loss": 0.2699,
      "step": 1195
    },
    {
      "epoch": 0.005625746728505979,
      "grad_norm": 3.96038556098938,
      "learning_rate": 0.00019934368723301932,
      "loss": 0.7296,
      "step": 1196
    },
    {
      "epoch": 0.005630450530118442,
      "grad_norm": 3.9397950172424316,
      "learning_rate": 0.00019934274425490584,
      "loss": 0.3104,
      "step": 1197
    },
    {
      "epoch": 0.005635154331730905,
      "grad_norm": 2.8164515495300293,
      "learning_rate": 0.00019934180127679238,
      "loss": 0.5098,
      "step": 1198
    },
    {
      "epoch": 0.005639858133343368,
      "grad_norm": 0.6950138211250305,
      "learning_rate": 0.0001993408582986789,
      "loss": 0.0363,
      "step": 1199
    },
    {
      "epoch": 0.005644561934955832,
      "grad_norm": 2.043074131011963,
      "learning_rate": 0.00019933991532056542,
      "loss": 0.3198,
      "step": 1200
    },
    {
      "epoch": 0.005649265736568295,
      "grad_norm": 4.8200297355651855,
      "learning_rate": 0.00019933897234245194,
      "loss": 0.6469,
      "step": 1201
    },
    {
      "epoch": 0.005653969538180758,
      "grad_norm": 7.070374488830566,
      "learning_rate": 0.00019933802936433846,
      "loss": 1.1239,
      "step": 1202
    },
    {
      "epoch": 0.005658673339793221,
      "grad_norm": 8.754776000976562,
      "learning_rate": 0.000199337086386225,
      "loss": 0.9654,
      "step": 1203
    },
    {
      "epoch": 0.005663377141405684,
      "grad_norm": 0.675666093826294,
      "learning_rate": 0.00019933614340811152,
      "loss": 0.0349,
      "step": 1204
    },
    {
      "epoch": 0.005668080943018147,
      "grad_norm": 4.707622528076172,
      "learning_rate": 0.000199335200429998,
      "loss": 0.7683,
      "step": 1205
    },
    {
      "epoch": 0.005672784744630611,
      "grad_norm": 2.05842661857605,
      "learning_rate": 0.00019933425745188453,
      "loss": 0.2405,
      "step": 1206
    },
    {
      "epoch": 0.005677488546243074,
      "grad_norm": 1.4938149452209473,
      "learning_rate": 0.00019933331447377108,
      "loss": 0.1945,
      "step": 1207
    },
    {
      "epoch": 0.005682192347855537,
      "grad_norm": 1.8888393640518188,
      "learning_rate": 0.0001993323714956576,
      "loss": 0.1516,
      "step": 1208
    },
    {
      "epoch": 0.005686896149468,
      "grad_norm": 3.495593547821045,
      "learning_rate": 0.00019933142851754411,
      "loss": 0.4761,
      "step": 1209
    },
    {
      "epoch": 0.005691599951080463,
      "grad_norm": 2.378688335418701,
      "learning_rate": 0.00019933048553943063,
      "loss": 0.5007,
      "step": 1210
    },
    {
      "epoch": 0.005696303752692926,
      "grad_norm": 2.0711324214935303,
      "learning_rate": 0.00019932954256131715,
      "loss": 0.262,
      "step": 1211
    },
    {
      "epoch": 0.00570100755430539,
      "grad_norm": 2.422760248184204,
      "learning_rate": 0.0001993285995832037,
      "loss": 0.4428,
      "step": 1212
    },
    {
      "epoch": 0.005705711355917853,
      "grad_norm": 1.0392595529556274,
      "learning_rate": 0.00019932765660509022,
      "loss": 0.1849,
      "step": 1213
    },
    {
      "epoch": 0.005710415157530316,
      "grad_norm": 1.4100192785263062,
      "learning_rate": 0.00019932671362697673,
      "loss": 0.2219,
      "step": 1214
    },
    {
      "epoch": 0.005715118959142779,
      "grad_norm": 1.1109975576400757,
      "learning_rate": 0.00019932577064886325,
      "loss": 0.3005,
      "step": 1215
    },
    {
      "epoch": 0.005719822760755242,
      "grad_norm": 1.5160828828811646,
      "learning_rate": 0.00019932482767074977,
      "loss": 0.1723,
      "step": 1216
    },
    {
      "epoch": 0.005724526562367706,
      "grad_norm": 2.116989850997925,
      "learning_rate": 0.0001993238846926363,
      "loss": 0.2312,
      "step": 1217
    },
    {
      "epoch": 0.005729230363980169,
      "grad_norm": 1.290832281112671,
      "learning_rate": 0.0001993229417145228,
      "loss": 0.1477,
      "step": 1218
    },
    {
      "epoch": 0.005733934165592632,
      "grad_norm": 1.3675892353057861,
      "learning_rate": 0.00019932199873640933,
      "loss": 0.1483,
      "step": 1219
    },
    {
      "epoch": 0.005738637967205095,
      "grad_norm": 1.387751579284668,
      "learning_rate": 0.00019932105575829585,
      "loss": 0.1479,
      "step": 1220
    },
    {
      "epoch": 0.005743341768817558,
      "grad_norm": 2.6929235458374023,
      "learning_rate": 0.0001993201127801824,
      "loss": 0.2617,
      "step": 1221
    },
    {
      "epoch": 0.005748045570430021,
      "grad_norm": 3.8289926052093506,
      "learning_rate": 0.0001993191698020689,
      "loss": 0.5473,
      "step": 1222
    },
    {
      "epoch": 0.005752749372042485,
      "grad_norm": 3.6065146923065186,
      "learning_rate": 0.00019931822682395543,
      "loss": 0.2511,
      "step": 1223
    },
    {
      "epoch": 0.005757453173654948,
      "grad_norm": 1.1976337432861328,
      "learning_rate": 0.00019931728384584195,
      "loss": 0.157,
      "step": 1224
    },
    {
      "epoch": 0.005762156975267411,
      "grad_norm": 1.3144774436950684,
      "learning_rate": 0.00019931634086772847,
      "loss": 0.1356,
      "step": 1225
    },
    {
      "epoch": 0.005766860776879874,
      "grad_norm": 1.150099515914917,
      "learning_rate": 0.00019931539788961499,
      "loss": 0.1195,
      "step": 1226
    },
    {
      "epoch": 0.005771564578492337,
      "grad_norm": 3.6199307441711426,
      "learning_rate": 0.0001993144549115015,
      "loss": 0.5751,
      "step": 1227
    },
    {
      "epoch": 0.0057762683801048,
      "grad_norm": 2.053744316101074,
      "learning_rate": 0.00019931351193338802,
      "loss": 0.1009,
      "step": 1228
    },
    {
      "epoch": 0.005780972181717264,
      "grad_norm": 1.6871464252471924,
      "learning_rate": 0.00019931256895527454,
      "loss": 0.2022,
      "step": 1229
    },
    {
      "epoch": 0.005785675983329727,
      "grad_norm": 0.793158233165741,
      "learning_rate": 0.0001993116259771611,
      "loss": 0.0363,
      "step": 1230
    },
    {
      "epoch": 0.00579037978494219,
      "grad_norm": 4.424693584442139,
      "learning_rate": 0.0001993106829990476,
      "loss": 0.5557,
      "step": 1231
    },
    {
      "epoch": 0.005795083586554653,
      "grad_norm": 1.5429329872131348,
      "learning_rate": 0.00019930974002093412,
      "loss": 0.1128,
      "step": 1232
    },
    {
      "epoch": 0.005799787388167116,
      "grad_norm": 6.781052112579346,
      "learning_rate": 0.00019930879704282064,
      "loss": 0.7655,
      "step": 1233
    },
    {
      "epoch": 0.00580449118977958,
      "grad_norm": 4.749333381652832,
      "learning_rate": 0.0001993078540647072,
      "loss": 0.6344,
      "step": 1234
    },
    {
      "epoch": 0.005809194991392043,
      "grad_norm": 1.4649970531463623,
      "learning_rate": 0.0001993069110865937,
      "loss": 0.1411,
      "step": 1235
    },
    {
      "epoch": 0.005813898793004506,
      "grad_norm": 5.697296619415283,
      "learning_rate": 0.0001993059681084802,
      "loss": 0.8614,
      "step": 1236
    },
    {
      "epoch": 0.005818602594616969,
      "grad_norm": 1.2889690399169922,
      "learning_rate": 0.00019930502513036672,
      "loss": 0.1595,
      "step": 1237
    },
    {
      "epoch": 0.005823306396229432,
      "grad_norm": 0.6478897333145142,
      "learning_rate": 0.00019930408215225324,
      "loss": 0.0436,
      "step": 1238
    },
    {
      "epoch": 0.005828010197841895,
      "grad_norm": 2.3113772869110107,
      "learning_rate": 0.00019930313917413978,
      "loss": 0.2199,
      "step": 1239
    },
    {
      "epoch": 0.005832713999454359,
      "grad_norm": 3.2041616439819336,
      "learning_rate": 0.0001993021961960263,
      "loss": 0.3884,
      "step": 1240
    },
    {
      "epoch": 0.005837417801066822,
      "grad_norm": 5.104762554168701,
      "learning_rate": 0.00019930125321791282,
      "loss": 0.61,
      "step": 1241
    },
    {
      "epoch": 0.005842121602679285,
      "grad_norm": 2.0124590396881104,
      "learning_rate": 0.00019930031023979934,
      "loss": 0.165,
      "step": 1242
    },
    {
      "epoch": 0.0058468254042917485,
      "grad_norm": 1.0103362798690796,
      "learning_rate": 0.00019929936726168588,
      "loss": 0.0749,
      "step": 1243
    },
    {
      "epoch": 0.0058515292059042115,
      "grad_norm": 5.156274795532227,
      "learning_rate": 0.0001992984242835724,
      "loss": 0.4305,
      "step": 1244
    },
    {
      "epoch": 0.005856233007516675,
      "grad_norm": 9.353336334228516,
      "learning_rate": 0.00019929748130545892,
      "loss": 0.6514,
      "step": 1245
    },
    {
      "epoch": 0.0058609368091291384,
      "grad_norm": 1.3498950004577637,
      "learning_rate": 0.00019929653832734544,
      "loss": 0.1577,
      "step": 1246
    },
    {
      "epoch": 0.0058656406107416015,
      "grad_norm": 1.5542339086532593,
      "learning_rate": 0.00019929559534923196,
      "loss": 0.1679,
      "step": 1247
    },
    {
      "epoch": 0.0058703444123540645,
      "grad_norm": 0.8223026394844055,
      "learning_rate": 0.00019929465237111848,
      "loss": 0.0989,
      "step": 1248
    },
    {
      "epoch": 0.0058750482139665276,
      "grad_norm": 1.3289713859558105,
      "learning_rate": 0.000199293709393005,
      "loss": 0.0787,
      "step": 1249
    },
    {
      "epoch": 0.005879752015578991,
      "grad_norm": 3.784435749053955,
      "learning_rate": 0.00019929276641489151,
      "loss": 0.2394,
      "step": 1250
    },
    {
      "epoch": 0.0058844558171914545,
      "grad_norm": 3.639416217803955,
      "learning_rate": 0.00019929182343677803,
      "loss": 0.2231,
      "step": 1251
    },
    {
      "epoch": 0.0058891596188039175,
      "grad_norm": 1.6251678466796875,
      "learning_rate": 0.00019929088045866458,
      "loss": 0.1206,
      "step": 1252
    },
    {
      "epoch": 0.005893863420416381,
      "grad_norm": 0.8225566148757935,
      "learning_rate": 0.0001992899374805511,
      "loss": 0.0313,
      "step": 1253
    },
    {
      "epoch": 0.005898567222028844,
      "grad_norm": 5.806878089904785,
      "learning_rate": 0.00019928899450243762,
      "loss": 0.607,
      "step": 1254
    },
    {
      "epoch": 0.005903271023641307,
      "grad_norm": 3.8365721702575684,
      "learning_rate": 0.00019928805152432413,
      "loss": 0.2764,
      "step": 1255
    },
    {
      "epoch": 0.00590797482525377,
      "grad_norm": 2.233706474304199,
      "learning_rate": 0.00019928710854621065,
      "loss": 0.1201,
      "step": 1256
    },
    {
      "epoch": 0.005912678626866234,
      "grad_norm": 6.764200210571289,
      "learning_rate": 0.00019928616556809717,
      "loss": 0.8666,
      "step": 1257
    },
    {
      "epoch": 0.005917382428478697,
      "grad_norm": 1.445852518081665,
      "learning_rate": 0.0001992852225899837,
      "loss": 0.0577,
      "step": 1258
    },
    {
      "epoch": 0.00592208623009116,
      "grad_norm": 7.526480197906494,
      "learning_rate": 0.0001992842796118702,
      "loss": 0.7086,
      "step": 1259
    },
    {
      "epoch": 0.005926790031703623,
      "grad_norm": 5.058367729187012,
      "learning_rate": 0.00019928333663375673,
      "loss": 0.3856,
      "step": 1260
    },
    {
      "epoch": 0.005931493833316086,
      "grad_norm": 3.0840954780578613,
      "learning_rate": 0.00019928239365564325,
      "loss": 0.1952,
      "step": 1261
    },
    {
      "epoch": 0.00593619763492855,
      "grad_norm": 4.15273380279541,
      "learning_rate": 0.0001992814506775298,
      "loss": 0.7649,
      "step": 1262
    },
    {
      "epoch": 0.005940901436541013,
      "grad_norm": 0.03401263430714607,
      "learning_rate": 0.0001992805076994163,
      "loss": 0.0014,
      "step": 1263
    },
    {
      "epoch": 0.005945605238153476,
      "grad_norm": 4.920039176940918,
      "learning_rate": 0.00019927956472130283,
      "loss": 0.2747,
      "step": 1264
    },
    {
      "epoch": 0.005950309039765939,
      "grad_norm": 5.3256659507751465,
      "learning_rate": 0.00019927862174318935,
      "loss": 1.202,
      "step": 1265
    },
    {
      "epoch": 0.005955012841378402,
      "grad_norm": 1.491474986076355,
      "learning_rate": 0.0001992776787650759,
      "loss": 0.0917,
      "step": 1266
    },
    {
      "epoch": 0.005959716642990865,
      "grad_norm": 1.0424631834030151,
      "learning_rate": 0.00019927673578696239,
      "loss": 0.039,
      "step": 1267
    },
    {
      "epoch": 0.005964420444603329,
      "grad_norm": 6.376051902770996,
      "learning_rate": 0.0001992757928088489,
      "loss": 0.4321,
      "step": 1268
    },
    {
      "epoch": 0.005969124246215792,
      "grad_norm": 2.305828332901001,
      "learning_rate": 0.00019927484983073542,
      "loss": 0.2922,
      "step": 1269
    },
    {
      "epoch": 0.005973828047828255,
      "grad_norm": 1.7370834350585938,
      "learning_rate": 0.00019927390685262194,
      "loss": 0.1118,
      "step": 1270
    },
    {
      "epoch": 0.005978531849440718,
      "grad_norm": 1.6610260009765625,
      "learning_rate": 0.0001992729638745085,
      "loss": 0.0997,
      "step": 1271
    },
    {
      "epoch": 0.005983235651053181,
      "grad_norm": 0.698682427406311,
      "learning_rate": 0.000199272020896395,
      "loss": 0.0609,
      "step": 1272
    },
    {
      "epoch": 0.005987939452665644,
      "grad_norm": 1.8116564750671387,
      "learning_rate": 0.00019927107791828152,
      "loss": 0.1286,
      "step": 1273
    },
    {
      "epoch": 0.005992643254278108,
      "grad_norm": 1.4065539836883545,
      "learning_rate": 0.00019927013494016804,
      "loss": 0.2025,
      "step": 1274
    },
    {
      "epoch": 0.005997347055890571,
      "grad_norm": 2.437939405441284,
      "learning_rate": 0.0001992691919620546,
      "loss": 0.1756,
      "step": 1275
    },
    {
      "epoch": 0.006002050857503034,
      "grad_norm": 2.970208168029785,
      "learning_rate": 0.0001992682489839411,
      "loss": 0.2693,
      "step": 1276
    },
    {
      "epoch": 0.006006754659115497,
      "grad_norm": 3.8471930027008057,
      "learning_rate": 0.00019926730600582763,
      "loss": 0.4362,
      "step": 1277
    },
    {
      "epoch": 0.00601145846072796,
      "grad_norm": 4.280054569244385,
      "learning_rate": 0.00019926636302771414,
      "loss": 0.3732,
      "step": 1278
    },
    {
      "epoch": 0.006016162262340424,
      "grad_norm": 1.6444324254989624,
      "learning_rate": 0.00019926542004960064,
      "loss": 0.1465,
      "step": 1279
    },
    {
      "epoch": 0.006020866063952887,
      "grad_norm": 2.0941598415374756,
      "learning_rate": 0.00019926447707148718,
      "loss": 0.2193,
      "step": 1280
    },
    {
      "epoch": 0.00602556986556535,
      "grad_norm": 2.719475030899048,
      "learning_rate": 0.0001992635340933737,
      "loss": 0.4009,
      "step": 1281
    },
    {
      "epoch": 0.006030273667177813,
      "grad_norm": 1.496988296508789,
      "learning_rate": 0.00019926259111526022,
      "loss": 0.0873,
      "step": 1282
    },
    {
      "epoch": 0.006034977468790276,
      "grad_norm": 0.8769496083259583,
      "learning_rate": 0.00019926164813714674,
      "loss": 0.0746,
      "step": 1283
    },
    {
      "epoch": 0.006039681270402739,
      "grad_norm": 0.7686470746994019,
      "learning_rate": 0.00019926070515903328,
      "loss": 0.0763,
      "step": 1284
    },
    {
      "epoch": 0.006044385072015203,
      "grad_norm": 1.3727295398712158,
      "learning_rate": 0.0001992597621809198,
      "loss": 0.1291,
      "step": 1285
    },
    {
      "epoch": 0.006049088873627666,
      "grad_norm": 3.052811622619629,
      "learning_rate": 0.00019925881920280632,
      "loss": 0.3593,
      "step": 1286
    },
    {
      "epoch": 0.006053792675240129,
      "grad_norm": 2.0017738342285156,
      "learning_rate": 0.00019925787622469284,
      "loss": 0.0959,
      "step": 1287
    },
    {
      "epoch": 0.006058496476852592,
      "grad_norm": 1.1170976161956787,
      "learning_rate": 0.00019925693324657936,
      "loss": 0.1149,
      "step": 1288
    },
    {
      "epoch": 0.006063200278465055,
      "grad_norm": 1.0299327373504639,
      "learning_rate": 0.00019925599026846588,
      "loss": 0.0733,
      "step": 1289
    },
    {
      "epoch": 0.006067904080077518,
      "grad_norm": 3.037893772125244,
      "learning_rate": 0.0001992550472903524,
      "loss": 0.2554,
      "step": 1290
    },
    {
      "epoch": 0.006072607881689982,
      "grad_norm": 4.019340515136719,
      "learning_rate": 0.00019925410431223891,
      "loss": 0.2672,
      "step": 1291
    },
    {
      "epoch": 0.006077311683302445,
      "grad_norm": 3.297081470489502,
      "learning_rate": 0.00019925316133412543,
      "loss": 0.1376,
      "step": 1292
    },
    {
      "epoch": 0.006082015484914908,
      "grad_norm": 5.918375015258789,
      "learning_rate": 0.00019925221835601198,
      "loss": 0.5497,
      "step": 1293
    },
    {
      "epoch": 0.006086719286527371,
      "grad_norm": 2.914677381515503,
      "learning_rate": 0.0001992512753778985,
      "loss": 0.1728,
      "step": 1294
    },
    {
      "epoch": 0.006091423088139834,
      "grad_norm": 3.6379780769348145,
      "learning_rate": 0.00019925033239978502,
      "loss": 0.9724,
      "step": 1295
    },
    {
      "epoch": 0.006096126889752298,
      "grad_norm": 2.665971279144287,
      "learning_rate": 0.00019924938942167153,
      "loss": 0.3171,
      "step": 1296
    },
    {
      "epoch": 0.006100830691364761,
      "grad_norm": 4.7336530685424805,
      "learning_rate": 0.00019924844644355805,
      "loss": 0.4826,
      "step": 1297
    },
    {
      "epoch": 0.006105534492977224,
      "grad_norm": 6.398085594177246,
      "learning_rate": 0.00019924750346544457,
      "loss": 0.5827,
      "step": 1298
    },
    {
      "epoch": 0.006110238294589687,
      "grad_norm": 1.6176749467849731,
      "learning_rate": 0.0001992465604873311,
      "loss": 0.1043,
      "step": 1299
    },
    {
      "epoch": 0.00611494209620215,
      "grad_norm": 3.5241875648498535,
      "learning_rate": 0.0001992456175092176,
      "loss": 0.4338,
      "step": 1300
    },
    {
      "epoch": 0.006119645897814613,
      "grad_norm": 11.880297660827637,
      "learning_rate": 0.00019924467453110413,
      "loss": 0.398,
      "step": 1301
    },
    {
      "epoch": 0.006124349699427077,
      "grad_norm": 1.2258449792861938,
      "learning_rate": 0.00019924373155299067,
      "loss": 0.0423,
      "step": 1302
    },
    {
      "epoch": 0.00612905350103954,
      "grad_norm": 4.117990493774414,
      "learning_rate": 0.0001992427885748772,
      "loss": 0.7353,
      "step": 1303
    },
    {
      "epoch": 0.006133757302652003,
      "grad_norm": 5.710389614105225,
      "learning_rate": 0.0001992418455967637,
      "loss": 0.3904,
      "step": 1304
    },
    {
      "epoch": 0.006138461104264466,
      "grad_norm": 1.8665326833724976,
      "learning_rate": 0.00019924090261865023,
      "loss": 0.1605,
      "step": 1305
    },
    {
      "epoch": 0.0061431649058769295,
      "grad_norm": 2.659217357635498,
      "learning_rate": 0.00019923995964053675,
      "loss": 0.1989,
      "step": 1306
    },
    {
      "epoch": 0.0061478687074893925,
      "grad_norm": 6.0066304206848145,
      "learning_rate": 0.0001992390166624233,
      "loss": 1.0185,
      "step": 1307
    },
    {
      "epoch": 0.006152572509101856,
      "grad_norm": 4.032823085784912,
      "learning_rate": 0.0001992380736843098,
      "loss": 0.3713,
      "step": 1308
    },
    {
      "epoch": 0.0061572763107143194,
      "grad_norm": 7.590004920959473,
      "learning_rate": 0.00019923713070619633,
      "loss": 0.3636,
      "step": 1309
    },
    {
      "epoch": 0.0061619801123267825,
      "grad_norm": 4.815395355224609,
      "learning_rate": 0.00019923618772808282,
      "loss": 1.1883,
      "step": 1310
    },
    {
      "epoch": 0.0061666839139392455,
      "grad_norm": 3.3513877391815186,
      "learning_rate": 0.00019923524474996934,
      "loss": 0.2407,
      "step": 1311
    },
    {
      "epoch": 0.0061713877155517086,
      "grad_norm": 3.7744569778442383,
      "learning_rate": 0.0001992343017718559,
      "loss": 0.35,
      "step": 1312
    },
    {
      "epoch": 0.0061760915171641725,
      "grad_norm": 2.939973831176758,
      "learning_rate": 0.0001992333587937424,
      "loss": 0.3727,
      "step": 1313
    },
    {
      "epoch": 0.0061807953187766355,
      "grad_norm": 2.994934320449829,
      "learning_rate": 0.00019923241581562892,
      "loss": 0.2833,
      "step": 1314
    },
    {
      "epoch": 0.0061854991203890985,
      "grad_norm": 3.2441821098327637,
      "learning_rate": 0.00019923147283751544,
      "loss": 0.4573,
      "step": 1315
    },
    {
      "epoch": 0.006190202922001562,
      "grad_norm": 1.7011607885360718,
      "learning_rate": 0.000199230529859402,
      "loss": 0.1733,
      "step": 1316
    },
    {
      "epoch": 0.006194906723614025,
      "grad_norm": 2.092859983444214,
      "learning_rate": 0.0001992295868812885,
      "loss": 0.2686,
      "step": 1317
    },
    {
      "epoch": 0.006199610525226488,
      "grad_norm": 2.8448615074157715,
      "learning_rate": 0.00019922864390317503,
      "loss": 0.3486,
      "step": 1318
    },
    {
      "epoch": 0.006204314326838952,
      "grad_norm": 2.7913870811462402,
      "learning_rate": 0.00019922770092506154,
      "loss": 0.1783,
      "step": 1319
    },
    {
      "epoch": 0.006209018128451415,
      "grad_norm": 1.182358741760254,
      "learning_rate": 0.00019922675794694806,
      "loss": 0.0861,
      "step": 1320
    },
    {
      "epoch": 0.006213721930063878,
      "grad_norm": 0.905224621295929,
      "learning_rate": 0.00019922581496883458,
      "loss": 0.0614,
      "step": 1321
    },
    {
      "epoch": 0.006218425731676341,
      "grad_norm": 1.8957051038742065,
      "learning_rate": 0.0001992248719907211,
      "loss": 0.2419,
      "step": 1322
    },
    {
      "epoch": 0.006223129533288804,
      "grad_norm": 0.7903734445571899,
      "learning_rate": 0.00019922392901260762,
      "loss": 0.046,
      "step": 1323
    },
    {
      "epoch": 0.006227833334901268,
      "grad_norm": 1.4794456958770752,
      "learning_rate": 0.00019922298603449414,
      "loss": 0.1599,
      "step": 1324
    },
    {
      "epoch": 0.006232537136513731,
      "grad_norm": 2.730485439300537,
      "learning_rate": 0.00019922204305638068,
      "loss": 0.2082,
      "step": 1325
    },
    {
      "epoch": 0.006237240938126194,
      "grad_norm": 1.6693755388259888,
      "learning_rate": 0.0001992211000782672,
      "loss": 0.1617,
      "step": 1326
    },
    {
      "epoch": 0.006241944739738657,
      "grad_norm": 1.9373129606246948,
      "learning_rate": 0.00019922015710015372,
      "loss": 0.1611,
      "step": 1327
    },
    {
      "epoch": 0.00624664854135112,
      "grad_norm": 0.09699622541666031,
      "learning_rate": 0.00019921921412204024,
      "loss": 0.0046,
      "step": 1328
    },
    {
      "epoch": 0.006251352342963583,
      "grad_norm": 3.1648166179656982,
      "learning_rate": 0.00019921827114392676,
      "loss": 0.1035,
      "step": 1329
    },
    {
      "epoch": 0.006256056144576047,
      "grad_norm": 3.2026867866516113,
      "learning_rate": 0.00019921732816581328,
      "loss": 0.2819,
      "step": 1330
    },
    {
      "epoch": 0.00626075994618851,
      "grad_norm": 0.1931099146604538,
      "learning_rate": 0.0001992163851876998,
      "loss": 0.0073,
      "step": 1331
    },
    {
      "epoch": 0.006265463747800973,
      "grad_norm": 4.8043437004089355,
      "learning_rate": 0.00019921544220958631,
      "loss": 0.7336,
      "step": 1332
    },
    {
      "epoch": 0.006270167549413436,
      "grad_norm": 2.653977155685425,
      "learning_rate": 0.00019921449923147283,
      "loss": 0.3132,
      "step": 1333
    },
    {
      "epoch": 0.006274871351025899,
      "grad_norm": 0.1970120072364807,
      "learning_rate": 0.00019921355625335938,
      "loss": 0.0163,
      "step": 1334
    },
    {
      "epoch": 0.006279575152638362,
      "grad_norm": 5.042530536651611,
      "learning_rate": 0.0001992126132752459,
      "loss": 0.4437,
      "step": 1335
    },
    {
      "epoch": 0.006284278954250826,
      "grad_norm": 1.7543840408325195,
      "learning_rate": 0.00019921167029713242,
      "loss": 0.1282,
      "step": 1336
    },
    {
      "epoch": 0.006288982755863289,
      "grad_norm": 3.4680984020233154,
      "learning_rate": 0.00019921072731901893,
      "loss": 0.5903,
      "step": 1337
    },
    {
      "epoch": 0.006293686557475752,
      "grad_norm": 3.2494592666625977,
      "learning_rate": 0.00019920978434090545,
      "loss": 0.4127,
      "step": 1338
    },
    {
      "epoch": 0.006298390359088215,
      "grad_norm": 0.4063134789466858,
      "learning_rate": 0.000199208841362792,
      "loss": 0.022,
      "step": 1339
    },
    {
      "epoch": 0.006303094160700678,
      "grad_norm": 3.947680950164795,
      "learning_rate": 0.00019920789838467852,
      "loss": 0.5213,
      "step": 1340
    },
    {
      "epoch": 0.006307797962313142,
      "grad_norm": 2.05122709274292,
      "learning_rate": 0.000199206955406565,
      "loss": 0.2825,
      "step": 1341
    },
    {
      "epoch": 0.006312501763925605,
      "grad_norm": 2.9824588298797607,
      "learning_rate": 0.00019920601242845153,
      "loss": 0.3805,
      "step": 1342
    },
    {
      "epoch": 0.006317205565538068,
      "grad_norm": 3.9097533226013184,
      "learning_rate": 0.00019920506945033807,
      "loss": 0.3822,
      "step": 1343
    },
    {
      "epoch": 0.006321909367150531,
      "grad_norm": 2.188673734664917,
      "learning_rate": 0.0001992041264722246,
      "loss": 0.418,
      "step": 1344
    },
    {
      "epoch": 0.006326613168762994,
      "grad_norm": 0.7709594964981079,
      "learning_rate": 0.0001992031834941111,
      "loss": 0.0867,
      "step": 1345
    },
    {
      "epoch": 0.006331316970375457,
      "grad_norm": 2.9926815032958984,
      "learning_rate": 0.00019920224051599763,
      "loss": 0.8724,
      "step": 1346
    },
    {
      "epoch": 0.006336020771987921,
      "grad_norm": 3.136735200881958,
      "learning_rate": 0.00019920129753788415,
      "loss": 0.3397,
      "step": 1347
    },
    {
      "epoch": 0.006340724573600384,
      "grad_norm": 2.52642822265625,
      "learning_rate": 0.0001992003545597707,
      "loss": 0.2683,
      "step": 1348
    },
    {
      "epoch": 0.006345428375212847,
      "grad_norm": 0.8175086379051208,
      "learning_rate": 0.0001991994115816572,
      "loss": 0.1242,
      "step": 1349
    },
    {
      "epoch": 0.00635013217682531,
      "grad_norm": 1.3646725416183472,
      "learning_rate": 0.00019919846860354373,
      "loss": 0.1223,
      "step": 1350
    },
    {
      "epoch": 0.006354835978437773,
      "grad_norm": 5.086792469024658,
      "learning_rate": 0.00019919752562543025,
      "loss": 0.4509,
      "step": 1351
    },
    {
      "epoch": 0.006359539780050236,
      "grad_norm": 4.30047082901001,
      "learning_rate": 0.00019919658264731677,
      "loss": 0.6292,
      "step": 1352
    },
    {
      "epoch": 0.0063642435816627,
      "grad_norm": 5.5526509284973145,
      "learning_rate": 0.0001991956396692033,
      "loss": 0.7155,
      "step": 1353
    },
    {
      "epoch": 0.006368947383275163,
      "grad_norm": 3.9304115772247314,
      "learning_rate": 0.0001991946966910898,
      "loss": 0.262,
      "step": 1354
    },
    {
      "epoch": 0.006373651184887626,
      "grad_norm": 2.0851359367370605,
      "learning_rate": 0.00019919375371297632,
      "loss": 0.1657,
      "step": 1355
    },
    {
      "epoch": 0.006378354986500089,
      "grad_norm": 1.7218279838562012,
      "learning_rate": 0.00019919281073486284,
      "loss": 0.1328,
      "step": 1356
    },
    {
      "epoch": 0.006383058788112552,
      "grad_norm": 4.667328834533691,
      "learning_rate": 0.0001991918677567494,
      "loss": 0.5193,
      "step": 1357
    },
    {
      "epoch": 0.006387762589725016,
      "grad_norm": 1.2759580612182617,
      "learning_rate": 0.0001991909247786359,
      "loss": 0.1134,
      "step": 1358
    },
    {
      "epoch": 0.006392466391337479,
      "grad_norm": 2.8780972957611084,
      "learning_rate": 0.00019918998180052243,
      "loss": 0.382,
      "step": 1359
    },
    {
      "epoch": 0.006397170192949942,
      "grad_norm": 0.7783670425415039,
      "learning_rate": 0.00019918903882240894,
      "loss": 0.0576,
      "step": 1360
    },
    {
      "epoch": 0.006401873994562405,
      "grad_norm": 3.7707276344299316,
      "learning_rate": 0.00019918809584429546,
      "loss": 0.228,
      "step": 1361
    },
    {
      "epoch": 0.006406577796174868,
      "grad_norm": 1.7454590797424316,
      "learning_rate": 0.00019918715286618198,
      "loss": 0.2348,
      "step": 1362
    },
    {
      "epoch": 0.006411281597787331,
      "grad_norm": 2.0401275157928467,
      "learning_rate": 0.0001991862098880685,
      "loss": 0.2113,
      "step": 1363
    },
    {
      "epoch": 0.006415985399399795,
      "grad_norm": 4.700932502746582,
      "learning_rate": 0.00019918526690995502,
      "loss": 0.5186,
      "step": 1364
    },
    {
      "epoch": 0.006420689201012258,
      "grad_norm": 2.6380412578582764,
      "learning_rate": 0.00019918432393184154,
      "loss": 0.2397,
      "step": 1365
    },
    {
      "epoch": 0.006425393002624721,
      "grad_norm": 1.5414738655090332,
      "learning_rate": 0.00019918338095372808,
      "loss": 0.1874,
      "step": 1366
    },
    {
      "epoch": 0.006430096804237184,
      "grad_norm": 2.1091878414154053,
      "learning_rate": 0.0001991824379756146,
      "loss": 0.1549,
      "step": 1367
    },
    {
      "epoch": 0.006434800605849647,
      "grad_norm": 0.7958643436431885,
      "learning_rate": 0.00019918149499750112,
      "loss": 0.0767,
      "step": 1368
    },
    {
      "epoch": 0.0064395044074621105,
      "grad_norm": 1.7204638719558716,
      "learning_rate": 0.00019918055201938764,
      "loss": 0.1667,
      "step": 1369
    },
    {
      "epoch": 0.006444208209074574,
      "grad_norm": 2.175060272216797,
      "learning_rate": 0.00019917960904127416,
      "loss": 0.271,
      "step": 1370
    },
    {
      "epoch": 0.006448912010687037,
      "grad_norm": 2.502535820007324,
      "learning_rate": 0.0001991786660631607,
      "loss": 0.3904,
      "step": 1371
    },
    {
      "epoch": 0.0064536158122995005,
      "grad_norm": 0.6607105731964111,
      "learning_rate": 0.0001991777230850472,
      "loss": 0.0731,
      "step": 1372
    },
    {
      "epoch": 0.0064583196139119635,
      "grad_norm": 1.9826887845993042,
      "learning_rate": 0.00019917678010693371,
      "loss": 0.1946,
      "step": 1373
    },
    {
      "epoch": 0.0064630234155244265,
      "grad_norm": 3.546515703201294,
      "learning_rate": 0.00019917583712882023,
      "loss": 0.817,
      "step": 1374
    },
    {
      "epoch": 0.0064677272171368904,
      "grad_norm": 2.5036728382110596,
      "learning_rate": 0.00019917489415070678,
      "loss": 0.4435,
      "step": 1375
    },
    {
      "epoch": 0.0064724310187493535,
      "grad_norm": 1.0384302139282227,
      "learning_rate": 0.0001991739511725933,
      "loss": 0.1082,
      "step": 1376
    },
    {
      "epoch": 0.0064771348203618165,
      "grad_norm": 2.7292237281799316,
      "learning_rate": 0.00019917300819447982,
      "loss": 0.2158,
      "step": 1377
    },
    {
      "epoch": 0.0064818386219742796,
      "grad_norm": 2.4711930751800537,
      "learning_rate": 0.00019917206521636633,
      "loss": 0.1978,
      "step": 1378
    },
    {
      "epoch": 0.006486542423586743,
      "grad_norm": 0.7609228491783142,
      "learning_rate": 0.00019917112223825285,
      "loss": 0.0473,
      "step": 1379
    },
    {
      "epoch": 0.006491246225199206,
      "grad_norm": 0.9524661302566528,
      "learning_rate": 0.0001991701792601394,
      "loss": 0.0895,
      "step": 1380
    },
    {
      "epoch": 0.0064959500268116695,
      "grad_norm": 2.503368854522705,
      "learning_rate": 0.00019916923628202592,
      "loss": 0.3195,
      "step": 1381
    },
    {
      "epoch": 0.006500653828424133,
      "grad_norm": 0.4589749574661255,
      "learning_rate": 0.00019916829330391244,
      "loss": 0.0277,
      "step": 1382
    },
    {
      "epoch": 0.006505357630036596,
      "grad_norm": 2.4975218772888184,
      "learning_rate": 0.00019916735032579893,
      "loss": 0.2668,
      "step": 1383
    },
    {
      "epoch": 0.006510061431649059,
      "grad_norm": 5.081094741821289,
      "learning_rate": 0.00019916640734768547,
      "loss": 0.9388,
      "step": 1384
    },
    {
      "epoch": 0.006514765233261522,
      "grad_norm": 2.3634145259857178,
      "learning_rate": 0.000199165464369572,
      "loss": 0.3255,
      "step": 1385
    },
    {
      "epoch": 0.006519469034873985,
      "grad_norm": 5.837757587432861,
      "learning_rate": 0.0001991645213914585,
      "loss": 1.0654,
      "step": 1386
    },
    {
      "epoch": 0.006524172836486449,
      "grad_norm": 2.6881825923919678,
      "learning_rate": 0.00019916357841334503,
      "loss": 0.395,
      "step": 1387
    },
    {
      "epoch": 0.006528876638098912,
      "grad_norm": 4.49833345413208,
      "learning_rate": 0.00019916263543523155,
      "loss": 0.7888,
      "step": 1388
    },
    {
      "epoch": 0.006533580439711375,
      "grad_norm": 0.671392023563385,
      "learning_rate": 0.0001991616924571181,
      "loss": 0.0617,
      "step": 1389
    },
    {
      "epoch": 0.006538284241323838,
      "grad_norm": 0.2141418755054474,
      "learning_rate": 0.0001991607494790046,
      "loss": 0.0087,
      "step": 1390
    },
    {
      "epoch": 0.006542988042936301,
      "grad_norm": 4.5547637939453125,
      "learning_rate": 0.00019915980650089113,
      "loss": 0.5384,
      "step": 1391
    },
    {
      "epoch": 0.006547691844548765,
      "grad_norm": 4.375422954559326,
      "learning_rate": 0.00019915886352277765,
      "loss": 0.4163,
      "step": 1392
    },
    {
      "epoch": 0.006552395646161228,
      "grad_norm": 2.95135498046875,
      "learning_rate": 0.00019915792054466417,
      "loss": 0.2635,
      "step": 1393
    },
    {
      "epoch": 0.006557099447773691,
      "grad_norm": 2.2018022537231445,
      "learning_rate": 0.0001991569775665507,
      "loss": 0.4005,
      "step": 1394
    },
    {
      "epoch": 0.006561803249386154,
      "grad_norm": 0.7557845711708069,
      "learning_rate": 0.0001991560345884372,
      "loss": 0.1261,
      "step": 1395
    },
    {
      "epoch": 0.006566507050998617,
      "grad_norm": 1.8487602472305298,
      "learning_rate": 0.00019915509161032372,
      "loss": 0.5149,
      "step": 1396
    },
    {
      "epoch": 0.00657121085261108,
      "grad_norm": 2.1005938053131104,
      "learning_rate": 0.00019915414863221024,
      "loss": 0.3672,
      "step": 1397
    },
    {
      "epoch": 0.006575914654223544,
      "grad_norm": 2.2446391582489014,
      "learning_rate": 0.0001991532056540968,
      "loss": 0.7652,
      "step": 1398
    },
    {
      "epoch": 0.006580618455836007,
      "grad_norm": 2.5361857414245605,
      "learning_rate": 0.0001991522626759833,
      "loss": 0.3595,
      "step": 1399
    },
    {
      "epoch": 0.00658532225744847,
      "grad_norm": 1.6888619661331177,
      "learning_rate": 0.00019915131969786983,
      "loss": 0.3998,
      "step": 1400
    },
    {
      "epoch": 0.006590026059060933,
      "grad_norm": 3.044670581817627,
      "learning_rate": 0.00019915037671975634,
      "loss": 0.3477,
      "step": 1401
    },
    {
      "epoch": 0.006594729860673396,
      "grad_norm": 4.274088382720947,
      "learning_rate": 0.0001991494337416429,
      "loss": 0.6977,
      "step": 1402
    },
    {
      "epoch": 0.006599433662285859,
      "grad_norm": 2.7150449752807617,
      "learning_rate": 0.00019914849076352938,
      "loss": 0.392,
      "step": 1403
    },
    {
      "epoch": 0.006604137463898323,
      "grad_norm": 1.520392656326294,
      "learning_rate": 0.0001991475477854159,
      "loss": 0.2205,
      "step": 1404
    },
    {
      "epoch": 0.006608841265510786,
      "grad_norm": 2.037027597427368,
      "learning_rate": 0.00019914660480730242,
      "loss": 0.6189,
      "step": 1405
    },
    {
      "epoch": 0.006613545067123249,
      "grad_norm": 2.222280979156494,
      "learning_rate": 0.00019914566182918894,
      "loss": 0.3542,
      "step": 1406
    },
    {
      "epoch": 0.006618248868735712,
      "grad_norm": 3.267590284347534,
      "learning_rate": 0.00019914471885107548,
      "loss": 0.4173,
      "step": 1407
    },
    {
      "epoch": 0.006622952670348175,
      "grad_norm": 2.785785675048828,
      "learning_rate": 0.000199143775872962,
      "loss": 0.2641,
      "step": 1408
    },
    {
      "epoch": 0.006627656471960639,
      "grad_norm": 12.447946548461914,
      "learning_rate": 0.00019914283289484852,
      "loss": 0.2517,
      "step": 1409
    },
    {
      "epoch": 0.006632360273573102,
      "grad_norm": 3.600961685180664,
      "learning_rate": 0.00019914188991673504,
      "loss": 0.3981,
      "step": 1410
    },
    {
      "epoch": 0.006637064075185565,
      "grad_norm": 2.8569252490997314,
      "learning_rate": 0.00019914094693862156,
      "loss": 0.5635,
      "step": 1411
    },
    {
      "epoch": 0.006641767876798028,
      "grad_norm": 3.2600643634796143,
      "learning_rate": 0.0001991400039605081,
      "loss": 0.7017,
      "step": 1412
    },
    {
      "epoch": 0.006646471678410491,
      "grad_norm": 1.5967565774917603,
      "learning_rate": 0.00019913906098239462,
      "loss": 0.211,
      "step": 1413
    },
    {
      "epoch": 0.006651175480022954,
      "grad_norm": 2.103419542312622,
      "learning_rate": 0.00019913811800428111,
      "loss": 0.2545,
      "step": 1414
    },
    {
      "epoch": 0.006655879281635418,
      "grad_norm": 1.984075665473938,
      "learning_rate": 0.00019913717502616763,
      "loss": 0.2367,
      "step": 1415
    },
    {
      "epoch": 0.006660583083247881,
      "grad_norm": 2.8564300537109375,
      "learning_rate": 0.00019913623204805418,
      "loss": 0.3994,
      "step": 1416
    },
    {
      "epoch": 0.006665286884860344,
      "grad_norm": 1.5458862781524658,
      "learning_rate": 0.0001991352890699407,
      "loss": 0.3583,
      "step": 1417
    },
    {
      "epoch": 0.006669990686472807,
      "grad_norm": 1.397250771522522,
      "learning_rate": 0.00019913434609182722,
      "loss": 0.2167,
      "step": 1418
    },
    {
      "epoch": 0.00667469448808527,
      "grad_norm": 2.3576314449310303,
      "learning_rate": 0.00019913340311371373,
      "loss": 0.3804,
      "step": 1419
    },
    {
      "epoch": 0.006679398289697734,
      "grad_norm": 1.610963225364685,
      "learning_rate": 0.00019913246013560025,
      "loss": 0.1914,
      "step": 1420
    },
    {
      "epoch": 0.006684102091310197,
      "grad_norm": 0.7774103283882141,
      "learning_rate": 0.0001991315171574868,
      "loss": 0.0732,
      "step": 1421
    },
    {
      "epoch": 0.00668880589292266,
      "grad_norm": 13.439309120178223,
      "learning_rate": 0.00019913057417937332,
      "loss": 0.5524,
      "step": 1422
    },
    {
      "epoch": 0.006693509694535123,
      "grad_norm": 0.961188554763794,
      "learning_rate": 0.00019912963120125984,
      "loss": 0.1207,
      "step": 1423
    },
    {
      "epoch": 0.006698213496147586,
      "grad_norm": 1.2676563262939453,
      "learning_rate": 0.00019912868822314635,
      "loss": 0.1904,
      "step": 1424
    },
    {
      "epoch": 0.006702917297760049,
      "grad_norm": 1.8618130683898926,
      "learning_rate": 0.00019912774524503287,
      "loss": 0.194,
      "step": 1425
    },
    {
      "epoch": 0.006707621099372513,
      "grad_norm": 1.6804386377334595,
      "learning_rate": 0.0001991268022669194,
      "loss": 0.1317,
      "step": 1426
    },
    {
      "epoch": 0.006712324900984976,
      "grad_norm": 2.542173385620117,
      "learning_rate": 0.0001991258592888059,
      "loss": 0.3242,
      "step": 1427
    },
    {
      "epoch": 0.006717028702597439,
      "grad_norm": 1.5305466651916504,
      "learning_rate": 0.00019912491631069243,
      "loss": 0.1365,
      "step": 1428
    },
    {
      "epoch": 0.006721732504209902,
      "grad_norm": 0.3391966223716736,
      "learning_rate": 0.00019912397333257895,
      "loss": 0.0216,
      "step": 1429
    },
    {
      "epoch": 0.006726436305822365,
      "grad_norm": 2.9247663021087646,
      "learning_rate": 0.0001991230303544655,
      "loss": 0.5385,
      "step": 1430
    },
    {
      "epoch": 0.0067311401074348284,
      "grad_norm": 3.7968170642852783,
      "learning_rate": 0.000199122087376352,
      "loss": 0.6024,
      "step": 1431
    },
    {
      "epoch": 0.006735843909047292,
      "grad_norm": 2.049414873123169,
      "learning_rate": 0.00019912114439823853,
      "loss": 0.1616,
      "step": 1432
    },
    {
      "epoch": 0.006740547710659755,
      "grad_norm": 3.8054070472717285,
      "learning_rate": 0.00019912020142012505,
      "loss": 0.4561,
      "step": 1433
    },
    {
      "epoch": 0.006745251512272218,
      "grad_norm": 4.383414268493652,
      "learning_rate": 0.00019911925844201157,
      "loss": 0.507,
      "step": 1434
    },
    {
      "epoch": 0.0067499553138846815,
      "grad_norm": 2.059438467025757,
      "learning_rate": 0.0001991183154638981,
      "loss": 0.4461,
      "step": 1435
    },
    {
      "epoch": 0.0067546591154971445,
      "grad_norm": 2.832028388977051,
      "learning_rate": 0.0001991173724857846,
      "loss": 0.344,
      "step": 1436
    },
    {
      "epoch": 0.006759362917109608,
      "grad_norm": 1.315283179283142,
      "learning_rate": 0.00019911642950767112,
      "loss": 0.1053,
      "step": 1437
    },
    {
      "epoch": 0.0067640667187220714,
      "grad_norm": 1.0429794788360596,
      "learning_rate": 0.00019911548652955764,
      "loss": 0.0673,
      "step": 1438
    },
    {
      "epoch": 0.0067687705203345345,
      "grad_norm": 10.740277290344238,
      "learning_rate": 0.0001991145435514442,
      "loss": 0.4821,
      "step": 1439
    },
    {
      "epoch": 0.0067734743219469975,
      "grad_norm": 2.0408830642700195,
      "learning_rate": 0.0001991136005733307,
      "loss": 0.1478,
      "step": 1440
    },
    {
      "epoch": 0.0067781781235594606,
      "grad_norm": 0.5398014187812805,
      "learning_rate": 0.00019911265759521723,
      "loss": 0.0329,
      "step": 1441
    },
    {
      "epoch": 0.006782881925171924,
      "grad_norm": 1.07991361618042,
      "learning_rate": 0.00019911171461710374,
      "loss": 0.1796,
      "step": 1442
    },
    {
      "epoch": 0.0067875857267843875,
      "grad_norm": 4.16876220703125,
      "learning_rate": 0.0001991107716389903,
      "loss": 0.3654,
      "step": 1443
    },
    {
      "epoch": 0.0067922895283968505,
      "grad_norm": 1.696366548538208,
      "learning_rate": 0.0001991098286608768,
      "loss": 0.08,
      "step": 1444
    },
    {
      "epoch": 0.006796993330009314,
      "grad_norm": 3.2078402042388916,
      "learning_rate": 0.0001991088856827633,
      "loss": 0.2976,
      "step": 1445
    },
    {
      "epoch": 0.006801697131621777,
      "grad_norm": 2.092254877090454,
      "learning_rate": 0.00019910794270464982,
      "loss": 0.2181,
      "step": 1446
    },
    {
      "epoch": 0.00680640093323424,
      "grad_norm": 0.9521049857139587,
      "learning_rate": 0.00019910699972653634,
      "loss": 0.0665,
      "step": 1447
    },
    {
      "epoch": 0.006811104734846703,
      "grad_norm": 2.443721294403076,
      "learning_rate": 0.00019910605674842288,
      "loss": 0.1891,
      "step": 1448
    },
    {
      "epoch": 0.006815808536459167,
      "grad_norm": 0.6312785744667053,
      "learning_rate": 0.0001991051137703094,
      "loss": 0.0265,
      "step": 1449
    },
    {
      "epoch": 0.00682051233807163,
      "grad_norm": 0.8142788410186768,
      "learning_rate": 0.00019910417079219592,
      "loss": 0.0296,
      "step": 1450
    },
    {
      "epoch": 0.006825216139684093,
      "grad_norm": 7.120420455932617,
      "learning_rate": 0.00019910322781408244,
      "loss": 1.0179,
      "step": 1451
    },
    {
      "epoch": 0.006829919941296556,
      "grad_norm": 8.266618728637695,
      "learning_rate": 0.00019910228483596899,
      "loss": 0.6852,
      "step": 1452
    },
    {
      "epoch": 0.006834623742909019,
      "grad_norm": 4.806761264801025,
      "learning_rate": 0.0001991013418578555,
      "loss": 0.2074,
      "step": 1453
    },
    {
      "epoch": 0.006839327544521483,
      "grad_norm": 5.498729228973389,
      "learning_rate": 0.00019910039887974202,
      "loss": 0.5235,
      "step": 1454
    },
    {
      "epoch": 0.006844031346133946,
      "grad_norm": 2.038437843322754,
      "learning_rate": 0.00019909945590162854,
      "loss": 0.1448,
      "step": 1455
    },
    {
      "epoch": 0.006848735147746409,
      "grad_norm": 10.403788566589355,
      "learning_rate": 0.00019909851292351503,
      "loss": 0.2314,
      "step": 1456
    },
    {
      "epoch": 0.006853438949358872,
      "grad_norm": 8.04789924621582,
      "learning_rate": 0.00019909756994540158,
      "loss": 1.1807,
      "step": 1457
    },
    {
      "epoch": 0.006858142750971335,
      "grad_norm": 2.0530431270599365,
      "learning_rate": 0.0001990966269672881,
      "loss": 0.0919,
      "step": 1458
    },
    {
      "epoch": 0.006862846552583798,
      "grad_norm": 2.5555202960968018,
      "learning_rate": 0.00019909568398917462,
      "loss": 0.1374,
      "step": 1459
    },
    {
      "epoch": 0.006867550354196262,
      "grad_norm": 5.663936614990234,
      "learning_rate": 0.00019909474101106113,
      "loss": 0.961,
      "step": 1460
    },
    {
      "epoch": 0.006872254155808725,
      "grad_norm": 0.42344313859939575,
      "learning_rate": 0.00019909379803294765,
      "loss": 0.0156,
      "step": 1461
    },
    {
      "epoch": 0.006876957957421188,
      "grad_norm": 5.350970268249512,
      "learning_rate": 0.0001990928550548342,
      "loss": 0.4753,
      "step": 1462
    },
    {
      "epoch": 0.006881661759033651,
      "grad_norm": 4.2557268142700195,
      "learning_rate": 0.00019909191207672072,
      "loss": 0.5099,
      "step": 1463
    },
    {
      "epoch": 0.006886365560646114,
      "grad_norm": 2.2290570735931396,
      "learning_rate": 0.00019909096909860724,
      "loss": 0.1745,
      "step": 1464
    },
    {
      "epoch": 0.006891069362258577,
      "grad_norm": 2.7260773181915283,
      "learning_rate": 0.00019909002612049375,
      "loss": 0.2966,
      "step": 1465
    },
    {
      "epoch": 0.006895773163871041,
      "grad_norm": 2.1883420944213867,
      "learning_rate": 0.00019908908314238027,
      "loss": 0.149,
      "step": 1466
    },
    {
      "epoch": 0.006900476965483504,
      "grad_norm": 0.6175577640533447,
      "learning_rate": 0.0001990881401642668,
      "loss": 0.032,
      "step": 1467
    },
    {
      "epoch": 0.006905180767095967,
      "grad_norm": 0.544040858745575,
      "learning_rate": 0.0001990871971861533,
      "loss": 0.0286,
      "step": 1468
    },
    {
      "epoch": 0.00690988456870843,
      "grad_norm": 3.0093536376953125,
      "learning_rate": 0.00019908625420803983,
      "loss": 0.4559,
      "step": 1469
    },
    {
      "epoch": 0.006914588370320893,
      "grad_norm": 1.665278673171997,
      "learning_rate": 0.00019908531122992635,
      "loss": 0.0873,
      "step": 1470
    },
    {
      "epoch": 0.006919292171933357,
      "grad_norm": 5.53358268737793,
      "learning_rate": 0.0001990843682518129,
      "loss": 0.5145,
      "step": 1471
    },
    {
      "epoch": 0.00692399597354582,
      "grad_norm": 2.2321808338165283,
      "learning_rate": 0.0001990834252736994,
      "loss": 0.4545,
      "step": 1472
    },
    {
      "epoch": 0.006928699775158283,
      "grad_norm": 4.483903884887695,
      "learning_rate": 0.00019908248229558593,
      "loss": 0.9831,
      "step": 1473
    },
    {
      "epoch": 0.006933403576770746,
      "grad_norm": 2.4293270111083984,
      "learning_rate": 0.00019908153931747245,
      "loss": 0.2008,
      "step": 1474
    },
    {
      "epoch": 0.006938107378383209,
      "grad_norm": 1.5354743003845215,
      "learning_rate": 0.000199080596339359,
      "loss": 0.1568,
      "step": 1475
    },
    {
      "epoch": 0.006942811179995672,
      "grad_norm": 2.4184625148773193,
      "learning_rate": 0.0001990796533612455,
      "loss": 0.0638,
      "step": 1476
    },
    {
      "epoch": 0.006947514981608136,
      "grad_norm": 4.437269687652588,
      "learning_rate": 0.000199078710383132,
      "loss": 0.4352,
      "step": 1477
    },
    {
      "epoch": 0.006952218783220599,
      "grad_norm": 2.7684755325317383,
      "learning_rate": 0.00019907776740501852,
      "loss": 0.2304,
      "step": 1478
    },
    {
      "epoch": 0.006956922584833062,
      "grad_norm": 2.8454320430755615,
      "learning_rate": 0.00019907682442690504,
      "loss": 0.3246,
      "step": 1479
    },
    {
      "epoch": 0.006961626386445525,
      "grad_norm": 2.215761184692383,
      "learning_rate": 0.0001990758814487916,
      "loss": 0.2598,
      "step": 1480
    },
    {
      "epoch": 0.006966330188057988,
      "grad_norm": 2.214871406555176,
      "learning_rate": 0.0001990749384706781,
      "loss": 0.1307,
      "step": 1481
    },
    {
      "epoch": 0.006971033989670451,
      "grad_norm": 1.1508687734603882,
      "learning_rate": 0.00019907399549256463,
      "loss": 0.1246,
      "step": 1482
    },
    {
      "epoch": 0.006975737791282915,
      "grad_norm": 2.698467254638672,
      "learning_rate": 0.00019907305251445114,
      "loss": 0.3449,
      "step": 1483
    },
    {
      "epoch": 0.006980441592895378,
      "grad_norm": 1.8462392091751099,
      "learning_rate": 0.0001990721095363377,
      "loss": 0.1848,
      "step": 1484
    },
    {
      "epoch": 0.006985145394507841,
      "grad_norm": 6.624638557434082,
      "learning_rate": 0.0001990711665582242,
      "loss": 0.3611,
      "step": 1485
    },
    {
      "epoch": 0.006989849196120304,
      "grad_norm": 1.5050383806228638,
      "learning_rate": 0.00019907022358011073,
      "loss": 0.1452,
      "step": 1486
    },
    {
      "epoch": 0.006994552997732767,
      "grad_norm": 2.9412803649902344,
      "learning_rate": 0.00019906928060199722,
      "loss": 0.2747,
      "step": 1487
    },
    {
      "epoch": 0.006999256799345231,
      "grad_norm": 1.803712248802185,
      "learning_rate": 0.00019906833762388374,
      "loss": 0.1882,
      "step": 1488
    },
    {
      "epoch": 0.007003960600957694,
      "grad_norm": 2.179441452026367,
      "learning_rate": 0.00019906739464577028,
      "loss": 0.1346,
      "step": 1489
    },
    {
      "epoch": 0.007008664402570157,
      "grad_norm": 4.041566848754883,
      "learning_rate": 0.0001990664516676568,
      "loss": 0.5996,
      "step": 1490
    },
    {
      "epoch": 0.00701336820418262,
      "grad_norm": 2.58475923538208,
      "learning_rate": 0.00019906550868954332,
      "loss": 0.2454,
      "step": 1491
    },
    {
      "epoch": 0.007018072005795083,
      "grad_norm": 1.6086567640304565,
      "learning_rate": 0.00019906456571142984,
      "loss": 0.061,
      "step": 1492
    },
    {
      "epoch": 0.007022775807407546,
      "grad_norm": 1.6899641752243042,
      "learning_rate": 0.00019906362273331639,
      "loss": 0.0899,
      "step": 1493
    },
    {
      "epoch": 0.00702747960902001,
      "grad_norm": 2.037184953689575,
      "learning_rate": 0.0001990626797552029,
      "loss": 0.1991,
      "step": 1494
    },
    {
      "epoch": 0.007032183410632473,
      "grad_norm": 1.201207160949707,
      "learning_rate": 0.00019906173677708942,
      "loss": 0.1137,
      "step": 1495
    },
    {
      "epoch": 0.007036887212244936,
      "grad_norm": 1.898914098739624,
      "learning_rate": 0.00019906079379897594,
      "loss": 0.1756,
      "step": 1496
    },
    {
      "epoch": 0.007041591013857399,
      "grad_norm": 3.409877300262451,
      "learning_rate": 0.00019905985082086246,
      "loss": 0.6678,
      "step": 1497
    },
    {
      "epoch": 0.0070462948154698625,
      "grad_norm": 3.231858491897583,
      "learning_rate": 0.00019905890784274898,
      "loss": 0.5474,
      "step": 1498
    },
    {
      "epoch": 0.0070509986170823255,
      "grad_norm": 5.482954502105713,
      "learning_rate": 0.0001990579648646355,
      "loss": 0.2262,
      "step": 1499
    },
    {
      "epoch": 0.007055702418694789,
      "grad_norm": 1.6555808782577515,
      "learning_rate": 0.00019905702188652202,
      "loss": 0.1211,
      "step": 1500
    },
    {
      "epoch": 0.0070604062203072524,
      "grad_norm": 2.4366302490234375,
      "learning_rate": 0.00019905607890840853,
      "loss": 0.2164,
      "step": 1501
    },
    {
      "epoch": 0.0070651100219197155,
      "grad_norm": 3.3971059322357178,
      "learning_rate": 0.00019905513593029508,
      "loss": 0.3128,
      "step": 1502
    },
    {
      "epoch": 0.0070698138235321785,
      "grad_norm": 3.0080647468566895,
      "learning_rate": 0.0001990541929521816,
      "loss": 0.2526,
      "step": 1503
    },
    {
      "epoch": 0.007074517625144642,
      "grad_norm": 2.450955867767334,
      "learning_rate": 0.00019905324997406812,
      "loss": 0.1475,
      "step": 1504
    },
    {
      "epoch": 0.0070792214267571055,
      "grad_norm": 5.756219387054443,
      "learning_rate": 0.00019905230699595464,
      "loss": 0.4208,
      "step": 1505
    },
    {
      "epoch": 0.0070839252283695685,
      "grad_norm": 4.120522975921631,
      "learning_rate": 0.00019905136401784115,
      "loss": 0.3369,
      "step": 1506
    },
    {
      "epoch": 0.0070886290299820316,
      "grad_norm": 2.4948503971099854,
      "learning_rate": 0.00019905042103972767,
      "loss": 0.1831,
      "step": 1507
    },
    {
      "epoch": 0.007093332831594495,
      "grad_norm": 2.53379487991333,
      "learning_rate": 0.0001990494780616142,
      "loss": 0.1221,
      "step": 1508
    },
    {
      "epoch": 0.007098036633206958,
      "grad_norm": 1.535423755645752,
      "learning_rate": 0.0001990485350835007,
      "loss": 0.1534,
      "step": 1509
    },
    {
      "epoch": 0.007102740434819421,
      "grad_norm": 0.9368466138839722,
      "learning_rate": 0.00019904759210538723,
      "loss": 0.0593,
      "step": 1510
    },
    {
      "epoch": 0.007107444236431885,
      "grad_norm": 5.832233428955078,
      "learning_rate": 0.00019904664912727377,
      "loss": 0.2394,
      "step": 1511
    },
    {
      "epoch": 0.007112148038044348,
      "grad_norm": 1.7118804454803467,
      "learning_rate": 0.0001990457061491603,
      "loss": 0.1549,
      "step": 1512
    },
    {
      "epoch": 0.007116851839656811,
      "grad_norm": 0.8583953976631165,
      "learning_rate": 0.0001990447631710468,
      "loss": 0.0562,
      "step": 1513
    },
    {
      "epoch": 0.007121555641269274,
      "grad_norm": 7.437216758728027,
      "learning_rate": 0.00019904382019293333,
      "loss": 0.5879,
      "step": 1514
    },
    {
      "epoch": 0.007126259442881737,
      "grad_norm": 3.0423264503479004,
      "learning_rate": 0.00019904287721481985,
      "loss": 0.3856,
      "step": 1515
    },
    {
      "epoch": 0.007130963244494201,
      "grad_norm": 3.520587921142578,
      "learning_rate": 0.0001990419342367064,
      "loss": 0.2589,
      "step": 1516
    },
    {
      "epoch": 0.007135667046106664,
      "grad_norm": 5.40837287902832,
      "learning_rate": 0.00019904099125859291,
      "loss": 0.6494,
      "step": 1517
    },
    {
      "epoch": 0.007140370847719127,
      "grad_norm": 3.3999195098876953,
      "learning_rate": 0.0001990400482804794,
      "loss": 0.3274,
      "step": 1518
    },
    {
      "epoch": 0.00714507464933159,
      "grad_norm": 9.082784652709961,
      "learning_rate": 0.00019903910530236592,
      "loss": 0.9494,
      "step": 1519
    },
    {
      "epoch": 0.007149778450944053,
      "grad_norm": 4.172074794769287,
      "learning_rate": 0.00019903816232425244,
      "loss": 0.4622,
      "step": 1520
    },
    {
      "epoch": 0.007154482252556516,
      "grad_norm": 4.414341449737549,
      "learning_rate": 0.000199037219346139,
      "loss": 0.277,
      "step": 1521
    },
    {
      "epoch": 0.00715918605416898,
      "grad_norm": 1.6569184064865112,
      "learning_rate": 0.0001990362763680255,
      "loss": 0.1599,
      "step": 1522
    },
    {
      "epoch": 0.007163889855781443,
      "grad_norm": 3.0851776599884033,
      "learning_rate": 0.00019903533338991203,
      "loss": 0.4116,
      "step": 1523
    },
    {
      "epoch": 0.007168593657393906,
      "grad_norm": 3.0310823917388916,
      "learning_rate": 0.00019903439041179854,
      "loss": 0.2694,
      "step": 1524
    },
    {
      "epoch": 0.007173297459006369,
      "grad_norm": 2.8986172676086426,
      "learning_rate": 0.0001990334474336851,
      "loss": 0.1286,
      "step": 1525
    },
    {
      "epoch": 0.007178001260618832,
      "grad_norm": 0.8381897807121277,
      "learning_rate": 0.0001990325044555716,
      "loss": 0.0751,
      "step": 1526
    },
    {
      "epoch": 0.007182705062231295,
      "grad_norm": 1.0870673656463623,
      "learning_rate": 0.00019903156147745813,
      "loss": 0.1325,
      "step": 1527
    },
    {
      "epoch": 0.007187408863843759,
      "grad_norm": 1.6816426515579224,
      "learning_rate": 0.00019903061849934465,
      "loss": 0.2098,
      "step": 1528
    },
    {
      "epoch": 0.007192112665456222,
      "grad_norm": 6.085613250732422,
      "learning_rate": 0.00019902967552123116,
      "loss": 0.9937,
      "step": 1529
    },
    {
      "epoch": 0.007196816467068685,
      "grad_norm": 0.9858291149139404,
      "learning_rate": 0.00019902873254311768,
      "loss": 0.1192,
      "step": 1530
    },
    {
      "epoch": 0.007201520268681148,
      "grad_norm": 4.775461673736572,
      "learning_rate": 0.0001990277895650042,
      "loss": 0.5346,
      "step": 1531
    },
    {
      "epoch": 0.007206224070293611,
      "grad_norm": 1.5665034055709839,
      "learning_rate": 0.00019902684658689072,
      "loss": 0.3857,
      "step": 1532
    },
    {
      "epoch": 0.007210927871906075,
      "grad_norm": 2.7294039726257324,
      "learning_rate": 0.00019902590360877724,
      "loss": 0.4847,
      "step": 1533
    },
    {
      "epoch": 0.007215631673518538,
      "grad_norm": 3.447383165359497,
      "learning_rate": 0.00019902496063066378,
      "loss": 0.5959,
      "step": 1534
    },
    {
      "epoch": 0.007220335475131001,
      "grad_norm": 3.1211118698120117,
      "learning_rate": 0.0001990240176525503,
      "loss": 0.3974,
      "step": 1535
    },
    {
      "epoch": 0.007225039276743464,
      "grad_norm": 1.7299609184265137,
      "learning_rate": 0.00019902307467443682,
      "loss": 0.1973,
      "step": 1536
    },
    {
      "epoch": 0.007229743078355927,
      "grad_norm": 5.461467266082764,
      "learning_rate": 0.00019902213169632334,
      "loss": 0.5661,
      "step": 1537
    },
    {
      "epoch": 0.00723444687996839,
      "grad_norm": 1.9634644985198975,
      "learning_rate": 0.00019902118871820986,
      "loss": 0.2772,
      "step": 1538
    },
    {
      "epoch": 0.007239150681580854,
      "grad_norm": 0.828944742679596,
      "learning_rate": 0.00019902024574009638,
      "loss": 0.0678,
      "step": 1539
    },
    {
      "epoch": 0.007243854483193317,
      "grad_norm": 0.9554455280303955,
      "learning_rate": 0.0001990193027619829,
      "loss": 0.122,
      "step": 1540
    },
    {
      "epoch": 0.00724855828480578,
      "grad_norm": 0.7532808780670166,
      "learning_rate": 0.00019901835978386942,
      "loss": 0.0742,
      "step": 1541
    },
    {
      "epoch": 0.007253262086418243,
      "grad_norm": 1.1933282613754272,
      "learning_rate": 0.00019901741680575593,
      "loss": 0.1734,
      "step": 1542
    },
    {
      "epoch": 0.007257965888030706,
      "grad_norm": 1.5679826736450195,
      "learning_rate": 0.00019901647382764248,
      "loss": 0.2128,
      "step": 1543
    },
    {
      "epoch": 0.007262669689643169,
      "grad_norm": 0.9316748380661011,
      "learning_rate": 0.000199015530849529,
      "loss": 0.0925,
      "step": 1544
    },
    {
      "epoch": 0.007267373491255633,
      "grad_norm": 1.129256248474121,
      "learning_rate": 0.00019901458787141552,
      "loss": 0.1711,
      "step": 1545
    },
    {
      "epoch": 0.007272077292868096,
      "grad_norm": 4.774512767791748,
      "learning_rate": 0.00019901364489330204,
      "loss": 1.1239,
      "step": 1546
    },
    {
      "epoch": 0.007276781094480559,
      "grad_norm": 1.0059623718261719,
      "learning_rate": 0.00019901270191518855,
      "loss": 0.1474,
      "step": 1547
    },
    {
      "epoch": 0.007281484896093022,
      "grad_norm": 0.8260228037834167,
      "learning_rate": 0.0001990117589370751,
      "loss": 0.0874,
      "step": 1548
    },
    {
      "epoch": 0.007286188697705485,
      "grad_norm": 2.1862740516662598,
      "learning_rate": 0.0001990108159589616,
      "loss": 0.5407,
      "step": 1549
    },
    {
      "epoch": 0.007290892499317949,
      "grad_norm": 3.150632381439209,
      "learning_rate": 0.0001990098729808481,
      "loss": 0.287,
      "step": 1550
    },
    {
      "epoch": 0.007295596300930412,
      "grad_norm": 0.4887999892234802,
      "learning_rate": 0.00019900893000273463,
      "loss": 0.0306,
      "step": 1551
    },
    {
      "epoch": 0.007300300102542875,
      "grad_norm": 3.4156172275543213,
      "learning_rate": 0.00019900798702462117,
      "loss": 0.0404,
      "step": 1552
    },
    {
      "epoch": 0.007305003904155338,
      "grad_norm": 4.2180633544921875,
      "learning_rate": 0.0001990070440465077,
      "loss": 0.7546,
      "step": 1553
    },
    {
      "epoch": 0.007309707705767801,
      "grad_norm": 3.632542848587036,
      "learning_rate": 0.0001990061010683942,
      "loss": 0.2721,
      "step": 1554
    },
    {
      "epoch": 0.007314411507380264,
      "grad_norm": 0.9155692458152771,
      "learning_rate": 0.00019900515809028073,
      "loss": 0.1148,
      "step": 1555
    },
    {
      "epoch": 0.007319115308992728,
      "grad_norm": 5.565780162811279,
      "learning_rate": 0.00019900421511216725,
      "loss": 0.7153,
      "step": 1556
    },
    {
      "epoch": 0.007323819110605191,
      "grad_norm": 0.49045392870903015,
      "learning_rate": 0.0001990032721340538,
      "loss": 0.0348,
      "step": 1557
    },
    {
      "epoch": 0.007328522912217654,
      "grad_norm": 0.49134618043899536,
      "learning_rate": 0.00019900232915594031,
      "loss": 0.0499,
      "step": 1558
    },
    {
      "epoch": 0.007333226713830117,
      "grad_norm": 1.9040557146072388,
      "learning_rate": 0.00019900138617782683,
      "loss": 0.2589,
      "step": 1559
    },
    {
      "epoch": 0.00733793051544258,
      "grad_norm": 2.281604766845703,
      "learning_rate": 0.00019900044319971335,
      "loss": 0.2648,
      "step": 1560
    },
    {
      "epoch": 0.0073426343170550435,
      "grad_norm": 11.491667747497559,
      "learning_rate": 0.00019899950022159987,
      "loss": 0.7595,
      "step": 1561
    },
    {
      "epoch": 0.007347338118667507,
      "grad_norm": 0.6182972192764282,
      "learning_rate": 0.0001989985572434864,
      "loss": 0.0699,
      "step": 1562
    },
    {
      "epoch": 0.00735204192027997,
      "grad_norm": 2.5922319889068604,
      "learning_rate": 0.0001989976142653729,
      "loss": 0.3437,
      "step": 1563
    },
    {
      "epoch": 0.0073567457218924335,
      "grad_norm": 5.138021945953369,
      "learning_rate": 0.00019899667128725943,
      "loss": 0.4237,
      "step": 1564
    },
    {
      "epoch": 0.0073614495235048965,
      "grad_norm": 4.2352824211120605,
      "learning_rate": 0.00019899572830914594,
      "loss": 0.3887,
      "step": 1565
    },
    {
      "epoch": 0.0073661533251173595,
      "grad_norm": 3.3455257415771484,
      "learning_rate": 0.0001989947853310325,
      "loss": 0.1876,
      "step": 1566
    },
    {
      "epoch": 0.0073708571267298234,
      "grad_norm": 1.4736741781234741,
      "learning_rate": 0.000198993842352919,
      "loss": 0.1583,
      "step": 1567
    },
    {
      "epoch": 0.0073755609283422865,
      "grad_norm": 3.180309295654297,
      "learning_rate": 0.00019899289937480553,
      "loss": 0.2636,
      "step": 1568
    },
    {
      "epoch": 0.0073802647299547495,
      "grad_norm": 1.2760204076766968,
      "learning_rate": 0.00019899195639669205,
      "loss": 0.111,
      "step": 1569
    },
    {
      "epoch": 0.0073849685315672126,
      "grad_norm": 1.256264328956604,
      "learning_rate": 0.00019899101341857856,
      "loss": 0.1044,
      "step": 1570
    },
    {
      "epoch": 0.007389672333179676,
      "grad_norm": 1.41753351688385,
      "learning_rate": 0.00019899007044046508,
      "loss": 0.1597,
      "step": 1571
    },
    {
      "epoch": 0.007394376134792139,
      "grad_norm": 3.686314582824707,
      "learning_rate": 0.0001989891274623516,
      "loss": 0.4671,
      "step": 1572
    },
    {
      "epoch": 0.0073990799364046025,
      "grad_norm": 3.383988380432129,
      "learning_rate": 0.00019898818448423812,
      "loss": 0.4775,
      "step": 1573
    },
    {
      "epoch": 0.007403783738017066,
      "grad_norm": 0.46275943517684937,
      "learning_rate": 0.00019898724150612464,
      "loss": 0.0526,
      "step": 1574
    },
    {
      "epoch": 0.007408487539629529,
      "grad_norm": 4.965036392211914,
      "learning_rate": 0.00019898629852801118,
      "loss": 0.8119,
      "step": 1575
    },
    {
      "epoch": 0.007413191341241992,
      "grad_norm": 2.9518661499023438,
      "learning_rate": 0.0001989853555498977,
      "loss": 0.244,
      "step": 1576
    },
    {
      "epoch": 0.007417895142854455,
      "grad_norm": 0.4855504333972931,
      "learning_rate": 0.00019898441257178422,
      "loss": 0.0397,
      "step": 1577
    },
    {
      "epoch": 0.007422598944466918,
      "grad_norm": 0.7756426930427551,
      "learning_rate": 0.00019898346959367074,
      "loss": 0.06,
      "step": 1578
    },
    {
      "epoch": 0.007427302746079382,
      "grad_norm": 0.4038020968437195,
      "learning_rate": 0.00019898252661555726,
      "loss": 0.0319,
      "step": 1579
    },
    {
      "epoch": 0.007432006547691845,
      "grad_norm": 0.7220945358276367,
      "learning_rate": 0.00019898158363744378,
      "loss": 0.0691,
      "step": 1580
    },
    {
      "epoch": 0.007436710349304308,
      "grad_norm": 2.4435341358184814,
      "learning_rate": 0.0001989806406593303,
      "loss": 0.281,
      "step": 1581
    },
    {
      "epoch": 0.007441414150916771,
      "grad_norm": 1.023919701576233,
      "learning_rate": 0.00019897969768121682,
      "loss": 0.0906,
      "step": 1582
    },
    {
      "epoch": 0.007446117952529234,
      "grad_norm": 2.085477590560913,
      "learning_rate": 0.00019897875470310333,
      "loss": 0.1973,
      "step": 1583
    },
    {
      "epoch": 0.007450821754141698,
      "grad_norm": 3.209566831588745,
      "learning_rate": 0.00019897781172498988,
      "loss": 0.4936,
      "step": 1584
    },
    {
      "epoch": 0.007455525555754161,
      "grad_norm": 2.4954171180725098,
      "learning_rate": 0.0001989768687468764,
      "loss": 0.2364,
      "step": 1585
    },
    {
      "epoch": 0.007460229357366624,
      "grad_norm": 0.36700478196144104,
      "learning_rate": 0.00019897592576876292,
      "loss": 0.0244,
      "step": 1586
    },
    {
      "epoch": 0.007464933158979087,
      "grad_norm": 0.356790691614151,
      "learning_rate": 0.00019897498279064944,
      "loss": 0.0318,
      "step": 1587
    },
    {
      "epoch": 0.00746963696059155,
      "grad_norm": 5.213973045349121,
      "learning_rate": 0.00019897403981253595,
      "loss": 0.6925,
      "step": 1588
    },
    {
      "epoch": 0.007474340762204013,
      "grad_norm": 0.7700883150100708,
      "learning_rate": 0.0001989730968344225,
      "loss": 0.0383,
      "step": 1589
    },
    {
      "epoch": 0.007479044563816477,
      "grad_norm": 3.2975306510925293,
      "learning_rate": 0.00019897215385630902,
      "loss": 0.2322,
      "step": 1590
    },
    {
      "epoch": 0.00748374836542894,
      "grad_norm": 3.026475191116333,
      "learning_rate": 0.00019897121087819554,
      "loss": 0.7011,
      "step": 1591
    },
    {
      "epoch": 0.007488452167041403,
      "grad_norm": 1.5428786277770996,
      "learning_rate": 0.00019897026790008203,
      "loss": 0.1208,
      "step": 1592
    },
    {
      "epoch": 0.007493155968653866,
      "grad_norm": 0.2287512868642807,
      "learning_rate": 0.00019896932492196857,
      "loss": 0.0177,
      "step": 1593
    },
    {
      "epoch": 0.007497859770266329,
      "grad_norm": 0.2510547637939453,
      "learning_rate": 0.0001989683819438551,
      "loss": 0.0136,
      "step": 1594
    },
    {
      "epoch": 0.007502563571878793,
      "grad_norm": 3.1322262287139893,
      "learning_rate": 0.0001989674389657416,
      "loss": 0.7879,
      "step": 1595
    },
    {
      "epoch": 0.007507267373491256,
      "grad_norm": 1.6190037727355957,
      "learning_rate": 0.00019896649598762813,
      "loss": 0.1125,
      "step": 1596
    },
    {
      "epoch": 0.007511971175103719,
      "grad_norm": 4.079925060272217,
      "learning_rate": 0.00019896555300951465,
      "loss": 0.342,
      "step": 1597
    },
    {
      "epoch": 0.007516674976716182,
      "grad_norm": 4.0910234451293945,
      "learning_rate": 0.0001989646100314012,
      "loss": 0.3805,
      "step": 1598
    },
    {
      "epoch": 0.007521378778328645,
      "grad_norm": 2.9319515228271484,
      "learning_rate": 0.00019896366705328771,
      "loss": 0.2639,
      "step": 1599
    },
    {
      "epoch": 0.007526082579941108,
      "grad_norm": 3.7148468494415283,
      "learning_rate": 0.00019896272407517423,
      "loss": 0.6517,
      "step": 1600
    },
    {
      "epoch": 0.007530786381553572,
      "grad_norm": 6.484891414642334,
      "learning_rate": 0.00019896178109706075,
      "loss": 0.3276,
      "step": 1601
    },
    {
      "epoch": 0.007535490183166035,
      "grad_norm": 0.9091915488243103,
      "learning_rate": 0.00019896083811894727,
      "loss": 0.0602,
      "step": 1602
    },
    {
      "epoch": 0.007540193984778498,
      "grad_norm": 6.144850254058838,
      "learning_rate": 0.0001989598951408338,
      "loss": 0.7151,
      "step": 1603
    },
    {
      "epoch": 0.007544897786390961,
      "grad_norm": 1.2686628103256226,
      "learning_rate": 0.0001989589521627203,
      "loss": 0.0988,
      "step": 1604
    },
    {
      "epoch": 0.007549601588003424,
      "grad_norm": 2.5975377559661865,
      "learning_rate": 0.00019895800918460683,
      "loss": 0.1934,
      "step": 1605
    },
    {
      "epoch": 0.007554305389615887,
      "grad_norm": 0.3707684874534607,
      "learning_rate": 0.00019895706620649334,
      "loss": 0.0421,
      "step": 1606
    },
    {
      "epoch": 0.007559009191228351,
      "grad_norm": 2.7485878467559814,
      "learning_rate": 0.0001989561232283799,
      "loss": 0.1658,
      "step": 1607
    },
    {
      "epoch": 0.007563712992840814,
      "grad_norm": 2.0539510250091553,
      "learning_rate": 0.0001989551802502664,
      "loss": 0.2,
      "step": 1608
    },
    {
      "epoch": 0.007568416794453277,
      "grad_norm": 5.670045852661133,
      "learning_rate": 0.00019895423727215293,
      "loss": 1.0534,
      "step": 1609
    },
    {
      "epoch": 0.00757312059606574,
      "grad_norm": 3.4583325386047363,
      "learning_rate": 0.00019895329429403945,
      "loss": 0.6441,
      "step": 1610
    },
    {
      "epoch": 0.007577824397678203,
      "grad_norm": 3.83882737159729,
      "learning_rate": 0.00019895235131592596,
      "loss": 0.6339,
      "step": 1611
    },
    {
      "epoch": 0.007582528199290667,
      "grad_norm": 0.4055306911468506,
      "learning_rate": 0.00019895140833781248,
      "loss": 0.0373,
      "step": 1612
    },
    {
      "epoch": 0.00758723200090313,
      "grad_norm": 5.343654155731201,
      "learning_rate": 0.000198950465359699,
      "loss": 0.8576,
      "step": 1613
    },
    {
      "epoch": 0.007591935802515593,
      "grad_norm": 2.480494976043701,
      "learning_rate": 0.00019894952238158552,
      "loss": 0.3965,
      "step": 1614
    },
    {
      "epoch": 0.007596639604128056,
      "grad_norm": 0.7539643049240112,
      "learning_rate": 0.00019894857940347204,
      "loss": 0.0496,
      "step": 1615
    },
    {
      "epoch": 0.007601343405740519,
      "grad_norm": 1.507466197013855,
      "learning_rate": 0.00019894763642535858,
      "loss": 0.1095,
      "step": 1616
    },
    {
      "epoch": 0.007606047207352982,
      "grad_norm": 0.6925960779190063,
      "learning_rate": 0.0001989466934472451,
      "loss": 0.0523,
      "step": 1617
    },
    {
      "epoch": 0.007610751008965446,
      "grad_norm": 0.04966451972723007,
      "learning_rate": 0.00019894575046913162,
      "loss": 0.0026,
      "step": 1618
    },
    {
      "epoch": 0.007615454810577909,
      "grad_norm": 3.504945993423462,
      "learning_rate": 0.00019894480749101814,
      "loss": 0.4126,
      "step": 1619
    },
    {
      "epoch": 0.007620158612190372,
      "grad_norm": 3.0644478797912598,
      "learning_rate": 0.00019894386451290466,
      "loss": 0.2618,
      "step": 1620
    },
    {
      "epoch": 0.007624862413802835,
      "grad_norm": 2.1780364513397217,
      "learning_rate": 0.0001989429215347912,
      "loss": 0.2506,
      "step": 1621
    },
    {
      "epoch": 0.007629566215415298,
      "grad_norm": 3.6390109062194824,
      "learning_rate": 0.00019894197855667772,
      "loss": 0.5778,
      "step": 1622
    },
    {
      "epoch": 0.0076342700170277614,
      "grad_norm": 2.33213472366333,
      "learning_rate": 0.00019894103557856422,
      "loss": 0.4107,
      "step": 1623
    },
    {
      "epoch": 0.007638973818640225,
      "grad_norm": 3.5405375957489014,
      "learning_rate": 0.00019894009260045073,
      "loss": 0.4024,
      "step": 1624
    },
    {
      "epoch": 0.007643677620252688,
      "grad_norm": 2.0179529190063477,
      "learning_rate": 0.00019893914962233728,
      "loss": 0.1398,
      "step": 1625
    },
    {
      "epoch": 0.007648381421865151,
      "grad_norm": 0.7846934795379639,
      "learning_rate": 0.0001989382066442238,
      "loss": 0.0571,
      "step": 1626
    },
    {
      "epoch": 0.0076530852234776145,
      "grad_norm": 2.125006675720215,
      "learning_rate": 0.00019893726366611032,
      "loss": 0.218,
      "step": 1627
    },
    {
      "epoch": 0.0076577890250900775,
      "grad_norm": 2.6115846633911133,
      "learning_rate": 0.00019893632068799684,
      "loss": 0.2592,
      "step": 1628
    },
    {
      "epoch": 0.007662492826702541,
      "grad_norm": 3.7855734825134277,
      "learning_rate": 0.00019893537770988335,
      "loss": 0.4821,
      "step": 1629
    },
    {
      "epoch": 0.0076671966283150044,
      "grad_norm": 0.678255021572113,
      "learning_rate": 0.0001989344347317699,
      "loss": 0.0421,
      "step": 1630
    },
    {
      "epoch": 0.0076719004299274675,
      "grad_norm": 6.648968696594238,
      "learning_rate": 0.00019893349175365642,
      "loss": 0.9579,
      "step": 1631
    },
    {
      "epoch": 0.0076766042315399305,
      "grad_norm": 0.771108865737915,
      "learning_rate": 0.00019893254877554294,
      "loss": 0.0626,
      "step": 1632
    },
    {
      "epoch": 0.0076813080331523936,
      "grad_norm": 0.732408344745636,
      "learning_rate": 0.00019893160579742946,
      "loss": 0.0492,
      "step": 1633
    },
    {
      "epoch": 0.007686011834764857,
      "grad_norm": 1.6897647380828857,
      "learning_rate": 0.00019893066281931597,
      "loss": 0.2315,
      "step": 1634
    },
    {
      "epoch": 0.0076907156363773205,
      "grad_norm": 1.9170438051223755,
      "learning_rate": 0.0001989297198412025,
      "loss": 0.3678,
      "step": 1635
    },
    {
      "epoch": 0.0076954194379897835,
      "grad_norm": 1.243452787399292,
      "learning_rate": 0.000198928776863089,
      "loss": 0.0804,
      "step": 1636
    },
    {
      "epoch": 0.007700123239602247,
      "grad_norm": 0.7616702914237976,
      "learning_rate": 0.00019892783388497553,
      "loss": 0.0624,
      "step": 1637
    },
    {
      "epoch": 0.00770482704121471,
      "grad_norm": 0.9347981810569763,
      "learning_rate": 0.00019892689090686205,
      "loss": 0.1247,
      "step": 1638
    },
    {
      "epoch": 0.007709530842827173,
      "grad_norm": 4.4129533767700195,
      "learning_rate": 0.0001989259479287486,
      "loss": 0.7318,
      "step": 1639
    },
    {
      "epoch": 0.007714234644439636,
      "grad_norm": 2.9866912364959717,
      "learning_rate": 0.00019892500495063511,
      "loss": 0.7071,
      "step": 1640
    },
    {
      "epoch": 0.0077189384460521,
      "grad_norm": 1.5822582244873047,
      "learning_rate": 0.00019892406197252163,
      "loss": 0.2728,
      "step": 1641
    },
    {
      "epoch": 0.007723642247664563,
      "grad_norm": 1.3037450313568115,
      "learning_rate": 0.00019892311899440815,
      "loss": 0.2277,
      "step": 1642
    },
    {
      "epoch": 0.007728346049277026,
      "grad_norm": 1.4067535400390625,
      "learning_rate": 0.00019892217601629467,
      "loss": 0.1149,
      "step": 1643
    },
    {
      "epoch": 0.007733049850889489,
      "grad_norm": 4.214066505432129,
      "learning_rate": 0.0001989212330381812,
      "loss": 0.4755,
      "step": 1644
    },
    {
      "epoch": 0.007737753652501952,
      "grad_norm": 3.6728708744049072,
      "learning_rate": 0.0001989202900600677,
      "loss": 0.2377,
      "step": 1645
    },
    {
      "epoch": 0.007742457454114416,
      "grad_norm": 2.3796355724334717,
      "learning_rate": 0.00019891934708195423,
      "loss": 0.423,
      "step": 1646
    },
    {
      "epoch": 0.007747161255726879,
      "grad_norm": 0.2395624816417694,
      "learning_rate": 0.00019891840410384074,
      "loss": 0.0141,
      "step": 1647
    },
    {
      "epoch": 0.007751865057339342,
      "grad_norm": 1.4714523553848267,
      "learning_rate": 0.0001989174611257273,
      "loss": 0.2453,
      "step": 1648
    },
    {
      "epoch": 0.007756568858951805,
      "grad_norm": 0.8269250392913818,
      "learning_rate": 0.0001989165181476138,
      "loss": 0.1137,
      "step": 1649
    },
    {
      "epoch": 0.007761272660564268,
      "grad_norm": 1.2023777961730957,
      "learning_rate": 0.00019891557516950033,
      "loss": 0.2054,
      "step": 1650
    },
    {
      "epoch": 0.007765976462176731,
      "grad_norm": 2.600313186645508,
      "learning_rate": 0.00019891463219138685,
      "loss": 0.4091,
      "step": 1651
    },
    {
      "epoch": 0.007770680263789195,
      "grad_norm": 2.513038158416748,
      "learning_rate": 0.0001989136892132734,
      "loss": 0.2294,
      "step": 1652
    },
    {
      "epoch": 0.007775384065401658,
      "grad_norm": 3.421490430831909,
      "learning_rate": 0.0001989127462351599,
      "loss": 0.3258,
      "step": 1653
    },
    {
      "epoch": 0.007780087867014121,
      "grad_norm": 0.863810122013092,
      "learning_rate": 0.0001989118032570464,
      "loss": 0.1154,
      "step": 1654
    },
    {
      "epoch": 0.007784791668626584,
      "grad_norm": 1.5918519496917725,
      "learning_rate": 0.00019891086027893292,
      "loss": 0.1617,
      "step": 1655
    },
    {
      "epoch": 0.007789495470239047,
      "grad_norm": 0.10716669261455536,
      "learning_rate": 0.00019890991730081944,
      "loss": 0.0053,
      "step": 1656
    },
    {
      "epoch": 0.00779419927185151,
      "grad_norm": 4.288917064666748,
      "learning_rate": 0.00019890897432270598,
      "loss": 0.8794,
      "step": 1657
    },
    {
      "epoch": 0.007798903073463974,
      "grad_norm": 1.0743263959884644,
      "learning_rate": 0.0001989080313445925,
      "loss": 0.153,
      "step": 1658
    },
    {
      "epoch": 0.007803606875076437,
      "grad_norm": 2.4675910472869873,
      "learning_rate": 0.00019890708836647902,
      "loss": 0.2249,
      "step": 1659
    },
    {
      "epoch": 0.0078083106766889,
      "grad_norm": 3.1599090099334717,
      "learning_rate": 0.00019890614538836554,
      "loss": 0.4823,
      "step": 1660
    },
    {
      "epoch": 0.007813014478301364,
      "grad_norm": 0.7109806537628174,
      "learning_rate": 0.0001989052024102521,
      "loss": 0.0633,
      "step": 1661
    },
    {
      "epoch": 0.007817718279913826,
      "grad_norm": 2.9848098754882812,
      "learning_rate": 0.0001989042594321386,
      "loss": 0.4197,
      "step": 1662
    },
    {
      "epoch": 0.00782242208152629,
      "grad_norm": 4.789748668670654,
      "learning_rate": 0.00019890331645402512,
      "loss": 0.7195,
      "step": 1663
    },
    {
      "epoch": 0.007827125883138752,
      "grad_norm": 1.5685973167419434,
      "learning_rate": 0.00019890237347591164,
      "loss": 0.2284,
      "step": 1664
    },
    {
      "epoch": 0.007831829684751216,
      "grad_norm": 3.1936209201812744,
      "learning_rate": 0.00019890143049779813,
      "loss": 0.5974,
      "step": 1665
    },
    {
      "epoch": 0.00783653348636368,
      "grad_norm": 2.839496374130249,
      "learning_rate": 0.00019890048751968468,
      "loss": 0.5918,
      "step": 1666
    },
    {
      "epoch": 0.007841237287976142,
      "grad_norm": 3.6090574264526367,
      "learning_rate": 0.0001988995445415712,
      "loss": 0.438,
      "step": 1667
    },
    {
      "epoch": 0.007845941089588606,
      "grad_norm": 1.4341397285461426,
      "learning_rate": 0.00019889860156345772,
      "loss": 0.158,
      "step": 1668
    },
    {
      "epoch": 0.007850644891201068,
      "grad_norm": 1.189231514930725,
      "learning_rate": 0.00019889765858534424,
      "loss": 0.1063,
      "step": 1669
    },
    {
      "epoch": 0.007855348692813532,
      "grad_norm": 3.1547510623931885,
      "learning_rate": 0.00019889671560723075,
      "loss": 0.3148,
      "step": 1670
    },
    {
      "epoch": 0.007860052494425994,
      "grad_norm": 1.2049331665039062,
      "learning_rate": 0.0001988957726291173,
      "loss": 0.1304,
      "step": 1671
    },
    {
      "epoch": 0.007864756296038458,
      "grad_norm": 2.099280595779419,
      "learning_rate": 0.00019889482965100382,
      "loss": 0.2932,
      "step": 1672
    },
    {
      "epoch": 0.007869460097650922,
      "grad_norm": 0.7474635243415833,
      "learning_rate": 0.00019889388667289034,
      "loss": 0.1172,
      "step": 1673
    },
    {
      "epoch": 0.007874163899263384,
      "grad_norm": 0.9308487772941589,
      "learning_rate": 0.00019889294369477686,
      "loss": 0.1557,
      "step": 1674
    },
    {
      "epoch": 0.007878867700875848,
      "grad_norm": 2.078902244567871,
      "learning_rate": 0.00019889200071666337,
      "loss": 0.3609,
      "step": 1675
    },
    {
      "epoch": 0.00788357150248831,
      "grad_norm": 2.3032777309417725,
      "learning_rate": 0.0001988910577385499,
      "loss": 0.4114,
      "step": 1676
    },
    {
      "epoch": 0.007888275304100774,
      "grad_norm": 2.156663656234741,
      "learning_rate": 0.0001988901147604364,
      "loss": 0.185,
      "step": 1677
    },
    {
      "epoch": 0.007892979105713238,
      "grad_norm": 3.2542736530303955,
      "learning_rate": 0.00019888917178232293,
      "loss": 0.4556,
      "step": 1678
    },
    {
      "epoch": 0.0078976829073257,
      "grad_norm": 0.9339601993560791,
      "learning_rate": 0.00019888822880420945,
      "loss": 0.0925,
      "step": 1679
    },
    {
      "epoch": 0.007902386708938164,
      "grad_norm": 3.3332226276397705,
      "learning_rate": 0.000198887285826096,
      "loss": 0.2175,
      "step": 1680
    },
    {
      "epoch": 0.007907090510550626,
      "grad_norm": 2.0969018936157227,
      "learning_rate": 0.00019888634284798251,
      "loss": 0.2834,
      "step": 1681
    },
    {
      "epoch": 0.00791179431216309,
      "grad_norm": 0.4259168803691864,
      "learning_rate": 0.00019888539986986903,
      "loss": 0.0431,
      "step": 1682
    },
    {
      "epoch": 0.007916498113775554,
      "grad_norm": 1.8309026956558228,
      "learning_rate": 0.00019888445689175555,
      "loss": 0.1828,
      "step": 1683
    },
    {
      "epoch": 0.007921201915388016,
      "grad_norm": 2.0512068271636963,
      "learning_rate": 0.0001988835139136421,
      "loss": 0.1809,
      "step": 1684
    },
    {
      "epoch": 0.00792590571700048,
      "grad_norm": 1.7498528957366943,
      "learning_rate": 0.0001988825709355286,
      "loss": 0.267,
      "step": 1685
    },
    {
      "epoch": 0.007930609518612942,
      "grad_norm": 0.5268799066543579,
      "learning_rate": 0.0001988816279574151,
      "loss": 0.0266,
      "step": 1686
    },
    {
      "epoch": 0.007935313320225406,
      "grad_norm": 7.294299602508545,
      "learning_rate": 0.00019888068497930163,
      "loss": 0.7067,
      "step": 1687
    },
    {
      "epoch": 0.007940017121837869,
      "grad_norm": 0.8969483971595764,
      "learning_rate": 0.00019887974200118814,
      "loss": 0.0637,
      "step": 1688
    },
    {
      "epoch": 0.007944720923450332,
      "grad_norm": 0.22193239629268646,
      "learning_rate": 0.0001988787990230747,
      "loss": 0.0109,
      "step": 1689
    },
    {
      "epoch": 0.007949424725062796,
      "grad_norm": 0.2120736539363861,
      "learning_rate": 0.0001988778560449612,
      "loss": 0.0086,
      "step": 1690
    },
    {
      "epoch": 0.007954128526675259,
      "grad_norm": 1.6925476789474487,
      "learning_rate": 0.00019887691306684773,
      "loss": 0.1156,
      "step": 1691
    },
    {
      "epoch": 0.007958832328287722,
      "grad_norm": 4.162353515625,
      "learning_rate": 0.00019887597008873425,
      "loss": 0.3222,
      "step": 1692
    },
    {
      "epoch": 0.007963536129900185,
      "grad_norm": 3.637390375137329,
      "learning_rate": 0.0001988750271106208,
      "loss": 0.6465,
      "step": 1693
    },
    {
      "epoch": 0.007968239931512648,
      "grad_norm": 2.20692777633667,
      "learning_rate": 0.0001988740841325073,
      "loss": 0.1445,
      "step": 1694
    },
    {
      "epoch": 0.007972943733125112,
      "grad_norm": 0.4566425681114197,
      "learning_rate": 0.00019887314115439383,
      "loss": 0.0142,
      "step": 1695
    },
    {
      "epoch": 0.007977647534737575,
      "grad_norm": 1.7467913627624512,
      "learning_rate": 0.00019887219817628032,
      "loss": 0.0957,
      "step": 1696
    },
    {
      "epoch": 0.007982351336350038,
      "grad_norm": 4.879810333251953,
      "learning_rate": 0.00019887125519816684,
      "loss": 0.1989,
      "step": 1697
    },
    {
      "epoch": 0.0079870551379625,
      "grad_norm": 12.817580223083496,
      "learning_rate": 0.00019887031222005338,
      "loss": 1.617,
      "step": 1698
    },
    {
      "epoch": 0.007991758939574965,
      "grad_norm": 7.328233242034912,
      "learning_rate": 0.0001988693692419399,
      "loss": 1.2878,
      "step": 1699
    },
    {
      "epoch": 0.007996462741187428,
      "grad_norm": 6.230271816253662,
      "learning_rate": 0.00019886842626382642,
      "loss": 0.2992,
      "step": 1700
    },
    {
      "epoch": 0.00800116654279989,
      "grad_norm": 4.32506799697876,
      "learning_rate": 0.00019886748328571294,
      "loss": 0.4707,
      "step": 1701
    },
    {
      "epoch": 0.008005870344412355,
      "grad_norm": 1.0705132484436035,
      "learning_rate": 0.0001988665403075995,
      "loss": 0.037,
      "step": 1702
    },
    {
      "epoch": 0.008010574146024817,
      "grad_norm": 1.6260892152786255,
      "learning_rate": 0.000198865597329486,
      "loss": 0.0728,
      "step": 1703
    },
    {
      "epoch": 0.00801527794763728,
      "grad_norm": 3.5518906116485596,
      "learning_rate": 0.00019886465435137252,
      "loss": 0.346,
      "step": 1704
    },
    {
      "epoch": 0.008019981749249743,
      "grad_norm": 13.115615844726562,
      "learning_rate": 0.00019886371137325904,
      "loss": 0.7335,
      "step": 1705
    },
    {
      "epoch": 0.008024685550862207,
      "grad_norm": 1.825149416923523,
      "learning_rate": 0.00019886276839514556,
      "loss": 0.0967,
      "step": 1706
    },
    {
      "epoch": 0.00802938935247467,
      "grad_norm": 3.1004045009613037,
      "learning_rate": 0.00019886182541703208,
      "loss": 1.0509,
      "step": 1707
    },
    {
      "epoch": 0.008034093154087133,
      "grad_norm": 3.6263041496276855,
      "learning_rate": 0.0001988608824389186,
      "loss": 0.5054,
      "step": 1708
    },
    {
      "epoch": 0.008038796955699597,
      "grad_norm": 2.4702374935150146,
      "learning_rate": 0.00019885993946080512,
      "loss": 0.3182,
      "step": 1709
    },
    {
      "epoch": 0.008043500757312059,
      "grad_norm": 1.8121405839920044,
      "learning_rate": 0.00019885899648269164,
      "loss": 0.1231,
      "step": 1710
    },
    {
      "epoch": 0.008048204558924523,
      "grad_norm": 0.5865593552589417,
      "learning_rate": 0.00019885805350457818,
      "loss": 0.0173,
      "step": 1711
    },
    {
      "epoch": 0.008052908360536987,
      "grad_norm": 4.211760520935059,
      "learning_rate": 0.0001988571105264647,
      "loss": 0.3743,
      "step": 1712
    },
    {
      "epoch": 0.008057612162149449,
      "grad_norm": 2.89089298248291,
      "learning_rate": 0.00019885616754835122,
      "loss": 0.1178,
      "step": 1713
    },
    {
      "epoch": 0.008062315963761913,
      "grad_norm": 2.590308904647827,
      "learning_rate": 0.00019885522457023774,
      "loss": 0.193,
      "step": 1714
    },
    {
      "epoch": 0.008067019765374375,
      "grad_norm": 3.5527424812316895,
      "learning_rate": 0.00019885428159212426,
      "loss": 0.4944,
      "step": 1715
    },
    {
      "epoch": 0.008071723566986839,
      "grad_norm": 5.170905113220215,
      "learning_rate": 0.00019885333861401077,
      "loss": 0.7539,
      "step": 1716
    },
    {
      "epoch": 0.008076427368599303,
      "grad_norm": 1.4687334299087524,
      "learning_rate": 0.0001988523956358973,
      "loss": 0.2122,
      "step": 1717
    },
    {
      "epoch": 0.008081131170211765,
      "grad_norm": 1.5147504806518555,
      "learning_rate": 0.0001988514526577838,
      "loss": 0.2141,
      "step": 1718
    },
    {
      "epoch": 0.008085834971824229,
      "grad_norm": 3.060941219329834,
      "learning_rate": 0.00019885050967967033,
      "loss": 0.3661,
      "step": 1719
    },
    {
      "epoch": 0.008090538773436691,
      "grad_norm": 5.323693752288818,
      "learning_rate": 0.00019884956670155685,
      "loss": 0.9233,
      "step": 1720
    },
    {
      "epoch": 0.008095242575049155,
      "grad_norm": 1.7966225147247314,
      "learning_rate": 0.0001988486237234434,
      "loss": 0.2104,
      "step": 1721
    },
    {
      "epoch": 0.008099946376661617,
      "grad_norm": 6.095632553100586,
      "learning_rate": 0.00019884768074532991,
      "loss": 0.5713,
      "step": 1722
    },
    {
      "epoch": 0.008104650178274081,
      "grad_norm": 1.972665786743164,
      "learning_rate": 0.00019884673776721643,
      "loss": 0.2674,
      "step": 1723
    },
    {
      "epoch": 0.008109353979886545,
      "grad_norm": 5.693216323852539,
      "learning_rate": 0.00019884579478910295,
      "loss": 1.0816,
      "step": 1724
    },
    {
      "epoch": 0.008114057781499007,
      "grad_norm": 1.3859658241271973,
      "learning_rate": 0.0001988448518109895,
      "loss": 0.3275,
      "step": 1725
    },
    {
      "epoch": 0.008118761583111471,
      "grad_norm": 1.2262957096099854,
      "learning_rate": 0.00019884390883287602,
      "loss": 0.0812,
      "step": 1726
    },
    {
      "epoch": 0.008123465384723933,
      "grad_norm": 1.9073294401168823,
      "learning_rate": 0.0001988429658547625,
      "loss": 0.1959,
      "step": 1727
    },
    {
      "epoch": 0.008128169186336397,
      "grad_norm": 1.0299091339111328,
      "learning_rate": 0.00019884202287664903,
      "loss": 0.1322,
      "step": 1728
    },
    {
      "epoch": 0.008132872987948861,
      "grad_norm": 0.5334911346435547,
      "learning_rate": 0.00019884107989853554,
      "loss": 0.0453,
      "step": 1729
    },
    {
      "epoch": 0.008137576789561323,
      "grad_norm": 0.6485097408294678,
      "learning_rate": 0.0001988401369204221,
      "loss": 0.0407,
      "step": 1730
    },
    {
      "epoch": 0.008142280591173787,
      "grad_norm": 2.0376908779144287,
      "learning_rate": 0.0001988391939423086,
      "loss": 0.1498,
      "step": 1731
    },
    {
      "epoch": 0.00814698439278625,
      "grad_norm": 1.7103208303451538,
      "learning_rate": 0.00019883825096419513,
      "loss": 0.1887,
      "step": 1732
    },
    {
      "epoch": 0.008151688194398713,
      "grad_norm": 2.309669256210327,
      "learning_rate": 0.00019883730798608165,
      "loss": 0.4539,
      "step": 1733
    },
    {
      "epoch": 0.008156391996011177,
      "grad_norm": 4.552093982696533,
      "learning_rate": 0.0001988363650079682,
      "loss": 0.832,
      "step": 1734
    },
    {
      "epoch": 0.00816109579762364,
      "grad_norm": 0.6083732843399048,
      "learning_rate": 0.0001988354220298547,
      "loss": 0.0761,
      "step": 1735
    },
    {
      "epoch": 0.008165799599236103,
      "grad_norm": 1.013610601425171,
      "learning_rate": 0.00019883447905174123,
      "loss": 0.0595,
      "step": 1736
    },
    {
      "epoch": 0.008170503400848565,
      "grad_norm": 0.6131076812744141,
      "learning_rate": 0.00019883353607362775,
      "loss": 0.0551,
      "step": 1737
    },
    {
      "epoch": 0.00817520720246103,
      "grad_norm": 1.440894603729248,
      "learning_rate": 0.00019883259309551424,
      "loss": 0.1237,
      "step": 1738
    },
    {
      "epoch": 0.008179911004073493,
      "grad_norm": 0.917576014995575,
      "learning_rate": 0.00019883165011740078,
      "loss": 0.0561,
      "step": 1739
    },
    {
      "epoch": 0.008184614805685955,
      "grad_norm": 2.5494139194488525,
      "learning_rate": 0.0001988307071392873,
      "loss": 0.4899,
      "step": 1740
    },
    {
      "epoch": 0.00818931860729842,
      "grad_norm": 0.3420131802558899,
      "learning_rate": 0.00019882976416117382,
      "loss": 0.0309,
      "step": 1741
    },
    {
      "epoch": 0.008194022408910881,
      "grad_norm": 3.015580177307129,
      "learning_rate": 0.00019882882118306034,
      "loss": 0.2784,
      "step": 1742
    },
    {
      "epoch": 0.008198726210523345,
      "grad_norm": 5.480629920959473,
      "learning_rate": 0.0001988278782049469,
      "loss": 0.7884,
      "step": 1743
    },
    {
      "epoch": 0.008203430012135807,
      "grad_norm": 1.2253788709640503,
      "learning_rate": 0.0001988269352268334,
      "loss": 0.0686,
      "step": 1744
    },
    {
      "epoch": 0.008208133813748271,
      "grad_norm": 5.944839000701904,
      "learning_rate": 0.00019882599224871992,
      "loss": 0.9709,
      "step": 1745
    },
    {
      "epoch": 0.008212837615360735,
      "grad_norm": 2.6527349948883057,
      "learning_rate": 0.00019882504927060644,
      "loss": 0.6035,
      "step": 1746
    },
    {
      "epoch": 0.008217541416973197,
      "grad_norm": 3.2975969314575195,
      "learning_rate": 0.00019882410629249296,
      "loss": 0.7424,
      "step": 1747
    },
    {
      "epoch": 0.008222245218585661,
      "grad_norm": 1.6520073413848877,
      "learning_rate": 0.00019882316331437948,
      "loss": 0.1186,
      "step": 1748
    },
    {
      "epoch": 0.008226949020198123,
      "grad_norm": 0.12562130391597748,
      "learning_rate": 0.000198822220336266,
      "loss": 0.0067,
      "step": 1749
    },
    {
      "epoch": 0.008231652821810587,
      "grad_norm": 4.259496212005615,
      "learning_rate": 0.00019882127735815252,
      "loss": 0.4526,
      "step": 1750
    },
    {
      "epoch": 0.008236356623423051,
      "grad_norm": 0.5274729132652283,
      "learning_rate": 0.00019882033438003904,
      "loss": 0.0558,
      "step": 1751
    },
    {
      "epoch": 0.008241060425035513,
      "grad_norm": 0.14568476378917694,
      "learning_rate": 0.00019881939140192558,
      "loss": 0.0081,
      "step": 1752
    },
    {
      "epoch": 0.008245764226647977,
      "grad_norm": 2.785494089126587,
      "learning_rate": 0.0001988184484238121,
      "loss": 0.3924,
      "step": 1753
    },
    {
      "epoch": 0.00825046802826044,
      "grad_norm": 2.2078826427459717,
      "learning_rate": 0.00019881750544569862,
      "loss": 0.3062,
      "step": 1754
    },
    {
      "epoch": 0.008255171829872903,
      "grad_norm": 5.594730377197266,
      "learning_rate": 0.00019881656246758514,
      "loss": 0.4712,
      "step": 1755
    },
    {
      "epoch": 0.008259875631485367,
      "grad_norm": 3.4765875339508057,
      "learning_rate": 0.00019881561948947166,
      "loss": 0.5057,
      "step": 1756
    },
    {
      "epoch": 0.00826457943309783,
      "grad_norm": 2.902120590209961,
      "learning_rate": 0.0001988146765113582,
      "loss": 0.125,
      "step": 1757
    },
    {
      "epoch": 0.008269283234710293,
      "grad_norm": 1.5012187957763672,
      "learning_rate": 0.0001988137335332447,
      "loss": 0.1104,
      "step": 1758
    },
    {
      "epoch": 0.008273987036322756,
      "grad_norm": 2.502786874771118,
      "learning_rate": 0.0001988127905551312,
      "loss": 0.3622,
      "step": 1759
    },
    {
      "epoch": 0.00827869083793522,
      "grad_norm": 2.233971118927002,
      "learning_rate": 0.00019881184757701773,
      "loss": 0.3395,
      "step": 1760
    },
    {
      "epoch": 0.008283394639547682,
      "grad_norm": 1.357305645942688,
      "learning_rate": 0.00019881090459890428,
      "loss": 0.1566,
      "step": 1761
    },
    {
      "epoch": 0.008288098441160146,
      "grad_norm": 2.2919273376464844,
      "learning_rate": 0.0001988099616207908,
      "loss": 0.2597,
      "step": 1762
    },
    {
      "epoch": 0.00829280224277261,
      "grad_norm": 1.3462700843811035,
      "learning_rate": 0.00019880901864267731,
      "loss": 0.0879,
      "step": 1763
    },
    {
      "epoch": 0.008297506044385072,
      "grad_norm": 3.0283260345458984,
      "learning_rate": 0.00019880807566456383,
      "loss": 0.4224,
      "step": 1764
    },
    {
      "epoch": 0.008302209845997536,
      "grad_norm": 3.2912442684173584,
      "learning_rate": 0.00019880713268645035,
      "loss": 0.2765,
      "step": 1765
    },
    {
      "epoch": 0.008306913647609998,
      "grad_norm": 2.70709490776062,
      "learning_rate": 0.0001988061897083369,
      "loss": 0.1863,
      "step": 1766
    },
    {
      "epoch": 0.008311617449222462,
      "grad_norm": 2.773153781890869,
      "learning_rate": 0.00019880524673022342,
      "loss": 0.2419,
      "step": 1767
    },
    {
      "epoch": 0.008316321250834926,
      "grad_norm": 3.441305637359619,
      "learning_rate": 0.00019880430375210993,
      "loss": 0.3819,
      "step": 1768
    },
    {
      "epoch": 0.008321025052447388,
      "grad_norm": 0.9709202647209167,
      "learning_rate": 0.00019880336077399643,
      "loss": 0.0947,
      "step": 1769
    },
    {
      "epoch": 0.008325728854059852,
      "grad_norm": 4.884029388427734,
      "learning_rate": 0.00019880241779588297,
      "loss": 0.5859,
      "step": 1770
    },
    {
      "epoch": 0.008330432655672314,
      "grad_norm": 1.8057152032852173,
      "learning_rate": 0.0001988014748177695,
      "loss": 0.0989,
      "step": 1771
    },
    {
      "epoch": 0.008335136457284778,
      "grad_norm": 2.7540230751037598,
      "learning_rate": 0.000198800531839656,
      "loss": 0.307,
      "step": 1772
    },
    {
      "epoch": 0.008339840258897242,
      "grad_norm": 0.6474680304527283,
      "learning_rate": 0.00019879958886154253,
      "loss": 0.0417,
      "step": 1773
    },
    {
      "epoch": 0.008344544060509704,
      "grad_norm": 2.6356563568115234,
      "learning_rate": 0.00019879864588342905,
      "loss": 0.2093,
      "step": 1774
    },
    {
      "epoch": 0.008349247862122168,
      "grad_norm": 7.9431562423706055,
      "learning_rate": 0.0001987977029053156,
      "loss": 1.4843,
      "step": 1775
    },
    {
      "epoch": 0.00835395166373463,
      "grad_norm": 5.493814468383789,
      "learning_rate": 0.0001987967599272021,
      "loss": 0.3712,
      "step": 1776
    },
    {
      "epoch": 0.008358655465347094,
      "grad_norm": 2.0980284214019775,
      "learning_rate": 0.00019879581694908863,
      "loss": 0.2392,
      "step": 1777
    },
    {
      "epoch": 0.008363359266959556,
      "grad_norm": 2.8935229778289795,
      "learning_rate": 0.00019879487397097515,
      "loss": 0.437,
      "step": 1778
    },
    {
      "epoch": 0.00836806306857202,
      "grad_norm": 0.3329949975013733,
      "learning_rate": 0.00019879393099286167,
      "loss": 0.0204,
      "step": 1779
    },
    {
      "epoch": 0.008372766870184484,
      "grad_norm": 2.8743011951446533,
      "learning_rate": 0.00019879298801474818,
      "loss": 0.4084,
      "step": 1780
    },
    {
      "epoch": 0.008377470671796946,
      "grad_norm": 2.479177236557007,
      "learning_rate": 0.0001987920450366347,
      "loss": 0.1539,
      "step": 1781
    },
    {
      "epoch": 0.00838217447340941,
      "grad_norm": 3.1404407024383545,
      "learning_rate": 0.00019879110205852122,
      "loss": 0.3276,
      "step": 1782
    },
    {
      "epoch": 0.008386878275021872,
      "grad_norm": 3.297215700149536,
      "learning_rate": 0.00019879015908040774,
      "loss": 0.5119,
      "step": 1783
    },
    {
      "epoch": 0.008391582076634336,
      "grad_norm": 2.5488293170928955,
      "learning_rate": 0.0001987892161022943,
      "loss": 0.2915,
      "step": 1784
    },
    {
      "epoch": 0.0083962858782468,
      "grad_norm": 3.910601854324341,
      "learning_rate": 0.0001987882731241808,
      "loss": 0.5845,
      "step": 1785
    },
    {
      "epoch": 0.008400989679859262,
      "grad_norm": 0.6222986578941345,
      "learning_rate": 0.00019878733014606732,
      "loss": 0.0512,
      "step": 1786
    },
    {
      "epoch": 0.008405693481471726,
      "grad_norm": 4.457002639770508,
      "learning_rate": 0.00019878638716795384,
      "loss": 0.8182,
      "step": 1787
    },
    {
      "epoch": 0.008410397283084188,
      "grad_norm": 2.4662699699401855,
      "learning_rate": 0.00019878544418984036,
      "loss": 0.2527,
      "step": 1788
    },
    {
      "epoch": 0.008415101084696652,
      "grad_norm": 1.4068259000778198,
      "learning_rate": 0.00019878450121172688,
      "loss": 0.1942,
      "step": 1789
    },
    {
      "epoch": 0.008419804886309116,
      "grad_norm": 1.9796483516693115,
      "learning_rate": 0.0001987835582336134,
      "loss": 0.186,
      "step": 1790
    },
    {
      "epoch": 0.008424508687921578,
      "grad_norm": 2.94681715965271,
      "learning_rate": 0.00019878261525549992,
      "loss": 0.4823,
      "step": 1791
    },
    {
      "epoch": 0.008429212489534042,
      "grad_norm": 1.4101861715316772,
      "learning_rate": 0.00019878167227738644,
      "loss": 0.4362,
      "step": 1792
    },
    {
      "epoch": 0.008433916291146504,
      "grad_norm": 4.285127639770508,
      "learning_rate": 0.00019878072929927298,
      "loss": 0.8415,
      "step": 1793
    },
    {
      "epoch": 0.008438620092758968,
      "grad_norm": 1.7421550750732422,
      "learning_rate": 0.0001987797863211595,
      "loss": 0.204,
      "step": 1794
    },
    {
      "epoch": 0.00844332389437143,
      "grad_norm": 2.1271445751190186,
      "learning_rate": 0.00019877884334304602,
      "loss": 0.2393,
      "step": 1795
    },
    {
      "epoch": 0.008448027695983894,
      "grad_norm": 1.2741549015045166,
      "learning_rate": 0.00019877790036493254,
      "loss": 0.2839,
      "step": 1796
    },
    {
      "epoch": 0.008452731497596358,
      "grad_norm": 2.3143069744110107,
      "learning_rate": 0.00019877695738681906,
      "loss": 0.1887,
      "step": 1797
    },
    {
      "epoch": 0.00845743529920882,
      "grad_norm": 0.31793898344039917,
      "learning_rate": 0.0001987760144087056,
      "loss": 0.023,
      "step": 1798
    },
    {
      "epoch": 0.008462139100821284,
      "grad_norm": 1.5100829601287842,
      "learning_rate": 0.00019877507143059212,
      "loss": 0.2382,
      "step": 1799
    },
    {
      "epoch": 0.008466842902433746,
      "grad_norm": 2.3873603343963623,
      "learning_rate": 0.0001987741284524786,
      "loss": 0.495,
      "step": 1800
    },
    {
      "epoch": 0.00847154670404621,
      "grad_norm": 3.8885533809661865,
      "learning_rate": 0.00019877318547436513,
      "loss": 0.6112,
      "step": 1801
    },
    {
      "epoch": 0.008476250505658674,
      "grad_norm": 1.6652178764343262,
      "learning_rate": 0.00019877224249625168,
      "loss": 0.2533,
      "step": 1802
    },
    {
      "epoch": 0.008480954307271136,
      "grad_norm": 1.679044246673584,
      "learning_rate": 0.0001987712995181382,
      "loss": 0.1536,
      "step": 1803
    },
    {
      "epoch": 0.0084856581088836,
      "grad_norm": 0.9853795170783997,
      "learning_rate": 0.00019877035654002471,
      "loss": 0.1173,
      "step": 1804
    },
    {
      "epoch": 0.008490361910496062,
      "grad_norm": 1.1754813194274902,
      "learning_rate": 0.00019876941356191123,
      "loss": 0.1441,
      "step": 1805
    },
    {
      "epoch": 0.008495065712108526,
      "grad_norm": 1.736059546470642,
      "learning_rate": 0.00019876847058379775,
      "loss": 0.1241,
      "step": 1806
    },
    {
      "epoch": 0.00849976951372099,
      "grad_norm": 3.7681376934051514,
      "learning_rate": 0.0001987675276056843,
      "loss": 0.5292,
      "step": 1807
    },
    {
      "epoch": 0.008504473315333452,
      "grad_norm": 3.8850905895233154,
      "learning_rate": 0.00019876658462757082,
      "loss": 0.2683,
      "step": 1808
    },
    {
      "epoch": 0.008509177116945916,
      "grad_norm": 2.7350704669952393,
      "learning_rate": 0.00019876564164945733,
      "loss": 0.366,
      "step": 1809
    },
    {
      "epoch": 0.008513880918558378,
      "grad_norm": 1.7076199054718018,
      "learning_rate": 0.00019876469867134385,
      "loss": 0.2156,
      "step": 1810
    },
    {
      "epoch": 0.008518584720170842,
      "grad_norm": 3.466487407684326,
      "learning_rate": 0.00019876375569323037,
      "loss": 0.4935,
      "step": 1811
    },
    {
      "epoch": 0.008523288521783304,
      "grad_norm": 0.4489178955554962,
      "learning_rate": 0.0001987628127151169,
      "loss": 0.0321,
      "step": 1812
    },
    {
      "epoch": 0.008527992323395768,
      "grad_norm": 4.997873306274414,
      "learning_rate": 0.0001987618697370034,
      "loss": 0.3385,
      "step": 1813
    },
    {
      "epoch": 0.008532696125008232,
      "grad_norm": 4.035560131072998,
      "learning_rate": 0.00019876092675888993,
      "loss": 1.2215,
      "step": 1814
    },
    {
      "epoch": 0.008537399926620694,
      "grad_norm": 4.69710111618042,
      "learning_rate": 0.00019875998378077645,
      "loss": 0.5332,
      "step": 1815
    },
    {
      "epoch": 0.008542103728233158,
      "grad_norm": 0.775684654712677,
      "learning_rate": 0.000198759040802663,
      "loss": 0.0664,
      "step": 1816
    },
    {
      "epoch": 0.00854680752984562,
      "grad_norm": 1.6455274820327759,
      "learning_rate": 0.0001987580978245495,
      "loss": 0.3061,
      "step": 1817
    },
    {
      "epoch": 0.008551511331458084,
      "grad_norm": 7.9589619636535645,
      "learning_rate": 0.00019875715484643603,
      "loss": 0.8355,
      "step": 1818
    },
    {
      "epoch": 0.008556215133070548,
      "grad_norm": 2.3311054706573486,
      "learning_rate": 0.00019875621186832255,
      "loss": 0.2529,
      "step": 1819
    },
    {
      "epoch": 0.00856091893468301,
      "grad_norm": 1.67664635181427,
      "learning_rate": 0.00019875526889020907,
      "loss": 0.1434,
      "step": 1820
    },
    {
      "epoch": 0.008565622736295474,
      "grad_norm": 0.9768384099006653,
      "learning_rate": 0.00019875432591209558,
      "loss": 0.07,
      "step": 1821
    },
    {
      "epoch": 0.008570326537907937,
      "grad_norm": 3.484919548034668,
      "learning_rate": 0.0001987533829339821,
      "loss": 0.4944,
      "step": 1822
    },
    {
      "epoch": 0.0085750303395204,
      "grad_norm": 2.474827527999878,
      "learning_rate": 0.00019875243995586862,
      "loss": 0.2585,
      "step": 1823
    },
    {
      "epoch": 0.008579734141132864,
      "grad_norm": 2.772810935974121,
      "learning_rate": 0.00019875149697775514,
      "loss": 0.2563,
      "step": 1824
    },
    {
      "epoch": 0.008584437942745327,
      "grad_norm": 1.2734748125076294,
      "learning_rate": 0.00019875055399964169,
      "loss": 0.0641,
      "step": 1825
    },
    {
      "epoch": 0.00858914174435779,
      "grad_norm": 4.447396278381348,
      "learning_rate": 0.0001987496110215282,
      "loss": 0.9493,
      "step": 1826
    },
    {
      "epoch": 0.008593845545970253,
      "grad_norm": 2.606912612915039,
      "learning_rate": 0.00019874866804341472,
      "loss": 0.2263,
      "step": 1827
    },
    {
      "epoch": 0.008598549347582717,
      "grad_norm": 2.780980110168457,
      "learning_rate": 0.00019874772506530124,
      "loss": 0.3228,
      "step": 1828
    },
    {
      "epoch": 0.008603253149195179,
      "grad_norm": 1.1897577047348022,
      "learning_rate": 0.00019874678208718776,
      "loss": 0.1741,
      "step": 1829
    },
    {
      "epoch": 0.008607956950807643,
      "grad_norm": 0.9192363619804382,
      "learning_rate": 0.0001987458391090743,
      "loss": 0.1184,
      "step": 1830
    },
    {
      "epoch": 0.008612660752420107,
      "grad_norm": 3.7028427124023438,
      "learning_rate": 0.0001987448961309608,
      "loss": 0.4026,
      "step": 1831
    },
    {
      "epoch": 0.008617364554032569,
      "grad_norm": 2.1218273639678955,
      "learning_rate": 0.00019874395315284732,
      "loss": 0.3264,
      "step": 1832
    },
    {
      "epoch": 0.008622068355645033,
      "grad_norm": 3.2539641857147217,
      "learning_rate": 0.00019874301017473384,
      "loss": 0.2958,
      "step": 1833
    },
    {
      "epoch": 0.008626772157257495,
      "grad_norm": 1.8512144088745117,
      "learning_rate": 0.00019874206719662038,
      "loss": 0.3518,
      "step": 1834
    },
    {
      "epoch": 0.008631475958869959,
      "grad_norm": 0.8585800528526306,
      "learning_rate": 0.0001987411242185069,
      "loss": 0.0609,
      "step": 1835
    },
    {
      "epoch": 0.008636179760482423,
      "grad_norm": 1.6091874837875366,
      "learning_rate": 0.00019874018124039342,
      "loss": 0.1885,
      "step": 1836
    },
    {
      "epoch": 0.008640883562094885,
      "grad_norm": 0.730455756187439,
      "learning_rate": 0.00019873923826227994,
      "loss": 0.07,
      "step": 1837
    },
    {
      "epoch": 0.008645587363707349,
      "grad_norm": 3.3033523559570312,
      "learning_rate": 0.00019873829528416646,
      "loss": 0.4987,
      "step": 1838
    },
    {
      "epoch": 0.00865029116531981,
      "grad_norm": 0.35030558705329895,
      "learning_rate": 0.000198737352306053,
      "loss": 0.0334,
      "step": 1839
    },
    {
      "epoch": 0.008654994966932275,
      "grad_norm": 0.8767347931861877,
      "learning_rate": 0.00019873640932793952,
      "loss": 0.0836,
      "step": 1840
    },
    {
      "epoch": 0.008659698768544739,
      "grad_norm": 5.04138708114624,
      "learning_rate": 0.00019873546634982604,
      "loss": 0.3467,
      "step": 1841
    },
    {
      "epoch": 0.0086644025701572,
      "grad_norm": 4.5780839920043945,
      "learning_rate": 0.00019873452337171256,
      "loss": 0.8299,
      "step": 1842
    },
    {
      "epoch": 0.008669106371769665,
      "grad_norm": 2.144926071166992,
      "learning_rate": 0.00019873358039359908,
      "loss": 0.1878,
      "step": 1843
    },
    {
      "epoch": 0.008673810173382127,
      "grad_norm": 0.630431056022644,
      "learning_rate": 0.0001987326374154856,
      "loss": 0.0384,
      "step": 1844
    },
    {
      "epoch": 0.00867851397499459,
      "grad_norm": 0.48944827914237976,
      "learning_rate": 0.0001987316944373721,
      "loss": 0.066,
      "step": 1845
    },
    {
      "epoch": 0.008683217776607053,
      "grad_norm": 2.3297955989837646,
      "learning_rate": 0.00019873075145925863,
      "loss": 0.4283,
      "step": 1846
    },
    {
      "epoch": 0.008687921578219517,
      "grad_norm": 3.566269636154175,
      "learning_rate": 0.00019872980848114515,
      "loss": 0.8877,
      "step": 1847
    },
    {
      "epoch": 0.00869262537983198,
      "grad_norm": 1.0753118991851807,
      "learning_rate": 0.0001987288655030317,
      "loss": 0.0821,
      "step": 1848
    },
    {
      "epoch": 0.008697329181444443,
      "grad_norm": 0.21582050621509552,
      "learning_rate": 0.00019872792252491822,
      "loss": 0.0123,
      "step": 1849
    },
    {
      "epoch": 0.008702032983056907,
      "grad_norm": 2.5685794353485107,
      "learning_rate": 0.00019872697954680473,
      "loss": 0.3016,
      "step": 1850
    },
    {
      "epoch": 0.008706736784669369,
      "grad_norm": 1.4145278930664062,
      "learning_rate": 0.00019872603656869125,
      "loss": 0.0949,
      "step": 1851
    },
    {
      "epoch": 0.008711440586281833,
      "grad_norm": 4.329471588134766,
      "learning_rate": 0.00019872509359057777,
      "loss": 0.39,
      "step": 1852
    },
    {
      "epoch": 0.008716144387894297,
      "grad_norm": 0.9755269885063171,
      "learning_rate": 0.0001987241506124643,
      "loss": 0.0655,
      "step": 1853
    },
    {
      "epoch": 0.008720848189506759,
      "grad_norm": 2.1531081199645996,
      "learning_rate": 0.0001987232076343508,
      "loss": 0.2572,
      "step": 1854
    },
    {
      "epoch": 0.008725551991119223,
      "grad_norm": 4.298008441925049,
      "learning_rate": 0.00019872226465623733,
      "loss": 0.3759,
      "step": 1855
    },
    {
      "epoch": 0.008730255792731685,
      "grad_norm": 2.6593596935272217,
      "learning_rate": 0.00019872132167812385,
      "loss": 0.3401,
      "step": 1856
    },
    {
      "epoch": 0.008734959594344149,
      "grad_norm": 5.736881732940674,
      "learning_rate": 0.0001987203787000104,
      "loss": 0.3256,
      "step": 1857
    },
    {
      "epoch": 0.008739663395956613,
      "grad_norm": 1.3134866952896118,
      "learning_rate": 0.0001987194357218969,
      "loss": 0.088,
      "step": 1858
    },
    {
      "epoch": 0.008744367197569075,
      "grad_norm": 3.1335909366607666,
      "learning_rate": 0.00019871849274378343,
      "loss": 0.1542,
      "step": 1859
    },
    {
      "epoch": 0.008749070999181539,
      "grad_norm": 2.9443211555480957,
      "learning_rate": 0.00019871754976566995,
      "loss": 0.2092,
      "step": 1860
    },
    {
      "epoch": 0.008753774800794001,
      "grad_norm": 1.8636113405227661,
      "learning_rate": 0.0001987166067875565,
      "loss": 0.1265,
      "step": 1861
    },
    {
      "epoch": 0.008758478602406465,
      "grad_norm": 0.3278704583644867,
      "learning_rate": 0.00019871566380944298,
      "loss": 0.0223,
      "step": 1862
    },
    {
      "epoch": 0.008763182404018927,
      "grad_norm": 1.1759040355682373,
      "learning_rate": 0.0001987147208313295,
      "loss": 0.0776,
      "step": 1863
    },
    {
      "epoch": 0.008767886205631391,
      "grad_norm": 4.13474702835083,
      "learning_rate": 0.00019871377785321602,
      "loss": 0.3523,
      "step": 1864
    },
    {
      "epoch": 0.008772590007243855,
      "grad_norm": 4.96852445602417,
      "learning_rate": 0.00019871283487510254,
      "loss": 0.7035,
      "step": 1865
    },
    {
      "epoch": 0.008777293808856317,
      "grad_norm": 0.8130399584770203,
      "learning_rate": 0.00019871189189698909,
      "loss": 0.0671,
      "step": 1866
    },
    {
      "epoch": 0.008781997610468781,
      "grad_norm": 6.868462562561035,
      "learning_rate": 0.0001987109489188756,
      "loss": 0.9575,
      "step": 1867
    },
    {
      "epoch": 0.008786701412081243,
      "grad_norm": 1.415086030960083,
      "learning_rate": 0.00019871000594076212,
      "loss": 0.1221,
      "step": 1868
    },
    {
      "epoch": 0.008791405213693707,
      "grad_norm": 3.7330708503723145,
      "learning_rate": 0.00019870906296264864,
      "loss": 0.5531,
      "step": 1869
    },
    {
      "epoch": 0.008796109015306171,
      "grad_norm": 4.896450042724609,
      "learning_rate": 0.0001987081199845352,
      "loss": 0.9149,
      "step": 1870
    },
    {
      "epoch": 0.008800812816918633,
      "grad_norm": 2.4116430282592773,
      "learning_rate": 0.0001987071770064217,
      "loss": 0.2323,
      "step": 1871
    },
    {
      "epoch": 0.008805516618531097,
      "grad_norm": 1.5911169052124023,
      "learning_rate": 0.00019870623402830823,
      "loss": 0.1025,
      "step": 1872
    },
    {
      "epoch": 0.00881022042014356,
      "grad_norm": 0.5551995038986206,
      "learning_rate": 0.00019870529105019474,
      "loss": 0.0313,
      "step": 1873
    },
    {
      "epoch": 0.008814924221756023,
      "grad_norm": 3.013516902923584,
      "learning_rate": 0.00019870434807208124,
      "loss": 0.1625,
      "step": 1874
    },
    {
      "epoch": 0.008819628023368487,
      "grad_norm": 1.2565016746520996,
      "learning_rate": 0.00019870340509396778,
      "loss": 0.0663,
      "step": 1875
    },
    {
      "epoch": 0.00882433182498095,
      "grad_norm": 1.408740520477295,
      "learning_rate": 0.0001987024621158543,
      "loss": 0.1114,
      "step": 1876
    },
    {
      "epoch": 0.008829035626593413,
      "grad_norm": 0.7909154295921326,
      "learning_rate": 0.00019870151913774082,
      "loss": 0.047,
      "step": 1877
    },
    {
      "epoch": 0.008833739428205875,
      "grad_norm": 0.10233913362026215,
      "learning_rate": 0.00019870057615962734,
      "loss": 0.0048,
      "step": 1878
    },
    {
      "epoch": 0.00883844322981834,
      "grad_norm": 1.3106118440628052,
      "learning_rate": 0.00019869963318151386,
      "loss": 0.0676,
      "step": 1879
    },
    {
      "epoch": 0.008843147031430802,
      "grad_norm": 3.2693209648132324,
      "learning_rate": 0.0001986986902034004,
      "loss": 0.6303,
      "step": 1880
    },
    {
      "epoch": 0.008847850833043265,
      "grad_norm": 2.4600324630737305,
      "learning_rate": 0.00019869774722528692,
      "loss": 0.2207,
      "step": 1881
    },
    {
      "epoch": 0.00885255463465573,
      "grad_norm": 2.638523817062378,
      "learning_rate": 0.00019869680424717344,
      "loss": 0.3915,
      "step": 1882
    },
    {
      "epoch": 0.008857258436268192,
      "grad_norm": 0.927291214466095,
      "learning_rate": 0.00019869586126905996,
      "loss": 0.0683,
      "step": 1883
    },
    {
      "epoch": 0.008861962237880655,
      "grad_norm": 4.647659778594971,
      "learning_rate": 0.00019869491829094648,
      "loss": 0.2251,
      "step": 1884
    },
    {
      "epoch": 0.008866666039493118,
      "grad_norm": 1.4481110572814941,
      "learning_rate": 0.000198693975312833,
      "loss": 0.081,
      "step": 1885
    },
    {
      "epoch": 0.008871369841105581,
      "grad_norm": 2.510619640350342,
      "learning_rate": 0.0001986930323347195,
      "loss": 0.1654,
      "step": 1886
    },
    {
      "epoch": 0.008876073642718045,
      "grad_norm": 0.910617470741272,
      "learning_rate": 0.00019869208935660603,
      "loss": 0.0357,
      "step": 1887
    },
    {
      "epoch": 0.008880777444330508,
      "grad_norm": 2.848397731781006,
      "learning_rate": 0.00019869114637849255,
      "loss": 0.4632,
      "step": 1888
    },
    {
      "epoch": 0.008885481245942971,
      "grad_norm": 1.0333857536315918,
      "learning_rate": 0.0001986902034003791,
      "loss": 0.07,
      "step": 1889
    },
    {
      "epoch": 0.008890185047555434,
      "grad_norm": 3.8487038612365723,
      "learning_rate": 0.00019868926042226562,
      "loss": 0.6178,
      "step": 1890
    },
    {
      "epoch": 0.008894888849167898,
      "grad_norm": 3.6419758796691895,
      "learning_rate": 0.00019868831744415213,
      "loss": 0.2936,
      "step": 1891
    },
    {
      "epoch": 0.008899592650780361,
      "grad_norm": 0.32086536288261414,
      "learning_rate": 0.00019868737446603865,
      "loss": 0.0172,
      "step": 1892
    },
    {
      "epoch": 0.008904296452392824,
      "grad_norm": 3.533799409866333,
      "learning_rate": 0.00019868643148792517,
      "loss": 0.53,
      "step": 1893
    },
    {
      "epoch": 0.008909000254005288,
      "grad_norm": 4.078021049499512,
      "learning_rate": 0.0001986854885098117,
      "loss": 0.4695,
      "step": 1894
    },
    {
      "epoch": 0.00891370405561775,
      "grad_norm": 1.6964675188064575,
      "learning_rate": 0.0001986845455316982,
      "loss": 0.089,
      "step": 1895
    },
    {
      "epoch": 0.008918407857230214,
      "grad_norm": 2.3005881309509277,
      "learning_rate": 0.00019868360255358473,
      "loss": 0.1033,
      "step": 1896
    },
    {
      "epoch": 0.008923111658842676,
      "grad_norm": 2.7952468395233154,
      "learning_rate": 0.00019868265957547125,
      "loss": 0.5413,
      "step": 1897
    },
    {
      "epoch": 0.00892781546045514,
      "grad_norm": 1.4163062572479248,
      "learning_rate": 0.0001986817165973578,
      "loss": 0.1479,
      "step": 1898
    },
    {
      "epoch": 0.008932519262067604,
      "grad_norm": 2.068108320236206,
      "learning_rate": 0.0001986807736192443,
      "loss": 0.1478,
      "step": 1899
    },
    {
      "epoch": 0.008937223063680066,
      "grad_norm": 1.4307875633239746,
      "learning_rate": 0.00019867983064113083,
      "loss": 0.2771,
      "step": 1900
    },
    {
      "epoch": 0.00894192686529253,
      "grad_norm": 9.141895294189453,
      "learning_rate": 0.00019867888766301735,
      "loss": 1.3442,
      "step": 1901
    },
    {
      "epoch": 0.008946630666904992,
      "grad_norm": 6.174325942993164,
      "learning_rate": 0.0001986779446849039,
      "loss": 0.8181,
      "step": 1902
    },
    {
      "epoch": 0.008951334468517456,
      "grad_norm": 4.6447954177856445,
      "learning_rate": 0.0001986770017067904,
      "loss": 0.7486,
      "step": 1903
    },
    {
      "epoch": 0.00895603827012992,
      "grad_norm": 4.01623010635376,
      "learning_rate": 0.00019867605872867693,
      "loss": 0.6122,
      "step": 1904
    },
    {
      "epoch": 0.008960742071742382,
      "grad_norm": 2.759044647216797,
      "learning_rate": 0.00019867511575056342,
      "loss": 0.2394,
      "step": 1905
    },
    {
      "epoch": 0.008965445873354846,
      "grad_norm": 3.5522241592407227,
      "learning_rate": 0.00019867417277244994,
      "loss": 0.2663,
      "step": 1906
    },
    {
      "epoch": 0.008970149674967308,
      "grad_norm": 4.6010637283325195,
      "learning_rate": 0.00019867322979433649,
      "loss": 0.4952,
      "step": 1907
    },
    {
      "epoch": 0.008974853476579772,
      "grad_norm": 2.460287094116211,
      "learning_rate": 0.000198672286816223,
      "loss": 0.3118,
      "step": 1908
    },
    {
      "epoch": 0.008979557278192236,
      "grad_norm": 1.108132004737854,
      "learning_rate": 0.00019867134383810952,
      "loss": 0.0851,
      "step": 1909
    },
    {
      "epoch": 0.008984261079804698,
      "grad_norm": 2.7144267559051514,
      "learning_rate": 0.00019867040085999604,
      "loss": 0.2913,
      "step": 1910
    },
    {
      "epoch": 0.008988964881417162,
      "grad_norm": 2.1003799438476562,
      "learning_rate": 0.0001986694578818826,
      "loss": 0.192,
      "step": 1911
    },
    {
      "epoch": 0.008993668683029624,
      "grad_norm": 0.6295852065086365,
      "learning_rate": 0.0001986685149037691,
      "loss": 0.0922,
      "step": 1912
    },
    {
      "epoch": 0.008998372484642088,
      "grad_norm": 2.01629376411438,
      "learning_rate": 0.00019866757192565563,
      "loss": 0.258,
      "step": 1913
    },
    {
      "epoch": 0.009003076286254552,
      "grad_norm": 1.797904133796692,
      "learning_rate": 0.00019866662894754214,
      "loss": 0.1929,
      "step": 1914
    },
    {
      "epoch": 0.009007780087867014,
      "grad_norm": 1.5813263654708862,
      "learning_rate": 0.00019866568596942866,
      "loss": 0.1608,
      "step": 1915
    },
    {
      "epoch": 0.009012483889479478,
      "grad_norm": 1.0932843685150146,
      "learning_rate": 0.00019866474299131518,
      "loss": 0.0895,
      "step": 1916
    },
    {
      "epoch": 0.00901718769109194,
      "grad_norm": 1.4304441213607788,
      "learning_rate": 0.0001986638000132017,
      "loss": 0.1401,
      "step": 1917
    },
    {
      "epoch": 0.009021891492704404,
      "grad_norm": 3.078279495239258,
      "learning_rate": 0.00019866285703508822,
      "loss": 0.1686,
      "step": 1918
    },
    {
      "epoch": 0.009026595294316866,
      "grad_norm": 1.0045597553253174,
      "learning_rate": 0.00019866191405697474,
      "loss": 0.0959,
      "step": 1919
    },
    {
      "epoch": 0.00903129909592933,
      "grad_norm": 2.5375614166259766,
      "learning_rate": 0.00019866097107886128,
      "loss": 0.2722,
      "step": 1920
    },
    {
      "epoch": 0.009036002897541794,
      "grad_norm": 0.3027409315109253,
      "learning_rate": 0.0001986600281007478,
      "loss": 0.0226,
      "step": 1921
    },
    {
      "epoch": 0.009040706699154256,
      "grad_norm": 0.7669804096221924,
      "learning_rate": 0.00019865908512263432,
      "loss": 0.0699,
      "step": 1922
    },
    {
      "epoch": 0.00904541050076672,
      "grad_norm": 0.7776923775672913,
      "learning_rate": 0.00019865814214452084,
      "loss": 0.0708,
      "step": 1923
    },
    {
      "epoch": 0.009050114302379182,
      "grad_norm": 1.500665307044983,
      "learning_rate": 0.00019865719916640736,
      "loss": 0.1285,
      "step": 1924
    },
    {
      "epoch": 0.009054818103991646,
      "grad_norm": 1.372710108757019,
      "learning_rate": 0.00019865625618829388,
      "loss": 0.123,
      "step": 1925
    },
    {
      "epoch": 0.00905952190560411,
      "grad_norm": 5.636141777038574,
      "learning_rate": 0.0001986553132101804,
      "loss": 0.7461,
      "step": 1926
    },
    {
      "epoch": 0.009064225707216572,
      "grad_norm": 0.40516746044158936,
      "learning_rate": 0.0001986543702320669,
      "loss": 0.0452,
      "step": 1927
    },
    {
      "epoch": 0.009068929508829036,
      "grad_norm": 4.7608208656311035,
      "learning_rate": 0.00019865342725395343,
      "loss": 1.2364,
      "step": 1928
    },
    {
      "epoch": 0.009073633310441498,
      "grad_norm": 0.2147858738899231,
      "learning_rate": 0.00019865248427583995,
      "loss": 0.0178,
      "step": 1929
    },
    {
      "epoch": 0.009078337112053962,
      "grad_norm": 1.8435783386230469,
      "learning_rate": 0.0001986515412977265,
      "loss": 0.1151,
      "step": 1930
    },
    {
      "epoch": 0.009083040913666426,
      "grad_norm": 2.747436761856079,
      "learning_rate": 0.00019865059831961302,
      "loss": 0.1036,
      "step": 1931
    },
    {
      "epoch": 0.009087744715278888,
      "grad_norm": 3.1500349044799805,
      "learning_rate": 0.00019864965534149953,
      "loss": 0.5141,
      "step": 1932
    },
    {
      "epoch": 0.009092448516891352,
      "grad_norm": 4.507219314575195,
      "learning_rate": 0.00019864871236338605,
      "loss": 0.5812,
      "step": 1933
    },
    {
      "epoch": 0.009097152318503814,
      "grad_norm": 1.37589430809021,
      "learning_rate": 0.0001986477693852726,
      "loss": 0.0669,
      "step": 1934
    },
    {
      "epoch": 0.009101856120116278,
      "grad_norm": 4.7711262702941895,
      "learning_rate": 0.00019864682640715912,
      "loss": 0.2695,
      "step": 1935
    },
    {
      "epoch": 0.00910655992172874,
      "grad_norm": 0.3131839334964752,
      "learning_rate": 0.0001986458834290456,
      "loss": 0.0278,
      "step": 1936
    },
    {
      "epoch": 0.009111263723341204,
      "grad_norm": 0.09391012042760849,
      "learning_rate": 0.00019864494045093213,
      "loss": 0.005,
      "step": 1937
    },
    {
      "epoch": 0.009115967524953668,
      "grad_norm": 4.808467388153076,
      "learning_rate": 0.00019864399747281865,
      "loss": 0.5547,
      "step": 1938
    },
    {
      "epoch": 0.00912067132656613,
      "grad_norm": 4.312599182128906,
      "learning_rate": 0.0001986430544947052,
      "loss": 0.3662,
      "step": 1939
    },
    {
      "epoch": 0.009125375128178594,
      "grad_norm": 4.4823784828186035,
      "learning_rate": 0.0001986421115165917,
      "loss": 0.713,
      "step": 1940
    },
    {
      "epoch": 0.009130078929791056,
      "grad_norm": 1.8575862646102905,
      "learning_rate": 0.00019864116853847823,
      "loss": 0.112,
      "step": 1941
    },
    {
      "epoch": 0.00913478273140352,
      "grad_norm": 5.692812919616699,
      "learning_rate": 0.00019864022556036475,
      "loss": 0.411,
      "step": 1942
    },
    {
      "epoch": 0.009139486533015984,
      "grad_norm": 1.2337207794189453,
      "learning_rate": 0.0001986392825822513,
      "loss": 0.0745,
      "step": 1943
    },
    {
      "epoch": 0.009144190334628446,
      "grad_norm": 0.7123286128044128,
      "learning_rate": 0.0001986383396041378,
      "loss": 0.0443,
      "step": 1944
    },
    {
      "epoch": 0.00914889413624091,
      "grad_norm": 3.8500988483428955,
      "learning_rate": 0.00019863739662602433,
      "loss": 0.1951,
      "step": 1945
    },
    {
      "epoch": 0.009153597937853373,
      "grad_norm": 4.221866607666016,
      "learning_rate": 0.00019863645364791085,
      "loss": 0.8137,
      "step": 1946
    },
    {
      "epoch": 0.009158301739465836,
      "grad_norm": 0.6207980513572693,
      "learning_rate": 0.00019863551066979734,
      "loss": 0.0554,
      "step": 1947
    },
    {
      "epoch": 0.0091630055410783,
      "grad_norm": 0.9040563106536865,
      "learning_rate": 0.00019863456769168389,
      "loss": 0.0709,
      "step": 1948
    },
    {
      "epoch": 0.009167709342690763,
      "grad_norm": 3.285335063934326,
      "learning_rate": 0.0001986336247135704,
      "loss": 0.2879,
      "step": 1949
    },
    {
      "epoch": 0.009172413144303226,
      "grad_norm": 4.161096572875977,
      "learning_rate": 0.00019863268173545692,
      "loss": 0.863,
      "step": 1950
    },
    {
      "epoch": 0.009177116945915689,
      "grad_norm": 4.307877540588379,
      "learning_rate": 0.00019863173875734344,
      "loss": 0.2442,
      "step": 1951
    },
    {
      "epoch": 0.009181820747528152,
      "grad_norm": 0.5454793572425842,
      "learning_rate": 0.00019863079577923,
      "loss": 0.0282,
      "step": 1952
    },
    {
      "epoch": 0.009186524549140615,
      "grad_norm": 0.8692917823791504,
      "learning_rate": 0.0001986298528011165,
      "loss": 0.0396,
      "step": 1953
    },
    {
      "epoch": 0.009191228350753079,
      "grad_norm": 2.16548228263855,
      "learning_rate": 0.00019862890982300303,
      "loss": 0.1647,
      "step": 1954
    },
    {
      "epoch": 0.009195932152365542,
      "grad_norm": 3.2223758697509766,
      "learning_rate": 0.00019862796684488954,
      "loss": 0.2841,
      "step": 1955
    },
    {
      "epoch": 0.009200635953978005,
      "grad_norm": 0.48356181383132935,
      "learning_rate": 0.00019862702386677606,
      "loss": 0.0224,
      "step": 1956
    },
    {
      "epoch": 0.009205339755590469,
      "grad_norm": 5.698901653289795,
      "learning_rate": 0.00019862608088866258,
      "loss": 0.5199,
      "step": 1957
    },
    {
      "epoch": 0.00921004355720293,
      "grad_norm": 0.4518698453903198,
      "learning_rate": 0.0001986251379105491,
      "loss": 0.0418,
      "step": 1958
    },
    {
      "epoch": 0.009214747358815395,
      "grad_norm": 0.6569637060165405,
      "learning_rate": 0.00019862419493243562,
      "loss": 0.0538,
      "step": 1959
    },
    {
      "epoch": 0.009219451160427859,
      "grad_norm": 6.812992095947266,
      "learning_rate": 0.00019862325195432214,
      "loss": 0.6741,
      "step": 1960
    },
    {
      "epoch": 0.00922415496204032,
      "grad_norm": 1.859641194343567,
      "learning_rate": 0.00019862230897620868,
      "loss": 0.0861,
      "step": 1961
    },
    {
      "epoch": 0.009228858763652785,
      "grad_norm": 2.9031126499176025,
      "learning_rate": 0.0001986213659980952,
      "loss": 0.1397,
      "step": 1962
    },
    {
      "epoch": 0.009233562565265247,
      "grad_norm": 4.704668998718262,
      "learning_rate": 0.00019862042301998172,
      "loss": 0.4033,
      "step": 1963
    },
    {
      "epoch": 0.00923826636687771,
      "grad_norm": 1.761831521987915,
      "learning_rate": 0.00019861948004186824,
      "loss": 0.1139,
      "step": 1964
    },
    {
      "epoch": 0.009242970168490175,
      "grad_norm": 4.59731912612915,
      "learning_rate": 0.00019861853706375476,
      "loss": 0.6271,
      "step": 1965
    },
    {
      "epoch": 0.009247673970102637,
      "grad_norm": 2.828322172164917,
      "learning_rate": 0.0001986175940856413,
      "loss": 0.1961,
      "step": 1966
    },
    {
      "epoch": 0.0092523777717151,
      "grad_norm": 1.7503265142440796,
      "learning_rate": 0.0001986166511075278,
      "loss": 0.0741,
      "step": 1967
    },
    {
      "epoch": 0.009257081573327563,
      "grad_norm": 0.05261896178126335,
      "learning_rate": 0.0001986157081294143,
      "loss": 0.0032,
      "step": 1968
    },
    {
      "epoch": 0.009261785374940027,
      "grad_norm": 2.8311848640441895,
      "learning_rate": 0.00019861476515130083,
      "loss": 0.1522,
      "step": 1969
    },
    {
      "epoch": 0.009266489176552489,
      "grad_norm": 0.2605423629283905,
      "learning_rate": 0.00019861382217318738,
      "loss": 0.0167,
      "step": 1970
    },
    {
      "epoch": 0.009271192978164953,
      "grad_norm": 4.243223190307617,
      "learning_rate": 0.0001986128791950739,
      "loss": 0.5661,
      "step": 1971
    },
    {
      "epoch": 0.009275896779777417,
      "grad_norm": 8.250027656555176,
      "learning_rate": 0.00019861193621696042,
      "loss": 0.1956,
      "step": 1972
    },
    {
      "epoch": 0.009280600581389879,
      "grad_norm": 0.31811708211898804,
      "learning_rate": 0.00019861099323884693,
      "loss": 0.0167,
      "step": 1973
    },
    {
      "epoch": 0.009285304383002343,
      "grad_norm": 3.8845229148864746,
      "learning_rate": 0.00019861005026073345,
      "loss": 0.914,
      "step": 1974
    },
    {
      "epoch": 0.009290008184614805,
      "grad_norm": 4.219247817993164,
      "learning_rate": 0.00019860910728262,
      "loss": 0.4198,
      "step": 1975
    },
    {
      "epoch": 0.009294711986227269,
      "grad_norm": 1.6821366548538208,
      "learning_rate": 0.00019860816430450652,
      "loss": 0.1619,
      "step": 1976
    },
    {
      "epoch": 0.009299415787839733,
      "grad_norm": 5.881168842315674,
      "learning_rate": 0.00019860722132639304,
      "loss": 1.0253,
      "step": 1977
    },
    {
      "epoch": 0.009304119589452195,
      "grad_norm": 4.689562797546387,
      "learning_rate": 0.00019860627834827953,
      "loss": 0.4535,
      "step": 1978
    },
    {
      "epoch": 0.009308823391064659,
      "grad_norm": 0.8992546200752258,
      "learning_rate": 0.00019860533537016607,
      "loss": 0.0536,
      "step": 1979
    },
    {
      "epoch": 0.009313527192677121,
      "grad_norm": 4.680046558380127,
      "learning_rate": 0.0001986043923920526,
      "loss": 0.2149,
      "step": 1980
    },
    {
      "epoch": 0.009318230994289585,
      "grad_norm": 0.45089876651763916,
      "learning_rate": 0.0001986034494139391,
      "loss": 0.0226,
      "step": 1981
    },
    {
      "epoch": 0.009322934795902049,
      "grad_norm": 4.172562122344971,
      "learning_rate": 0.00019860250643582563,
      "loss": 0.3549,
      "step": 1982
    },
    {
      "epoch": 0.009327638597514511,
      "grad_norm": 1.5220776796340942,
      "learning_rate": 0.00019860156345771215,
      "loss": 0.1406,
      "step": 1983
    },
    {
      "epoch": 0.009332342399126975,
      "grad_norm": 1.0390819311141968,
      "learning_rate": 0.0001986006204795987,
      "loss": 0.0745,
      "step": 1984
    },
    {
      "epoch": 0.009337046200739437,
      "grad_norm": 2.36112380027771,
      "learning_rate": 0.0001985996775014852,
      "loss": 0.1766,
      "step": 1985
    },
    {
      "epoch": 0.009341750002351901,
      "grad_norm": 1.8418055772781372,
      "learning_rate": 0.00019859873452337173,
      "loss": 0.1275,
      "step": 1986
    },
    {
      "epoch": 0.009346453803964363,
      "grad_norm": 0.7968916893005371,
      "learning_rate": 0.00019859779154525825,
      "loss": 0.0577,
      "step": 1987
    },
    {
      "epoch": 0.009351157605576827,
      "grad_norm": 2.2047131061553955,
      "learning_rate": 0.00019859684856714477,
      "loss": 0.2483,
      "step": 1988
    },
    {
      "epoch": 0.009355861407189291,
      "grad_norm": 0.27032703161239624,
      "learning_rate": 0.00019859590558903129,
      "loss": 0.0188,
      "step": 1989
    },
    {
      "epoch": 0.009360565208801753,
      "grad_norm": 4.282246112823486,
      "learning_rate": 0.0001985949626109178,
      "loss": 0.535,
      "step": 1990
    },
    {
      "epoch": 0.009365269010414217,
      "grad_norm": 1.6889479160308838,
      "learning_rate": 0.00019859401963280432,
      "loss": 0.0856,
      "step": 1991
    },
    {
      "epoch": 0.00936997281202668,
      "grad_norm": 0.4769444763660431,
      "learning_rate": 0.00019859307665469084,
      "loss": 0.0297,
      "step": 1992
    },
    {
      "epoch": 0.009374676613639143,
      "grad_norm": 0.08098479360342026,
      "learning_rate": 0.0001985921336765774,
      "loss": 0.0049,
      "step": 1993
    },
    {
      "epoch": 0.009379380415251607,
      "grad_norm": 0.8298177719116211,
      "learning_rate": 0.0001985911906984639,
      "loss": 0.0597,
      "step": 1994
    },
    {
      "epoch": 0.00938408421686407,
      "grad_norm": 3.7495486736297607,
      "learning_rate": 0.00019859024772035043,
      "loss": 0.1914,
      "step": 1995
    },
    {
      "epoch": 0.009388788018476533,
      "grad_norm": 2.3238260746002197,
      "learning_rate": 0.00019858930474223694,
      "loss": 0.2834,
      "step": 1996
    },
    {
      "epoch": 0.009393491820088995,
      "grad_norm": 2.7130086421966553,
      "learning_rate": 0.00019858836176412346,
      "loss": 0.2436,
      "step": 1997
    },
    {
      "epoch": 0.00939819562170146,
      "grad_norm": 0.06514612585306168,
      "learning_rate": 0.00019858741878600998,
      "loss": 0.0027,
      "step": 1998
    },
    {
      "epoch": 0.009402899423313923,
      "grad_norm": 3.8861749172210693,
      "learning_rate": 0.0001985864758078965,
      "loss": 0.6673,
      "step": 1999
    },
    {
      "epoch": 0.009407603224926385,
      "grad_norm": 2.6497976779937744,
      "learning_rate": 0.00019858553282978302,
      "loss": 0.4135,
      "step": 2000
    },
    {
      "epoch": 0.00941230702653885,
      "grad_norm": 1.9480136632919312,
      "learning_rate": 0.00019858458985166954,
      "loss": 0.1331,
      "step": 2001
    },
    {
      "epoch": 0.009417010828151311,
      "grad_norm": 4.433010578155518,
      "learning_rate": 0.00019858364687355608,
      "loss": 0.2678,
      "step": 2002
    },
    {
      "epoch": 0.009421714629763775,
      "grad_norm": 4.375907897949219,
      "learning_rate": 0.0001985827038954426,
      "loss": 0.4734,
      "step": 2003
    },
    {
      "epoch": 0.009426418431376237,
      "grad_norm": 0.4008784294128418,
      "learning_rate": 0.00019858176091732912,
      "loss": 0.0216,
      "step": 2004
    },
    {
      "epoch": 0.009431122232988701,
      "grad_norm": 2.3873260021209717,
      "learning_rate": 0.00019858081793921564,
      "loss": 0.1674,
      "step": 2005
    },
    {
      "epoch": 0.009435826034601165,
      "grad_norm": 3.8415472507476807,
      "learning_rate": 0.00019857987496110216,
      "loss": 0.2061,
      "step": 2006
    },
    {
      "epoch": 0.009440529836213627,
      "grad_norm": 4.19901180267334,
      "learning_rate": 0.0001985789319829887,
      "loss": 0.6754,
      "step": 2007
    },
    {
      "epoch": 0.009445233637826091,
      "grad_norm": 2.832975149154663,
      "learning_rate": 0.00019857798900487522,
      "loss": 0.2283,
      "step": 2008
    },
    {
      "epoch": 0.009449937439438554,
      "grad_norm": 1.6771951913833618,
      "learning_rate": 0.0001985770460267617,
      "loss": 0.1636,
      "step": 2009
    },
    {
      "epoch": 0.009454641241051017,
      "grad_norm": 2.3483023643493652,
      "learning_rate": 0.00019857610304864823,
      "loss": 0.2106,
      "step": 2010
    },
    {
      "epoch": 0.009459345042663481,
      "grad_norm": 2.1009206771850586,
      "learning_rate": 0.00019857516007053478,
      "loss": 0.2854,
      "step": 2011
    },
    {
      "epoch": 0.009464048844275944,
      "grad_norm": 1.4412719011306763,
      "learning_rate": 0.0001985742170924213,
      "loss": 0.1282,
      "step": 2012
    },
    {
      "epoch": 0.009468752645888407,
      "grad_norm": 1.8708709478378296,
      "learning_rate": 0.00019857327411430782,
      "loss": 0.2556,
      "step": 2013
    },
    {
      "epoch": 0.00947345644750087,
      "grad_norm": 1.7086881399154663,
      "learning_rate": 0.00019857233113619433,
      "loss": 0.1398,
      "step": 2014
    },
    {
      "epoch": 0.009478160249113333,
      "grad_norm": 2.1973929405212402,
      "learning_rate": 0.00019857138815808085,
      "loss": 0.1603,
      "step": 2015
    },
    {
      "epoch": 0.009482864050725797,
      "grad_norm": 3.2620718479156494,
      "learning_rate": 0.0001985704451799674,
      "loss": 0.2353,
      "step": 2016
    },
    {
      "epoch": 0.00948756785233826,
      "grad_norm": 1.9105995893478394,
      "learning_rate": 0.00019856950220185392,
      "loss": 0.1918,
      "step": 2017
    },
    {
      "epoch": 0.009492271653950723,
      "grad_norm": 4.6702880859375,
      "learning_rate": 0.00019856855922374044,
      "loss": 0.4637,
      "step": 2018
    },
    {
      "epoch": 0.009496975455563186,
      "grad_norm": 3.122480869293213,
      "learning_rate": 0.00019856761624562695,
      "loss": 0.3025,
      "step": 2019
    },
    {
      "epoch": 0.00950167925717565,
      "grad_norm": 1.0047403573989868,
      "learning_rate": 0.00019856667326751347,
      "loss": 0.2186,
      "step": 2020
    },
    {
      "epoch": 0.009506383058788112,
      "grad_norm": 2.6238863468170166,
      "learning_rate": 0.0001985657302894,
      "loss": 0.1858,
      "step": 2021
    },
    {
      "epoch": 0.009511086860400576,
      "grad_norm": 4.542938232421875,
      "learning_rate": 0.0001985647873112865,
      "loss": 1.2399,
      "step": 2022
    },
    {
      "epoch": 0.00951579066201304,
      "grad_norm": 0.3482508659362793,
      "learning_rate": 0.00019856384433317303,
      "loss": 0.0245,
      "step": 2023
    },
    {
      "epoch": 0.009520494463625502,
      "grad_norm": 2.952622413635254,
      "learning_rate": 0.00019856290135505955,
      "loss": 0.7121,
      "step": 2024
    },
    {
      "epoch": 0.009525198265237966,
      "grad_norm": 2.2869296073913574,
      "learning_rate": 0.0001985619583769461,
      "loss": 0.4813,
      "step": 2025
    },
    {
      "epoch": 0.009529902066850428,
      "grad_norm": 1.5826035737991333,
      "learning_rate": 0.0001985610153988326,
      "loss": 0.2869,
      "step": 2026
    },
    {
      "epoch": 0.009534605868462892,
      "grad_norm": 3.4794623851776123,
      "learning_rate": 0.00019856007242071913,
      "loss": 0.6591,
      "step": 2027
    },
    {
      "epoch": 0.009539309670075356,
      "grad_norm": 3.144705295562744,
      "learning_rate": 0.00019855912944260565,
      "loss": 0.6354,
      "step": 2028
    },
    {
      "epoch": 0.009544013471687818,
      "grad_norm": 0.7512005567550659,
      "learning_rate": 0.00019855818646449217,
      "loss": 0.1185,
      "step": 2029
    },
    {
      "epoch": 0.009548717273300282,
      "grad_norm": 1.482723355293274,
      "learning_rate": 0.00019855724348637869,
      "loss": 0.1463,
      "step": 2030
    },
    {
      "epoch": 0.009553421074912744,
      "grad_norm": 0.9442422389984131,
      "learning_rate": 0.0001985563005082652,
      "loss": 0.0855,
      "step": 2031
    },
    {
      "epoch": 0.009558124876525208,
      "grad_norm": 0.9468145966529846,
      "learning_rate": 0.00019855535753015172,
      "loss": 0.0963,
      "step": 2032
    },
    {
      "epoch": 0.009562828678137672,
      "grad_norm": 1.1344637870788574,
      "learning_rate": 0.00019855441455203824,
      "loss": 0.1503,
      "step": 2033
    },
    {
      "epoch": 0.009567532479750134,
      "grad_norm": 2.5294878482818604,
      "learning_rate": 0.0001985534715739248,
      "loss": 0.4728,
      "step": 2034
    },
    {
      "epoch": 0.009572236281362598,
      "grad_norm": 3.472659111022949,
      "learning_rate": 0.0001985525285958113,
      "loss": 0.3486,
      "step": 2035
    },
    {
      "epoch": 0.00957694008297506,
      "grad_norm": 0.3443320691585541,
      "learning_rate": 0.00019855158561769783,
      "loss": 0.0232,
      "step": 2036
    },
    {
      "epoch": 0.009581643884587524,
      "grad_norm": 1.1376553773880005,
      "learning_rate": 0.00019855064263958434,
      "loss": 0.1059,
      "step": 2037
    },
    {
      "epoch": 0.009586347686199986,
      "grad_norm": 2.851501703262329,
      "learning_rate": 0.00019854969966147086,
      "loss": 0.2962,
      "step": 2038
    },
    {
      "epoch": 0.00959105148781245,
      "grad_norm": 2.2675600051879883,
      "learning_rate": 0.0001985487566833574,
      "loss": 0.1859,
      "step": 2039
    },
    {
      "epoch": 0.009595755289424914,
      "grad_norm": 4.275279521942139,
      "learning_rate": 0.0001985478137052439,
      "loss": 0.4549,
      "step": 2040
    },
    {
      "epoch": 0.009600459091037376,
      "grad_norm": 1.1991171836853027,
      "learning_rate": 0.00019854687072713042,
      "loss": 0.1876,
      "step": 2041
    },
    {
      "epoch": 0.00960516289264984,
      "grad_norm": 1.8503615856170654,
      "learning_rate": 0.00019854592774901694,
      "loss": 0.3912,
      "step": 2042
    },
    {
      "epoch": 0.009609866694262302,
      "grad_norm": 2.8133692741394043,
      "learning_rate": 0.00019854498477090348,
      "loss": 0.3597,
      "step": 2043
    },
    {
      "epoch": 0.009614570495874766,
      "grad_norm": 1.6805357933044434,
      "learning_rate": 0.00019854404179279,
      "loss": 0.1343,
      "step": 2044
    },
    {
      "epoch": 0.00961927429748723,
      "grad_norm": 3.5069005489349365,
      "learning_rate": 0.00019854309881467652,
      "loss": 0.7026,
      "step": 2045
    },
    {
      "epoch": 0.009623978099099692,
      "grad_norm": 3.607426404953003,
      "learning_rate": 0.00019854215583656304,
      "loss": 0.5268,
      "step": 2046
    },
    {
      "epoch": 0.009628681900712156,
      "grad_norm": 1.6961454153060913,
      "learning_rate": 0.00019854121285844956,
      "loss": 0.2915,
      "step": 2047
    },
    {
      "epoch": 0.009633385702324618,
      "grad_norm": 0.3103017508983612,
      "learning_rate": 0.0001985402698803361,
      "loss": 0.0251,
      "step": 2048
    },
    {
      "epoch": 0.009638089503937082,
      "grad_norm": 1.6264241933822632,
      "learning_rate": 0.00019853932690222262,
      "loss": 0.2251,
      "step": 2049
    },
    {
      "epoch": 0.009642793305549546,
      "grad_norm": 0.7655535936355591,
      "learning_rate": 0.00019853838392410914,
      "loss": 0.0952,
      "step": 2050
    },
    {
      "epoch": 0.009647497107162008,
      "grad_norm": 0.7885769009590149,
      "learning_rate": 0.00019853744094599563,
      "loss": 0.0691,
      "step": 2051
    },
    {
      "epoch": 0.009652200908774472,
      "grad_norm": 0.5672537684440613,
      "learning_rate": 0.00019853649796788218,
      "loss": 0.0514,
      "step": 2052
    },
    {
      "epoch": 0.009656904710386934,
      "grad_norm": 0.7237324118614197,
      "learning_rate": 0.0001985355549897687,
      "loss": 0.0616,
      "step": 2053
    },
    {
      "epoch": 0.009661608511999398,
      "grad_norm": 3.211331844329834,
      "learning_rate": 0.00019853461201165522,
      "loss": 0.6796,
      "step": 2054
    },
    {
      "epoch": 0.00966631231361186,
      "grad_norm": 3.2660071849823,
      "learning_rate": 0.00019853366903354173,
      "loss": 0.5097,
      "step": 2055
    },
    {
      "epoch": 0.009671016115224324,
      "grad_norm": 2.292741537094116,
      "learning_rate": 0.00019853272605542825,
      "loss": 0.4023,
      "step": 2056
    },
    {
      "epoch": 0.009675719916836788,
      "grad_norm": 3.6358892917633057,
      "learning_rate": 0.0001985317830773148,
      "loss": 0.5461,
      "step": 2057
    },
    {
      "epoch": 0.00968042371844925,
      "grad_norm": 3.6753032207489014,
      "learning_rate": 0.00019853084009920132,
      "loss": 0.7981,
      "step": 2058
    },
    {
      "epoch": 0.009685127520061714,
      "grad_norm": 3.006383180618286,
      "learning_rate": 0.00019852989712108784,
      "loss": 0.333,
      "step": 2059
    },
    {
      "epoch": 0.009689831321674176,
      "grad_norm": 2.2926762104034424,
      "learning_rate": 0.00019852895414297435,
      "loss": 0.59,
      "step": 2060
    },
    {
      "epoch": 0.00969453512328664,
      "grad_norm": 1.6415152549743652,
      "learning_rate": 0.00019852801116486087,
      "loss": 0.1934,
      "step": 2061
    },
    {
      "epoch": 0.009699238924899104,
      "grad_norm": 1.9285532236099243,
      "learning_rate": 0.0001985270681867474,
      "loss": 0.3722,
      "step": 2062
    },
    {
      "epoch": 0.009703942726511566,
      "grad_norm": 2.254483938217163,
      "learning_rate": 0.0001985261252086339,
      "loss": 0.5094,
      "step": 2063
    },
    {
      "epoch": 0.00970864652812403,
      "grad_norm": 0.42530497908592224,
      "learning_rate": 0.00019852518223052043,
      "loss": 0.0369,
      "step": 2064
    },
    {
      "epoch": 0.009713350329736492,
      "grad_norm": 3.097665548324585,
      "learning_rate": 0.00019852423925240695,
      "loss": 0.6075,
      "step": 2065
    },
    {
      "epoch": 0.009718054131348956,
      "grad_norm": 2.9728448390960693,
      "learning_rate": 0.0001985232962742935,
      "loss": 0.4569,
      "step": 2066
    },
    {
      "epoch": 0.00972275793296142,
      "grad_norm": 0.8437008857727051,
      "learning_rate": 0.00019852235329618,
      "loss": 0.1109,
      "step": 2067
    },
    {
      "epoch": 0.009727461734573882,
      "grad_norm": 1.129393219947815,
      "learning_rate": 0.00019852141031806653,
      "loss": 0.1314,
      "step": 2068
    },
    {
      "epoch": 0.009732165536186346,
      "grad_norm": 1.5671600103378296,
      "learning_rate": 0.00019852046733995305,
      "loss": 0.3562,
      "step": 2069
    },
    {
      "epoch": 0.009736869337798808,
      "grad_norm": 3.839158535003662,
      "learning_rate": 0.0001985195243618396,
      "loss": 1.2868,
      "step": 2070
    },
    {
      "epoch": 0.009741573139411272,
      "grad_norm": 1.6155858039855957,
      "learning_rate": 0.00019851858138372609,
      "loss": 0.3243,
      "step": 2071
    },
    {
      "epoch": 0.009746276941023735,
      "grad_norm": 0.9867823123931885,
      "learning_rate": 0.0001985176384056126,
      "loss": 0.2144,
      "step": 2072
    },
    {
      "epoch": 0.009750980742636198,
      "grad_norm": 2.657972812652588,
      "learning_rate": 0.00019851669542749912,
      "loss": 0.2865,
      "step": 2073
    },
    {
      "epoch": 0.009755684544248662,
      "grad_norm": 1.4940869808197021,
      "learning_rate": 0.00019851575244938564,
      "loss": 0.1658,
      "step": 2074
    },
    {
      "epoch": 0.009760388345861125,
      "grad_norm": 1.2279125452041626,
      "learning_rate": 0.0001985148094712722,
      "loss": 0.2347,
      "step": 2075
    },
    {
      "epoch": 0.009765092147473588,
      "grad_norm": 2.822490930557251,
      "learning_rate": 0.0001985138664931587,
      "loss": 0.5039,
      "step": 2076
    },
    {
      "epoch": 0.00976979594908605,
      "grad_norm": 0.8199083805084229,
      "learning_rate": 0.00019851292351504523,
      "loss": 0.0838,
      "step": 2077
    },
    {
      "epoch": 0.009774499750698514,
      "grad_norm": 0.9238651990890503,
      "learning_rate": 0.00019851198053693174,
      "loss": 0.1161,
      "step": 2078
    },
    {
      "epoch": 0.009779203552310978,
      "grad_norm": 2.463096857070923,
      "learning_rate": 0.0001985110375588183,
      "loss": 0.658,
      "step": 2079
    },
    {
      "epoch": 0.00978390735392344,
      "grad_norm": 1.3421237468719482,
      "learning_rate": 0.0001985100945807048,
      "loss": 0.3864,
      "step": 2080
    },
    {
      "epoch": 0.009788611155535904,
      "grad_norm": 2.942460775375366,
      "learning_rate": 0.00019850915160259133,
      "loss": 0.5789,
      "step": 2081
    },
    {
      "epoch": 0.009793314957148367,
      "grad_norm": 3.1492953300476074,
      "learning_rate": 0.00019850820862447782,
      "loss": 0.4043,
      "step": 2082
    },
    {
      "epoch": 0.00979801875876083,
      "grad_norm": 0.6869024038314819,
      "learning_rate": 0.00019850726564636434,
      "loss": 0.1138,
      "step": 2083
    },
    {
      "epoch": 0.009802722560373294,
      "grad_norm": 0.5224452018737793,
      "learning_rate": 0.00019850632266825088,
      "loss": 0.0688,
      "step": 2084
    },
    {
      "epoch": 0.009807426361985757,
      "grad_norm": 1.8564574718475342,
      "learning_rate": 0.0001985053796901374,
      "loss": 0.4244,
      "step": 2085
    },
    {
      "epoch": 0.00981213016359822,
      "grad_norm": 2.0517332553863525,
      "learning_rate": 0.00019850443671202392,
      "loss": 0.4974,
      "step": 2086
    },
    {
      "epoch": 0.009816833965210683,
      "grad_norm": 2.445746421813965,
      "learning_rate": 0.00019850349373391044,
      "loss": 0.3587,
      "step": 2087
    },
    {
      "epoch": 0.009821537766823147,
      "grad_norm": 0.9665172100067139,
      "learning_rate": 0.00019850255075579696,
      "loss": 0.2868,
      "step": 2088
    },
    {
      "epoch": 0.009826241568435609,
      "grad_norm": 2.5684895515441895,
      "learning_rate": 0.0001985016077776835,
      "loss": 0.4811,
      "step": 2089
    },
    {
      "epoch": 0.009830945370048073,
      "grad_norm": 1.0559139251708984,
      "learning_rate": 0.00019850066479957002,
      "loss": 0.338,
      "step": 2090
    },
    {
      "epoch": 0.009835649171660537,
      "grad_norm": 1.6719917058944702,
      "learning_rate": 0.00019849972182145654,
      "loss": 0.1924,
      "step": 2091
    },
    {
      "epoch": 0.009840352973272999,
      "grad_norm": 1.5877234935760498,
      "learning_rate": 0.00019849877884334306,
      "loss": 0.421,
      "step": 2092
    },
    {
      "epoch": 0.009845056774885463,
      "grad_norm": 1.1455696821212769,
      "learning_rate": 0.00019849783586522958,
      "loss": 0.2154,
      "step": 2093
    },
    {
      "epoch": 0.009849760576497925,
      "grad_norm": 1.6535484790802002,
      "learning_rate": 0.0001984968928871161,
      "loss": 0.3865,
      "step": 2094
    },
    {
      "epoch": 0.009854464378110389,
      "grad_norm": 1.914207935333252,
      "learning_rate": 0.00019849594990900262,
      "loss": 0.3318,
      "step": 2095
    },
    {
      "epoch": 0.009859168179722853,
      "grad_norm": 1.0440720319747925,
      "learning_rate": 0.00019849500693088913,
      "loss": 0.0935,
      "step": 2096
    },
    {
      "epoch": 0.009863871981335315,
      "grad_norm": 1.6160210371017456,
      "learning_rate": 0.00019849406395277565,
      "loss": 0.3936,
      "step": 2097
    },
    {
      "epoch": 0.009868575782947779,
      "grad_norm": 0.8808043003082275,
      "learning_rate": 0.0001984931209746622,
      "loss": 0.1358,
      "step": 2098
    },
    {
      "epoch": 0.009873279584560241,
      "grad_norm": 0.7468775510787964,
      "learning_rate": 0.00019849217799654872,
      "loss": 0.0812,
      "step": 2099
    },
    {
      "epoch": 0.009877983386172705,
      "grad_norm": 0.9535284042358398,
      "learning_rate": 0.00019849123501843524,
      "loss": 0.1841,
      "step": 2100
    },
    {
      "epoch": 0.009882687187785169,
      "grad_norm": 5.700603008270264,
      "learning_rate": 0.00019849029204032175,
      "loss": 0.6585,
      "step": 2101
    },
    {
      "epoch": 0.009887390989397631,
      "grad_norm": 3.9028408527374268,
      "learning_rate": 0.00019848934906220827,
      "loss": 0.4271,
      "step": 2102
    },
    {
      "epoch": 0.009892094791010095,
      "grad_norm": 6.121903896331787,
      "learning_rate": 0.0001984884060840948,
      "loss": 0.605,
      "step": 2103
    },
    {
      "epoch": 0.009896798592622557,
      "grad_norm": 2.353320360183716,
      "learning_rate": 0.0001984874631059813,
      "loss": 0.344,
      "step": 2104
    },
    {
      "epoch": 0.009901502394235021,
      "grad_norm": 2.659351348876953,
      "learning_rate": 0.00019848652012786783,
      "loss": 0.2844,
      "step": 2105
    },
    {
      "epoch": 0.009906206195847485,
      "grad_norm": 0.9129908680915833,
      "learning_rate": 0.00019848557714975435,
      "loss": 0.1153,
      "step": 2106
    },
    {
      "epoch": 0.009910909997459947,
      "grad_norm": 2.639758825302124,
      "learning_rate": 0.0001984846341716409,
      "loss": 0.2381,
      "step": 2107
    },
    {
      "epoch": 0.009915613799072411,
      "grad_norm": 1.225609540939331,
      "learning_rate": 0.0001984836911935274,
      "loss": 0.0814,
      "step": 2108
    },
    {
      "epoch": 0.009920317600684873,
      "grad_norm": 2.762814521789551,
      "learning_rate": 0.00019848274821541393,
      "loss": 0.5017,
      "step": 2109
    },
    {
      "epoch": 0.009925021402297337,
      "grad_norm": 0.23799943923950195,
      "learning_rate": 0.00019848180523730045,
      "loss": 0.0169,
      "step": 2110
    },
    {
      "epoch": 0.009929725203909799,
      "grad_norm": 0.8442642092704773,
      "learning_rate": 0.000198480862259187,
      "loss": 0.0719,
      "step": 2111
    },
    {
      "epoch": 0.009934429005522263,
      "grad_norm": 2.7436158657073975,
      "learning_rate": 0.0001984799192810735,
      "loss": 0.3677,
      "step": 2112
    },
    {
      "epoch": 0.009939132807134727,
      "grad_norm": 1.8241996765136719,
      "learning_rate": 0.00019847897630296,
      "loss": 0.1309,
      "step": 2113
    },
    {
      "epoch": 0.009943836608747189,
      "grad_norm": 1.1385438442230225,
      "learning_rate": 0.00019847803332484652,
      "loss": 0.0936,
      "step": 2114
    },
    {
      "epoch": 0.009948540410359653,
      "grad_norm": 3.0264196395874023,
      "learning_rate": 0.00019847709034673304,
      "loss": 0.3306,
      "step": 2115
    },
    {
      "epoch": 0.009953244211972115,
      "grad_norm": 0.5725715160369873,
      "learning_rate": 0.0001984761473686196,
      "loss": 0.0413,
      "step": 2116
    },
    {
      "epoch": 0.009957948013584579,
      "grad_norm": 2.0279388427734375,
      "learning_rate": 0.0001984752043905061,
      "loss": 0.2211,
      "step": 2117
    },
    {
      "epoch": 0.009962651815197043,
      "grad_norm": 1.406970500946045,
      "learning_rate": 0.00019847426141239263,
      "loss": 0.1519,
      "step": 2118
    },
    {
      "epoch": 0.009967355616809505,
      "grad_norm": 6.4709930419921875,
      "learning_rate": 0.00019847331843427914,
      "loss": 0.9857,
      "step": 2119
    },
    {
      "epoch": 0.009972059418421969,
      "grad_norm": 1.6020983457565308,
      "learning_rate": 0.0001984723754561657,
      "loss": 0.2175,
      "step": 2120
    },
    {
      "epoch": 0.009976763220034431,
      "grad_norm": 2.962219476699829,
      "learning_rate": 0.0001984714324780522,
      "loss": 0.1817,
      "step": 2121
    },
    {
      "epoch": 0.009981467021646895,
      "grad_norm": 2.160841941833496,
      "learning_rate": 0.00019847048949993873,
      "loss": 0.3465,
      "step": 2122
    },
    {
      "epoch": 0.009986170823259359,
      "grad_norm": 0.4425000846385956,
      "learning_rate": 0.00019846954652182525,
      "loss": 0.0276,
      "step": 2123
    },
    {
      "epoch": 0.009990874624871821,
      "grad_norm": 3.581526041030884,
      "learning_rate": 0.00019846860354371176,
      "loss": 0.3371,
      "step": 2124
    },
    {
      "epoch": 0.009995578426484285,
      "grad_norm": 3.385859251022339,
      "learning_rate": 0.00019846766056559828,
      "loss": 0.6436,
      "step": 2125
    },
    {
      "epoch": 0.010000282228096747,
      "grad_norm": 2.1586520671844482,
      "learning_rate": 0.0001984667175874848,
      "loss": 0.2088,
      "step": 2126
    },
    {
      "epoch": 0.010004986029709211,
      "grad_norm": 0.14233210682868958,
      "learning_rate": 0.00019846577460937132,
      "loss": 0.0084,
      "step": 2127
    },
    {
      "epoch": 0.010009689831321673,
      "grad_norm": 5.165597915649414,
      "learning_rate": 0.00019846483163125784,
      "loss": 0.9603,
      "step": 2128
    },
    {
      "epoch": 0.010014393632934137,
      "grad_norm": 3.377014636993408,
      "learning_rate": 0.00019846388865314438,
      "loss": 0.2642,
      "step": 2129
    },
    {
      "epoch": 0.010019097434546601,
      "grad_norm": 4.109095096588135,
      "learning_rate": 0.0001984629456750309,
      "loss": 0.464,
      "step": 2130
    },
    {
      "epoch": 0.010023801236159063,
      "grad_norm": 1.3515856266021729,
      "learning_rate": 0.00019846200269691742,
      "loss": 0.2072,
      "step": 2131
    },
    {
      "epoch": 0.010028505037771527,
      "grad_norm": 1.8522403240203857,
      "learning_rate": 0.00019846105971880394,
      "loss": 0.397,
      "step": 2132
    },
    {
      "epoch": 0.01003320883938399,
      "grad_norm": 1.573773741722107,
      "learning_rate": 0.00019846011674069046,
      "loss": 0.1635,
      "step": 2133
    },
    {
      "epoch": 0.010037912640996453,
      "grad_norm": 2.040066719055176,
      "learning_rate": 0.00019845917376257698,
      "loss": 0.2045,
      "step": 2134
    },
    {
      "epoch": 0.010042616442608917,
      "grad_norm": 1.5157294273376465,
      "learning_rate": 0.0001984582307844635,
      "loss": 0.1689,
      "step": 2135
    },
    {
      "epoch": 0.01004732024422138,
      "grad_norm": 0.6542640924453735,
      "learning_rate": 0.00019845728780635001,
      "loss": 0.0416,
      "step": 2136
    },
    {
      "epoch": 0.010052024045833843,
      "grad_norm": 2.704838275909424,
      "learning_rate": 0.00019845634482823653,
      "loss": 0.4471,
      "step": 2137
    },
    {
      "epoch": 0.010056727847446306,
      "grad_norm": 1.9680200815200806,
      "learning_rate": 0.00019845540185012305,
      "loss": 0.3474,
      "step": 2138
    },
    {
      "epoch": 0.01006143164905877,
      "grad_norm": 3.4222395420074463,
      "learning_rate": 0.0001984544588720096,
      "loss": 0.5961,
      "step": 2139
    },
    {
      "epoch": 0.010066135450671233,
      "grad_norm": 1.6525986194610596,
      "learning_rate": 0.00019845351589389612,
      "loss": 0.3878,
      "step": 2140
    },
    {
      "epoch": 0.010070839252283696,
      "grad_norm": 1.5390222072601318,
      "learning_rate": 0.00019845257291578264,
      "loss": 0.2952,
      "step": 2141
    },
    {
      "epoch": 0.01007554305389616,
      "grad_norm": 4.568881511688232,
      "learning_rate": 0.00019845162993766915,
      "loss": 1.1256,
      "step": 2142
    },
    {
      "epoch": 0.010080246855508622,
      "grad_norm": 3.2034387588500977,
      "learning_rate": 0.0001984506869595557,
      "loss": 0.6683,
      "step": 2143
    },
    {
      "epoch": 0.010084950657121085,
      "grad_norm": 2.2398200035095215,
      "learning_rate": 0.0001984497439814422,
      "loss": 0.5366,
      "step": 2144
    },
    {
      "epoch": 0.010089654458733548,
      "grad_norm": 1.3842642307281494,
      "learning_rate": 0.0001984488010033287,
      "loss": 0.1405,
      "step": 2145
    },
    {
      "epoch": 0.010094358260346012,
      "grad_norm": 0.4106450080871582,
      "learning_rate": 0.00019844785802521523,
      "loss": 0.0492,
      "step": 2146
    },
    {
      "epoch": 0.010099062061958475,
      "grad_norm": 0.5777968168258667,
      "learning_rate": 0.00019844691504710175,
      "loss": 0.0558,
      "step": 2147
    },
    {
      "epoch": 0.010103765863570938,
      "grad_norm": 0.781911313533783,
      "learning_rate": 0.0001984459720689883,
      "loss": 0.1837,
      "step": 2148
    },
    {
      "epoch": 0.010108469665183402,
      "grad_norm": 0.5662887692451477,
      "learning_rate": 0.0001984450290908748,
      "loss": 0.2118,
      "step": 2149
    },
    {
      "epoch": 0.010113173466795864,
      "grad_norm": 2.2091662883758545,
      "learning_rate": 0.00019844408611276133,
      "loss": 0.6726,
      "step": 2150
    },
    {
      "epoch": 0.010117877268408328,
      "grad_norm": 2.9910037517547607,
      "learning_rate": 0.00019844314313464785,
      "loss": 0.5031,
      "step": 2151
    },
    {
      "epoch": 0.010122581070020792,
      "grad_norm": 2.204303503036499,
      "learning_rate": 0.0001984422001565344,
      "loss": 0.4562,
      "step": 2152
    },
    {
      "epoch": 0.010127284871633254,
      "grad_norm": 0.36375489830970764,
      "learning_rate": 0.0001984412571784209,
      "loss": 0.0328,
      "step": 2153
    },
    {
      "epoch": 0.010131988673245718,
      "grad_norm": 1.321006178855896,
      "learning_rate": 0.00019844031420030743,
      "loss": 0.2721,
      "step": 2154
    },
    {
      "epoch": 0.01013669247485818,
      "grad_norm": 4.652657985687256,
      "learning_rate": 0.00019843937122219395,
      "loss": 0.8022,
      "step": 2155
    },
    {
      "epoch": 0.010141396276470644,
      "grad_norm": 3.23979115486145,
      "learning_rate": 0.00019843842824408044,
      "loss": 0.5544,
      "step": 2156
    },
    {
      "epoch": 0.010146100078083108,
      "grad_norm": 7.137925148010254,
      "learning_rate": 0.000198437485265967,
      "loss": 0.7024,
      "step": 2157
    },
    {
      "epoch": 0.01015080387969557,
      "grad_norm": 1.0779310464859009,
      "learning_rate": 0.0001984365422878535,
      "loss": 0.1031,
      "step": 2158
    },
    {
      "epoch": 0.010155507681308034,
      "grad_norm": 1.0176482200622559,
      "learning_rate": 0.00019843559930974003,
      "loss": 0.3028,
      "step": 2159
    },
    {
      "epoch": 0.010160211482920496,
      "grad_norm": 1.8350672721862793,
      "learning_rate": 0.00019843465633162654,
      "loss": 0.2186,
      "step": 2160
    },
    {
      "epoch": 0.01016491528453296,
      "grad_norm": 0.4799902141094208,
      "learning_rate": 0.0001984337133535131,
      "loss": 0.093,
      "step": 2161
    },
    {
      "epoch": 0.010169619086145422,
      "grad_norm": 0.5726094841957092,
      "learning_rate": 0.0001984327703753996,
      "loss": 0.1156,
      "step": 2162
    },
    {
      "epoch": 0.010174322887757886,
      "grad_norm": 1.9317418336868286,
      "learning_rate": 0.00019843182739728613,
      "loss": 0.2857,
      "step": 2163
    },
    {
      "epoch": 0.01017902668937035,
      "grad_norm": 2.1388165950775146,
      "learning_rate": 0.00019843088441917265,
      "loss": 0.5222,
      "step": 2164
    },
    {
      "epoch": 0.010183730490982812,
      "grad_norm": 1.7264355421066284,
      "learning_rate": 0.00019842994144105916,
      "loss": 0.3528,
      "step": 2165
    },
    {
      "epoch": 0.010188434292595276,
      "grad_norm": 0.7781792283058167,
      "learning_rate": 0.00019842899846294568,
      "loss": 0.2629,
      "step": 2166
    },
    {
      "epoch": 0.010193138094207738,
      "grad_norm": 0.6403849124908447,
      "learning_rate": 0.0001984280554848322,
      "loss": 0.0646,
      "step": 2167
    },
    {
      "epoch": 0.010197841895820202,
      "grad_norm": 1.1150219440460205,
      "learning_rate": 0.00019842711250671872,
      "loss": 0.1404,
      "step": 2168
    },
    {
      "epoch": 0.010202545697432666,
      "grad_norm": 1.0376497507095337,
      "learning_rate": 0.00019842616952860524,
      "loss": 0.2238,
      "step": 2169
    },
    {
      "epoch": 0.010207249499045128,
      "grad_norm": 1.2235469818115234,
      "learning_rate": 0.00019842522655049178,
      "loss": 0.1403,
      "step": 2170
    },
    {
      "epoch": 0.010211953300657592,
      "grad_norm": 2.6769723892211914,
      "learning_rate": 0.0001984242835723783,
      "loss": 0.3202,
      "step": 2171
    },
    {
      "epoch": 0.010216657102270054,
      "grad_norm": 2.519240379333496,
      "learning_rate": 0.00019842334059426482,
      "loss": 0.4448,
      "step": 2172
    },
    {
      "epoch": 0.010221360903882518,
      "grad_norm": 1.0857610702514648,
      "learning_rate": 0.00019842239761615134,
      "loss": 0.19,
      "step": 2173
    },
    {
      "epoch": 0.010226064705494982,
      "grad_norm": 1.6320821046829224,
      "learning_rate": 0.00019842145463803786,
      "loss": 0.3287,
      "step": 2174
    },
    {
      "epoch": 0.010230768507107444,
      "grad_norm": 0.1276800036430359,
      "learning_rate": 0.00019842051165992438,
      "loss": 0.0083,
      "step": 2175
    },
    {
      "epoch": 0.010235472308719908,
      "grad_norm": 0.8468027710914612,
      "learning_rate": 0.0001984195686818109,
      "loss": 0.0775,
      "step": 2176
    },
    {
      "epoch": 0.01024017611033237,
      "grad_norm": 0.4755682647228241,
      "learning_rate": 0.00019841862570369741,
      "loss": 0.0543,
      "step": 2177
    },
    {
      "epoch": 0.010244879911944834,
      "grad_norm": 2.66896390914917,
      "learning_rate": 0.00019841768272558393,
      "loss": 1.0403,
      "step": 2178
    },
    {
      "epoch": 0.010249583713557296,
      "grad_norm": 3.696568727493286,
      "learning_rate": 0.00019841673974747048,
      "loss": 0.8193,
      "step": 2179
    },
    {
      "epoch": 0.01025428751516976,
      "grad_norm": 2.734875202178955,
      "learning_rate": 0.000198415796769357,
      "loss": 0.7069,
      "step": 2180
    },
    {
      "epoch": 0.010258991316782224,
      "grad_norm": 2.5331296920776367,
      "learning_rate": 0.00019841485379124352,
      "loss": 0.4723,
      "step": 2181
    },
    {
      "epoch": 0.010263695118394686,
      "grad_norm": 0.6726731061935425,
      "learning_rate": 0.00019841391081313004,
      "loss": 0.0891,
      "step": 2182
    },
    {
      "epoch": 0.01026839892000715,
      "grad_norm": 2.4188172817230225,
      "learning_rate": 0.00019841296783501655,
      "loss": 0.764,
      "step": 2183
    },
    {
      "epoch": 0.010273102721619612,
      "grad_norm": 1.2904964685440063,
      "learning_rate": 0.0001984120248569031,
      "loss": 0.1567,
      "step": 2184
    },
    {
      "epoch": 0.010277806523232076,
      "grad_norm": 0.8907520174980164,
      "learning_rate": 0.00019841108187878962,
      "loss": 0.1336,
      "step": 2185
    },
    {
      "epoch": 0.01028251032484454,
      "grad_norm": 1.504075527191162,
      "learning_rate": 0.00019841013890067614,
      "loss": 0.2065,
      "step": 2186
    },
    {
      "epoch": 0.010287214126457002,
      "grad_norm": 1.348915457725525,
      "learning_rate": 0.00019840919592256263,
      "loss": 0.4186,
      "step": 2187
    },
    {
      "epoch": 0.010291917928069466,
      "grad_norm": 0.6001793742179871,
      "learning_rate": 0.00019840825294444915,
      "loss": 0.0749,
      "step": 2188
    },
    {
      "epoch": 0.010296621729681928,
      "grad_norm": 1.4686939716339111,
      "learning_rate": 0.0001984073099663357,
      "loss": 0.2774,
      "step": 2189
    },
    {
      "epoch": 0.010301325531294392,
      "grad_norm": 1.5230129957199097,
      "learning_rate": 0.0001984063669882222,
      "loss": 0.236,
      "step": 2190
    },
    {
      "epoch": 0.010306029332906856,
      "grad_norm": 0.7278214693069458,
      "learning_rate": 0.00019840542401010873,
      "loss": 0.0772,
      "step": 2191
    },
    {
      "epoch": 0.010310733134519318,
      "grad_norm": 1.7107527256011963,
      "learning_rate": 0.00019840448103199525,
      "loss": 0.2094,
      "step": 2192
    },
    {
      "epoch": 0.010315436936131782,
      "grad_norm": 0.23891636729240417,
      "learning_rate": 0.0001984035380538818,
      "loss": 0.0189,
      "step": 2193
    },
    {
      "epoch": 0.010320140737744244,
      "grad_norm": 0.9282350540161133,
      "learning_rate": 0.0001984025950757683,
      "loss": 0.1408,
      "step": 2194
    },
    {
      "epoch": 0.010324844539356708,
      "grad_norm": 0.48461517691612244,
      "learning_rate": 0.00019840165209765483,
      "loss": 0.0497,
      "step": 2195
    },
    {
      "epoch": 0.01032954834096917,
      "grad_norm": 1.0950733423233032,
      "learning_rate": 0.00019840070911954135,
      "loss": 0.1179,
      "step": 2196
    },
    {
      "epoch": 0.010334252142581634,
      "grad_norm": 3.016165018081665,
      "learning_rate": 0.00019839976614142787,
      "loss": 0.6624,
      "step": 2197
    },
    {
      "epoch": 0.010338955944194098,
      "grad_norm": 1.2582809925079346,
      "learning_rate": 0.0001983988231633144,
      "loss": 0.1836,
      "step": 2198
    },
    {
      "epoch": 0.01034365974580656,
      "grad_norm": 0.9781376719474792,
      "learning_rate": 0.0001983978801852009,
      "loss": 0.2774,
      "step": 2199
    },
    {
      "epoch": 0.010348363547419024,
      "grad_norm": 0.7103979587554932,
      "learning_rate": 0.00019839693720708743,
      "loss": 0.078,
      "step": 2200
    },
    {
      "epoch": 0.010353067349031487,
      "grad_norm": 0.8385601043701172,
      "learning_rate": 0.00019839599422897394,
      "loss": 0.052,
      "step": 2201
    },
    {
      "epoch": 0.01035777115064395,
      "grad_norm": 1.4475435018539429,
      "learning_rate": 0.0001983950512508605,
      "loss": 0.2422,
      "step": 2202
    },
    {
      "epoch": 0.010362474952256414,
      "grad_norm": 5.164323806762695,
      "learning_rate": 0.000198394108272747,
      "loss": 0.4593,
      "step": 2203
    },
    {
      "epoch": 0.010367178753868877,
      "grad_norm": 1.3113300800323486,
      "learning_rate": 0.00019839316529463353,
      "loss": 0.1277,
      "step": 2204
    },
    {
      "epoch": 0.01037188255548134,
      "grad_norm": 1.4814456701278687,
      "learning_rate": 0.00019839222231652005,
      "loss": 0.0931,
      "step": 2205
    },
    {
      "epoch": 0.010376586357093803,
      "grad_norm": 3.630401849746704,
      "learning_rate": 0.00019839127933840656,
      "loss": 0.6633,
      "step": 2206
    },
    {
      "epoch": 0.010381290158706266,
      "grad_norm": 1.0084242820739746,
      "learning_rate": 0.00019839033636029308,
      "loss": 0.2032,
      "step": 2207
    },
    {
      "epoch": 0.01038599396031873,
      "grad_norm": 0.5265902280807495,
      "learning_rate": 0.0001983893933821796,
      "loss": 0.0515,
      "step": 2208
    },
    {
      "epoch": 0.010390697761931193,
      "grad_norm": 2.002939462661743,
      "learning_rate": 0.00019838845040406612,
      "loss": 0.1761,
      "step": 2209
    },
    {
      "epoch": 0.010395401563543656,
      "grad_norm": 1.5370640754699707,
      "learning_rate": 0.00019838750742595264,
      "loss": 0.3482,
      "step": 2210
    },
    {
      "epoch": 0.010400105365156119,
      "grad_norm": 1.8726047277450562,
      "learning_rate": 0.00019838656444783918,
      "loss": 0.252,
      "step": 2211
    },
    {
      "epoch": 0.010404809166768583,
      "grad_norm": 0.8388247489929199,
      "learning_rate": 0.0001983856214697257,
      "loss": 0.0577,
      "step": 2212
    },
    {
      "epoch": 0.010409512968381045,
      "grad_norm": 1.064494013786316,
      "learning_rate": 0.00019838467849161222,
      "loss": 0.0674,
      "step": 2213
    },
    {
      "epoch": 0.010414216769993509,
      "grad_norm": 0.46862226724624634,
      "learning_rate": 0.00019838373551349874,
      "loss": 0.0335,
      "step": 2214
    },
    {
      "epoch": 0.010418920571605973,
      "grad_norm": 3.8774707317352295,
      "learning_rate": 0.00019838279253538526,
      "loss": 0.3734,
      "step": 2215
    },
    {
      "epoch": 0.010423624373218435,
      "grad_norm": 3.4409608840942383,
      "learning_rate": 0.0001983818495572718,
      "loss": 0.4319,
      "step": 2216
    },
    {
      "epoch": 0.010428328174830899,
      "grad_norm": 3.231396436691284,
      "learning_rate": 0.00019838090657915832,
      "loss": 0.6697,
      "step": 2217
    },
    {
      "epoch": 0.01043303197644336,
      "grad_norm": 4.249982833862305,
      "learning_rate": 0.00019837996360104481,
      "loss": 0.5077,
      "step": 2218
    },
    {
      "epoch": 0.010437735778055825,
      "grad_norm": 0.6521630883216858,
      "learning_rate": 0.00019837902062293133,
      "loss": 0.0525,
      "step": 2219
    },
    {
      "epoch": 0.010442439579668289,
      "grad_norm": 1.614594578742981,
      "learning_rate": 0.00019837807764481788,
      "loss": 0.1258,
      "step": 2220
    },
    {
      "epoch": 0.01044714338128075,
      "grad_norm": 1.8819711208343506,
      "learning_rate": 0.0001983771346667044,
      "loss": 0.1257,
      "step": 2221
    },
    {
      "epoch": 0.010451847182893215,
      "grad_norm": 1.944233775138855,
      "learning_rate": 0.00019837619168859092,
      "loss": 0.1811,
      "step": 2222
    },
    {
      "epoch": 0.010456550984505677,
      "grad_norm": 4.494632720947266,
      "learning_rate": 0.00019837524871047744,
      "loss": 0.5895,
      "step": 2223
    },
    {
      "epoch": 0.01046125478611814,
      "grad_norm": 5.353002548217773,
      "learning_rate": 0.00019837430573236395,
      "loss": 0.6776,
      "step": 2224
    },
    {
      "epoch": 0.010465958587730605,
      "grad_norm": 5.2794952392578125,
      "learning_rate": 0.0001983733627542505,
      "loss": 1.2858,
      "step": 2225
    },
    {
      "epoch": 0.010470662389343067,
      "grad_norm": 7.596147537231445,
      "learning_rate": 0.00019837241977613702,
      "loss": 1.1829,
      "step": 2226
    },
    {
      "epoch": 0.01047536619095553,
      "grad_norm": 0.5854461789131165,
      "learning_rate": 0.00019837147679802354,
      "loss": 0.0328,
      "step": 2227
    },
    {
      "epoch": 0.010480069992567993,
      "grad_norm": 1.445433259010315,
      "learning_rate": 0.00019837053381991006,
      "loss": 0.142,
      "step": 2228
    },
    {
      "epoch": 0.010484773794180457,
      "grad_norm": 4.605005264282227,
      "learning_rate": 0.00019836959084179657,
      "loss": 1.1401,
      "step": 2229
    },
    {
      "epoch": 0.010489477595792919,
      "grad_norm": 1.5291510820388794,
      "learning_rate": 0.0001983686478636831,
      "loss": 0.156,
      "step": 2230
    },
    {
      "epoch": 0.010494181397405383,
      "grad_norm": 1.3625760078430176,
      "learning_rate": 0.0001983677048855696,
      "loss": 0.3044,
      "step": 2231
    },
    {
      "epoch": 0.010498885199017847,
      "grad_norm": 1.253119945526123,
      "learning_rate": 0.00019836676190745613,
      "loss": 0.1448,
      "step": 2232
    },
    {
      "epoch": 0.010503589000630309,
      "grad_norm": 0.9958257675170898,
      "learning_rate": 0.00019836581892934265,
      "loss": 0.3415,
      "step": 2233
    },
    {
      "epoch": 0.010508292802242773,
      "grad_norm": 1.328467845916748,
      "learning_rate": 0.0001983648759512292,
      "loss": 0.168,
      "step": 2234
    },
    {
      "epoch": 0.010512996603855235,
      "grad_norm": 1.0258467197418213,
      "learning_rate": 0.0001983639329731157,
      "loss": 0.1241,
      "step": 2235
    },
    {
      "epoch": 0.010517700405467699,
      "grad_norm": 1.8064836263656616,
      "learning_rate": 0.00019836298999500223,
      "loss": 0.3654,
      "step": 2236
    },
    {
      "epoch": 0.010522404207080163,
      "grad_norm": 0.5046388506889343,
      "learning_rate": 0.00019836204701688875,
      "loss": 0.1049,
      "step": 2237
    },
    {
      "epoch": 0.010527108008692625,
      "grad_norm": 2.884132146835327,
      "learning_rate": 0.00019836110403877527,
      "loss": 0.5822,
      "step": 2238
    },
    {
      "epoch": 0.010531811810305089,
      "grad_norm": 2.7762961387634277,
      "learning_rate": 0.0001983601610606618,
      "loss": 0.4521,
      "step": 2239
    },
    {
      "epoch": 0.010536515611917551,
      "grad_norm": 1.742840051651001,
      "learning_rate": 0.0001983592180825483,
      "loss": 0.2297,
      "step": 2240
    },
    {
      "epoch": 0.010541219413530015,
      "grad_norm": 1.7221167087554932,
      "learning_rate": 0.00019835827510443483,
      "loss": 0.3344,
      "step": 2241
    },
    {
      "epoch": 0.010545923215142479,
      "grad_norm": 2.0732412338256836,
      "learning_rate": 0.00019835733212632134,
      "loss": 0.4088,
      "step": 2242
    },
    {
      "epoch": 0.010550627016754941,
      "grad_norm": 0.666049063205719,
      "learning_rate": 0.0001983563891482079,
      "loss": 0.0601,
      "step": 2243
    },
    {
      "epoch": 0.010555330818367405,
      "grad_norm": 0.9373392462730408,
      "learning_rate": 0.0001983554461700944,
      "loss": 0.2521,
      "step": 2244
    },
    {
      "epoch": 0.010560034619979867,
      "grad_norm": 0.6215453743934631,
      "learning_rate": 0.00019835450319198093,
      "loss": 0.1701,
      "step": 2245
    },
    {
      "epoch": 0.010564738421592331,
      "grad_norm": 1.4363337755203247,
      "learning_rate": 0.00019835356021386745,
      "loss": 0.1535,
      "step": 2246
    },
    {
      "epoch": 0.010569442223204793,
      "grad_norm": 2.7137134075164795,
      "learning_rate": 0.00019835261723575396,
      "loss": 0.56,
      "step": 2247
    },
    {
      "epoch": 0.010574146024817257,
      "grad_norm": 3.285017967224121,
      "learning_rate": 0.0001983516742576405,
      "loss": 0.7056,
      "step": 2248
    },
    {
      "epoch": 0.010578849826429721,
      "grad_norm": 0.5522463321685791,
      "learning_rate": 0.000198350731279527,
      "loss": 0.0487,
      "step": 2249
    },
    {
      "epoch": 0.010583553628042183,
      "grad_norm": 1.2034869194030762,
      "learning_rate": 0.00019834978830141352,
      "loss": 0.1133,
      "step": 2250
    },
    {
      "epoch": 0.010588257429654647,
      "grad_norm": 5.139450550079346,
      "learning_rate": 0.00019834884532330004,
      "loss": 0.6061,
      "step": 2251
    },
    {
      "epoch": 0.01059296123126711,
      "grad_norm": 1.2349295616149902,
      "learning_rate": 0.00019834790234518658,
      "loss": 0.1435,
      "step": 2252
    },
    {
      "epoch": 0.010597665032879573,
      "grad_norm": 2.2237446308135986,
      "learning_rate": 0.0001983469593670731,
      "loss": 0.3043,
      "step": 2253
    },
    {
      "epoch": 0.010602368834492037,
      "grad_norm": 2.76901912689209,
      "learning_rate": 0.00019834601638895962,
      "loss": 0.3326,
      "step": 2254
    },
    {
      "epoch": 0.0106070726361045,
      "grad_norm": 0.4262849688529968,
      "learning_rate": 0.00019834507341084614,
      "loss": 0.0363,
      "step": 2255
    },
    {
      "epoch": 0.010611776437716963,
      "grad_norm": 4.517763137817383,
      "learning_rate": 0.00019834413043273266,
      "loss": 0.4285,
      "step": 2256
    },
    {
      "epoch": 0.010616480239329425,
      "grad_norm": 3.1228764057159424,
      "learning_rate": 0.0001983431874546192,
      "loss": 0.2053,
      "step": 2257
    },
    {
      "epoch": 0.01062118404094189,
      "grad_norm": 2.094383478164673,
      "learning_rate": 0.00019834224447650572,
      "loss": 0.3703,
      "step": 2258
    },
    {
      "epoch": 0.010625887842554353,
      "grad_norm": 1.2628334760665894,
      "learning_rate": 0.00019834130149839224,
      "loss": 0.1146,
      "step": 2259
    },
    {
      "epoch": 0.010630591644166815,
      "grad_norm": 0.9446836709976196,
      "learning_rate": 0.00019834035852027873,
      "loss": 0.0705,
      "step": 2260
    },
    {
      "epoch": 0.01063529544577928,
      "grad_norm": 2.3182289600372314,
      "learning_rate": 0.00019833941554216528,
      "loss": 0.5411,
      "step": 2261
    },
    {
      "epoch": 0.010639999247391741,
      "grad_norm": 1.5498381853103638,
      "learning_rate": 0.0001983384725640518,
      "loss": 0.3133,
      "step": 2262
    },
    {
      "epoch": 0.010644703049004205,
      "grad_norm": 3.2691140174865723,
      "learning_rate": 0.00019833752958593832,
      "loss": 0.3777,
      "step": 2263
    },
    {
      "epoch": 0.010649406850616668,
      "grad_norm": 1.0311437845230103,
      "learning_rate": 0.00019833658660782484,
      "loss": 0.1483,
      "step": 2264
    },
    {
      "epoch": 0.010654110652229131,
      "grad_norm": 1.2936441898345947,
      "learning_rate": 0.00019833564362971135,
      "loss": 0.1088,
      "step": 2265
    },
    {
      "epoch": 0.010658814453841595,
      "grad_norm": 2.858048677444458,
      "learning_rate": 0.0001983347006515979,
      "loss": 0.3584,
      "step": 2266
    },
    {
      "epoch": 0.010663518255454058,
      "grad_norm": 0.5494986772537231,
      "learning_rate": 0.00019833375767348442,
      "loss": 0.0852,
      "step": 2267
    },
    {
      "epoch": 0.010668222057066521,
      "grad_norm": 0.7419120669364929,
      "learning_rate": 0.00019833281469537094,
      "loss": 0.0999,
      "step": 2268
    },
    {
      "epoch": 0.010672925858678984,
      "grad_norm": 2.3751680850982666,
      "learning_rate": 0.00019833187171725746,
      "loss": 0.3888,
      "step": 2269
    },
    {
      "epoch": 0.010677629660291447,
      "grad_norm": 2.1402692794799805,
      "learning_rate": 0.00019833092873914397,
      "loss": 0.4633,
      "step": 2270
    },
    {
      "epoch": 0.010682333461903911,
      "grad_norm": 0.8376036882400513,
      "learning_rate": 0.0001983299857610305,
      "loss": 0.0985,
      "step": 2271
    },
    {
      "epoch": 0.010687037263516374,
      "grad_norm": 2.7211997509002686,
      "learning_rate": 0.000198329042782917,
      "loss": 0.4823,
      "step": 2272
    },
    {
      "epoch": 0.010691741065128837,
      "grad_norm": 3.8785595893859863,
      "learning_rate": 0.00019832809980480353,
      "loss": 0.6579,
      "step": 2273
    },
    {
      "epoch": 0.0106964448667413,
      "grad_norm": 3.101792335510254,
      "learning_rate": 0.00019832715682669005,
      "loss": 0.3204,
      "step": 2274
    },
    {
      "epoch": 0.010701148668353764,
      "grad_norm": 4.892071723937988,
      "learning_rate": 0.0001983262138485766,
      "loss": 0.5226,
      "step": 2275
    },
    {
      "epoch": 0.010705852469966227,
      "grad_norm": 0.6321303248405457,
      "learning_rate": 0.0001983252708704631,
      "loss": 0.0733,
      "step": 2276
    },
    {
      "epoch": 0.01071055627157869,
      "grad_norm": 4.521658897399902,
      "learning_rate": 0.00019832432789234963,
      "loss": 1.022,
      "step": 2277
    },
    {
      "epoch": 0.010715260073191154,
      "grad_norm": 2.4365360736846924,
      "learning_rate": 0.00019832338491423615,
      "loss": 0.2983,
      "step": 2278
    },
    {
      "epoch": 0.010719963874803616,
      "grad_norm": 2.7868492603302,
      "learning_rate": 0.00019832244193612267,
      "loss": 0.3953,
      "step": 2279
    },
    {
      "epoch": 0.01072466767641608,
      "grad_norm": 2.063406467437744,
      "learning_rate": 0.0001983214989580092,
      "loss": 0.1964,
      "step": 2280
    },
    {
      "epoch": 0.010729371478028544,
      "grad_norm": 5.045990467071533,
      "learning_rate": 0.0001983205559798957,
      "loss": 0.5406,
      "step": 2281
    },
    {
      "epoch": 0.010734075279641006,
      "grad_norm": 1.7656006813049316,
      "learning_rate": 0.00019831961300178223,
      "loss": 0.1507,
      "step": 2282
    },
    {
      "epoch": 0.01073877908125347,
      "grad_norm": 1.090066909790039,
      "learning_rate": 0.00019831867002366874,
      "loss": 0.0935,
      "step": 2283
    },
    {
      "epoch": 0.010743482882865932,
      "grad_norm": 2.3441061973571777,
      "learning_rate": 0.0001983177270455553,
      "loss": 0.5836,
      "step": 2284
    },
    {
      "epoch": 0.010748186684478396,
      "grad_norm": 1.0968612432479858,
      "learning_rate": 0.0001983167840674418,
      "loss": 0.1043,
      "step": 2285
    },
    {
      "epoch": 0.010752890486090858,
      "grad_norm": 0.7230786681175232,
      "learning_rate": 0.00019831584108932833,
      "loss": 0.0882,
      "step": 2286
    },
    {
      "epoch": 0.010757594287703322,
      "grad_norm": 3.4900870323181152,
      "learning_rate": 0.00019831489811121485,
      "loss": 1.1402,
      "step": 2287
    },
    {
      "epoch": 0.010762298089315786,
      "grad_norm": 2.9745328426361084,
      "learning_rate": 0.0001983139551331014,
      "loss": 0.5967,
      "step": 2288
    },
    {
      "epoch": 0.010767001890928248,
      "grad_norm": 1.0975507497787476,
      "learning_rate": 0.0001983130121549879,
      "loss": 0.1137,
      "step": 2289
    },
    {
      "epoch": 0.010771705692540712,
      "grad_norm": 2.3168392181396484,
      "learning_rate": 0.00019831206917687443,
      "loss": 0.3539,
      "step": 2290
    },
    {
      "epoch": 0.010776409494153174,
      "grad_norm": 0.7727434635162354,
      "learning_rate": 0.00019831112619876092,
      "loss": 0.0784,
      "step": 2291
    },
    {
      "epoch": 0.010781113295765638,
      "grad_norm": 1.4732143878936768,
      "learning_rate": 0.00019831018322064744,
      "loss": 0.181,
      "step": 2292
    },
    {
      "epoch": 0.010785817097378102,
      "grad_norm": 0.44743579626083374,
      "learning_rate": 0.00019830924024253398,
      "loss": 0.0459,
      "step": 2293
    },
    {
      "epoch": 0.010790520898990564,
      "grad_norm": 0.32153189182281494,
      "learning_rate": 0.0001983082972644205,
      "loss": 0.0253,
      "step": 2294
    },
    {
      "epoch": 0.010795224700603028,
      "grad_norm": 1.2477411031723022,
      "learning_rate": 0.00019830735428630702,
      "loss": 0.0962,
      "step": 2295
    },
    {
      "epoch": 0.01079992850221549,
      "grad_norm": 2.8735485076904297,
      "learning_rate": 0.00019830641130819354,
      "loss": 0.686,
      "step": 2296
    },
    {
      "epoch": 0.010804632303827954,
      "grad_norm": 2.4542465209960938,
      "learning_rate": 0.00019830546833008006,
      "loss": 0.39,
      "step": 2297
    },
    {
      "epoch": 0.010809336105440418,
      "grad_norm": 0.6663770079612732,
      "learning_rate": 0.0001983045253519666,
      "loss": 0.0585,
      "step": 2298
    },
    {
      "epoch": 0.01081403990705288,
      "grad_norm": 4.828481197357178,
      "learning_rate": 0.00019830358237385312,
      "loss": 1.0941,
      "step": 2299
    },
    {
      "epoch": 0.010818743708665344,
      "grad_norm": 1.1886368989944458,
      "learning_rate": 0.00019830263939573964,
      "loss": 0.1569,
      "step": 2300
    },
    {
      "epoch": 0.010823447510277806,
      "grad_norm": 3.6031205654144287,
      "learning_rate": 0.00019830169641762616,
      "loss": 0.6655,
      "step": 2301
    },
    {
      "epoch": 0.01082815131189027,
      "grad_norm": 1.7570794820785522,
      "learning_rate": 0.00019830075343951268,
      "loss": 0.132,
      "step": 2302
    },
    {
      "epoch": 0.010832855113502732,
      "grad_norm": 2.3901381492614746,
      "learning_rate": 0.0001982998104613992,
      "loss": 0.4656,
      "step": 2303
    },
    {
      "epoch": 0.010837558915115196,
      "grad_norm": 6.5319414138793945,
      "learning_rate": 0.00019829886748328572,
      "loss": 0.5951,
      "step": 2304
    },
    {
      "epoch": 0.01084226271672766,
      "grad_norm": 1.5676621198654175,
      "learning_rate": 0.00019829792450517224,
      "loss": 0.1929,
      "step": 2305
    },
    {
      "epoch": 0.010846966518340122,
      "grad_norm": 2.4520063400268555,
      "learning_rate": 0.00019829698152705875,
      "loss": 0.4263,
      "step": 2306
    },
    {
      "epoch": 0.010851670319952586,
      "grad_norm": 2.413917064666748,
      "learning_rate": 0.0001982960385489453,
      "loss": 0.3205,
      "step": 2307
    },
    {
      "epoch": 0.010856374121565048,
      "grad_norm": 3.0621469020843506,
      "learning_rate": 0.00019829509557083182,
      "loss": 0.4742,
      "step": 2308
    },
    {
      "epoch": 0.010861077923177512,
      "grad_norm": 1.6167715787887573,
      "learning_rate": 0.00019829415259271834,
      "loss": 0.2437,
      "step": 2309
    },
    {
      "epoch": 0.010865781724789976,
      "grad_norm": 2.0194549560546875,
      "learning_rate": 0.00019829320961460486,
      "loss": 0.4913,
      "step": 2310
    },
    {
      "epoch": 0.010870485526402438,
      "grad_norm": 0.8005443811416626,
      "learning_rate": 0.00019829226663649137,
      "loss": 0.0557,
      "step": 2311
    },
    {
      "epoch": 0.010875189328014902,
      "grad_norm": 4.919615745544434,
      "learning_rate": 0.0001982913236583779,
      "loss": 0.3151,
      "step": 2312
    },
    {
      "epoch": 0.010879893129627364,
      "grad_norm": 1.4404816627502441,
      "learning_rate": 0.0001982903806802644,
      "loss": 0.18,
      "step": 2313
    },
    {
      "epoch": 0.010884596931239828,
      "grad_norm": 6.4715800285339355,
      "learning_rate": 0.00019828943770215093,
      "loss": 1.0841,
      "step": 2314
    },
    {
      "epoch": 0.010889300732852292,
      "grad_norm": 2.0656216144561768,
      "learning_rate": 0.00019828849472403745,
      "loss": 0.3754,
      "step": 2315
    },
    {
      "epoch": 0.010894004534464754,
      "grad_norm": 4.295805931091309,
      "learning_rate": 0.000198287551745924,
      "loss": 0.6809,
      "step": 2316
    },
    {
      "epoch": 0.010898708336077218,
      "grad_norm": 2.8468234539031982,
      "learning_rate": 0.0001982866087678105,
      "loss": 0.849,
      "step": 2317
    },
    {
      "epoch": 0.01090341213768968,
      "grad_norm": 1.3094816207885742,
      "learning_rate": 0.00019828566578969703,
      "loss": 0.2958,
      "step": 2318
    },
    {
      "epoch": 0.010908115939302144,
      "grad_norm": 1.1059147119522095,
      "learning_rate": 0.00019828472281158355,
      "loss": 0.1696,
      "step": 2319
    },
    {
      "epoch": 0.010912819740914606,
      "grad_norm": 0.6295298933982849,
      "learning_rate": 0.0001982837798334701,
      "loss": 0.0548,
      "step": 2320
    },
    {
      "epoch": 0.01091752354252707,
      "grad_norm": 2.1237356662750244,
      "learning_rate": 0.00019828283685535661,
      "loss": 0.4671,
      "step": 2321
    },
    {
      "epoch": 0.010922227344139534,
      "grad_norm": 1.5812463760375977,
      "learning_rate": 0.0001982818938772431,
      "loss": 0.4383,
      "step": 2322
    },
    {
      "epoch": 0.010926931145751996,
      "grad_norm": 1.019834041595459,
      "learning_rate": 0.00019828095089912963,
      "loss": 0.164,
      "step": 2323
    },
    {
      "epoch": 0.01093163494736446,
      "grad_norm": 1.444732904434204,
      "learning_rate": 0.00019828000792101614,
      "loss": 0.3318,
      "step": 2324
    },
    {
      "epoch": 0.010936338748976922,
      "grad_norm": 1.5405269861221313,
      "learning_rate": 0.0001982790649429027,
      "loss": 0.4103,
      "step": 2325
    },
    {
      "epoch": 0.010941042550589386,
      "grad_norm": 1.6571656465530396,
      "learning_rate": 0.0001982781219647892,
      "loss": 0.2911,
      "step": 2326
    },
    {
      "epoch": 0.01094574635220185,
      "grad_norm": 0.9713258743286133,
      "learning_rate": 0.00019827717898667573,
      "loss": 0.1389,
      "step": 2327
    },
    {
      "epoch": 0.010950450153814312,
      "grad_norm": 0.877895712852478,
      "learning_rate": 0.00019827623600856225,
      "loss": 0.0972,
      "step": 2328
    },
    {
      "epoch": 0.010955153955426776,
      "grad_norm": 1.0739057064056396,
      "learning_rate": 0.0001982752930304488,
      "loss": 0.3225,
      "step": 2329
    },
    {
      "epoch": 0.010959857757039239,
      "grad_norm": 0.6686445474624634,
      "learning_rate": 0.0001982743500523353,
      "loss": 0.0657,
      "step": 2330
    },
    {
      "epoch": 0.010964561558651702,
      "grad_norm": 0.6065624952316284,
      "learning_rate": 0.00019827340707422183,
      "loss": 0.1085,
      "step": 2331
    },
    {
      "epoch": 0.010969265360264166,
      "grad_norm": 1.413105845451355,
      "learning_rate": 0.00019827246409610835,
      "loss": 0.4142,
      "step": 2332
    },
    {
      "epoch": 0.010973969161876629,
      "grad_norm": 4.2737860679626465,
      "learning_rate": 0.00019827152111799484,
      "loss": 0.8687,
      "step": 2333
    },
    {
      "epoch": 0.010978672963489092,
      "grad_norm": 0.8784523606300354,
      "learning_rate": 0.00019827057813988138,
      "loss": 0.1789,
      "step": 2334
    },
    {
      "epoch": 0.010983376765101555,
      "grad_norm": 2.8121273517608643,
      "learning_rate": 0.0001982696351617679,
      "loss": 0.7468,
      "step": 2335
    },
    {
      "epoch": 0.010988080566714018,
      "grad_norm": 2.0546607971191406,
      "learning_rate": 0.00019826869218365442,
      "loss": 0.4957,
      "step": 2336
    },
    {
      "epoch": 0.01099278436832648,
      "grad_norm": 0.7807263135910034,
      "learning_rate": 0.00019826774920554094,
      "loss": 0.0869,
      "step": 2337
    },
    {
      "epoch": 0.010997488169938945,
      "grad_norm": 1.3851896524429321,
      "learning_rate": 0.00019826680622742749,
      "loss": 0.1565,
      "step": 2338
    },
    {
      "epoch": 0.011002191971551408,
      "grad_norm": 2.1161248683929443,
      "learning_rate": 0.000198265863249314,
      "loss": 0.2387,
      "step": 2339
    },
    {
      "epoch": 0.01100689577316387,
      "grad_norm": 1.5731797218322754,
      "learning_rate": 0.00019826492027120052,
      "loss": 0.3463,
      "step": 2340
    },
    {
      "epoch": 0.011011599574776335,
      "grad_norm": 1.1750551462173462,
      "learning_rate": 0.00019826397729308704,
      "loss": 0.2258,
      "step": 2341
    },
    {
      "epoch": 0.011016303376388797,
      "grad_norm": 4.470129489898682,
      "learning_rate": 0.00019826303431497356,
      "loss": 1.0554,
      "step": 2342
    },
    {
      "epoch": 0.01102100717800126,
      "grad_norm": 0.2530858516693115,
      "learning_rate": 0.00019826209133686008,
      "loss": 0.01,
      "step": 2343
    },
    {
      "epoch": 0.011025710979613725,
      "grad_norm": 0.14074748754501343,
      "learning_rate": 0.0001982611483587466,
      "loss": 0.0106,
      "step": 2344
    },
    {
      "epoch": 0.011030414781226187,
      "grad_norm": 1.2208750247955322,
      "learning_rate": 0.00019826020538063312,
      "loss": 0.3124,
      "step": 2345
    },
    {
      "epoch": 0.01103511858283865,
      "grad_norm": 2.6698391437530518,
      "learning_rate": 0.00019825926240251964,
      "loss": 0.8546,
      "step": 2346
    },
    {
      "epoch": 0.011039822384451113,
      "grad_norm": 2.26369309425354,
      "learning_rate": 0.00019825831942440615,
      "loss": 0.4247,
      "step": 2347
    },
    {
      "epoch": 0.011044526186063577,
      "grad_norm": 0.4017873704433441,
      "learning_rate": 0.0001982573764462927,
      "loss": 0.0366,
      "step": 2348
    },
    {
      "epoch": 0.01104922998767604,
      "grad_norm": 1.1896064281463623,
      "learning_rate": 0.00019825643346817922,
      "loss": 0.2412,
      "step": 2349
    },
    {
      "epoch": 0.011053933789288503,
      "grad_norm": 0.8668099641799927,
      "learning_rate": 0.00019825549049006574,
      "loss": 0.2036,
      "step": 2350
    },
    {
      "epoch": 0.011058637590900967,
      "grad_norm": 1.1250098943710327,
      "learning_rate": 0.00019825454751195226,
      "loss": 0.0957,
      "step": 2351
    },
    {
      "epoch": 0.011063341392513429,
      "grad_norm": 0.8360164165496826,
      "learning_rate": 0.0001982536045338388,
      "loss": 0.1965,
      "step": 2352
    },
    {
      "epoch": 0.011068045194125893,
      "grad_norm": 1.5970449447631836,
      "learning_rate": 0.0001982526615557253,
      "loss": 0.145,
      "step": 2353
    },
    {
      "epoch": 0.011072748995738355,
      "grad_norm": 1.226471185684204,
      "learning_rate": 0.0001982517185776118,
      "loss": 0.149,
      "step": 2354
    },
    {
      "epoch": 0.011077452797350819,
      "grad_norm": 2.733546018600464,
      "learning_rate": 0.00019825077559949833,
      "loss": 0.5943,
      "step": 2355
    },
    {
      "epoch": 0.011082156598963283,
      "grad_norm": 2.5023913383483887,
      "learning_rate": 0.00019824983262138485,
      "loss": 0.5793,
      "step": 2356
    },
    {
      "epoch": 0.011086860400575745,
      "grad_norm": 2.261408567428589,
      "learning_rate": 0.0001982488896432714,
      "loss": 0.4823,
      "step": 2357
    },
    {
      "epoch": 0.011091564202188209,
      "grad_norm": 0.7210854887962341,
      "learning_rate": 0.0001982479466651579,
      "loss": 0.088,
      "step": 2358
    },
    {
      "epoch": 0.011096268003800671,
      "grad_norm": 0.7294961214065552,
      "learning_rate": 0.00019824700368704443,
      "loss": 0.1369,
      "step": 2359
    },
    {
      "epoch": 0.011100971805413135,
      "grad_norm": 1.018038272857666,
      "learning_rate": 0.00019824606070893095,
      "loss": 0.2235,
      "step": 2360
    },
    {
      "epoch": 0.011105675607025599,
      "grad_norm": 1.3235948085784912,
      "learning_rate": 0.0001982451177308175,
      "loss": 0.225,
      "step": 2361
    },
    {
      "epoch": 0.011110379408638061,
      "grad_norm": 1.6032743453979492,
      "learning_rate": 0.00019824417475270401,
      "loss": 0.3398,
      "step": 2362
    },
    {
      "epoch": 0.011115083210250525,
      "grad_norm": 0.4874812960624695,
      "learning_rate": 0.00019824323177459053,
      "loss": 0.0723,
      "step": 2363
    },
    {
      "epoch": 0.011119787011862987,
      "grad_norm": 1.7143499851226807,
      "learning_rate": 0.00019824228879647702,
      "loss": 0.212,
      "step": 2364
    },
    {
      "epoch": 0.011124490813475451,
      "grad_norm": 0.7605493664741516,
      "learning_rate": 0.00019824134581836354,
      "loss": 0.0758,
      "step": 2365
    },
    {
      "epoch": 0.011129194615087915,
      "grad_norm": 2.269376039505005,
      "learning_rate": 0.0001982404028402501,
      "loss": 0.3953,
      "step": 2366
    },
    {
      "epoch": 0.011133898416700377,
      "grad_norm": 0.3896499276161194,
      "learning_rate": 0.0001982394598621366,
      "loss": 0.0289,
      "step": 2367
    },
    {
      "epoch": 0.011138602218312841,
      "grad_norm": 3.5189032554626465,
      "learning_rate": 0.00019823851688402313,
      "loss": 0.6174,
      "step": 2368
    },
    {
      "epoch": 0.011143306019925303,
      "grad_norm": 1.2590949535369873,
      "learning_rate": 0.00019823757390590965,
      "loss": 0.1438,
      "step": 2369
    },
    {
      "epoch": 0.011148009821537767,
      "grad_norm": 2.3062338829040527,
      "learning_rate": 0.0001982366309277962,
      "loss": 0.4504,
      "step": 2370
    },
    {
      "epoch": 0.01115271362315023,
      "grad_norm": 3.1734118461608887,
      "learning_rate": 0.0001982356879496827,
      "loss": 0.8319,
      "step": 2371
    },
    {
      "epoch": 0.011157417424762693,
      "grad_norm": 2.6659317016601562,
      "learning_rate": 0.00019823474497156923,
      "loss": 0.4855,
      "step": 2372
    },
    {
      "epoch": 0.011162121226375157,
      "grad_norm": 0.16507527232170105,
      "learning_rate": 0.00019823380199345575,
      "loss": 0.0139,
      "step": 2373
    },
    {
      "epoch": 0.01116682502798762,
      "grad_norm": 2.3460476398468018,
      "learning_rate": 0.00019823285901534227,
      "loss": 0.3278,
      "step": 2374
    },
    {
      "epoch": 0.011171528829600083,
      "grad_norm": 2.8664164543151855,
      "learning_rate": 0.00019823191603722878,
      "loss": 0.463,
      "step": 2375
    },
    {
      "epoch": 0.011176232631212545,
      "grad_norm": 0.8181930184364319,
      "learning_rate": 0.0001982309730591153,
      "loss": 0.1282,
      "step": 2376
    },
    {
      "epoch": 0.01118093643282501,
      "grad_norm": 3.4771728515625,
      "learning_rate": 0.00019823003008100182,
      "loss": 0.8785,
      "step": 2377
    },
    {
      "epoch": 0.011185640234437473,
      "grad_norm": 4.112370014190674,
      "learning_rate": 0.00019822908710288834,
      "loss": 0.9963,
      "step": 2378
    },
    {
      "epoch": 0.011190344036049935,
      "grad_norm": 0.25966572761535645,
      "learning_rate": 0.00019822814412477489,
      "loss": 0.0203,
      "step": 2379
    },
    {
      "epoch": 0.0111950478376624,
      "grad_norm": 1.8929563760757446,
      "learning_rate": 0.0001982272011466614,
      "loss": 0.4311,
      "step": 2380
    },
    {
      "epoch": 0.011199751639274861,
      "grad_norm": 1.4959379434585571,
      "learning_rate": 0.00019822625816854792,
      "loss": 0.2031,
      "step": 2381
    },
    {
      "epoch": 0.011204455440887325,
      "grad_norm": 1.714566946029663,
      "learning_rate": 0.00019822531519043444,
      "loss": 0.4685,
      "step": 2382
    },
    {
      "epoch": 0.01120915924249979,
      "grad_norm": 2.516655683517456,
      "learning_rate": 0.00019822437221232096,
      "loss": 0.4407,
      "step": 2383
    },
    {
      "epoch": 0.011213863044112251,
      "grad_norm": 1.1816580295562744,
      "learning_rate": 0.00019822342923420748,
      "loss": 0.2606,
      "step": 2384
    },
    {
      "epoch": 0.011218566845724715,
      "grad_norm": 0.5628942251205444,
      "learning_rate": 0.000198222486256094,
      "loss": 0.0558,
      "step": 2385
    },
    {
      "epoch": 0.011223270647337177,
      "grad_norm": 0.6653311252593994,
      "learning_rate": 0.00019822154327798052,
      "loss": 0.2831,
      "step": 2386
    },
    {
      "epoch": 0.011227974448949641,
      "grad_norm": 0.8269336223602295,
      "learning_rate": 0.00019822060029986704,
      "loss": 0.1423,
      "step": 2387
    },
    {
      "epoch": 0.011232678250562103,
      "grad_norm": 0.6823490262031555,
      "learning_rate": 0.00019821965732175358,
      "loss": 0.1126,
      "step": 2388
    },
    {
      "epoch": 0.011237382052174567,
      "grad_norm": 0.9608281254768372,
      "learning_rate": 0.0001982187143436401,
      "loss": 0.134,
      "step": 2389
    },
    {
      "epoch": 0.011242085853787031,
      "grad_norm": 3.5793581008911133,
      "learning_rate": 0.00019821777136552662,
      "loss": 0.6411,
      "step": 2390
    },
    {
      "epoch": 0.011246789655399493,
      "grad_norm": 0.7091776132583618,
      "learning_rate": 0.00019821682838741314,
      "loss": 0.0885,
      "step": 2391
    },
    {
      "epoch": 0.011251493457011957,
      "grad_norm": 0.49099960923194885,
      "learning_rate": 0.00019821588540929966,
      "loss": 0.0821,
      "step": 2392
    },
    {
      "epoch": 0.01125619725862442,
      "grad_norm": 0.47901439666748047,
      "learning_rate": 0.0001982149424311862,
      "loss": 0.055,
      "step": 2393
    },
    {
      "epoch": 0.011260901060236883,
      "grad_norm": 0.8624131679534912,
      "learning_rate": 0.00019821399945307272,
      "loss": 0.2378,
      "step": 2394
    },
    {
      "epoch": 0.011265604861849347,
      "grad_norm": 0.6637465953826904,
      "learning_rate": 0.0001982130564749592,
      "loss": 0.0749,
      "step": 2395
    },
    {
      "epoch": 0.01127030866346181,
      "grad_norm": 1.99415922164917,
      "learning_rate": 0.00019821211349684573,
      "loss": 0.2896,
      "step": 2396
    },
    {
      "epoch": 0.011275012465074273,
      "grad_norm": 1.721381664276123,
      "learning_rate": 0.00019821117051873225,
      "loss": 0.3593,
      "step": 2397
    },
    {
      "epoch": 0.011279716266686736,
      "grad_norm": 1.808576226234436,
      "learning_rate": 0.0001982102275406188,
      "loss": 0.1586,
      "step": 2398
    },
    {
      "epoch": 0.0112844200682992,
      "grad_norm": 1.8755377531051636,
      "learning_rate": 0.0001982092845625053,
      "loss": 0.4083,
      "step": 2399
    },
    {
      "epoch": 0.011289123869911663,
      "grad_norm": 0.4787181615829468,
      "learning_rate": 0.00019820834158439183,
      "loss": 0.052,
      "step": 2400
    },
    {
      "epoch": 0.011293827671524126,
      "grad_norm": 2.7263472080230713,
      "learning_rate": 0.00019820739860627835,
      "loss": 0.1348,
      "step": 2401
    },
    {
      "epoch": 0.01129853147313659,
      "grad_norm": 1.1657806634902954,
      "learning_rate": 0.0001982064556281649,
      "loss": 0.1045,
      "step": 2402
    },
    {
      "epoch": 0.011303235274749052,
      "grad_norm": 1.583101511001587,
      "learning_rate": 0.00019820551265005141,
      "loss": 0.1476,
      "step": 2403
    },
    {
      "epoch": 0.011307939076361516,
      "grad_norm": 2.3032009601593018,
      "learning_rate": 0.00019820456967193793,
      "loss": 0.4611,
      "step": 2404
    },
    {
      "epoch": 0.011312642877973978,
      "grad_norm": 2.1254589557647705,
      "learning_rate": 0.00019820362669382445,
      "loss": 0.2273,
      "step": 2405
    },
    {
      "epoch": 0.011317346679586442,
      "grad_norm": 1.8613847494125366,
      "learning_rate": 0.00019820268371571097,
      "loss": 0.274,
      "step": 2406
    },
    {
      "epoch": 0.011322050481198906,
      "grad_norm": 2.3091914653778076,
      "learning_rate": 0.0001982017407375975,
      "loss": 0.3297,
      "step": 2407
    },
    {
      "epoch": 0.011326754282811368,
      "grad_norm": 0.8390915393829346,
      "learning_rate": 0.000198200797759484,
      "loss": 0.0618,
      "step": 2408
    },
    {
      "epoch": 0.011331458084423832,
      "grad_norm": 4.115633010864258,
      "learning_rate": 0.00019819985478137053,
      "loss": 1.2298,
      "step": 2409
    },
    {
      "epoch": 0.011336161886036294,
      "grad_norm": 2.463996648788452,
      "learning_rate": 0.00019819891180325705,
      "loss": 0.1973,
      "step": 2410
    },
    {
      "epoch": 0.011340865687648758,
      "grad_norm": 0.9465429782867432,
      "learning_rate": 0.0001981979688251436,
      "loss": 0.0539,
      "step": 2411
    },
    {
      "epoch": 0.011345569489261222,
      "grad_norm": 3.7729549407958984,
      "learning_rate": 0.0001981970258470301,
      "loss": 0.5819,
      "step": 2412
    },
    {
      "epoch": 0.011350273290873684,
      "grad_norm": 2.1740963459014893,
      "learning_rate": 0.00019819608286891663,
      "loss": 0.5208,
      "step": 2413
    },
    {
      "epoch": 0.011354977092486148,
      "grad_norm": 0.9803938269615173,
      "learning_rate": 0.00019819513989080315,
      "loss": 0.1015,
      "step": 2414
    },
    {
      "epoch": 0.01135968089409861,
      "grad_norm": 2.3178765773773193,
      "learning_rate": 0.00019819419691268967,
      "loss": 0.4023,
      "step": 2415
    },
    {
      "epoch": 0.011364384695711074,
      "grad_norm": 0.725051760673523,
      "learning_rate": 0.00019819325393457618,
      "loss": 0.0285,
      "step": 2416
    },
    {
      "epoch": 0.011369088497323538,
      "grad_norm": 1.932142972946167,
      "learning_rate": 0.0001981923109564627,
      "loss": 0.3331,
      "step": 2417
    },
    {
      "epoch": 0.011373792298936,
      "grad_norm": 0.9271976947784424,
      "learning_rate": 0.00019819136797834922,
      "loss": 0.0692,
      "step": 2418
    },
    {
      "epoch": 0.011378496100548464,
      "grad_norm": 2.34016489982605,
      "learning_rate": 0.00019819042500023574,
      "loss": 0.2778,
      "step": 2419
    },
    {
      "epoch": 0.011383199902160926,
      "grad_norm": 1.2882640361785889,
      "learning_rate": 0.00019818948202212229,
      "loss": 0.2078,
      "step": 2420
    },
    {
      "epoch": 0.01138790370377339,
      "grad_norm": 1.017608642578125,
      "learning_rate": 0.0001981885390440088,
      "loss": 0.1438,
      "step": 2421
    },
    {
      "epoch": 0.011392607505385852,
      "grad_norm": 1.7727071046829224,
      "learning_rate": 0.00019818759606589532,
      "loss": 0.1791,
      "step": 2422
    },
    {
      "epoch": 0.011397311306998316,
      "grad_norm": 2.13043212890625,
      "learning_rate": 0.00019818665308778184,
      "loss": 0.2839,
      "step": 2423
    },
    {
      "epoch": 0.01140201510861078,
      "grad_norm": 1.5942827463150024,
      "learning_rate": 0.00019818571010966836,
      "loss": 0.2251,
      "step": 2424
    },
    {
      "epoch": 0.011406718910223242,
      "grad_norm": 2.6545307636260986,
      "learning_rate": 0.0001981847671315549,
      "loss": 0.3676,
      "step": 2425
    },
    {
      "epoch": 0.011411422711835706,
      "grad_norm": 2.3149898052215576,
      "learning_rate": 0.0001981838241534414,
      "loss": 0.2313,
      "step": 2426
    },
    {
      "epoch": 0.011416126513448168,
      "grad_norm": 1.05778968334198,
      "learning_rate": 0.00019818288117532792,
      "loss": 0.0891,
      "step": 2427
    },
    {
      "epoch": 0.011420830315060632,
      "grad_norm": 3.2445197105407715,
      "learning_rate": 0.00019818193819721444,
      "loss": 0.582,
      "step": 2428
    },
    {
      "epoch": 0.011425534116673096,
      "grad_norm": 1.183402419090271,
      "learning_rate": 0.00019818099521910098,
      "loss": 0.0932,
      "step": 2429
    },
    {
      "epoch": 0.011430237918285558,
      "grad_norm": 4.862755298614502,
      "learning_rate": 0.0001981800522409875,
      "loss": 1.2854,
      "step": 2430
    },
    {
      "epoch": 0.011434941719898022,
      "grad_norm": 1.8874727487564087,
      "learning_rate": 0.00019817910926287402,
      "loss": 0.2314,
      "step": 2431
    },
    {
      "epoch": 0.011439645521510484,
      "grad_norm": 2.177607297897339,
      "learning_rate": 0.00019817816628476054,
      "loss": 0.2936,
      "step": 2432
    },
    {
      "epoch": 0.011444349323122948,
      "grad_norm": 0.7705857753753662,
      "learning_rate": 0.00019817722330664706,
      "loss": 0.0501,
      "step": 2433
    },
    {
      "epoch": 0.011449053124735412,
      "grad_norm": 1.563193678855896,
      "learning_rate": 0.0001981762803285336,
      "loss": 0.1661,
      "step": 2434
    },
    {
      "epoch": 0.011453756926347874,
      "grad_norm": 1.7085477113723755,
      "learning_rate": 0.00019817533735042012,
      "loss": 0.1512,
      "step": 2435
    },
    {
      "epoch": 0.011458460727960338,
      "grad_norm": 0.06748808175325394,
      "learning_rate": 0.00019817439437230664,
      "loss": 0.0039,
      "step": 2436
    },
    {
      "epoch": 0.0114631645295728,
      "grad_norm": 4.437750339508057,
      "learning_rate": 0.00019817345139419316,
      "loss": 0.1773,
      "step": 2437
    },
    {
      "epoch": 0.011467868331185264,
      "grad_norm": 3.445072650909424,
      "learning_rate": 0.00019817250841607968,
      "loss": 0.5788,
      "step": 2438
    },
    {
      "epoch": 0.011472572132797726,
      "grad_norm": 6.441549777984619,
      "learning_rate": 0.0001981715654379662,
      "loss": 0.7851,
      "step": 2439
    },
    {
      "epoch": 0.01147727593441019,
      "grad_norm": 2.765779972076416,
      "learning_rate": 0.0001981706224598527,
      "loss": 0.5324,
      "step": 2440
    },
    {
      "epoch": 0.011481979736022654,
      "grad_norm": 2.3779823780059814,
      "learning_rate": 0.00019816967948173923,
      "loss": 0.3083,
      "step": 2441
    },
    {
      "epoch": 0.011486683537635116,
      "grad_norm": 2.3862905502319336,
      "learning_rate": 0.00019816873650362575,
      "loss": 0.307,
      "step": 2442
    },
    {
      "epoch": 0.01149138733924758,
      "grad_norm": 1.130448579788208,
      "learning_rate": 0.0001981677935255123,
      "loss": 0.1212,
      "step": 2443
    },
    {
      "epoch": 0.011496091140860042,
      "grad_norm": 2.4130194187164307,
      "learning_rate": 0.00019816685054739881,
      "loss": 0.4241,
      "step": 2444
    },
    {
      "epoch": 0.011500794942472506,
      "grad_norm": 1.5404860973358154,
      "learning_rate": 0.00019816590756928533,
      "loss": 0.137,
      "step": 2445
    },
    {
      "epoch": 0.01150549874408497,
      "grad_norm": 0.9718731045722961,
      "learning_rate": 0.00019816496459117185,
      "loss": 0.1057,
      "step": 2446
    },
    {
      "epoch": 0.011510202545697432,
      "grad_norm": 1.2687417268753052,
      "learning_rate": 0.00019816402161305837,
      "loss": 0.3036,
      "step": 2447
    },
    {
      "epoch": 0.011514906347309896,
      "grad_norm": 1.424466609954834,
      "learning_rate": 0.0001981630786349449,
      "loss": 0.2281,
      "step": 2448
    },
    {
      "epoch": 0.011519610148922358,
      "grad_norm": 1.593610167503357,
      "learning_rate": 0.0001981621356568314,
      "loss": 0.2233,
      "step": 2449
    },
    {
      "epoch": 0.011524313950534822,
      "grad_norm": 0.9686912298202515,
      "learning_rate": 0.00019816119267871793,
      "loss": 0.2016,
      "step": 2450
    },
    {
      "epoch": 0.011529017752147286,
      "grad_norm": 0.9448218941688538,
      "learning_rate": 0.00019816024970060445,
      "loss": 0.1284,
      "step": 2451
    },
    {
      "epoch": 0.011533721553759748,
      "grad_norm": 3.778312921524048,
      "learning_rate": 0.000198159306722491,
      "loss": 0.384,
      "step": 2452
    },
    {
      "epoch": 0.011538425355372212,
      "grad_norm": 1.1346240043640137,
      "learning_rate": 0.0001981583637443775,
      "loss": 0.0931,
      "step": 2453
    },
    {
      "epoch": 0.011543129156984674,
      "grad_norm": 2.1593685150146484,
      "learning_rate": 0.00019815742076626403,
      "loss": 0.3897,
      "step": 2454
    },
    {
      "epoch": 0.011547832958597138,
      "grad_norm": 1.1922920942306519,
      "learning_rate": 0.00019815647778815055,
      "loss": 0.1123,
      "step": 2455
    },
    {
      "epoch": 0.0115525367602096,
      "grad_norm": 1.4532833099365234,
      "learning_rate": 0.00019815553481003707,
      "loss": 0.1112,
      "step": 2456
    },
    {
      "epoch": 0.011557240561822064,
      "grad_norm": 3.6037628650665283,
      "learning_rate": 0.00019815459183192358,
      "loss": 0.4255,
      "step": 2457
    },
    {
      "epoch": 0.011561944363434528,
      "grad_norm": 3.075120687484741,
      "learning_rate": 0.0001981536488538101,
      "loss": 0.3082,
      "step": 2458
    },
    {
      "epoch": 0.01156664816504699,
      "grad_norm": 2.0735361576080322,
      "learning_rate": 0.00019815270587569662,
      "loss": 0.3722,
      "step": 2459
    },
    {
      "epoch": 0.011571351966659454,
      "grad_norm": 1.2325680255889893,
      "learning_rate": 0.00019815176289758314,
      "loss": 0.1032,
      "step": 2460
    },
    {
      "epoch": 0.011576055768271917,
      "grad_norm": 1.8928576707839966,
      "learning_rate": 0.00019815081991946969,
      "loss": 0.1913,
      "step": 2461
    },
    {
      "epoch": 0.01158075956988438,
      "grad_norm": 2.820483922958374,
      "learning_rate": 0.0001981498769413562,
      "loss": 0.6231,
      "step": 2462
    },
    {
      "epoch": 0.011585463371496844,
      "grad_norm": 3.9074041843414307,
      "learning_rate": 0.00019814893396324272,
      "loss": 0.3293,
      "step": 2463
    },
    {
      "epoch": 0.011590167173109307,
      "grad_norm": 4.1751861572265625,
      "learning_rate": 0.00019814799098512924,
      "loss": 0.7045,
      "step": 2464
    },
    {
      "epoch": 0.01159487097472177,
      "grad_norm": 2.995431661605835,
      "learning_rate": 0.00019814704800701576,
      "loss": 0.785,
      "step": 2465
    },
    {
      "epoch": 0.011599574776334233,
      "grad_norm": 2.4669394493103027,
      "learning_rate": 0.0001981461050289023,
      "loss": 0.3018,
      "step": 2466
    },
    {
      "epoch": 0.011604278577946697,
      "grad_norm": 1.2639925479888916,
      "learning_rate": 0.00019814516205078882,
      "loss": 0.1903,
      "step": 2467
    },
    {
      "epoch": 0.01160898237955916,
      "grad_norm": 0.49419620633125305,
      "learning_rate": 0.00019814421907267534,
      "loss": 0.0745,
      "step": 2468
    },
    {
      "epoch": 0.011613686181171623,
      "grad_norm": 1.4181357622146606,
      "learning_rate": 0.00019814327609456184,
      "loss": 0.2055,
      "step": 2469
    },
    {
      "epoch": 0.011618389982784087,
      "grad_norm": 1.536342978477478,
      "learning_rate": 0.00019814233311644838,
      "loss": 0.3,
      "step": 2470
    },
    {
      "epoch": 0.011623093784396549,
      "grad_norm": 2.5166895389556885,
      "learning_rate": 0.0001981413901383349,
      "loss": 0.2619,
      "step": 2471
    },
    {
      "epoch": 0.011627797586009013,
      "grad_norm": 1.242052435874939,
      "learning_rate": 0.00019814044716022142,
      "loss": 0.1173,
      "step": 2472
    },
    {
      "epoch": 0.011632501387621477,
      "grad_norm": 0.39308610558509827,
      "learning_rate": 0.00019813950418210794,
      "loss": 0.0318,
      "step": 2473
    },
    {
      "epoch": 0.011637205189233939,
      "grad_norm": 1.1487972736358643,
      "learning_rate": 0.00019813856120399446,
      "loss": 0.1672,
      "step": 2474
    },
    {
      "epoch": 0.011641908990846403,
      "grad_norm": 2.282228946685791,
      "learning_rate": 0.000198137618225881,
      "loss": 0.2411,
      "step": 2475
    },
    {
      "epoch": 0.011646612792458865,
      "grad_norm": 1.9462727308273315,
      "learning_rate": 0.00019813667524776752,
      "loss": 0.2386,
      "step": 2476
    },
    {
      "epoch": 0.011651316594071329,
      "grad_norm": 0.40289855003356934,
      "learning_rate": 0.00019813573226965404,
      "loss": 0.0325,
      "step": 2477
    },
    {
      "epoch": 0.01165602039568379,
      "grad_norm": 1.5901689529418945,
      "learning_rate": 0.00019813478929154056,
      "loss": 0.2082,
      "step": 2478
    },
    {
      "epoch": 0.011660724197296255,
      "grad_norm": 3.2818336486816406,
      "learning_rate": 0.00019813384631342708,
      "loss": 0.7351,
      "step": 2479
    },
    {
      "epoch": 0.011665427998908719,
      "grad_norm": 1.948394536972046,
      "learning_rate": 0.0001981329033353136,
      "loss": 0.2124,
      "step": 2480
    },
    {
      "epoch": 0.01167013180052118,
      "grad_norm": 0.5054841041564941,
      "learning_rate": 0.0001981319603572001,
      "loss": 0.0465,
      "step": 2481
    },
    {
      "epoch": 0.011674835602133645,
      "grad_norm": 0.21172140538692474,
      "learning_rate": 0.00019813101737908663,
      "loss": 0.017,
      "step": 2482
    },
    {
      "epoch": 0.011679539403746107,
      "grad_norm": 1.6946210861206055,
      "learning_rate": 0.00019813007440097315,
      "loss": 0.175,
      "step": 2483
    },
    {
      "epoch": 0.01168424320535857,
      "grad_norm": 4.699731826782227,
      "learning_rate": 0.0001981291314228597,
      "loss": 0.3741,
      "step": 2484
    },
    {
      "epoch": 0.011688947006971035,
      "grad_norm": 3.310652494430542,
      "learning_rate": 0.00019812818844474621,
      "loss": 1.1772,
      "step": 2485
    },
    {
      "epoch": 0.011693650808583497,
      "grad_norm": 1.5948691368103027,
      "learning_rate": 0.00019812724546663273,
      "loss": 0.2785,
      "step": 2486
    },
    {
      "epoch": 0.01169835461019596,
      "grad_norm": 2.3461196422576904,
      "learning_rate": 0.00019812630248851925,
      "loss": 0.3432,
      "step": 2487
    },
    {
      "epoch": 0.011703058411808423,
      "grad_norm": 0.26972976326942444,
      "learning_rate": 0.00019812535951040577,
      "loss": 0.0201,
      "step": 2488
    },
    {
      "epoch": 0.011707762213420887,
      "grad_norm": 1.9483920335769653,
      "learning_rate": 0.0001981244165322923,
      "loss": 0.3659,
      "step": 2489
    },
    {
      "epoch": 0.01171246601503335,
      "grad_norm": 2.913945436477661,
      "learning_rate": 0.0001981234735541788,
      "loss": 0.2846,
      "step": 2490
    },
    {
      "epoch": 0.011717169816645813,
      "grad_norm": 1.0252429246902466,
      "learning_rate": 0.00019812253057606533,
      "loss": 0.1063,
      "step": 2491
    },
    {
      "epoch": 0.011721873618258277,
      "grad_norm": 2.194037914276123,
      "learning_rate": 0.00019812158759795185,
      "loss": 0.2476,
      "step": 2492
    },
    {
      "epoch": 0.011726577419870739,
      "grad_norm": 2.2589399814605713,
      "learning_rate": 0.0001981206446198384,
      "loss": 0.2602,
      "step": 2493
    },
    {
      "epoch": 0.011731281221483203,
      "grad_norm": 1.7096763849258423,
      "learning_rate": 0.0001981197016417249,
      "loss": 0.2437,
      "step": 2494
    },
    {
      "epoch": 0.011735985023095665,
      "grad_norm": 2.2768361568450928,
      "learning_rate": 0.00019811875866361143,
      "loss": 0.2605,
      "step": 2495
    },
    {
      "epoch": 0.011740688824708129,
      "grad_norm": 1.0729637145996094,
      "learning_rate": 0.00019811781568549795,
      "loss": 0.0922,
      "step": 2496
    },
    {
      "epoch": 0.011745392626320593,
      "grad_norm": 2.076690673828125,
      "learning_rate": 0.0001981168727073845,
      "loss": 0.1471,
      "step": 2497
    },
    {
      "epoch": 0.011750096427933055,
      "grad_norm": 1.3032305240631104,
      "learning_rate": 0.000198115929729271,
      "loss": 0.1004,
      "step": 2498
    },
    {
      "epoch": 0.011754800229545519,
      "grad_norm": 3.8107409477233887,
      "learning_rate": 0.00019811498675115753,
      "loss": 0.4029,
      "step": 2499
    },
    {
      "epoch": 0.011759504031157981,
      "grad_norm": 2.2884268760681152,
      "learning_rate": 0.00019811404377304402,
      "loss": 0.1664,
      "step": 2500
    },
    {
      "epoch": 0.011764207832770445,
      "grad_norm": 2.0703673362731934,
      "learning_rate": 0.00019811310079493054,
      "loss": 0.2721,
      "step": 2501
    },
    {
      "epoch": 0.011768911634382909,
      "grad_norm": 1.9218354225158691,
      "learning_rate": 0.00019811215781681709,
      "loss": 0.3168,
      "step": 2502
    },
    {
      "epoch": 0.011773615435995371,
      "grad_norm": 2.0381743907928467,
      "learning_rate": 0.0001981112148387036,
      "loss": 0.2582,
      "step": 2503
    },
    {
      "epoch": 0.011778319237607835,
      "grad_norm": 3.3027567863464355,
      "learning_rate": 0.00019811027186059012,
      "loss": 0.4692,
      "step": 2504
    },
    {
      "epoch": 0.011783023039220297,
      "grad_norm": 3.751288652420044,
      "learning_rate": 0.00019810932888247664,
      "loss": 0.4861,
      "step": 2505
    },
    {
      "epoch": 0.011787726840832761,
      "grad_norm": 3.9246132373809814,
      "learning_rate": 0.00019810838590436316,
      "loss": 0.5722,
      "step": 2506
    },
    {
      "epoch": 0.011792430642445225,
      "grad_norm": 0.9191484451293945,
      "learning_rate": 0.0001981074429262497,
      "loss": 0.0882,
      "step": 2507
    },
    {
      "epoch": 0.011797134444057687,
      "grad_norm": 2.9856183528900146,
      "learning_rate": 0.00019810649994813622,
      "loss": 0.4694,
      "step": 2508
    },
    {
      "epoch": 0.011801838245670151,
      "grad_norm": 0.3991037607192993,
      "learning_rate": 0.00019810555697002274,
      "loss": 0.0436,
      "step": 2509
    },
    {
      "epoch": 0.011806542047282613,
      "grad_norm": 2.9773828983306885,
      "learning_rate": 0.00019810461399190926,
      "loss": 0.5153,
      "step": 2510
    },
    {
      "epoch": 0.011811245848895077,
      "grad_norm": 2.398897171020508,
      "learning_rate": 0.00019810367101379578,
      "loss": 0.2079,
      "step": 2511
    },
    {
      "epoch": 0.01181594965050754,
      "grad_norm": 2.249206304550171,
      "learning_rate": 0.0001981027280356823,
      "loss": 0.1721,
      "step": 2512
    },
    {
      "epoch": 0.011820653452120003,
      "grad_norm": 1.0432095527648926,
      "learning_rate": 0.00019810178505756882,
      "loss": 0.0905,
      "step": 2513
    },
    {
      "epoch": 0.011825357253732467,
      "grad_norm": 3.843829393386841,
      "learning_rate": 0.00019810084207945534,
      "loss": 0.3161,
      "step": 2514
    },
    {
      "epoch": 0.01183006105534493,
      "grad_norm": 2.9492523670196533,
      "learning_rate": 0.00019809989910134186,
      "loss": 0.2277,
      "step": 2515
    },
    {
      "epoch": 0.011834764856957393,
      "grad_norm": 3.073009967803955,
      "learning_rate": 0.0001980989561232284,
      "loss": 0.2425,
      "step": 2516
    },
    {
      "epoch": 0.011839468658569855,
      "grad_norm": 2.016566038131714,
      "learning_rate": 0.00019809801314511492,
      "loss": 0.2764,
      "step": 2517
    },
    {
      "epoch": 0.01184417246018232,
      "grad_norm": 1.8964108228683472,
      "learning_rate": 0.00019809707016700144,
      "loss": 0.1684,
      "step": 2518
    },
    {
      "epoch": 0.011848876261794783,
      "grad_norm": 1.846630334854126,
      "learning_rate": 0.00019809612718888796,
      "loss": 0.2166,
      "step": 2519
    },
    {
      "epoch": 0.011853580063407245,
      "grad_norm": 4.611847877502441,
      "learning_rate": 0.00019809518421077448,
      "loss": 0.6644,
      "step": 2520
    },
    {
      "epoch": 0.01185828386501971,
      "grad_norm": 1.5084342956542969,
      "learning_rate": 0.000198094241232661,
      "loss": 0.17,
      "step": 2521
    },
    {
      "epoch": 0.011862987666632172,
      "grad_norm": 3.586652994155884,
      "learning_rate": 0.0001980932982545475,
      "loss": 0.6948,
      "step": 2522
    },
    {
      "epoch": 0.011867691468244635,
      "grad_norm": 3.56257700920105,
      "learning_rate": 0.00019809235527643403,
      "loss": 0.2193,
      "step": 2523
    },
    {
      "epoch": 0.0118723952698571,
      "grad_norm": 2.5311615467071533,
      "learning_rate": 0.00019809141229832055,
      "loss": 0.4449,
      "step": 2524
    },
    {
      "epoch": 0.011877099071469562,
      "grad_norm": 0.5578433275222778,
      "learning_rate": 0.0001980904693202071,
      "loss": 0.0674,
      "step": 2525
    },
    {
      "epoch": 0.011881802873082025,
      "grad_norm": 1.5918176174163818,
      "learning_rate": 0.00019808952634209361,
      "loss": 0.188,
      "step": 2526
    },
    {
      "epoch": 0.011886506674694488,
      "grad_norm": 0.4271109998226166,
      "learning_rate": 0.00019808858336398013,
      "loss": 0.0311,
      "step": 2527
    },
    {
      "epoch": 0.011891210476306951,
      "grad_norm": 1.5629994869232178,
      "learning_rate": 0.00019808764038586665,
      "loss": 0.2494,
      "step": 2528
    },
    {
      "epoch": 0.011895914277919414,
      "grad_norm": 2.824834108352661,
      "learning_rate": 0.0001980866974077532,
      "loss": 0.567,
      "step": 2529
    },
    {
      "epoch": 0.011900618079531878,
      "grad_norm": 2.3493967056274414,
      "learning_rate": 0.0001980857544296397,
      "loss": 0.3704,
      "step": 2530
    },
    {
      "epoch": 0.011905321881144341,
      "grad_norm": 4.542910575866699,
      "learning_rate": 0.0001980848114515262,
      "loss": 0.7869,
      "step": 2531
    },
    {
      "epoch": 0.011910025682756804,
      "grad_norm": 1.5542019605636597,
      "learning_rate": 0.00019808386847341273,
      "loss": 0.1763,
      "step": 2532
    },
    {
      "epoch": 0.011914729484369268,
      "grad_norm": 0.8600497841835022,
      "learning_rate": 0.00019808292549529925,
      "loss": 0.0903,
      "step": 2533
    },
    {
      "epoch": 0.01191943328598173,
      "grad_norm": 0.5204780697822571,
      "learning_rate": 0.0001980819825171858,
      "loss": 0.0391,
      "step": 2534
    },
    {
      "epoch": 0.011924137087594194,
      "grad_norm": 0.5160027742385864,
      "learning_rate": 0.0001980810395390723,
      "loss": 0.0407,
      "step": 2535
    },
    {
      "epoch": 0.011928840889206658,
      "grad_norm": 3.377201557159424,
      "learning_rate": 0.00019808009656095883,
      "loss": 0.5335,
      "step": 2536
    },
    {
      "epoch": 0.01193354469081912,
      "grad_norm": 0.7095991969108582,
      "learning_rate": 0.00019807915358284535,
      "loss": 0.0606,
      "step": 2537
    },
    {
      "epoch": 0.011938248492431584,
      "grad_norm": 3.049787998199463,
      "learning_rate": 0.0001980782106047319,
      "loss": 0.4898,
      "step": 2538
    },
    {
      "epoch": 0.011942952294044046,
      "grad_norm": 1.9797781705856323,
      "learning_rate": 0.0001980772676266184,
      "loss": 0.3387,
      "step": 2539
    },
    {
      "epoch": 0.01194765609565651,
      "grad_norm": 3.9044137001037598,
      "learning_rate": 0.00019807632464850493,
      "loss": 0.8182,
      "step": 2540
    },
    {
      "epoch": 0.011952359897268974,
      "grad_norm": 0.8970770835876465,
      "learning_rate": 0.00019807538167039145,
      "loss": 0.1306,
      "step": 2541
    },
    {
      "epoch": 0.011957063698881436,
      "grad_norm": 0.26653188467025757,
      "learning_rate": 0.00019807443869227794,
      "loss": 0.0188,
      "step": 2542
    },
    {
      "epoch": 0.0119617675004939,
      "grad_norm": 2.7573118209838867,
      "learning_rate": 0.00019807349571416449,
      "loss": 0.4822,
      "step": 2543
    },
    {
      "epoch": 0.011966471302106362,
      "grad_norm": 0.16202716529369354,
      "learning_rate": 0.000198072552736051,
      "loss": 0.009,
      "step": 2544
    },
    {
      "epoch": 0.011971175103718826,
      "grad_norm": 1.8402736186981201,
      "learning_rate": 0.00019807160975793752,
      "loss": 0.3024,
      "step": 2545
    },
    {
      "epoch": 0.011975878905331288,
      "grad_norm": 1.9367974996566772,
      "learning_rate": 0.00019807066677982404,
      "loss": 0.4377,
      "step": 2546
    },
    {
      "epoch": 0.011980582706943752,
      "grad_norm": 2.012364387512207,
      "learning_rate": 0.0001980697238017106,
      "loss": 0.3272,
      "step": 2547
    },
    {
      "epoch": 0.011985286508556216,
      "grad_norm": 1.4567837715148926,
      "learning_rate": 0.0001980687808235971,
      "loss": 0.2563,
      "step": 2548
    },
    {
      "epoch": 0.011989990310168678,
      "grad_norm": 1.2287946939468384,
      "learning_rate": 0.00019806783784548362,
      "loss": 0.2075,
      "step": 2549
    },
    {
      "epoch": 0.011994694111781142,
      "grad_norm": 0.8975484371185303,
      "learning_rate": 0.00019806689486737014,
      "loss": 0.0491,
      "step": 2550
    },
    {
      "epoch": 0.011999397913393604,
      "grad_norm": 2.898841619491577,
      "learning_rate": 0.00019806595188925666,
      "loss": 0.1995,
      "step": 2551
    },
    {
      "epoch": 0.012004101715006068,
      "grad_norm": 4.705866813659668,
      "learning_rate": 0.00019806500891114318,
      "loss": 0.4479,
      "step": 2552
    },
    {
      "epoch": 0.012008805516618532,
      "grad_norm": 0.8255510926246643,
      "learning_rate": 0.0001980640659330297,
      "loss": 0.0845,
      "step": 2553
    },
    {
      "epoch": 0.012013509318230994,
      "grad_norm": 3.3653507232666016,
      "learning_rate": 0.00019806312295491622,
      "loss": 0.4385,
      "step": 2554
    },
    {
      "epoch": 0.012018213119843458,
      "grad_norm": 1.8068641424179077,
      "learning_rate": 0.00019806217997680274,
      "loss": 0.1566,
      "step": 2555
    },
    {
      "epoch": 0.01202291692145592,
      "grad_norm": 0.8460500240325928,
      "learning_rate": 0.00019806123699868926,
      "loss": 0.1316,
      "step": 2556
    },
    {
      "epoch": 0.012027620723068384,
      "grad_norm": 1.6134552955627441,
      "learning_rate": 0.0001980602940205758,
      "loss": 0.1253,
      "step": 2557
    },
    {
      "epoch": 0.012032324524680848,
      "grad_norm": 1.6800028085708618,
      "learning_rate": 0.00019805935104246232,
      "loss": 0.2305,
      "step": 2558
    },
    {
      "epoch": 0.01203702832629331,
      "grad_norm": 1.4548358917236328,
      "learning_rate": 0.00019805840806434884,
      "loss": 0.1541,
      "step": 2559
    },
    {
      "epoch": 0.012041732127905774,
      "grad_norm": 0.9569388628005981,
      "learning_rate": 0.00019805746508623536,
      "loss": 0.0821,
      "step": 2560
    },
    {
      "epoch": 0.012046435929518236,
      "grad_norm": 0.9708883762359619,
      "learning_rate": 0.00019805652210812188,
      "loss": 0.1462,
      "step": 2561
    },
    {
      "epoch": 0.0120511397311307,
      "grad_norm": 3.0880398750305176,
      "learning_rate": 0.0001980555791300084,
      "loss": 0.5519,
      "step": 2562
    },
    {
      "epoch": 0.012055843532743162,
      "grad_norm": 2.665733814239502,
      "learning_rate": 0.0001980546361518949,
      "loss": 0.2885,
      "step": 2563
    },
    {
      "epoch": 0.012060547334355626,
      "grad_norm": 3.7148635387420654,
      "learning_rate": 0.00019805369317378143,
      "loss": 0.7599,
      "step": 2564
    },
    {
      "epoch": 0.01206525113596809,
      "grad_norm": 0.37335774302482605,
      "learning_rate": 0.00019805275019566795,
      "loss": 0.0319,
      "step": 2565
    },
    {
      "epoch": 0.012069954937580552,
      "grad_norm": 3.5128257274627686,
      "learning_rate": 0.0001980518072175545,
      "loss": 0.5294,
      "step": 2566
    },
    {
      "epoch": 0.012074658739193016,
      "grad_norm": 1.6121606826782227,
      "learning_rate": 0.00019805086423944101,
      "loss": 0.2739,
      "step": 2567
    },
    {
      "epoch": 0.012079362540805478,
      "grad_norm": 2.4950623512268066,
      "learning_rate": 0.00019804992126132753,
      "loss": 0.3239,
      "step": 2568
    },
    {
      "epoch": 0.012084066342417942,
      "grad_norm": 0.9218420386314392,
      "learning_rate": 0.00019804897828321405,
      "loss": 0.1036,
      "step": 2569
    },
    {
      "epoch": 0.012088770144030406,
      "grad_norm": 4.689274787902832,
      "learning_rate": 0.0001980480353051006,
      "loss": 0.4136,
      "step": 2570
    },
    {
      "epoch": 0.012093473945642868,
      "grad_norm": 1.3116787672042847,
      "learning_rate": 0.00019804709232698712,
      "loss": 0.1061,
      "step": 2571
    },
    {
      "epoch": 0.012098177747255332,
      "grad_norm": 4.958982467651367,
      "learning_rate": 0.00019804614934887363,
      "loss": 0.9456,
      "step": 2572
    },
    {
      "epoch": 0.012102881548867794,
      "grad_norm": 0.9842046499252319,
      "learning_rate": 0.00019804520637076013,
      "loss": 0.098,
      "step": 2573
    },
    {
      "epoch": 0.012107585350480258,
      "grad_norm": 1.3865983486175537,
      "learning_rate": 0.00019804426339264665,
      "loss": 0.1278,
      "step": 2574
    },
    {
      "epoch": 0.012112289152092722,
      "grad_norm": 7.043516635894775,
      "learning_rate": 0.0001980433204145332,
      "loss": 0.7089,
      "step": 2575
    },
    {
      "epoch": 0.012116992953705184,
      "grad_norm": 1.812571406364441,
      "learning_rate": 0.0001980423774364197,
      "loss": 0.1592,
      "step": 2576
    },
    {
      "epoch": 0.012121696755317648,
      "grad_norm": 1.1475884914398193,
      "learning_rate": 0.00019804143445830623,
      "loss": 0.1056,
      "step": 2577
    },
    {
      "epoch": 0.01212640055693011,
      "grad_norm": 3.265251874923706,
      "learning_rate": 0.00019804049148019275,
      "loss": 0.6975,
      "step": 2578
    },
    {
      "epoch": 0.012131104358542574,
      "grad_norm": 1.187157154083252,
      "learning_rate": 0.0001980395485020793,
      "loss": 0.1257,
      "step": 2579
    },
    {
      "epoch": 0.012135808160155036,
      "grad_norm": 1.3641613721847534,
      "learning_rate": 0.0001980386055239658,
      "loss": 0.133,
      "step": 2580
    },
    {
      "epoch": 0.0121405119617675,
      "grad_norm": 2.5796713829040527,
      "learning_rate": 0.00019803766254585233,
      "loss": 0.3632,
      "step": 2581
    },
    {
      "epoch": 0.012145215763379964,
      "grad_norm": 2.2008659839630127,
      "learning_rate": 0.00019803671956773885,
      "loss": 0.1886,
      "step": 2582
    },
    {
      "epoch": 0.012149919564992426,
      "grad_norm": 1.6226469278335571,
      "learning_rate": 0.00019803577658962537,
      "loss": 0.2246,
      "step": 2583
    },
    {
      "epoch": 0.01215462336660489,
      "grad_norm": 2.7873940467834473,
      "learning_rate": 0.00019803483361151189,
      "loss": 0.5469,
      "step": 2584
    },
    {
      "epoch": 0.012159327168217353,
      "grad_norm": 1.8136652708053589,
      "learning_rate": 0.0001980338906333984,
      "loss": 0.5838,
      "step": 2585
    },
    {
      "epoch": 0.012164030969829816,
      "grad_norm": 3.9853086471557617,
      "learning_rate": 0.00019803294765528492,
      "loss": 0.3613,
      "step": 2586
    },
    {
      "epoch": 0.01216873477144228,
      "grad_norm": 1.4645051956176758,
      "learning_rate": 0.00019803200467717144,
      "loss": 0.2588,
      "step": 2587
    },
    {
      "epoch": 0.012173438573054743,
      "grad_norm": 4.065470218658447,
      "learning_rate": 0.000198031061699058,
      "loss": 0.5086,
      "step": 2588
    },
    {
      "epoch": 0.012178142374667206,
      "grad_norm": 1.6325215101242065,
      "learning_rate": 0.0001980301187209445,
      "loss": 0.1693,
      "step": 2589
    },
    {
      "epoch": 0.012182846176279669,
      "grad_norm": 2.542534351348877,
      "learning_rate": 0.00019802917574283102,
      "loss": 0.1865,
      "step": 2590
    },
    {
      "epoch": 0.012187549977892132,
      "grad_norm": 1.5946259498596191,
      "learning_rate": 0.00019802823276471754,
      "loss": 0.2537,
      "step": 2591
    },
    {
      "epoch": 0.012192253779504596,
      "grad_norm": 0.7086212038993835,
      "learning_rate": 0.00019802728978660406,
      "loss": 0.0709,
      "step": 2592
    },
    {
      "epoch": 0.012196957581117059,
      "grad_norm": 1.1119804382324219,
      "learning_rate": 0.00019802634680849058,
      "loss": 0.1816,
      "step": 2593
    },
    {
      "epoch": 0.012201661382729522,
      "grad_norm": 0.7224805951118469,
      "learning_rate": 0.0001980254038303771,
      "loss": 0.0965,
      "step": 2594
    },
    {
      "epoch": 0.012206365184341985,
      "grad_norm": 0.8775892853736877,
      "learning_rate": 0.00019802446085226362,
      "loss": 0.1531,
      "step": 2595
    },
    {
      "epoch": 0.012211068985954449,
      "grad_norm": 0.5312280654907227,
      "learning_rate": 0.00019802351787415014,
      "loss": 0.0929,
      "step": 2596
    },
    {
      "epoch": 0.01221577278756691,
      "grad_norm": 1.2727724313735962,
      "learning_rate": 0.00019802257489603668,
      "loss": 0.1529,
      "step": 2597
    },
    {
      "epoch": 0.012220476589179375,
      "grad_norm": 1.2054837942123413,
      "learning_rate": 0.0001980216319179232,
      "loss": 0.1392,
      "step": 2598
    },
    {
      "epoch": 0.012225180390791839,
      "grad_norm": 3.2410292625427246,
      "learning_rate": 0.00019802068893980972,
      "loss": 0.3,
      "step": 2599
    },
    {
      "epoch": 0.0122298841924043,
      "grad_norm": 0.8083617091178894,
      "learning_rate": 0.00019801974596169624,
      "loss": 0.0994,
      "step": 2600
    },
    {
      "epoch": 0.012234587994016765,
      "grad_norm": 4.061936855316162,
      "learning_rate": 0.00019801880298358276,
      "loss": 0.3412,
      "step": 2601
    },
    {
      "epoch": 0.012239291795629227,
      "grad_norm": 3.454437494277954,
      "learning_rate": 0.0001980178600054693,
      "loss": 0.5241,
      "step": 2602
    },
    {
      "epoch": 0.01224399559724169,
      "grad_norm": 1.6829438209533691,
      "learning_rate": 0.00019801691702735582,
      "loss": 0.1828,
      "step": 2603
    },
    {
      "epoch": 0.012248699398854155,
      "grad_norm": 0.8906373977661133,
      "learning_rate": 0.0001980159740492423,
      "loss": 0.0692,
      "step": 2604
    },
    {
      "epoch": 0.012253403200466617,
      "grad_norm": 0.6737685203552246,
      "learning_rate": 0.00019801503107112883,
      "loss": 0.084,
      "step": 2605
    },
    {
      "epoch": 0.01225810700207908,
      "grad_norm": 2.6246354579925537,
      "learning_rate": 0.00019801408809301535,
      "loss": 0.2594,
      "step": 2606
    },
    {
      "epoch": 0.012262810803691543,
      "grad_norm": 3.985417366027832,
      "learning_rate": 0.0001980131451149019,
      "loss": 0.4928,
      "step": 2607
    },
    {
      "epoch": 0.012267514605304007,
      "grad_norm": 3.041466236114502,
      "learning_rate": 0.00019801220213678841,
      "loss": 0.4307,
      "step": 2608
    },
    {
      "epoch": 0.01227221840691647,
      "grad_norm": 3.9242186546325684,
      "learning_rate": 0.00019801125915867493,
      "loss": 0.7529,
      "step": 2609
    },
    {
      "epoch": 0.012276922208528933,
      "grad_norm": 2.7373409271240234,
      "learning_rate": 0.00019801031618056145,
      "loss": 0.2252,
      "step": 2610
    },
    {
      "epoch": 0.012281626010141397,
      "grad_norm": 5.690968036651611,
      "learning_rate": 0.000198009373202448,
      "loss": 0.6693,
      "step": 2611
    },
    {
      "epoch": 0.012286329811753859,
      "grad_norm": 9.384255409240723,
      "learning_rate": 0.00019800843022433452,
      "loss": 1.3036,
      "step": 2612
    },
    {
      "epoch": 0.012291033613366323,
      "grad_norm": 2.6941206455230713,
      "learning_rate": 0.00019800748724622103,
      "loss": 0.1874,
      "step": 2613
    },
    {
      "epoch": 0.012295737414978785,
      "grad_norm": 3.0316596031188965,
      "learning_rate": 0.00019800654426810755,
      "loss": 0.6653,
      "step": 2614
    },
    {
      "epoch": 0.012300441216591249,
      "grad_norm": 2.6299631595611572,
      "learning_rate": 0.00019800560128999405,
      "loss": 0.2884,
      "step": 2615
    },
    {
      "epoch": 0.012305145018203713,
      "grad_norm": 1.3194172382354736,
      "learning_rate": 0.0001980046583118806,
      "loss": 0.2869,
      "step": 2616
    },
    {
      "epoch": 0.012309848819816175,
      "grad_norm": 0.9122987985610962,
      "learning_rate": 0.0001980037153337671,
      "loss": 0.0397,
      "step": 2617
    },
    {
      "epoch": 0.012314552621428639,
      "grad_norm": 1.6527049541473389,
      "learning_rate": 0.00019800277235565363,
      "loss": 0.2879,
      "step": 2618
    },
    {
      "epoch": 0.012319256423041101,
      "grad_norm": 0.6074760556221008,
      "learning_rate": 0.00019800182937754015,
      "loss": 0.0463,
      "step": 2619
    },
    {
      "epoch": 0.012323960224653565,
      "grad_norm": 1.099339485168457,
      "learning_rate": 0.0001980008863994267,
      "loss": 0.0789,
      "step": 2620
    },
    {
      "epoch": 0.012328664026266029,
      "grad_norm": 1.4584153890609741,
      "learning_rate": 0.0001979999434213132,
      "loss": 0.1955,
      "step": 2621
    },
    {
      "epoch": 0.012333367827878491,
      "grad_norm": 3.5590944290161133,
      "learning_rate": 0.00019799900044319973,
      "loss": 0.7853,
      "step": 2622
    },
    {
      "epoch": 0.012338071629490955,
      "grad_norm": 6.480471611022949,
      "learning_rate": 0.00019799805746508625,
      "loss": 0.9853,
      "step": 2623
    },
    {
      "epoch": 0.012342775431103417,
      "grad_norm": 2.3735105991363525,
      "learning_rate": 0.00019799711448697277,
      "loss": 0.2561,
      "step": 2624
    },
    {
      "epoch": 0.012347479232715881,
      "grad_norm": 1.9742505550384521,
      "learning_rate": 0.00019799617150885929,
      "loss": 0.2469,
      "step": 2625
    },
    {
      "epoch": 0.012352183034328345,
      "grad_norm": 3.114816188812256,
      "learning_rate": 0.0001979952285307458,
      "loss": 0.6638,
      "step": 2626
    },
    {
      "epoch": 0.012356886835940807,
      "grad_norm": 1.7097526788711548,
      "learning_rate": 0.00019799428555263232,
      "loss": 0.5264,
      "step": 2627
    },
    {
      "epoch": 0.012361590637553271,
      "grad_norm": 0.9657158255577087,
      "learning_rate": 0.00019799334257451884,
      "loss": 0.1263,
      "step": 2628
    },
    {
      "epoch": 0.012366294439165733,
      "grad_norm": 2.0480570793151855,
      "learning_rate": 0.0001979923995964054,
      "loss": 0.3878,
      "step": 2629
    },
    {
      "epoch": 0.012370998240778197,
      "grad_norm": 0.7499891519546509,
      "learning_rate": 0.0001979914566182919,
      "loss": 0.124,
      "step": 2630
    },
    {
      "epoch": 0.01237570204239066,
      "grad_norm": 4.5066094398498535,
      "learning_rate": 0.00019799051364017842,
      "loss": 0.5138,
      "step": 2631
    },
    {
      "epoch": 0.012380405844003123,
      "grad_norm": 6.635505676269531,
      "learning_rate": 0.00019798957066206494,
      "loss": 0.3817,
      "step": 2632
    },
    {
      "epoch": 0.012385109645615587,
      "grad_norm": 3.388746500015259,
      "learning_rate": 0.00019798862768395146,
      "loss": 0.4869,
      "step": 2633
    },
    {
      "epoch": 0.01238981344722805,
      "grad_norm": 0.4203975796699524,
      "learning_rate": 0.000197987684705838,
      "loss": 0.0349,
      "step": 2634
    },
    {
      "epoch": 0.012394517248840513,
      "grad_norm": 2.872861862182617,
      "learning_rate": 0.0001979867417277245,
      "loss": 0.3499,
      "step": 2635
    },
    {
      "epoch": 0.012399221050452975,
      "grad_norm": 1.3292909860610962,
      "learning_rate": 0.00019798579874961102,
      "loss": 0.1619,
      "step": 2636
    },
    {
      "epoch": 0.01240392485206544,
      "grad_norm": 2.0044052600860596,
      "learning_rate": 0.00019798485577149754,
      "loss": 0.2495,
      "step": 2637
    },
    {
      "epoch": 0.012408628653677903,
      "grad_norm": 1.0062949657440186,
      "learning_rate": 0.00019798391279338408,
      "loss": 0.0817,
      "step": 2638
    },
    {
      "epoch": 0.012413332455290365,
      "grad_norm": 3.285897731781006,
      "learning_rate": 0.0001979829698152706,
      "loss": 0.4422,
      "step": 2639
    },
    {
      "epoch": 0.01241803625690283,
      "grad_norm": 1.9997142553329468,
      "learning_rate": 0.00019798202683715712,
      "loss": 0.3245,
      "step": 2640
    },
    {
      "epoch": 0.012422740058515291,
      "grad_norm": 1.5408358573913574,
      "learning_rate": 0.00019798108385904364,
      "loss": 0.2601,
      "step": 2641
    },
    {
      "epoch": 0.012427443860127755,
      "grad_norm": 0.44999179244041443,
      "learning_rate": 0.00019798014088093016,
      "loss": 0.0495,
      "step": 2642
    },
    {
      "epoch": 0.01243214766174022,
      "grad_norm": 0.7675291895866394,
      "learning_rate": 0.0001979791979028167,
      "loss": 0.0636,
      "step": 2643
    },
    {
      "epoch": 0.012436851463352681,
      "grad_norm": 4.196427822113037,
      "learning_rate": 0.00019797825492470322,
      "loss": 1.0631,
      "step": 2644
    },
    {
      "epoch": 0.012441555264965145,
      "grad_norm": 0.2162400782108307,
      "learning_rate": 0.00019797731194658974,
      "loss": 0.0139,
      "step": 2645
    },
    {
      "epoch": 0.012446259066577607,
      "grad_norm": 1.1269489526748657,
      "learning_rate": 0.00019797636896847623,
      "loss": 0.114,
      "step": 2646
    },
    {
      "epoch": 0.012450962868190071,
      "grad_norm": 2.238931894302368,
      "learning_rate": 0.00019797542599036278,
      "loss": 0.4499,
      "step": 2647
    },
    {
      "epoch": 0.012455666669802535,
      "grad_norm": 0.8484088778495789,
      "learning_rate": 0.0001979744830122493,
      "loss": 0.1357,
      "step": 2648
    },
    {
      "epoch": 0.012460370471414997,
      "grad_norm": 1.1703304052352905,
      "learning_rate": 0.00019797354003413581,
      "loss": 0.2743,
      "step": 2649
    },
    {
      "epoch": 0.012465074273027461,
      "grad_norm": 1.0912961959838867,
      "learning_rate": 0.00019797259705602233,
      "loss": 0.1877,
      "step": 2650
    },
    {
      "epoch": 0.012469778074639924,
      "grad_norm": 3.6544010639190674,
      "learning_rate": 0.00019797165407790885,
      "loss": 0.7151,
      "step": 2651
    },
    {
      "epoch": 0.012474481876252387,
      "grad_norm": 0.5901654362678528,
      "learning_rate": 0.0001979707110997954,
      "loss": 0.0656,
      "step": 2652
    },
    {
      "epoch": 0.01247918567786485,
      "grad_norm": 1.1498985290527344,
      "learning_rate": 0.00019796976812168192,
      "loss": 0.2285,
      "step": 2653
    },
    {
      "epoch": 0.012483889479477314,
      "grad_norm": 3.8270263671875,
      "learning_rate": 0.00019796882514356843,
      "loss": 0.3057,
      "step": 2654
    },
    {
      "epoch": 0.012488593281089777,
      "grad_norm": 4.598294734954834,
      "learning_rate": 0.00019796788216545495,
      "loss": 0.6301,
      "step": 2655
    },
    {
      "epoch": 0.01249329708270224,
      "grad_norm": 3.353900671005249,
      "learning_rate": 0.00019796693918734147,
      "loss": 0.7055,
      "step": 2656
    },
    {
      "epoch": 0.012498000884314703,
      "grad_norm": 0.8954707384109497,
      "learning_rate": 0.000197965996209228,
      "loss": 0.154,
      "step": 2657
    },
    {
      "epoch": 0.012502704685927166,
      "grad_norm": 2.0749144554138184,
      "learning_rate": 0.0001979650532311145,
      "loss": 0.2428,
      "step": 2658
    },
    {
      "epoch": 0.01250740848753963,
      "grad_norm": 0.9555361866950989,
      "learning_rate": 0.00019796411025300103,
      "loss": 0.1182,
      "step": 2659
    },
    {
      "epoch": 0.012512112289152093,
      "grad_norm": 1.8515305519104004,
      "learning_rate": 0.00019796316727488755,
      "loss": 0.3444,
      "step": 2660
    },
    {
      "epoch": 0.012516816090764556,
      "grad_norm": 1.7806715965270996,
      "learning_rate": 0.0001979622242967741,
      "loss": 0.2556,
      "step": 2661
    },
    {
      "epoch": 0.01252151989237702,
      "grad_norm": 3.1497714519500732,
      "learning_rate": 0.0001979612813186606,
      "loss": 0.7632,
      "step": 2662
    },
    {
      "epoch": 0.012526223693989482,
      "grad_norm": 1.6281706094741821,
      "learning_rate": 0.00019796033834054713,
      "loss": 0.2276,
      "step": 2663
    },
    {
      "epoch": 0.012530927495601946,
      "grad_norm": 1.9211543798446655,
      "learning_rate": 0.00019795939536243365,
      "loss": 0.2415,
      "step": 2664
    },
    {
      "epoch": 0.01253563129721441,
      "grad_norm": 1.9361307621002197,
      "learning_rate": 0.00019795845238432017,
      "loss": 0.381,
      "step": 2665
    },
    {
      "epoch": 0.012540335098826872,
      "grad_norm": 0.728889524936676,
      "learning_rate": 0.00019795750940620669,
      "loss": 0.0731,
      "step": 2666
    },
    {
      "epoch": 0.012545038900439336,
      "grad_norm": 0.9244268536567688,
      "learning_rate": 0.0001979565664280932,
      "loss": 0.1385,
      "step": 2667
    },
    {
      "epoch": 0.012549742702051798,
      "grad_norm": 0.8023003935813904,
      "learning_rate": 0.00019795562344997972,
      "loss": 0.0969,
      "step": 2668
    },
    {
      "epoch": 0.012554446503664262,
      "grad_norm": 0.6671333909034729,
      "learning_rate": 0.00019795468047186624,
      "loss": 0.1046,
      "step": 2669
    },
    {
      "epoch": 0.012559150305276724,
      "grad_norm": 1.2715634107589722,
      "learning_rate": 0.0001979537374937528,
      "loss": 0.1659,
      "step": 2670
    },
    {
      "epoch": 0.012563854106889188,
      "grad_norm": 0.6316545009613037,
      "learning_rate": 0.0001979527945156393,
      "loss": 0.0714,
      "step": 2671
    },
    {
      "epoch": 0.012568557908501652,
      "grad_norm": 0.4387318193912506,
      "learning_rate": 0.00019795185153752582,
      "loss": 0.0319,
      "step": 2672
    },
    {
      "epoch": 0.012573261710114114,
      "grad_norm": 0.49246400594711304,
      "learning_rate": 0.00019795090855941234,
      "loss": 0.0433,
      "step": 2673
    },
    {
      "epoch": 0.012577965511726578,
      "grad_norm": 3.622680902481079,
      "learning_rate": 0.00019794996558129886,
      "loss": 0.4637,
      "step": 2674
    },
    {
      "epoch": 0.01258266931333904,
      "grad_norm": 3.6990981101989746,
      "learning_rate": 0.0001979490226031854,
      "loss": 0.9184,
      "step": 2675
    },
    {
      "epoch": 0.012587373114951504,
      "grad_norm": 0.5671023726463318,
      "learning_rate": 0.00019794807962507193,
      "loss": 0.052,
      "step": 2676
    },
    {
      "epoch": 0.012592076916563968,
      "grad_norm": 1.3688730001449585,
      "learning_rate": 0.00019794713664695842,
      "loss": 0.2453,
      "step": 2677
    },
    {
      "epoch": 0.01259678071817643,
      "grad_norm": 4.185766696929932,
      "learning_rate": 0.00019794619366884494,
      "loss": 0.7104,
      "step": 2678
    },
    {
      "epoch": 0.012601484519788894,
      "grad_norm": 2.2320189476013184,
      "learning_rate": 0.00019794525069073148,
      "loss": 0.5169,
      "step": 2679
    },
    {
      "epoch": 0.012606188321401356,
      "grad_norm": 2.562394618988037,
      "learning_rate": 0.000197944307712618,
      "loss": 0.5151,
      "step": 2680
    },
    {
      "epoch": 0.01261089212301382,
      "grad_norm": 1.6826831102371216,
      "learning_rate": 0.00019794336473450452,
      "loss": 0.4219,
      "step": 2681
    },
    {
      "epoch": 0.012615595924626284,
      "grad_norm": 1.5684928894042969,
      "learning_rate": 0.00019794242175639104,
      "loss": 0.1792,
      "step": 2682
    },
    {
      "epoch": 0.012620299726238746,
      "grad_norm": 1.7446727752685547,
      "learning_rate": 0.00019794147877827756,
      "loss": 0.3851,
      "step": 2683
    },
    {
      "epoch": 0.01262500352785121,
      "grad_norm": 2.5701873302459717,
      "learning_rate": 0.0001979405358001641,
      "loss": 0.4195,
      "step": 2684
    },
    {
      "epoch": 0.012629707329463672,
      "grad_norm": 1.456620693206787,
      "learning_rate": 0.00019793959282205062,
      "loss": 0.2932,
      "step": 2685
    },
    {
      "epoch": 0.012634411131076136,
      "grad_norm": 1.2436107397079468,
      "learning_rate": 0.00019793864984393714,
      "loss": 0.151,
      "step": 2686
    },
    {
      "epoch": 0.012639114932688598,
      "grad_norm": 4.237247943878174,
      "learning_rate": 0.00019793770686582366,
      "loss": 1.0524,
      "step": 2687
    },
    {
      "epoch": 0.012643818734301062,
      "grad_norm": 2.7385194301605225,
      "learning_rate": 0.00019793676388771018,
      "loss": 0.4936,
      "step": 2688
    },
    {
      "epoch": 0.012648522535913526,
      "grad_norm": 0.501785159111023,
      "learning_rate": 0.0001979358209095967,
      "loss": 0.048,
      "step": 2689
    },
    {
      "epoch": 0.012653226337525988,
      "grad_norm": 2.0563580989837646,
      "learning_rate": 0.00019793487793148321,
      "loss": 0.5048,
      "step": 2690
    },
    {
      "epoch": 0.012657930139138452,
      "grad_norm": 1.7798006534576416,
      "learning_rate": 0.00019793393495336973,
      "loss": 0.4051,
      "step": 2691
    },
    {
      "epoch": 0.012662633940750914,
      "grad_norm": 1.9956060647964478,
      "learning_rate": 0.00019793299197525625,
      "loss": 0.5915,
      "step": 2692
    },
    {
      "epoch": 0.012667337742363378,
      "grad_norm": 1.907699704170227,
      "learning_rate": 0.0001979320489971428,
      "loss": 0.28,
      "step": 2693
    },
    {
      "epoch": 0.012672041543975842,
      "grad_norm": 1.243460774421692,
      "learning_rate": 0.00019793110601902932,
      "loss": 0.1637,
      "step": 2694
    },
    {
      "epoch": 0.012676745345588304,
      "grad_norm": 1.0484647750854492,
      "learning_rate": 0.00019793016304091583,
      "loss": 0.1304,
      "step": 2695
    },
    {
      "epoch": 0.012681449147200768,
      "grad_norm": 4.337580680847168,
      "learning_rate": 0.00019792922006280235,
      "loss": 0.5621,
      "step": 2696
    },
    {
      "epoch": 0.01268615294881323,
      "grad_norm": 2.5215094089508057,
      "learning_rate": 0.00019792827708468887,
      "loss": 0.7666,
      "step": 2697
    },
    {
      "epoch": 0.012690856750425694,
      "grad_norm": 2.463006019592285,
      "learning_rate": 0.0001979273341065754,
      "loss": 0.5113,
      "step": 2698
    },
    {
      "epoch": 0.012695560552038158,
      "grad_norm": 0.6732184886932373,
      "learning_rate": 0.0001979263911284619,
      "loss": 0.0725,
      "step": 2699
    },
    {
      "epoch": 0.01270026435365062,
      "grad_norm": 5.575409889221191,
      "learning_rate": 0.00019792544815034843,
      "loss": 0.6705,
      "step": 2700
    },
    {
      "epoch": 0.012704968155263084,
      "grad_norm": 2.07979154586792,
      "learning_rate": 0.00019792450517223495,
      "loss": 0.1583,
      "step": 2701
    },
    {
      "epoch": 0.012709671956875546,
      "grad_norm": 0.7171790599822998,
      "learning_rate": 0.0001979235621941215,
      "loss": 0.0729,
      "step": 2702
    },
    {
      "epoch": 0.01271437575848801,
      "grad_norm": 1.2802598476409912,
      "learning_rate": 0.000197922619216008,
      "loss": 0.1768,
      "step": 2703
    },
    {
      "epoch": 0.012719079560100472,
      "grad_norm": 1.665776014328003,
      "learning_rate": 0.00019792167623789453,
      "loss": 0.4405,
      "step": 2704
    },
    {
      "epoch": 0.012723783361712936,
      "grad_norm": 3.069411039352417,
      "learning_rate": 0.00019792073325978105,
      "loss": 0.4051,
      "step": 2705
    },
    {
      "epoch": 0.0127284871633254,
      "grad_norm": 1.079978585243225,
      "learning_rate": 0.0001979197902816676,
      "loss": 0.1378,
      "step": 2706
    },
    {
      "epoch": 0.012733190964937862,
      "grad_norm": 2.191873073577881,
      "learning_rate": 0.0001979188473035541,
      "loss": 0.1596,
      "step": 2707
    },
    {
      "epoch": 0.012737894766550326,
      "grad_norm": 0.5538754463195801,
      "learning_rate": 0.0001979179043254406,
      "loss": 0.0898,
      "step": 2708
    },
    {
      "epoch": 0.012742598568162788,
      "grad_norm": 1.3235350847244263,
      "learning_rate": 0.00019791696134732712,
      "loss": 0.1788,
      "step": 2709
    },
    {
      "epoch": 0.012747302369775252,
      "grad_norm": 1.4431982040405273,
      "learning_rate": 0.00019791601836921364,
      "loss": 0.1844,
      "step": 2710
    },
    {
      "epoch": 0.012752006171387716,
      "grad_norm": 0.7845215797424316,
      "learning_rate": 0.0001979150753911002,
      "loss": 0.0939,
      "step": 2711
    },
    {
      "epoch": 0.012756709973000178,
      "grad_norm": 1.3994930982589722,
      "learning_rate": 0.0001979141324129867,
      "loss": 0.1354,
      "step": 2712
    },
    {
      "epoch": 0.012761413774612642,
      "grad_norm": 2.4622669219970703,
      "learning_rate": 0.00019791318943487322,
      "loss": 0.2256,
      "step": 2713
    },
    {
      "epoch": 0.012766117576225105,
      "grad_norm": 3.925163984298706,
      "learning_rate": 0.00019791224645675974,
      "loss": 0.6895,
      "step": 2714
    },
    {
      "epoch": 0.012770821377837568,
      "grad_norm": 1.3283123970031738,
      "learning_rate": 0.00019791130347864626,
      "loss": 0.1122,
      "step": 2715
    },
    {
      "epoch": 0.012775525179450032,
      "grad_norm": 0.5289915204048157,
      "learning_rate": 0.0001979103605005328,
      "loss": 0.0557,
      "step": 2716
    },
    {
      "epoch": 0.012780228981062495,
      "grad_norm": 2.6909875869750977,
      "learning_rate": 0.00019790941752241933,
      "loss": 0.4331,
      "step": 2717
    },
    {
      "epoch": 0.012784932782674958,
      "grad_norm": 4.827743053436279,
      "learning_rate": 0.00019790847454430584,
      "loss": 0.7045,
      "step": 2718
    },
    {
      "epoch": 0.01278963658428742,
      "grad_norm": 3.731858253479004,
      "learning_rate": 0.00019790753156619236,
      "loss": 0.5908,
      "step": 2719
    },
    {
      "epoch": 0.012794340385899884,
      "grad_norm": 2.2481582164764404,
      "learning_rate": 0.00019790658858807888,
      "loss": 0.2569,
      "step": 2720
    },
    {
      "epoch": 0.012799044187512347,
      "grad_norm": 1.4324229955673218,
      "learning_rate": 0.0001979056456099654,
      "loss": 0.1089,
      "step": 2721
    },
    {
      "epoch": 0.01280374798912481,
      "grad_norm": 5.191833019256592,
      "learning_rate": 0.00019790470263185192,
      "loss": 0.7678,
      "step": 2722
    },
    {
      "epoch": 0.012808451790737274,
      "grad_norm": 5.075355529785156,
      "learning_rate": 0.00019790375965373844,
      "loss": 1.178,
      "step": 2723
    },
    {
      "epoch": 0.012813155592349737,
      "grad_norm": 3.6550674438476562,
      "learning_rate": 0.00019790281667562496,
      "loss": 0.1851,
      "step": 2724
    },
    {
      "epoch": 0.0128178593939622,
      "grad_norm": 3.712315797805786,
      "learning_rate": 0.0001979018736975115,
      "loss": 0.74,
      "step": 2725
    },
    {
      "epoch": 0.012822563195574663,
      "grad_norm": 0.26194947957992554,
      "learning_rate": 0.00019790093071939802,
      "loss": 0.0273,
      "step": 2726
    },
    {
      "epoch": 0.012827266997187127,
      "grad_norm": 1.3537061214447021,
      "learning_rate": 0.00019789998774128454,
      "loss": 0.1345,
      "step": 2727
    },
    {
      "epoch": 0.01283197079879959,
      "grad_norm": 3.2001240253448486,
      "learning_rate": 0.00019789904476317106,
      "loss": 0.6752,
      "step": 2728
    },
    {
      "epoch": 0.012836674600412053,
      "grad_norm": 0.4945340156555176,
      "learning_rate": 0.00019789810178505758,
      "loss": 0.0521,
      "step": 2729
    },
    {
      "epoch": 0.012841378402024517,
      "grad_norm": 2.422276258468628,
      "learning_rate": 0.0001978971588069441,
      "loss": 0.58,
      "step": 2730
    },
    {
      "epoch": 0.012846082203636979,
      "grad_norm": 2.6143007278442383,
      "learning_rate": 0.00019789621582883061,
      "loss": 0.6056,
      "step": 2731
    },
    {
      "epoch": 0.012850786005249443,
      "grad_norm": 1.3880653381347656,
      "learning_rate": 0.00019789527285071713,
      "loss": 0.2529,
      "step": 2732
    },
    {
      "epoch": 0.012855489806861907,
      "grad_norm": 0.7349313497543335,
      "learning_rate": 0.00019789432987260365,
      "loss": 0.1172,
      "step": 2733
    },
    {
      "epoch": 0.012860193608474369,
      "grad_norm": 0.831524670124054,
      "learning_rate": 0.0001978933868944902,
      "loss": 0.1553,
      "step": 2734
    },
    {
      "epoch": 0.012864897410086833,
      "grad_norm": 1.6956787109375,
      "learning_rate": 0.00019789244391637672,
      "loss": 0.223,
      "step": 2735
    },
    {
      "epoch": 0.012869601211699295,
      "grad_norm": 2.2425265312194824,
      "learning_rate": 0.00019789150093826323,
      "loss": 0.2894,
      "step": 2736
    },
    {
      "epoch": 0.012874305013311759,
      "grad_norm": 0.5284700989723206,
      "learning_rate": 0.00019789055796014975,
      "loss": 0.0361,
      "step": 2737
    },
    {
      "epoch": 0.012879008814924221,
      "grad_norm": 1.6631498336791992,
      "learning_rate": 0.0001978896149820363,
      "loss": 0.2726,
      "step": 2738
    },
    {
      "epoch": 0.012883712616536685,
      "grad_norm": 1.1657088994979858,
      "learning_rate": 0.0001978886720039228,
      "loss": 0.1329,
      "step": 2739
    },
    {
      "epoch": 0.012888416418149149,
      "grad_norm": 1.8864418268203735,
      "learning_rate": 0.0001978877290258093,
      "loss": 0.3189,
      "step": 2740
    },
    {
      "epoch": 0.012893120219761611,
      "grad_norm": 1.3367301225662231,
      "learning_rate": 0.00019788678604769583,
      "loss": 0.1813,
      "step": 2741
    },
    {
      "epoch": 0.012897824021374075,
      "grad_norm": 1.1698756217956543,
      "learning_rate": 0.00019788584306958235,
      "loss": 0.2046,
      "step": 2742
    },
    {
      "epoch": 0.012902527822986537,
      "grad_norm": 1.9733256101608276,
      "learning_rate": 0.0001978849000914689,
      "loss": 0.4351,
      "step": 2743
    },
    {
      "epoch": 0.012907231624599001,
      "grad_norm": 3.277961015701294,
      "learning_rate": 0.0001978839571133554,
      "loss": 0.9081,
      "step": 2744
    },
    {
      "epoch": 0.012911935426211465,
      "grad_norm": 2.945483446121216,
      "learning_rate": 0.00019788301413524193,
      "loss": 0.5009,
      "step": 2745
    },
    {
      "epoch": 0.012916639227823927,
      "grad_norm": 1.0473741292953491,
      "learning_rate": 0.00019788207115712845,
      "loss": 0.2478,
      "step": 2746
    },
    {
      "epoch": 0.012921343029436391,
      "grad_norm": 0.8308587074279785,
      "learning_rate": 0.000197881128179015,
      "loss": 0.2069,
      "step": 2747
    },
    {
      "epoch": 0.012926046831048853,
      "grad_norm": 1.4784350395202637,
      "learning_rate": 0.0001978801852009015,
      "loss": 0.3778,
      "step": 2748
    },
    {
      "epoch": 0.012930750632661317,
      "grad_norm": 1.5217405557632446,
      "learning_rate": 0.00019787924222278803,
      "loss": 0.2511,
      "step": 2749
    },
    {
      "epoch": 0.012935454434273781,
      "grad_norm": 1.8072149753570557,
      "learning_rate": 0.00019787829924467455,
      "loss": 0.2211,
      "step": 2750
    },
    {
      "epoch": 0.012940158235886243,
      "grad_norm": 3.040539264678955,
      "learning_rate": 0.00019787735626656104,
      "loss": 0.5179,
      "step": 2751
    },
    {
      "epoch": 0.012944862037498707,
      "grad_norm": 0.4399884343147278,
      "learning_rate": 0.0001978764132884476,
      "loss": 0.0508,
      "step": 2752
    },
    {
      "epoch": 0.012949565839111169,
      "grad_norm": 2.1175925731658936,
      "learning_rate": 0.0001978754703103341,
      "loss": 0.3668,
      "step": 2753
    },
    {
      "epoch": 0.012954269640723633,
      "grad_norm": 1.6562780141830444,
      "learning_rate": 0.00019787452733222062,
      "loss": 0.259,
      "step": 2754
    },
    {
      "epoch": 0.012958973442336095,
      "grad_norm": 3.784193754196167,
      "learning_rate": 0.00019787358435410714,
      "loss": 0.6928,
      "step": 2755
    },
    {
      "epoch": 0.012963677243948559,
      "grad_norm": 4.491770267486572,
      "learning_rate": 0.0001978726413759937,
      "loss": 0.8627,
      "step": 2756
    },
    {
      "epoch": 0.012968381045561023,
      "grad_norm": 1.356396198272705,
      "learning_rate": 0.0001978716983978802,
      "loss": 0.1654,
      "step": 2757
    },
    {
      "epoch": 0.012973084847173485,
      "grad_norm": 4.006329536437988,
      "learning_rate": 0.00019787075541976673,
      "loss": 0.7549,
      "step": 2758
    },
    {
      "epoch": 0.012977788648785949,
      "grad_norm": 4.609677791595459,
      "learning_rate": 0.00019786981244165324,
      "loss": 0.6066,
      "step": 2759
    },
    {
      "epoch": 0.012982492450398411,
      "grad_norm": 1.7859395742416382,
      "learning_rate": 0.00019786886946353976,
      "loss": 0.3637,
      "step": 2760
    },
    {
      "epoch": 0.012987196252010875,
      "grad_norm": 2.7245562076568604,
      "learning_rate": 0.00019786792648542628,
      "loss": 0.5748,
      "step": 2761
    },
    {
      "epoch": 0.012991900053623339,
      "grad_norm": 0.3477442264556885,
      "learning_rate": 0.0001978669835073128,
      "loss": 0.0361,
      "step": 2762
    },
    {
      "epoch": 0.012996603855235801,
      "grad_norm": 2.776435613632202,
      "learning_rate": 0.00019786604052919932,
      "loss": 0.6889,
      "step": 2763
    },
    {
      "epoch": 0.013001307656848265,
      "grad_norm": 2.1473278999328613,
      "learning_rate": 0.00019786509755108584,
      "loss": 0.2886,
      "step": 2764
    },
    {
      "epoch": 0.013006011458460727,
      "grad_norm": 0.7345065474510193,
      "learning_rate": 0.00019786415457297236,
      "loss": 0.0818,
      "step": 2765
    },
    {
      "epoch": 0.013010715260073191,
      "grad_norm": 1.1328626871109009,
      "learning_rate": 0.0001978632115948589,
      "loss": 0.1567,
      "step": 2766
    },
    {
      "epoch": 0.013015419061685655,
      "grad_norm": 1.2157095670700073,
      "learning_rate": 0.00019786226861674542,
      "loss": 0.095,
      "step": 2767
    },
    {
      "epoch": 0.013020122863298117,
      "grad_norm": 0.8936434984207153,
      "learning_rate": 0.00019786132563863194,
      "loss": 0.1141,
      "step": 2768
    },
    {
      "epoch": 0.013024826664910581,
      "grad_norm": 1.2305107116699219,
      "learning_rate": 0.00019786038266051846,
      "loss": 0.1547,
      "step": 2769
    },
    {
      "epoch": 0.013029530466523043,
      "grad_norm": 1.1841247081756592,
      "learning_rate": 0.00019785943968240498,
      "loss": 0.1894,
      "step": 2770
    },
    {
      "epoch": 0.013034234268135507,
      "grad_norm": 1.452366828918457,
      "learning_rate": 0.0001978584967042915,
      "loss": 0.151,
      "step": 2771
    },
    {
      "epoch": 0.01303893806974797,
      "grad_norm": 3.5492630004882812,
      "learning_rate": 0.00019785755372617801,
      "loss": 0.4474,
      "step": 2772
    },
    {
      "epoch": 0.013043641871360433,
      "grad_norm": 1.2502068281173706,
      "learning_rate": 0.00019785661074806453,
      "loss": 0.1357,
      "step": 2773
    },
    {
      "epoch": 0.013048345672972897,
      "grad_norm": 0.4862353801727295,
      "learning_rate": 0.00019785566776995105,
      "loss": 0.0681,
      "step": 2774
    },
    {
      "epoch": 0.01305304947458536,
      "grad_norm": 1.3946533203125,
      "learning_rate": 0.0001978547247918376,
      "loss": 0.2666,
      "step": 2775
    },
    {
      "epoch": 0.013057753276197823,
      "grad_norm": 1.4392656087875366,
      "learning_rate": 0.00019785378181372412,
      "loss": 0.3704,
      "step": 2776
    },
    {
      "epoch": 0.013062457077810286,
      "grad_norm": 2.1305394172668457,
      "learning_rate": 0.00019785283883561063,
      "loss": 0.3599,
      "step": 2777
    },
    {
      "epoch": 0.01306716087942275,
      "grad_norm": 0.7235758304595947,
      "learning_rate": 0.00019785189585749715,
      "loss": 0.0681,
      "step": 2778
    },
    {
      "epoch": 0.013071864681035213,
      "grad_norm": 0.9037313461303711,
      "learning_rate": 0.0001978509528793837,
      "loss": 0.0537,
      "step": 2779
    },
    {
      "epoch": 0.013076568482647676,
      "grad_norm": 1.4158114194869995,
      "learning_rate": 0.00019785000990127022,
      "loss": 0.2232,
      "step": 2780
    },
    {
      "epoch": 0.01308127228426014,
      "grad_norm": 0.6954839825630188,
      "learning_rate": 0.0001978490669231567,
      "loss": 0.1116,
      "step": 2781
    },
    {
      "epoch": 0.013085976085872602,
      "grad_norm": 1.579830527305603,
      "learning_rate": 0.00019784812394504323,
      "loss": 0.1551,
      "step": 2782
    },
    {
      "epoch": 0.013090679887485066,
      "grad_norm": 2.2010462284088135,
      "learning_rate": 0.00019784718096692975,
      "loss": 0.5475,
      "step": 2783
    },
    {
      "epoch": 0.01309538368909753,
      "grad_norm": 1.439206838607788,
      "learning_rate": 0.0001978462379888163,
      "loss": 0.1855,
      "step": 2784
    },
    {
      "epoch": 0.013100087490709992,
      "grad_norm": 1.680843472480774,
      "learning_rate": 0.0001978452950107028,
      "loss": 0.3548,
      "step": 2785
    },
    {
      "epoch": 0.013104791292322455,
      "grad_norm": 1.593355417251587,
      "learning_rate": 0.00019784435203258933,
      "loss": 0.375,
      "step": 2786
    },
    {
      "epoch": 0.013109495093934918,
      "grad_norm": 2.6798384189605713,
      "learning_rate": 0.00019784340905447585,
      "loss": 0.4337,
      "step": 2787
    },
    {
      "epoch": 0.013114198895547382,
      "grad_norm": 1.362717628479004,
      "learning_rate": 0.0001978424660763624,
      "loss": 0.2697,
      "step": 2788
    },
    {
      "epoch": 0.013118902697159844,
      "grad_norm": 0.83148592710495,
      "learning_rate": 0.0001978415230982489,
      "loss": 0.142,
      "step": 2789
    },
    {
      "epoch": 0.013123606498772308,
      "grad_norm": 3.2466983795166016,
      "learning_rate": 0.00019784058012013543,
      "loss": 0.6696,
      "step": 2790
    },
    {
      "epoch": 0.013128310300384772,
      "grad_norm": 2.0272395610809326,
      "learning_rate": 0.00019783963714202195,
      "loss": 0.445,
      "step": 2791
    },
    {
      "epoch": 0.013133014101997234,
      "grad_norm": 1.523019790649414,
      "learning_rate": 0.00019783869416390847,
      "loss": 0.2441,
      "step": 2792
    },
    {
      "epoch": 0.013137717903609698,
      "grad_norm": 1.4024548530578613,
      "learning_rate": 0.000197837751185795,
      "loss": 0.4054,
      "step": 2793
    },
    {
      "epoch": 0.01314242170522216,
      "grad_norm": 1.506809949874878,
      "learning_rate": 0.0001978368082076815,
      "loss": 0.2841,
      "step": 2794
    },
    {
      "epoch": 0.013147125506834624,
      "grad_norm": 2.376338481903076,
      "learning_rate": 0.00019783586522956802,
      "loss": 0.3532,
      "step": 2795
    },
    {
      "epoch": 0.013151829308447088,
      "grad_norm": 1.1002403497695923,
      "learning_rate": 0.00019783492225145454,
      "loss": 0.084,
      "step": 2796
    },
    {
      "epoch": 0.01315653311005955,
      "grad_norm": 2.9523110389709473,
      "learning_rate": 0.0001978339792733411,
      "loss": 0.5829,
      "step": 2797
    },
    {
      "epoch": 0.013161236911672014,
      "grad_norm": 1.4394323825836182,
      "learning_rate": 0.0001978330362952276,
      "loss": 0.2138,
      "step": 2798
    },
    {
      "epoch": 0.013165940713284476,
      "grad_norm": 1.58159601688385,
      "learning_rate": 0.00019783209331711413,
      "loss": 0.2559,
      "step": 2799
    },
    {
      "epoch": 0.01317064451489694,
      "grad_norm": 1.2776333093643188,
      "learning_rate": 0.00019783115033900064,
      "loss": 0.2812,
      "step": 2800
    },
    {
      "epoch": 0.013175348316509404,
      "grad_norm": 1.098767876625061,
      "learning_rate": 0.00019783020736088716,
      "loss": 0.1745,
      "step": 2801
    },
    {
      "epoch": 0.013180052118121866,
      "grad_norm": 0.46830669045448303,
      "learning_rate": 0.00019782926438277368,
      "loss": 0.0425,
      "step": 2802
    },
    {
      "epoch": 0.01318475591973433,
      "grad_norm": 2.3956501483917236,
      "learning_rate": 0.0001978283214046602,
      "loss": 0.3832,
      "step": 2803
    },
    {
      "epoch": 0.013189459721346792,
      "grad_norm": 2.4626212120056152,
      "learning_rate": 0.00019782737842654672,
      "loss": 0.2268,
      "step": 2804
    },
    {
      "epoch": 0.013194163522959256,
      "grad_norm": 0.972263753414154,
      "learning_rate": 0.00019782643544843324,
      "loss": 0.1225,
      "step": 2805
    },
    {
      "epoch": 0.013198867324571718,
      "grad_norm": 2.896083354949951,
      "learning_rate": 0.00019782549247031978,
      "loss": 0.3958,
      "step": 2806
    },
    {
      "epoch": 0.013203571126184182,
      "grad_norm": 0.7589747905731201,
      "learning_rate": 0.0001978245494922063,
      "loss": 0.1396,
      "step": 2807
    },
    {
      "epoch": 0.013208274927796646,
      "grad_norm": 3.3707261085510254,
      "learning_rate": 0.00019782360651409282,
      "loss": 0.6604,
      "step": 2808
    },
    {
      "epoch": 0.013212978729409108,
      "grad_norm": 1.1643568277359009,
      "learning_rate": 0.00019782266353597934,
      "loss": 0.1522,
      "step": 2809
    },
    {
      "epoch": 0.013217682531021572,
      "grad_norm": 1.3718055486679077,
      "learning_rate": 0.00019782172055786586,
      "loss": 0.2433,
      "step": 2810
    },
    {
      "epoch": 0.013222386332634034,
      "grad_norm": 0.6615086197853088,
      "learning_rate": 0.0001978207775797524,
      "loss": 0.0635,
      "step": 2811
    },
    {
      "epoch": 0.013227090134246498,
      "grad_norm": 1.077634572982788,
      "learning_rate": 0.0001978198346016389,
      "loss": 0.0996,
      "step": 2812
    },
    {
      "epoch": 0.013231793935858962,
      "grad_norm": 0.8997638821601868,
      "learning_rate": 0.00019781889162352541,
      "loss": 0.1589,
      "step": 2813
    },
    {
      "epoch": 0.013236497737471424,
      "grad_norm": 1.262187123298645,
      "learning_rate": 0.00019781794864541193,
      "loss": 0.12,
      "step": 2814
    },
    {
      "epoch": 0.013241201539083888,
      "grad_norm": 2.0181775093078613,
      "learning_rate": 0.00019781700566729845,
      "loss": 0.0459,
      "step": 2815
    },
    {
      "epoch": 0.01324590534069635,
      "grad_norm": 3.3003711700439453,
      "learning_rate": 0.000197816062689185,
      "loss": 0.4054,
      "step": 2816
    },
    {
      "epoch": 0.013250609142308814,
      "grad_norm": 1.0079864263534546,
      "learning_rate": 0.00019781511971107152,
      "loss": 0.1088,
      "step": 2817
    },
    {
      "epoch": 0.013255312943921278,
      "grad_norm": 8.72926139831543,
      "learning_rate": 0.00019781417673295803,
      "loss": 0.1721,
      "step": 2818
    },
    {
      "epoch": 0.01326001674553374,
      "grad_norm": 1.6006269454956055,
      "learning_rate": 0.00019781323375484455,
      "loss": 0.1481,
      "step": 2819
    },
    {
      "epoch": 0.013264720547146204,
      "grad_norm": 2.3395533561706543,
      "learning_rate": 0.0001978122907767311,
      "loss": 0.0815,
      "step": 2820
    },
    {
      "epoch": 0.013269424348758666,
      "grad_norm": 0.92242032289505,
      "learning_rate": 0.00019781134779861762,
      "loss": 0.0338,
      "step": 2821
    },
    {
      "epoch": 0.01327412815037113,
      "grad_norm": 3.9034550189971924,
      "learning_rate": 0.00019781040482050414,
      "loss": 0.2608,
      "step": 2822
    },
    {
      "epoch": 0.013278831951983594,
      "grad_norm": 2.1217916011810303,
      "learning_rate": 0.00019780946184239066,
      "loss": 0.136,
      "step": 2823
    },
    {
      "epoch": 0.013283535753596056,
      "grad_norm": 1.8673440217971802,
      "learning_rate": 0.00019780851886427715,
      "loss": 0.1797,
      "step": 2824
    },
    {
      "epoch": 0.01328823955520852,
      "grad_norm": 4.026325702667236,
      "learning_rate": 0.0001978075758861637,
      "loss": 0.3604,
      "step": 2825
    },
    {
      "epoch": 0.013292943356820982,
      "grad_norm": 2.256037712097168,
      "learning_rate": 0.0001978066329080502,
      "loss": 0.1697,
      "step": 2826
    },
    {
      "epoch": 0.013297647158433446,
      "grad_norm": 0.47419634461402893,
      "learning_rate": 0.00019780568992993673,
      "loss": 0.0187,
      "step": 2827
    },
    {
      "epoch": 0.013302350960045908,
      "grad_norm": 1.3282997608184814,
      "learning_rate": 0.00019780474695182325,
      "loss": 0.075,
      "step": 2828
    },
    {
      "epoch": 0.013307054761658372,
      "grad_norm": 6.188976287841797,
      "learning_rate": 0.0001978038039737098,
      "loss": 0.5757,
      "step": 2829
    },
    {
      "epoch": 0.013311758563270836,
      "grad_norm": 5.585386753082275,
      "learning_rate": 0.0001978028609955963,
      "loss": 0.7175,
      "step": 2830
    },
    {
      "epoch": 0.013316462364883298,
      "grad_norm": 2.717055320739746,
      "learning_rate": 0.00019780191801748283,
      "loss": 0.2394,
      "step": 2831
    },
    {
      "epoch": 0.013321166166495762,
      "grad_norm": 5.759832859039307,
      "learning_rate": 0.00019780097503936935,
      "loss": 0.7985,
      "step": 2832
    },
    {
      "epoch": 0.013325869968108224,
      "grad_norm": 6.234359264373779,
      "learning_rate": 0.00019780003206125587,
      "loss": 1.2888,
      "step": 2833
    },
    {
      "epoch": 0.013330573769720688,
      "grad_norm": 2.751328468322754,
      "learning_rate": 0.0001977990890831424,
      "loss": 0.326,
      "step": 2834
    },
    {
      "epoch": 0.013335277571333152,
      "grad_norm": 2.9689533710479736,
      "learning_rate": 0.0001977981461050289,
      "loss": 0.488,
      "step": 2835
    },
    {
      "epoch": 0.013339981372945614,
      "grad_norm": 5.52858304977417,
      "learning_rate": 0.00019779720312691542,
      "loss": 0.8432,
      "step": 2836
    },
    {
      "epoch": 0.013344685174558078,
      "grad_norm": 0.9980677962303162,
      "learning_rate": 0.00019779626014880194,
      "loss": 0.0757,
      "step": 2837
    },
    {
      "epoch": 0.01334938897617054,
      "grad_norm": 0.967671275138855,
      "learning_rate": 0.0001977953171706885,
      "loss": 0.1362,
      "step": 2838
    },
    {
      "epoch": 0.013354092777783004,
      "grad_norm": 3.7332279682159424,
      "learning_rate": 0.000197794374192575,
      "loss": 0.2167,
      "step": 2839
    },
    {
      "epoch": 0.013358796579395468,
      "grad_norm": 2.3589279651641846,
      "learning_rate": 0.00019779343121446153,
      "loss": 0.4208,
      "step": 2840
    },
    {
      "epoch": 0.01336350038100793,
      "grad_norm": 3.2028446197509766,
      "learning_rate": 0.00019779248823634804,
      "loss": 0.5941,
      "step": 2841
    },
    {
      "epoch": 0.013368204182620394,
      "grad_norm": 0.8726177215576172,
      "learning_rate": 0.00019779154525823456,
      "loss": 0.1883,
      "step": 2842
    },
    {
      "epoch": 0.013372907984232857,
      "grad_norm": 1.0509487390518188,
      "learning_rate": 0.00019779060228012108,
      "loss": 0.1532,
      "step": 2843
    },
    {
      "epoch": 0.01337761178584532,
      "grad_norm": 4.429160118103027,
      "learning_rate": 0.0001977896593020076,
      "loss": 0.4836,
      "step": 2844
    },
    {
      "epoch": 0.013382315587457783,
      "grad_norm": 1.5113294124603271,
      "learning_rate": 0.00019778871632389412,
      "loss": 0.4634,
      "step": 2845
    },
    {
      "epoch": 0.013387019389070247,
      "grad_norm": 2.011472225189209,
      "learning_rate": 0.00019778777334578064,
      "loss": 0.4266,
      "step": 2846
    },
    {
      "epoch": 0.01339172319068271,
      "grad_norm": 0.05154822766780853,
      "learning_rate": 0.00019778683036766718,
      "loss": 0.0033,
      "step": 2847
    },
    {
      "epoch": 0.013396426992295173,
      "grad_norm": 2.4787752628326416,
      "learning_rate": 0.0001977858873895537,
      "loss": 0.2835,
      "step": 2848
    },
    {
      "epoch": 0.013401130793907636,
      "grad_norm": 2.888673782348633,
      "learning_rate": 0.00019778494441144022,
      "loss": 0.4206,
      "step": 2849
    },
    {
      "epoch": 0.013405834595520099,
      "grad_norm": 1.5629925727844238,
      "learning_rate": 0.00019778400143332674,
      "loss": 0.3118,
      "step": 2850
    },
    {
      "epoch": 0.013410538397132563,
      "grad_norm": 0.8731178045272827,
      "learning_rate": 0.00019778305845521326,
      "loss": 0.0173,
      "step": 2851
    },
    {
      "epoch": 0.013415242198745026,
      "grad_norm": 3.8122434616088867,
      "learning_rate": 0.0001977821154770998,
      "loss": 0.3952,
      "step": 2852
    },
    {
      "epoch": 0.013419946000357489,
      "grad_norm": 2.485255718231201,
      "learning_rate": 0.00019778117249898632,
      "loss": 0.3835,
      "step": 2853
    },
    {
      "epoch": 0.013424649801969953,
      "grad_norm": 0.5552130937576294,
      "learning_rate": 0.00019778022952087284,
      "loss": 0.0464,
      "step": 2854
    },
    {
      "epoch": 0.013429353603582415,
      "grad_norm": 2.9137396812438965,
      "learning_rate": 0.00019777928654275933,
      "loss": 0.2686,
      "step": 2855
    },
    {
      "epoch": 0.013434057405194879,
      "grad_norm": 3.580070972442627,
      "learning_rate": 0.00019777834356464588,
      "loss": 0.1665,
      "step": 2856
    },
    {
      "epoch": 0.013438761206807343,
      "grad_norm": 2.1461069583892822,
      "learning_rate": 0.0001977774005865324,
      "loss": 0.3014,
      "step": 2857
    },
    {
      "epoch": 0.013443465008419805,
      "grad_norm": 2.771324872970581,
      "learning_rate": 0.00019777645760841892,
      "loss": 0.4655,
      "step": 2858
    },
    {
      "epoch": 0.013448168810032269,
      "grad_norm": 3.119110584259033,
      "learning_rate": 0.00019777551463030543,
      "loss": 0.4243,
      "step": 2859
    },
    {
      "epoch": 0.01345287261164473,
      "grad_norm": 1.6667497158050537,
      "learning_rate": 0.00019777457165219195,
      "loss": 0.2814,
      "step": 2860
    },
    {
      "epoch": 0.013457576413257195,
      "grad_norm": 2.073639154434204,
      "learning_rate": 0.0001977736286740785,
      "loss": 0.1638,
      "step": 2861
    },
    {
      "epoch": 0.013462280214869657,
      "grad_norm": 2.5751328468322754,
      "learning_rate": 0.00019777268569596502,
      "loss": 0.2594,
      "step": 2862
    },
    {
      "epoch": 0.01346698401648212,
      "grad_norm": 2.2324767112731934,
      "learning_rate": 0.00019777174271785154,
      "loss": 0.308,
      "step": 2863
    },
    {
      "epoch": 0.013471687818094585,
      "grad_norm": 2.368873119354248,
      "learning_rate": 0.00019777079973973806,
      "loss": 0.2827,
      "step": 2864
    },
    {
      "epoch": 0.013476391619707047,
      "grad_norm": 1.5985738039016724,
      "learning_rate": 0.00019776985676162457,
      "loss": 0.4717,
      "step": 2865
    },
    {
      "epoch": 0.01348109542131951,
      "grad_norm": 0.7418145537376404,
      "learning_rate": 0.0001977689137835111,
      "loss": 0.1063,
      "step": 2866
    },
    {
      "epoch": 0.013485799222931973,
      "grad_norm": 1.7944636344909668,
      "learning_rate": 0.0001977679708053976,
      "loss": 0.1717,
      "step": 2867
    },
    {
      "epoch": 0.013490503024544437,
      "grad_norm": 2.389728546142578,
      "learning_rate": 0.00019776702782728413,
      "loss": 0.2226,
      "step": 2868
    },
    {
      "epoch": 0.0134952068261569,
      "grad_norm": 3.986734628677368,
      "learning_rate": 0.00019776608484917065,
      "loss": 0.6893,
      "step": 2869
    },
    {
      "epoch": 0.013499910627769363,
      "grad_norm": 0.7164063453674316,
      "learning_rate": 0.0001977651418710572,
      "loss": 0.0788,
      "step": 2870
    },
    {
      "epoch": 0.013504614429381827,
      "grad_norm": 2.9347476959228516,
      "learning_rate": 0.0001977641988929437,
      "loss": 0.5542,
      "step": 2871
    },
    {
      "epoch": 0.013509318230994289,
      "grad_norm": 0.9513353705406189,
      "learning_rate": 0.00019776325591483023,
      "loss": 0.1008,
      "step": 2872
    },
    {
      "epoch": 0.013514022032606753,
      "grad_norm": 3.9183290004730225,
      "learning_rate": 0.00019776231293671675,
      "loss": 0.3328,
      "step": 2873
    },
    {
      "epoch": 0.013518725834219217,
      "grad_norm": 2.408752918243408,
      "learning_rate": 0.00019776136995860327,
      "loss": 0.4079,
      "step": 2874
    },
    {
      "epoch": 0.013523429635831679,
      "grad_norm": 2.2849247455596924,
      "learning_rate": 0.0001977604269804898,
      "loss": 0.2918,
      "step": 2875
    },
    {
      "epoch": 0.013528133437444143,
      "grad_norm": 2.4602739810943604,
      "learning_rate": 0.0001977594840023763,
      "loss": 0.5147,
      "step": 2876
    },
    {
      "epoch": 0.013532837239056605,
      "grad_norm": 0.6617171168327332,
      "learning_rate": 0.00019775854102426282,
      "loss": 0.0848,
      "step": 2877
    },
    {
      "epoch": 0.013537541040669069,
      "grad_norm": 0.9137336015701294,
      "learning_rate": 0.00019775759804614934,
      "loss": 0.0948,
      "step": 2878
    },
    {
      "epoch": 0.013542244842281531,
      "grad_norm": 1.4181170463562012,
      "learning_rate": 0.0001977566550680359,
      "loss": 0.165,
      "step": 2879
    },
    {
      "epoch": 0.013546948643893995,
      "grad_norm": 0.447442889213562,
      "learning_rate": 0.0001977557120899224,
      "loss": 0.0349,
      "step": 2880
    },
    {
      "epoch": 0.013551652445506459,
      "grad_norm": 2.117727279663086,
      "learning_rate": 0.00019775476911180893,
      "loss": 0.5475,
      "step": 2881
    },
    {
      "epoch": 0.013556356247118921,
      "grad_norm": 2.331247568130493,
      "learning_rate": 0.00019775382613369544,
      "loss": 0.2389,
      "step": 2882
    },
    {
      "epoch": 0.013561060048731385,
      "grad_norm": 0.20756438374519348,
      "learning_rate": 0.00019775288315558196,
      "loss": 0.0156,
      "step": 2883
    },
    {
      "epoch": 0.013565763850343847,
      "grad_norm": 1.8916364908218384,
      "learning_rate": 0.0001977519401774685,
      "loss": 0.1962,
      "step": 2884
    },
    {
      "epoch": 0.013570467651956311,
      "grad_norm": 2.822420835494995,
      "learning_rate": 0.00019775099719935503,
      "loss": 0.9352,
      "step": 2885
    },
    {
      "epoch": 0.013575171453568775,
      "grad_norm": 2.084717273712158,
      "learning_rate": 0.00019775005422124152,
      "loss": 0.1537,
      "step": 2886
    },
    {
      "epoch": 0.013579875255181237,
      "grad_norm": 1.814671516418457,
      "learning_rate": 0.00019774911124312804,
      "loss": 0.3141,
      "step": 2887
    },
    {
      "epoch": 0.013584579056793701,
      "grad_norm": 0.35270676016807556,
      "learning_rate": 0.00019774816826501458,
      "loss": 0.0426,
      "step": 2888
    },
    {
      "epoch": 0.013589282858406163,
      "grad_norm": 1.1291102170944214,
      "learning_rate": 0.0001977472252869011,
      "loss": 0.2625,
      "step": 2889
    },
    {
      "epoch": 0.013593986660018627,
      "grad_norm": 1.2462806701660156,
      "learning_rate": 0.00019774628230878762,
      "loss": 0.2022,
      "step": 2890
    },
    {
      "epoch": 0.013598690461631091,
      "grad_norm": 1.2088948488235474,
      "learning_rate": 0.00019774533933067414,
      "loss": 0.2252,
      "step": 2891
    },
    {
      "epoch": 0.013603394263243553,
      "grad_norm": 1.2659553289413452,
      "learning_rate": 0.00019774439635256066,
      "loss": 0.2204,
      "step": 2892
    },
    {
      "epoch": 0.013608098064856017,
      "grad_norm": 1.86069655418396,
      "learning_rate": 0.0001977434533744472,
      "loss": 0.2673,
      "step": 2893
    },
    {
      "epoch": 0.01361280186646848,
      "grad_norm": 1.4490617513656616,
      "learning_rate": 0.00019774251039633372,
      "loss": 0.27,
      "step": 2894
    },
    {
      "epoch": 0.013617505668080943,
      "grad_norm": 1.3853923082351685,
      "learning_rate": 0.00019774156741822024,
      "loss": 0.3302,
      "step": 2895
    },
    {
      "epoch": 0.013622209469693405,
      "grad_norm": 0.5785676836967468,
      "learning_rate": 0.00019774062444010676,
      "loss": 0.054,
      "step": 2896
    },
    {
      "epoch": 0.01362691327130587,
      "grad_norm": 1.0450483560562134,
      "learning_rate": 0.00019773968146199328,
      "loss": 0.1094,
      "step": 2897
    },
    {
      "epoch": 0.013631617072918333,
      "grad_norm": 0.6482528448104858,
      "learning_rate": 0.0001977387384838798,
      "loss": 0.0621,
      "step": 2898
    },
    {
      "epoch": 0.013636320874530795,
      "grad_norm": 1.2655881643295288,
      "learning_rate": 0.00019773779550576632,
      "loss": 0.1595,
      "step": 2899
    },
    {
      "epoch": 0.01364102467614326,
      "grad_norm": 1.3485915660858154,
      "learning_rate": 0.00019773685252765283,
      "loss": 0.2376,
      "step": 2900
    },
    {
      "epoch": 0.013645728477755721,
      "grad_norm": 1.1752643585205078,
      "learning_rate": 0.00019773590954953935,
      "loss": 0.0771,
      "step": 2901
    },
    {
      "epoch": 0.013650432279368185,
      "grad_norm": 1.0749801397323608,
      "learning_rate": 0.0001977349665714259,
      "loss": 0.0778,
      "step": 2902
    },
    {
      "epoch": 0.01365513608098065,
      "grad_norm": 1.5833771228790283,
      "learning_rate": 0.00019773402359331242,
      "loss": 0.1502,
      "step": 2903
    },
    {
      "epoch": 0.013659839882593111,
      "grad_norm": 1.0613360404968262,
      "learning_rate": 0.00019773308061519894,
      "loss": 0.1364,
      "step": 2904
    },
    {
      "epoch": 0.013664543684205575,
      "grad_norm": 1.423197865486145,
      "learning_rate": 0.00019773213763708546,
      "loss": 0.1452,
      "step": 2905
    },
    {
      "epoch": 0.013669247485818038,
      "grad_norm": 1.7810908555984497,
      "learning_rate": 0.00019773119465897197,
      "loss": 0.2206,
      "step": 2906
    },
    {
      "epoch": 0.013673951287430501,
      "grad_norm": 2.363640308380127,
      "learning_rate": 0.0001977302516808585,
      "loss": 0.4681,
      "step": 2907
    },
    {
      "epoch": 0.013678655089042965,
      "grad_norm": 2.289841413497925,
      "learning_rate": 0.000197729308702745,
      "loss": 0.2835,
      "step": 2908
    },
    {
      "epoch": 0.013683358890655428,
      "grad_norm": 2.344860792160034,
      "learning_rate": 0.00019772836572463153,
      "loss": 0.3082,
      "step": 2909
    },
    {
      "epoch": 0.013688062692267891,
      "grad_norm": 1.4004929065704346,
      "learning_rate": 0.00019772742274651805,
      "loss": 0.0878,
      "step": 2910
    },
    {
      "epoch": 0.013692766493880354,
      "grad_norm": 3.888485908508301,
      "learning_rate": 0.0001977264797684046,
      "loss": 0.5816,
      "step": 2911
    },
    {
      "epoch": 0.013697470295492817,
      "grad_norm": 3.7989141941070557,
      "learning_rate": 0.0001977255367902911,
      "loss": 0.3008,
      "step": 2912
    },
    {
      "epoch": 0.01370217409710528,
      "grad_norm": 3.014822244644165,
      "learning_rate": 0.00019772459381217763,
      "loss": 0.4727,
      "step": 2913
    },
    {
      "epoch": 0.013706877898717744,
      "grad_norm": 3.209871292114258,
      "learning_rate": 0.00019772365083406415,
      "loss": 0.7167,
      "step": 2914
    },
    {
      "epoch": 0.013711581700330207,
      "grad_norm": 0.5269498229026794,
      "learning_rate": 0.00019772270785595067,
      "loss": 0.0529,
      "step": 2915
    },
    {
      "epoch": 0.01371628550194267,
      "grad_norm": 2.6286544799804688,
      "learning_rate": 0.00019772176487783721,
      "loss": 0.3014,
      "step": 2916
    },
    {
      "epoch": 0.013720989303555134,
      "grad_norm": 12.342302322387695,
      "learning_rate": 0.0001977208218997237,
      "loss": 0.6997,
      "step": 2917
    },
    {
      "epoch": 0.013725693105167596,
      "grad_norm": 1.8515031337738037,
      "learning_rate": 0.00019771987892161022,
      "loss": 0.1852,
      "step": 2918
    },
    {
      "epoch": 0.01373039690678006,
      "grad_norm": 0.39741063117980957,
      "learning_rate": 0.00019771893594349674,
      "loss": 0.0212,
      "step": 2919
    },
    {
      "epoch": 0.013735100708392524,
      "grad_norm": 2.9627676010131836,
      "learning_rate": 0.0001977179929653833,
      "loss": 0.3193,
      "step": 2920
    },
    {
      "epoch": 0.013739804510004986,
      "grad_norm": 1.3465694189071655,
      "learning_rate": 0.0001977170499872698,
      "loss": 0.1307,
      "step": 2921
    },
    {
      "epoch": 0.01374450831161745,
      "grad_norm": 1.266767144203186,
      "learning_rate": 0.00019771610700915633,
      "loss": 0.1242,
      "step": 2922
    },
    {
      "epoch": 0.013749212113229912,
      "grad_norm": 0.762292206287384,
      "learning_rate": 0.00019771516403104284,
      "loss": 0.0466,
      "step": 2923
    },
    {
      "epoch": 0.013753915914842376,
      "grad_norm": 2.0707452297210693,
      "learning_rate": 0.00019771422105292936,
      "loss": 0.1846,
      "step": 2924
    },
    {
      "epoch": 0.01375861971645484,
      "grad_norm": 3.8857154846191406,
      "learning_rate": 0.0001977132780748159,
      "loss": 0.7728,
      "step": 2925
    },
    {
      "epoch": 0.013763323518067302,
      "grad_norm": 1.6388318538665771,
      "learning_rate": 0.00019771233509670243,
      "loss": 0.239,
      "step": 2926
    },
    {
      "epoch": 0.013768027319679766,
      "grad_norm": 0.7663395404815674,
      "learning_rate": 0.00019771139211858895,
      "loss": 0.0504,
      "step": 2927
    },
    {
      "epoch": 0.013772731121292228,
      "grad_norm": 4.693391799926758,
      "learning_rate": 0.00019771044914047544,
      "loss": 0.9317,
      "step": 2928
    },
    {
      "epoch": 0.013777434922904692,
      "grad_norm": 0.5523601174354553,
      "learning_rate": 0.00019770950616236198,
      "loss": 0.0363,
      "step": 2929
    },
    {
      "epoch": 0.013782138724517154,
      "grad_norm": 1.1632767915725708,
      "learning_rate": 0.0001977085631842485,
      "loss": 0.1081,
      "step": 2930
    },
    {
      "epoch": 0.013786842526129618,
      "grad_norm": 0.5943047404289246,
      "learning_rate": 0.00019770762020613502,
      "loss": 0.0669,
      "step": 2931
    },
    {
      "epoch": 0.013791546327742082,
      "grad_norm": 0.8196186423301697,
      "learning_rate": 0.00019770667722802154,
      "loss": 0.0839,
      "step": 2932
    },
    {
      "epoch": 0.013796250129354544,
      "grad_norm": 3.8594188690185547,
      "learning_rate": 0.00019770573424990806,
      "loss": 0.5577,
      "step": 2933
    },
    {
      "epoch": 0.013800953930967008,
      "grad_norm": 0.4753585457801819,
      "learning_rate": 0.0001977047912717946,
      "loss": 0.0506,
      "step": 2934
    },
    {
      "epoch": 0.01380565773257947,
      "grad_norm": 2.391615629196167,
      "learning_rate": 0.00019770384829368112,
      "loss": 0.4174,
      "step": 2935
    },
    {
      "epoch": 0.013810361534191934,
      "grad_norm": 0.5014589428901672,
      "learning_rate": 0.00019770290531556764,
      "loss": 0.0442,
      "step": 2936
    },
    {
      "epoch": 0.013815065335804398,
      "grad_norm": 1.7085014581680298,
      "learning_rate": 0.00019770196233745416,
      "loss": 0.2965,
      "step": 2937
    },
    {
      "epoch": 0.01381976913741686,
      "grad_norm": 0.5267037153244019,
      "learning_rate": 0.00019770101935934068,
      "loss": 0.0497,
      "step": 2938
    },
    {
      "epoch": 0.013824472939029324,
      "grad_norm": 0.9892097115516663,
      "learning_rate": 0.0001977000763812272,
      "loss": 0.0531,
      "step": 2939
    },
    {
      "epoch": 0.013829176740641786,
      "grad_norm": 1.2437564134597778,
      "learning_rate": 0.00019769913340311372,
      "loss": 0.0972,
      "step": 2940
    },
    {
      "epoch": 0.01383388054225425,
      "grad_norm": 1.3680073022842407,
      "learning_rate": 0.00019769819042500023,
      "loss": 0.1296,
      "step": 2941
    },
    {
      "epoch": 0.013838584343866714,
      "grad_norm": 0.6319824457168579,
      "learning_rate": 0.00019769724744688675,
      "loss": 0.0353,
      "step": 2942
    },
    {
      "epoch": 0.013843288145479176,
      "grad_norm": 1.7531129121780396,
      "learning_rate": 0.0001976963044687733,
      "loss": 0.1307,
      "step": 2943
    },
    {
      "epoch": 0.01384799194709164,
      "grad_norm": 1.4405683279037476,
      "learning_rate": 0.00019769536149065982,
      "loss": 0.1689,
      "step": 2944
    },
    {
      "epoch": 0.013852695748704102,
      "grad_norm": 0.7627254128456116,
      "learning_rate": 0.00019769441851254634,
      "loss": 0.0494,
      "step": 2945
    },
    {
      "epoch": 0.013857399550316566,
      "grad_norm": 2.017812490463257,
      "learning_rate": 0.00019769347553443285,
      "loss": 0.2095,
      "step": 2946
    },
    {
      "epoch": 0.013862103351929028,
      "grad_norm": 4.267820835113525,
      "learning_rate": 0.0001976925325563194,
      "loss": 0.4101,
      "step": 2947
    },
    {
      "epoch": 0.013866807153541492,
      "grad_norm": 2.199039936065674,
      "learning_rate": 0.0001976915895782059,
      "loss": 0.115,
      "step": 2948
    },
    {
      "epoch": 0.013871510955153956,
      "grad_norm": 3.3415656089782715,
      "learning_rate": 0.0001976906466000924,
      "loss": 0.9102,
      "step": 2949
    },
    {
      "epoch": 0.013876214756766418,
      "grad_norm": 3.160404920578003,
      "learning_rate": 0.00019768970362197893,
      "loss": 0.1387,
      "step": 2950
    },
    {
      "epoch": 0.013880918558378882,
      "grad_norm": 2.585021495819092,
      "learning_rate": 0.00019768876064386545,
      "loss": 0.1245,
      "step": 2951
    },
    {
      "epoch": 0.013885622359991344,
      "grad_norm": 4.094600200653076,
      "learning_rate": 0.000197687817665752,
      "loss": 0.3901,
      "step": 2952
    },
    {
      "epoch": 0.013890326161603808,
      "grad_norm": 3.9623448848724365,
      "learning_rate": 0.0001976868746876385,
      "loss": 0.5618,
      "step": 2953
    },
    {
      "epoch": 0.013895029963216272,
      "grad_norm": 1.1542658805847168,
      "learning_rate": 0.00019768593170952503,
      "loss": 0.1006,
      "step": 2954
    },
    {
      "epoch": 0.013899733764828734,
      "grad_norm": 1.0744450092315674,
      "learning_rate": 0.00019768498873141155,
      "loss": 0.1134,
      "step": 2955
    },
    {
      "epoch": 0.013904437566441198,
      "grad_norm": 6.407408237457275,
      "learning_rate": 0.0001976840457532981,
      "loss": 0.7972,
      "step": 2956
    },
    {
      "epoch": 0.01390914136805366,
      "grad_norm": 3.3023228645324707,
      "learning_rate": 0.00019768310277518461,
      "loss": 0.6037,
      "step": 2957
    },
    {
      "epoch": 0.013913845169666124,
      "grad_norm": 4.555481910705566,
      "learning_rate": 0.00019768215979707113,
      "loss": 0.2865,
      "step": 2958
    },
    {
      "epoch": 0.013918548971278588,
      "grad_norm": 6.292239665985107,
      "learning_rate": 0.00019768121681895762,
      "loss": 1.1754,
      "step": 2959
    },
    {
      "epoch": 0.01392325277289105,
      "grad_norm": 2.3837780952453613,
      "learning_rate": 0.00019768027384084414,
      "loss": 0.1875,
      "step": 2960
    },
    {
      "epoch": 0.013927956574503514,
      "grad_norm": 2.031881332397461,
      "learning_rate": 0.0001976793308627307,
      "loss": 0.1222,
      "step": 2961
    },
    {
      "epoch": 0.013932660376115976,
      "grad_norm": 2.946242332458496,
      "learning_rate": 0.0001976783878846172,
      "loss": 0.2524,
      "step": 2962
    },
    {
      "epoch": 0.01393736417772844,
      "grad_norm": 3.185981512069702,
      "learning_rate": 0.00019767744490650373,
      "loss": 0.2624,
      "step": 2963
    },
    {
      "epoch": 0.013942067979340902,
      "grad_norm": 2.7868759632110596,
      "learning_rate": 0.00019767650192839024,
      "loss": 0.2825,
      "step": 2964
    },
    {
      "epoch": 0.013946771780953366,
      "grad_norm": 3.0101253986358643,
      "learning_rate": 0.0001976755589502768,
      "loss": 0.3365,
      "step": 2965
    },
    {
      "epoch": 0.01395147558256583,
      "grad_norm": 1.9588545560836792,
      "learning_rate": 0.0001976746159721633,
      "loss": 0.1518,
      "step": 2966
    },
    {
      "epoch": 0.013956179384178292,
      "grad_norm": 1.6430333852767944,
      "learning_rate": 0.00019767367299404983,
      "loss": 0.2027,
      "step": 2967
    },
    {
      "epoch": 0.013960883185790756,
      "grad_norm": 1.4970213174819946,
      "learning_rate": 0.00019767273001593635,
      "loss": 0.1526,
      "step": 2968
    },
    {
      "epoch": 0.013965586987403219,
      "grad_norm": 1.0030611753463745,
      "learning_rate": 0.00019767178703782287,
      "loss": 0.0876,
      "step": 2969
    },
    {
      "epoch": 0.013970290789015682,
      "grad_norm": 3.5206897258758545,
      "learning_rate": 0.00019767084405970938,
      "loss": 0.7052,
      "step": 2970
    },
    {
      "epoch": 0.013974994590628146,
      "grad_norm": 0.5948641300201416,
      "learning_rate": 0.0001976699010815959,
      "loss": 0.0497,
      "step": 2971
    },
    {
      "epoch": 0.013979698392240609,
      "grad_norm": 2.963456392288208,
      "learning_rate": 0.00019766895810348242,
      "loss": 0.4626,
      "step": 2972
    },
    {
      "epoch": 0.013984402193853072,
      "grad_norm": 2.295708417892456,
      "learning_rate": 0.00019766801512536894,
      "loss": 0.3028,
      "step": 2973
    },
    {
      "epoch": 0.013989105995465535,
      "grad_norm": 4.539839267730713,
      "learning_rate": 0.00019766707214725546,
      "loss": 0.5994,
      "step": 2974
    },
    {
      "epoch": 0.013993809797077999,
      "grad_norm": 1.007148265838623,
      "learning_rate": 0.000197666129169142,
      "loss": 0.1072,
      "step": 2975
    },
    {
      "epoch": 0.013998513598690462,
      "grad_norm": 3.672236204147339,
      "learning_rate": 0.00019766518619102852,
      "loss": 0.6386,
      "step": 2976
    },
    {
      "epoch": 0.014003217400302925,
      "grad_norm": 5.404967784881592,
      "learning_rate": 0.00019766424321291504,
      "loss": 0.3979,
      "step": 2977
    },
    {
      "epoch": 0.014007921201915388,
      "grad_norm": 1.5573549270629883,
      "learning_rate": 0.00019766330023480156,
      "loss": 0.1543,
      "step": 2978
    },
    {
      "epoch": 0.01401262500352785,
      "grad_norm": 2.4044578075408936,
      "learning_rate": 0.00019766235725668808,
      "loss": 0.1731,
      "step": 2979
    },
    {
      "epoch": 0.014017328805140315,
      "grad_norm": 2.4529783725738525,
      "learning_rate": 0.0001976614142785746,
      "loss": 0.3212,
      "step": 2980
    },
    {
      "epoch": 0.014022032606752777,
      "grad_norm": 0.6878150701522827,
      "learning_rate": 0.00019766047130046112,
      "loss": 0.0516,
      "step": 2981
    },
    {
      "epoch": 0.01402673640836524,
      "grad_norm": 0.5569244027137756,
      "learning_rate": 0.00019765952832234763,
      "loss": 0.0452,
      "step": 2982
    },
    {
      "epoch": 0.014031440209977705,
      "grad_norm": 2.328188896179199,
      "learning_rate": 0.00019765858534423415,
      "loss": 0.4752,
      "step": 2983
    },
    {
      "epoch": 0.014036144011590167,
      "grad_norm": 3.021247625350952,
      "learning_rate": 0.0001976576423661207,
      "loss": 0.5268,
      "step": 2984
    },
    {
      "epoch": 0.01404084781320263,
      "grad_norm": 1.2675296068191528,
      "learning_rate": 0.00019765669938800722,
      "loss": 0.147,
      "step": 2985
    },
    {
      "epoch": 0.014045551614815093,
      "grad_norm": 2.8818185329437256,
      "learning_rate": 0.00019765575640989374,
      "loss": 0.4219,
      "step": 2986
    },
    {
      "epoch": 0.014050255416427557,
      "grad_norm": 0.8780827522277832,
      "learning_rate": 0.00019765481343178025,
      "loss": 0.1152,
      "step": 2987
    },
    {
      "epoch": 0.01405495921804002,
      "grad_norm": 2.431185245513916,
      "learning_rate": 0.0001976538704536668,
      "loss": 0.483,
      "step": 2988
    },
    {
      "epoch": 0.014059663019652483,
      "grad_norm": 1.413307547569275,
      "learning_rate": 0.00019765292747555332,
      "loss": 0.3177,
      "step": 2989
    },
    {
      "epoch": 0.014064366821264947,
      "grad_norm": 1.9730833768844604,
      "learning_rate": 0.0001976519844974398,
      "loss": 0.2748,
      "step": 2990
    },
    {
      "epoch": 0.014069070622877409,
      "grad_norm": 0.7174835801124573,
      "learning_rate": 0.00019765104151932633,
      "loss": 0.1534,
      "step": 2991
    },
    {
      "epoch": 0.014073774424489873,
      "grad_norm": 0.7869643568992615,
      "learning_rate": 0.00019765009854121285,
      "loss": 0.1143,
      "step": 2992
    },
    {
      "epoch": 0.014078478226102337,
      "grad_norm": 1.997499942779541,
      "learning_rate": 0.0001976491555630994,
      "loss": 0.2667,
      "step": 2993
    },
    {
      "epoch": 0.014083182027714799,
      "grad_norm": 0.3325344920158386,
      "learning_rate": 0.0001976482125849859,
      "loss": 0.0224,
      "step": 2994
    },
    {
      "epoch": 0.014087885829327263,
      "grad_norm": 1.5418391227722168,
      "learning_rate": 0.00019764726960687243,
      "loss": 0.1493,
      "step": 2995
    },
    {
      "epoch": 0.014092589630939725,
      "grad_norm": 2.5465657711029053,
      "learning_rate": 0.00019764632662875895,
      "loss": 0.4447,
      "step": 2996
    },
    {
      "epoch": 0.014097293432552189,
      "grad_norm": 0.5394986867904663,
      "learning_rate": 0.0001976453836506455,
      "loss": 0.0832,
      "step": 2997
    },
    {
      "epoch": 0.014101997234164651,
      "grad_norm": 0.6316916346549988,
      "learning_rate": 0.00019764444067253201,
      "loss": 0.0506,
      "step": 2998
    },
    {
      "epoch": 0.014106701035777115,
      "grad_norm": 2.2913224697113037,
      "learning_rate": 0.00019764349769441853,
      "loss": 0.4387,
      "step": 2999
    },
    {
      "epoch": 0.014111404837389579,
      "grad_norm": 1.7153167724609375,
      "learning_rate": 0.00019764255471630505,
      "loss": 0.2512,
      "step": 3000
    },
    {
      "epoch": 0.014116108639002041,
      "grad_norm": 1.0700522661209106,
      "learning_rate": 0.00019764161173819154,
      "loss": 0.0492,
      "step": 3001
    },
    {
      "epoch": 0.014120812440614505,
      "grad_norm": 5.258289813995361,
      "learning_rate": 0.0001976406687600781,
      "loss": 0.4018,
      "step": 3002
    },
    {
      "epoch": 0.014125516242226967,
      "grad_norm": 2.9091386795043945,
      "learning_rate": 0.0001976397257819646,
      "loss": 0.2417,
      "step": 3003
    },
    {
      "epoch": 0.014130220043839431,
      "grad_norm": 4.546091079711914,
      "learning_rate": 0.00019763878280385113,
      "loss": 0.3464,
      "step": 3004
    },
    {
      "epoch": 0.014134923845451895,
      "grad_norm": 0.5779658555984497,
      "learning_rate": 0.00019763783982573764,
      "loss": 0.037,
      "step": 3005
    },
    {
      "epoch": 0.014139627647064357,
      "grad_norm": 3.449437141418457,
      "learning_rate": 0.0001976368968476242,
      "loss": 0.5988,
      "step": 3006
    },
    {
      "epoch": 0.014144331448676821,
      "grad_norm": 0.8658197522163391,
      "learning_rate": 0.0001976359538695107,
      "loss": 0.076,
      "step": 3007
    },
    {
      "epoch": 0.014149035250289283,
      "grad_norm": 6.50562047958374,
      "learning_rate": 0.00019763501089139723,
      "loss": 0.4209,
      "step": 3008
    },
    {
      "epoch": 0.014153739051901747,
      "grad_norm": 2.2284936904907227,
      "learning_rate": 0.00019763406791328375,
      "loss": 0.3577,
      "step": 3009
    },
    {
      "epoch": 0.014158442853514211,
      "grad_norm": 2.3681271076202393,
      "learning_rate": 0.00019763312493517027,
      "loss": 0.3161,
      "step": 3010
    },
    {
      "epoch": 0.014163146655126673,
      "grad_norm": 0.8219378590583801,
      "learning_rate": 0.00019763218195705678,
      "loss": 0.0863,
      "step": 3011
    },
    {
      "epoch": 0.014167850456739137,
      "grad_norm": 0.5516422390937805,
      "learning_rate": 0.0001976312389789433,
      "loss": 0.0706,
      "step": 3012
    },
    {
      "epoch": 0.0141725542583516,
      "grad_norm": 2.4825491905212402,
      "learning_rate": 0.00019763029600082982,
      "loss": 0.2313,
      "step": 3013
    },
    {
      "epoch": 0.014177258059964063,
      "grad_norm": 2.9284470081329346,
      "learning_rate": 0.00019762935302271634,
      "loss": 0.5349,
      "step": 3014
    },
    {
      "epoch": 0.014181961861576527,
      "grad_norm": 1.5927152633666992,
      "learning_rate": 0.00019762841004460289,
      "loss": 0.2058,
      "step": 3015
    },
    {
      "epoch": 0.01418666566318899,
      "grad_norm": 5.117640495300293,
      "learning_rate": 0.0001976274670664894,
      "loss": 0.53,
      "step": 3016
    },
    {
      "epoch": 0.014191369464801453,
      "grad_norm": 2.642728567123413,
      "learning_rate": 0.00019762652408837592,
      "loss": 0.5531,
      "step": 3017
    },
    {
      "epoch": 0.014196073266413915,
      "grad_norm": 3.7321038246154785,
      "learning_rate": 0.00019762558111026244,
      "loss": 0.619,
      "step": 3018
    },
    {
      "epoch": 0.01420077706802638,
      "grad_norm": 0.6471593976020813,
      "learning_rate": 0.00019762463813214896,
      "loss": 0.044,
      "step": 3019
    },
    {
      "epoch": 0.014205480869638841,
      "grad_norm": 1.9284571409225464,
      "learning_rate": 0.0001976236951540355,
      "loss": 0.2205,
      "step": 3020
    },
    {
      "epoch": 0.014210184671251305,
      "grad_norm": 0.48996269702911377,
      "learning_rate": 0.000197622752175922,
      "loss": 0.0372,
      "step": 3021
    },
    {
      "epoch": 0.01421488847286377,
      "grad_norm": 1.0532485246658325,
      "learning_rate": 0.00019762180919780852,
      "loss": 0.1485,
      "step": 3022
    },
    {
      "epoch": 0.014219592274476231,
      "grad_norm": 0.8573077321052551,
      "learning_rate": 0.00019762086621969503,
      "loss": 0.1059,
      "step": 3023
    },
    {
      "epoch": 0.014224296076088695,
      "grad_norm": 0.2881937623023987,
      "learning_rate": 0.00019761992324158155,
      "loss": 0.0485,
      "step": 3024
    },
    {
      "epoch": 0.014228999877701157,
      "grad_norm": 3.4867403507232666,
      "learning_rate": 0.0001976189802634681,
      "loss": 0.6877,
      "step": 3025
    },
    {
      "epoch": 0.014233703679313621,
      "grad_norm": 2.9005510807037354,
      "learning_rate": 0.00019761803728535462,
      "loss": 0.6264,
      "step": 3026
    },
    {
      "epoch": 0.014238407480926085,
      "grad_norm": 0.9728841781616211,
      "learning_rate": 0.00019761709430724114,
      "loss": 0.0675,
      "step": 3027
    },
    {
      "epoch": 0.014243111282538547,
      "grad_norm": 1.810450792312622,
      "learning_rate": 0.00019761615132912765,
      "loss": 0.2435,
      "step": 3028
    },
    {
      "epoch": 0.014247815084151011,
      "grad_norm": 0.6359496116638184,
      "learning_rate": 0.0001976152083510142,
      "loss": 0.0703,
      "step": 3029
    },
    {
      "epoch": 0.014252518885763473,
      "grad_norm": 1.018123984336853,
      "learning_rate": 0.00019761426537290072,
      "loss": 0.1002,
      "step": 3030
    },
    {
      "epoch": 0.014257222687375937,
      "grad_norm": 1.9102312326431274,
      "learning_rate": 0.00019761332239478724,
      "loss": 0.1461,
      "step": 3031
    },
    {
      "epoch": 0.014261926488988401,
      "grad_norm": 1.3915135860443115,
      "learning_rate": 0.00019761237941667373,
      "loss": 0.2635,
      "step": 3032
    },
    {
      "epoch": 0.014266630290600863,
      "grad_norm": 0.46672379970550537,
      "learning_rate": 0.00019761143643856025,
      "loss": 0.0588,
      "step": 3033
    },
    {
      "epoch": 0.014271334092213327,
      "grad_norm": 1.697271704673767,
      "learning_rate": 0.0001976104934604468,
      "loss": 0.2631,
      "step": 3034
    },
    {
      "epoch": 0.01427603789382579,
      "grad_norm": 2.5412840843200684,
      "learning_rate": 0.0001976095504823333,
      "loss": 0.2373,
      "step": 3035
    },
    {
      "epoch": 0.014280741695438253,
      "grad_norm": 0.31178805232048035,
      "learning_rate": 0.00019760860750421983,
      "loss": 0.0203,
      "step": 3036
    },
    {
      "epoch": 0.014285445497050716,
      "grad_norm": 3.642991065979004,
      "learning_rate": 0.00019760766452610635,
      "loss": 0.6504,
      "step": 3037
    },
    {
      "epoch": 0.01429014929866318,
      "grad_norm": 0.5616593360900879,
      "learning_rate": 0.0001976067215479929,
      "loss": 0.0451,
      "step": 3038
    },
    {
      "epoch": 0.014294853100275643,
      "grad_norm": 0.978516697883606,
      "learning_rate": 0.00019760577856987941,
      "loss": 0.0745,
      "step": 3039
    },
    {
      "epoch": 0.014299556901888106,
      "grad_norm": 2.208103656768799,
      "learning_rate": 0.00019760483559176593,
      "loss": 0.3246,
      "step": 3040
    },
    {
      "epoch": 0.01430426070350057,
      "grad_norm": 14.016441345214844,
      "learning_rate": 0.00019760389261365245,
      "loss": 0.412,
      "step": 3041
    },
    {
      "epoch": 0.014308964505113032,
      "grad_norm": 0.6868658065795898,
      "learning_rate": 0.00019760294963553897,
      "loss": 0.0507,
      "step": 3042
    },
    {
      "epoch": 0.014313668306725496,
      "grad_norm": 0.2819099426269531,
      "learning_rate": 0.0001976020066574255,
      "loss": 0.0202,
      "step": 3043
    },
    {
      "epoch": 0.01431837210833796,
      "grad_norm": 4.438449859619141,
      "learning_rate": 0.000197601063679312,
      "loss": 0.9109,
      "step": 3044
    },
    {
      "epoch": 0.014323075909950422,
      "grad_norm": 0.7656089663505554,
      "learning_rate": 0.00019760012070119853,
      "loss": 0.0474,
      "step": 3045
    },
    {
      "epoch": 0.014327779711562886,
      "grad_norm": 0.2043965756893158,
      "learning_rate": 0.00019759917772308504,
      "loss": 0.0277,
      "step": 3046
    },
    {
      "epoch": 0.014332483513175348,
      "grad_norm": 2.2448222637176514,
      "learning_rate": 0.0001975982347449716,
      "loss": 0.2158,
      "step": 3047
    },
    {
      "epoch": 0.014337187314787812,
      "grad_norm": 0.4073202311992645,
      "learning_rate": 0.0001975972917668581,
      "loss": 0.025,
      "step": 3048
    },
    {
      "epoch": 0.014341891116400276,
      "grad_norm": 0.6307663917541504,
      "learning_rate": 0.00019759634878874463,
      "loss": 0.0389,
      "step": 3049
    },
    {
      "epoch": 0.014346594918012738,
      "grad_norm": 3.9016339778900146,
      "learning_rate": 0.00019759540581063115,
      "loss": 0.3901,
      "step": 3050
    },
    {
      "epoch": 0.014351298719625202,
      "grad_norm": 6.127216339111328,
      "learning_rate": 0.00019759446283251767,
      "loss": 0.5924,
      "step": 3051
    },
    {
      "epoch": 0.014356002521237664,
      "grad_norm": 0.4025786817073822,
      "learning_rate": 0.00019759351985440418,
      "loss": 0.0279,
      "step": 3052
    },
    {
      "epoch": 0.014360706322850128,
      "grad_norm": 3.507009506225586,
      "learning_rate": 0.0001975925768762907,
      "loss": 0.4322,
      "step": 3053
    },
    {
      "epoch": 0.01436541012446259,
      "grad_norm": 2.5944676399230957,
      "learning_rate": 0.00019759163389817722,
      "loss": 0.2936,
      "step": 3054
    },
    {
      "epoch": 0.014370113926075054,
      "grad_norm": 3.502295732498169,
      "learning_rate": 0.00019759069092006374,
      "loss": 0.2189,
      "step": 3055
    },
    {
      "epoch": 0.014374817727687518,
      "grad_norm": 7.011454105377197,
      "learning_rate": 0.00019758974794195029,
      "loss": 0.7507,
      "step": 3056
    },
    {
      "epoch": 0.01437952152929998,
      "grad_norm": 1.1582245826721191,
      "learning_rate": 0.0001975888049638368,
      "loss": 0.1281,
      "step": 3057
    },
    {
      "epoch": 0.014384225330912444,
      "grad_norm": 4.047021865844727,
      "learning_rate": 0.00019758786198572332,
      "loss": 0.368,
      "step": 3058
    },
    {
      "epoch": 0.014388929132524906,
      "grad_norm": 2.3594911098480225,
      "learning_rate": 0.00019758691900760984,
      "loss": 0.4135,
      "step": 3059
    },
    {
      "epoch": 0.01439363293413737,
      "grad_norm": 2.103754997253418,
      "learning_rate": 0.00019758597602949636,
      "loss": 0.1613,
      "step": 3060
    },
    {
      "epoch": 0.014398336735749834,
      "grad_norm": 1.6634691953659058,
      "learning_rate": 0.0001975850330513829,
      "loss": 0.1182,
      "step": 3061
    },
    {
      "epoch": 0.014403040537362296,
      "grad_norm": 4.85214376449585,
      "learning_rate": 0.00019758409007326942,
      "loss": 0.7149,
      "step": 3062
    },
    {
      "epoch": 0.01440774433897476,
      "grad_norm": 0.6124941110610962,
      "learning_rate": 0.00019758314709515592,
      "loss": 0.0233,
      "step": 3063
    },
    {
      "epoch": 0.014412448140587222,
      "grad_norm": 2.227055311203003,
      "learning_rate": 0.00019758220411704243,
      "loss": 0.2815,
      "step": 3064
    },
    {
      "epoch": 0.014417151942199686,
      "grad_norm": 3.312832832336426,
      "learning_rate": 0.00019758126113892898,
      "loss": 0.3689,
      "step": 3065
    },
    {
      "epoch": 0.01442185574381215,
      "grad_norm": 2.8695614337921143,
      "learning_rate": 0.0001975803181608155,
      "loss": 0.3236,
      "step": 3066
    },
    {
      "epoch": 0.014426559545424612,
      "grad_norm": 5.9155731201171875,
      "learning_rate": 0.00019757937518270202,
      "loss": 0.7076,
      "step": 3067
    },
    {
      "epoch": 0.014431263347037076,
      "grad_norm": 1.413021206855774,
      "learning_rate": 0.00019757843220458854,
      "loss": 0.1527,
      "step": 3068
    },
    {
      "epoch": 0.014435967148649538,
      "grad_norm": 2.209526538848877,
      "learning_rate": 0.00019757748922647505,
      "loss": 0.17,
      "step": 3069
    },
    {
      "epoch": 0.014440670950262002,
      "grad_norm": 4.833800315856934,
      "learning_rate": 0.0001975765462483616,
      "loss": 1.4394,
      "step": 3070
    },
    {
      "epoch": 0.014445374751874464,
      "grad_norm": 3.512331008911133,
      "learning_rate": 0.00019757560327024812,
      "loss": 0.4569,
      "step": 3071
    },
    {
      "epoch": 0.014450078553486928,
      "grad_norm": 3.8882174491882324,
      "learning_rate": 0.00019757466029213464,
      "loss": 0.8259,
      "step": 3072
    },
    {
      "epoch": 0.014454782355099392,
      "grad_norm": 1.9115445613861084,
      "learning_rate": 0.00019757371731402116,
      "loss": 0.1759,
      "step": 3073
    },
    {
      "epoch": 0.014459486156711854,
      "grad_norm": 0.7905717492103577,
      "learning_rate": 0.00019757277433590768,
      "loss": 0.0691,
      "step": 3074
    },
    {
      "epoch": 0.014464189958324318,
      "grad_norm": 3.7921416759490967,
      "learning_rate": 0.0001975718313577942,
      "loss": 0.2079,
      "step": 3075
    },
    {
      "epoch": 0.01446889375993678,
      "grad_norm": 1.928411602973938,
      "learning_rate": 0.0001975708883796807,
      "loss": 0.2287,
      "step": 3076
    },
    {
      "epoch": 0.014473597561549244,
      "grad_norm": 0.9506804943084717,
      "learning_rate": 0.00019756994540156723,
      "loss": 0.1103,
      "step": 3077
    },
    {
      "epoch": 0.014478301363161708,
      "grad_norm": 1.7281779050827026,
      "learning_rate": 0.00019756900242345375,
      "loss": 0.1383,
      "step": 3078
    },
    {
      "epoch": 0.01448300516477417,
      "grad_norm": 0.7795252799987793,
      "learning_rate": 0.0001975680594453403,
      "loss": 0.0595,
      "step": 3079
    },
    {
      "epoch": 0.014487708966386634,
      "grad_norm": 1.7890561819076538,
      "learning_rate": 0.00019756711646722681,
      "loss": 0.2637,
      "step": 3080
    },
    {
      "epoch": 0.014492412767999096,
      "grad_norm": 3.368032932281494,
      "learning_rate": 0.00019756617348911333,
      "loss": 0.7994,
      "step": 3081
    },
    {
      "epoch": 0.01449711656961156,
      "grad_norm": 1.927058219909668,
      "learning_rate": 0.00019756523051099985,
      "loss": 0.3034,
      "step": 3082
    },
    {
      "epoch": 0.014501820371224024,
      "grad_norm": 1.4312806129455566,
      "learning_rate": 0.00019756428753288637,
      "loss": 0.1336,
      "step": 3083
    },
    {
      "epoch": 0.014506524172836486,
      "grad_norm": 1.672982931137085,
      "learning_rate": 0.0001975633445547729,
      "loss": 0.2238,
      "step": 3084
    },
    {
      "epoch": 0.01451122797444895,
      "grad_norm": 3.121267557144165,
      "learning_rate": 0.0001975624015766594,
      "loss": 0.3044,
      "step": 3085
    },
    {
      "epoch": 0.014515931776061412,
      "grad_norm": 1.9708526134490967,
      "learning_rate": 0.00019756145859854593,
      "loss": 0.3519,
      "step": 3086
    },
    {
      "epoch": 0.014520635577673876,
      "grad_norm": 0.803438127040863,
      "learning_rate": 0.00019756051562043244,
      "loss": 0.087,
      "step": 3087
    },
    {
      "epoch": 0.014525339379286338,
      "grad_norm": 3.9172351360321045,
      "learning_rate": 0.000197559572642319,
      "loss": 0.4985,
      "step": 3088
    },
    {
      "epoch": 0.014530043180898802,
      "grad_norm": 0.3838404715061188,
      "learning_rate": 0.0001975586296642055,
      "loss": 0.036,
      "step": 3089
    },
    {
      "epoch": 0.014534746982511266,
      "grad_norm": 2.332826614379883,
      "learning_rate": 0.00019755768668609203,
      "loss": 0.6212,
      "step": 3090
    },
    {
      "epoch": 0.014539450784123728,
      "grad_norm": 1.1388468742370605,
      "learning_rate": 0.00019755674370797855,
      "loss": 0.0877,
      "step": 3091
    },
    {
      "epoch": 0.014544154585736192,
      "grad_norm": 4.401154041290283,
      "learning_rate": 0.00019755580072986507,
      "loss": 1.0804,
      "step": 3092
    },
    {
      "epoch": 0.014548858387348654,
      "grad_norm": 1.9692052602767944,
      "learning_rate": 0.0001975548577517516,
      "loss": 0.3952,
      "step": 3093
    },
    {
      "epoch": 0.014553562188961118,
      "grad_norm": 4.299432754516602,
      "learning_rate": 0.0001975539147736381,
      "loss": 1.1783,
      "step": 3094
    },
    {
      "epoch": 0.014558265990573582,
      "grad_norm": 1.0947773456573486,
      "learning_rate": 0.00019755297179552462,
      "loss": 0.1085,
      "step": 3095
    },
    {
      "epoch": 0.014562969792186044,
      "grad_norm": 1.5078290700912476,
      "learning_rate": 0.00019755202881741114,
      "loss": 0.1585,
      "step": 3096
    },
    {
      "epoch": 0.014567673593798508,
      "grad_norm": 0.6000860929489136,
      "learning_rate": 0.00019755108583929769,
      "loss": 0.0503,
      "step": 3097
    },
    {
      "epoch": 0.01457237739541097,
      "grad_norm": 1.8623924255371094,
      "learning_rate": 0.0001975501428611842,
      "loss": 0.288,
      "step": 3098
    },
    {
      "epoch": 0.014577081197023434,
      "grad_norm": 1.2730214595794678,
      "learning_rate": 0.00019754919988307072,
      "loss": 0.2898,
      "step": 3099
    },
    {
      "epoch": 0.014581784998635898,
      "grad_norm": 0.3629268705844879,
      "learning_rate": 0.00019754825690495724,
      "loss": 0.0254,
      "step": 3100
    },
    {
      "epoch": 0.01458648880024836,
      "grad_norm": 1.78961980342865,
      "learning_rate": 0.00019754731392684376,
      "loss": 0.1177,
      "step": 3101
    },
    {
      "epoch": 0.014591192601860824,
      "grad_norm": 1.3094408512115479,
      "learning_rate": 0.0001975463709487303,
      "loss": 0.1728,
      "step": 3102
    },
    {
      "epoch": 0.014595896403473287,
      "grad_norm": 4.303289413452148,
      "learning_rate": 0.00019754542797061682,
      "loss": 0.4398,
      "step": 3103
    },
    {
      "epoch": 0.01460060020508575,
      "grad_norm": 2.5545480251312256,
      "learning_rate": 0.00019754448499250334,
      "loss": 0.3504,
      "step": 3104
    },
    {
      "epoch": 0.014605304006698213,
      "grad_norm": 1.5467051267623901,
      "learning_rate": 0.00019754354201438986,
      "loss": 0.2203,
      "step": 3105
    },
    {
      "epoch": 0.014610007808310677,
      "grad_norm": 6.455044269561768,
      "learning_rate": 0.00019754259903627638,
      "loss": 0.1564,
      "step": 3106
    },
    {
      "epoch": 0.01461471160992314,
      "grad_norm": 1.2618881464004517,
      "learning_rate": 0.0001975416560581629,
      "loss": 0.1638,
      "step": 3107
    },
    {
      "epoch": 0.014619415411535603,
      "grad_norm": 1.0204061269760132,
      "learning_rate": 0.00019754071308004942,
      "loss": 0.1022,
      "step": 3108
    },
    {
      "epoch": 0.014624119213148067,
      "grad_norm": 3.0225157737731934,
      "learning_rate": 0.00019753977010193594,
      "loss": 0.3145,
      "step": 3109
    },
    {
      "epoch": 0.014628823014760529,
      "grad_norm": 1.0761802196502686,
      "learning_rate": 0.00019753882712382245,
      "loss": 0.0579,
      "step": 3110
    },
    {
      "epoch": 0.014633526816372993,
      "grad_norm": 4.8617024421691895,
      "learning_rate": 0.000197537884145709,
      "loss": 1.0639,
      "step": 3111
    },
    {
      "epoch": 0.014638230617985457,
      "grad_norm": 1.1274008750915527,
      "learning_rate": 0.00019753694116759552,
      "loss": 0.1228,
      "step": 3112
    },
    {
      "epoch": 0.014642934419597919,
      "grad_norm": 3.8792309761047363,
      "learning_rate": 0.00019753599818948204,
      "loss": 0.1132,
      "step": 3113
    },
    {
      "epoch": 0.014647638221210383,
      "grad_norm": 1.5575640201568604,
      "learning_rate": 0.00019753505521136856,
      "loss": 0.2135,
      "step": 3114
    },
    {
      "epoch": 0.014652342022822845,
      "grad_norm": 1.9492818117141724,
      "learning_rate": 0.00019753411223325508,
      "loss": 0.187,
      "step": 3115
    },
    {
      "epoch": 0.014657045824435309,
      "grad_norm": 1.68877375125885,
      "learning_rate": 0.0001975331692551416,
      "loss": 0.2803,
      "step": 3116
    },
    {
      "epoch": 0.014661749626047773,
      "grad_norm": 2.964049816131592,
      "learning_rate": 0.0001975322262770281,
      "loss": 0.4245,
      "step": 3117
    },
    {
      "epoch": 0.014666453427660235,
      "grad_norm": 0.5474739074707031,
      "learning_rate": 0.00019753128329891463,
      "loss": 0.0652,
      "step": 3118
    },
    {
      "epoch": 0.014671157229272699,
      "grad_norm": 2.4555728435516357,
      "learning_rate": 0.00019753034032080115,
      "loss": 0.3462,
      "step": 3119
    },
    {
      "epoch": 0.01467586103088516,
      "grad_norm": 3.2027015686035156,
      "learning_rate": 0.0001975293973426877,
      "loss": 0.4615,
      "step": 3120
    },
    {
      "epoch": 0.014680564832497625,
      "grad_norm": 3.300323247909546,
      "learning_rate": 0.00019752845436457421,
      "loss": 0.4872,
      "step": 3121
    },
    {
      "epoch": 0.014685268634110087,
      "grad_norm": 2.719205856323242,
      "learning_rate": 0.00019752751138646073,
      "loss": 0.4332,
      "step": 3122
    },
    {
      "epoch": 0.01468997243572255,
      "grad_norm": 1.321174144744873,
      "learning_rate": 0.00019752656840834725,
      "loss": 0.1353,
      "step": 3123
    },
    {
      "epoch": 0.014694676237335015,
      "grad_norm": 0.5575764179229736,
      "learning_rate": 0.00019752562543023377,
      "loss": 0.0503,
      "step": 3124
    },
    {
      "epoch": 0.014699380038947477,
      "grad_norm": 3.7776684761047363,
      "learning_rate": 0.0001975246824521203,
      "loss": 0.8605,
      "step": 3125
    },
    {
      "epoch": 0.01470408384055994,
      "grad_norm": 2.167480945587158,
      "learning_rate": 0.0001975237394740068,
      "loss": 0.3086,
      "step": 3126
    },
    {
      "epoch": 0.014708787642172403,
      "grad_norm": 2.770503520965576,
      "learning_rate": 0.00019752279649589333,
      "loss": 0.3839,
      "step": 3127
    },
    {
      "epoch": 0.014713491443784867,
      "grad_norm": 2.4745748043060303,
      "learning_rate": 0.00019752185351777984,
      "loss": 0.4826,
      "step": 3128
    },
    {
      "epoch": 0.01471819524539733,
      "grad_norm": 1.235529899597168,
      "learning_rate": 0.0001975209105396664,
      "loss": 0.1479,
      "step": 3129
    },
    {
      "epoch": 0.014722899047009793,
      "grad_norm": 2.7872605323791504,
      "learning_rate": 0.0001975199675615529,
      "loss": 0.8636,
      "step": 3130
    },
    {
      "epoch": 0.014727602848622257,
      "grad_norm": 2.9387195110321045,
      "learning_rate": 0.00019751902458343943,
      "loss": 0.2368,
      "step": 3131
    },
    {
      "epoch": 0.014732306650234719,
      "grad_norm": 0.28241512179374695,
      "learning_rate": 0.00019751808160532595,
      "loss": 0.0219,
      "step": 3132
    },
    {
      "epoch": 0.014737010451847183,
      "grad_norm": 3.1855669021606445,
      "learning_rate": 0.00019751713862721246,
      "loss": 0.2802,
      "step": 3133
    },
    {
      "epoch": 0.014741714253459647,
      "grad_norm": 0.9018104672431946,
      "learning_rate": 0.000197516195649099,
      "loss": 0.114,
      "step": 3134
    },
    {
      "epoch": 0.014746418055072109,
      "grad_norm": 1.3847273588180542,
      "learning_rate": 0.00019751525267098553,
      "loss": 0.081,
      "step": 3135
    },
    {
      "epoch": 0.014751121856684573,
      "grad_norm": 1.6929017305374146,
      "learning_rate": 0.00019751430969287205,
      "loss": 0.2045,
      "step": 3136
    },
    {
      "epoch": 0.014755825658297035,
      "grad_norm": 1.667626142501831,
      "learning_rate": 0.00019751336671475854,
      "loss": 0.2431,
      "step": 3137
    },
    {
      "epoch": 0.014760529459909499,
      "grad_norm": 4.313732147216797,
      "learning_rate": 0.00019751242373664509,
      "loss": 0.6904,
      "step": 3138
    },
    {
      "epoch": 0.014765233261521961,
      "grad_norm": 1.342461347579956,
      "learning_rate": 0.0001975114807585316,
      "loss": 0.1638,
      "step": 3139
    },
    {
      "epoch": 0.014769937063134425,
      "grad_norm": 1.3876347541809082,
      "learning_rate": 0.00019751053778041812,
      "loss": 0.3397,
      "step": 3140
    },
    {
      "epoch": 0.014774640864746889,
      "grad_norm": 1.5892343521118164,
      "learning_rate": 0.00019750959480230464,
      "loss": 0.2197,
      "step": 3141
    },
    {
      "epoch": 0.014779344666359351,
      "grad_norm": 1.1982618570327759,
      "learning_rate": 0.00019750865182419116,
      "loss": 0.1021,
      "step": 3142
    },
    {
      "epoch": 0.014784048467971815,
      "grad_norm": 0.2064136415719986,
      "learning_rate": 0.0001975077088460777,
      "loss": 0.011,
      "step": 3143
    },
    {
      "epoch": 0.014788752269584277,
      "grad_norm": 0.2886723279953003,
      "learning_rate": 0.00019750676586796422,
      "loss": 0.0244,
      "step": 3144
    },
    {
      "epoch": 0.014793456071196741,
      "grad_norm": 2.675333023071289,
      "learning_rate": 0.00019750582288985074,
      "loss": 0.559,
      "step": 3145
    },
    {
      "epoch": 0.014798159872809205,
      "grad_norm": 2.1804587841033936,
      "learning_rate": 0.00019750487991173726,
      "loss": 0.322,
      "step": 3146
    },
    {
      "epoch": 0.014802863674421667,
      "grad_norm": 1.345977783203125,
      "learning_rate": 0.00019750393693362378,
      "loss": 0.2439,
      "step": 3147
    },
    {
      "epoch": 0.014807567476034131,
      "grad_norm": 2.451643466949463,
      "learning_rate": 0.0001975029939555103,
      "loss": 0.2987,
      "step": 3148
    },
    {
      "epoch": 0.014812271277646593,
      "grad_norm": 0.9313154220581055,
      "learning_rate": 0.00019750205097739682,
      "loss": 0.0812,
      "step": 3149
    },
    {
      "epoch": 0.014816975079259057,
      "grad_norm": 2.234097719192505,
      "learning_rate": 0.00019750110799928334,
      "loss": 0.3238,
      "step": 3150
    },
    {
      "epoch": 0.014821678880871521,
      "grad_norm": 2.1106648445129395,
      "learning_rate": 0.00019750016502116985,
      "loss": 0.2485,
      "step": 3151
    },
    {
      "epoch": 0.014826382682483983,
      "grad_norm": 0.9751706123352051,
      "learning_rate": 0.0001974992220430564,
      "loss": 0.0876,
      "step": 3152
    },
    {
      "epoch": 0.014831086484096447,
      "grad_norm": 1.8181064128875732,
      "learning_rate": 0.00019749827906494292,
      "loss": 0.1987,
      "step": 3153
    },
    {
      "epoch": 0.01483579028570891,
      "grad_norm": 1.6196311712265015,
      "learning_rate": 0.00019749733608682944,
      "loss": 0.3526,
      "step": 3154
    },
    {
      "epoch": 0.014840494087321373,
      "grad_norm": 3.829141855239868,
      "learning_rate": 0.00019749639310871596,
      "loss": 0.8996,
      "step": 3155
    },
    {
      "epoch": 0.014845197888933835,
      "grad_norm": 1.5744065046310425,
      "learning_rate": 0.00019749545013060248,
      "loss": 0.2592,
      "step": 3156
    },
    {
      "epoch": 0.0148499016905463,
      "grad_norm": 1.2027267217636108,
      "learning_rate": 0.000197494507152489,
      "loss": 0.1269,
      "step": 3157
    },
    {
      "epoch": 0.014854605492158763,
      "grad_norm": 1.4402283430099487,
      "learning_rate": 0.0001974935641743755,
      "loss": 0.1611,
      "step": 3158
    },
    {
      "epoch": 0.014859309293771225,
      "grad_norm": 2.516205072402954,
      "learning_rate": 0.00019749262119626203,
      "loss": 0.1948,
      "step": 3159
    },
    {
      "epoch": 0.01486401309538369,
      "grad_norm": 0.6684200763702393,
      "learning_rate": 0.00019749167821814855,
      "loss": 0.0666,
      "step": 3160
    },
    {
      "epoch": 0.014868716896996152,
      "grad_norm": 1.3716248273849487,
      "learning_rate": 0.0001974907352400351,
      "loss": 0.2129,
      "step": 3161
    },
    {
      "epoch": 0.014873420698608615,
      "grad_norm": 1.7228832244873047,
      "learning_rate": 0.00019748979226192161,
      "loss": 0.2932,
      "step": 3162
    },
    {
      "epoch": 0.01487812450022108,
      "grad_norm": 1.9554892778396606,
      "learning_rate": 0.00019748884928380813,
      "loss": 0.2037,
      "step": 3163
    },
    {
      "epoch": 0.014882828301833542,
      "grad_norm": 1.5712612867355347,
      "learning_rate": 0.00019748790630569465,
      "loss": 0.1882,
      "step": 3164
    },
    {
      "epoch": 0.014887532103446005,
      "grad_norm": 3.576477289199829,
      "learning_rate": 0.0001974869633275812,
      "loss": 0.5344,
      "step": 3165
    },
    {
      "epoch": 0.014892235905058468,
      "grad_norm": 0.9301039576530457,
      "learning_rate": 0.00019748602034946772,
      "loss": 0.0738,
      "step": 3166
    },
    {
      "epoch": 0.014896939706670932,
      "grad_norm": 0.6645947098731995,
      "learning_rate": 0.00019748507737135423,
      "loss": 0.0596,
      "step": 3167
    },
    {
      "epoch": 0.014901643508283395,
      "grad_norm": 1.8256862163543701,
      "learning_rate": 0.00019748413439324073,
      "loss": 0.1718,
      "step": 3168
    },
    {
      "epoch": 0.014906347309895858,
      "grad_norm": 1.976415991783142,
      "learning_rate": 0.00019748319141512724,
      "loss": 0.2814,
      "step": 3169
    },
    {
      "epoch": 0.014911051111508321,
      "grad_norm": 1.9626113176345825,
      "learning_rate": 0.0001974822484370138,
      "loss": 0.1142,
      "step": 3170
    },
    {
      "epoch": 0.014915754913120784,
      "grad_norm": 1.900948405265808,
      "learning_rate": 0.0001974813054589003,
      "loss": 0.1298,
      "step": 3171
    },
    {
      "epoch": 0.014920458714733248,
      "grad_norm": 3.4554412364959717,
      "learning_rate": 0.00019748036248078683,
      "loss": 0.284,
      "step": 3172
    },
    {
      "epoch": 0.01492516251634571,
      "grad_norm": 3.158935070037842,
      "learning_rate": 0.00019747941950267335,
      "loss": 0.6657,
      "step": 3173
    },
    {
      "epoch": 0.014929866317958174,
      "grad_norm": 3.659792184829712,
      "learning_rate": 0.00019747847652455986,
      "loss": 0.7108,
      "step": 3174
    },
    {
      "epoch": 0.014934570119570638,
      "grad_norm": 0.7063148021697998,
      "learning_rate": 0.0001974775335464464,
      "loss": 0.0772,
      "step": 3175
    },
    {
      "epoch": 0.0149392739211831,
      "grad_norm": 2.8681445121765137,
      "learning_rate": 0.00019747659056833293,
      "loss": 0.6463,
      "step": 3176
    },
    {
      "epoch": 0.014943977722795564,
      "grad_norm": 2.5893490314483643,
      "learning_rate": 0.00019747564759021945,
      "loss": 0.6431,
      "step": 3177
    },
    {
      "epoch": 0.014948681524408026,
      "grad_norm": 2.26235032081604,
      "learning_rate": 0.00019747470461210597,
      "loss": 0.2151,
      "step": 3178
    },
    {
      "epoch": 0.01495338532602049,
      "grad_norm": 2.2553107738494873,
      "learning_rate": 0.00019747376163399249,
      "loss": 0.2269,
      "step": 3179
    },
    {
      "epoch": 0.014958089127632954,
      "grad_norm": 0.2595382034778595,
      "learning_rate": 0.000197472818655879,
      "loss": 0.0164,
      "step": 3180
    },
    {
      "epoch": 0.014962792929245416,
      "grad_norm": 1.1980434656143188,
      "learning_rate": 0.00019747187567776552,
      "loss": 0.1386,
      "step": 3181
    },
    {
      "epoch": 0.01496749673085788,
      "grad_norm": 2.3317863941192627,
      "learning_rate": 0.00019747093269965204,
      "loss": 0.3133,
      "step": 3182
    },
    {
      "epoch": 0.014972200532470342,
      "grad_norm": 1.0069732666015625,
      "learning_rate": 0.00019746998972153856,
      "loss": 0.1342,
      "step": 3183
    },
    {
      "epoch": 0.014976904334082806,
      "grad_norm": 2.7852511405944824,
      "learning_rate": 0.0001974690467434251,
      "loss": 0.9634,
      "step": 3184
    },
    {
      "epoch": 0.01498160813569527,
      "grad_norm": 0.8918610215187073,
      "learning_rate": 0.00019746810376531162,
      "loss": 0.062,
      "step": 3185
    },
    {
      "epoch": 0.014986311937307732,
      "grad_norm": 4.216745853424072,
      "learning_rate": 0.00019746716078719814,
      "loss": 0.8468,
      "step": 3186
    },
    {
      "epoch": 0.014991015738920196,
      "grad_norm": 0.415095716714859,
      "learning_rate": 0.00019746621780908466,
      "loss": 0.0266,
      "step": 3187
    },
    {
      "epoch": 0.014995719540532658,
      "grad_norm": 0.6642314791679382,
      "learning_rate": 0.00019746527483097118,
      "loss": 0.0931,
      "step": 3188
    },
    {
      "epoch": 0.015000423342145122,
      "grad_norm": 0.40566447377204895,
      "learning_rate": 0.0001974643318528577,
      "loss": 0.0321,
      "step": 3189
    },
    {
      "epoch": 0.015005127143757586,
      "grad_norm": 1.1993482112884521,
      "learning_rate": 0.00019746338887474422,
      "loss": 0.1601,
      "step": 3190
    },
    {
      "epoch": 0.015009830945370048,
      "grad_norm": 1.054469108581543,
      "learning_rate": 0.00019746244589663074,
      "loss": 0.0973,
      "step": 3191
    },
    {
      "epoch": 0.015014534746982512,
      "grad_norm": 3.4791104793548584,
      "learning_rate": 0.00019746150291851725,
      "loss": 0.7138,
      "step": 3192
    },
    {
      "epoch": 0.015019238548594974,
      "grad_norm": 0.647689938545227,
      "learning_rate": 0.0001974605599404038,
      "loss": 0.0536,
      "step": 3193
    },
    {
      "epoch": 0.015023942350207438,
      "grad_norm": 2.6656391620635986,
      "learning_rate": 0.00019745961696229032,
      "loss": 0.7711,
      "step": 3194
    },
    {
      "epoch": 0.0150286461518199,
      "grad_norm": 0.22917130589485168,
      "learning_rate": 0.00019745867398417684,
      "loss": 0.0151,
      "step": 3195
    },
    {
      "epoch": 0.015033349953432364,
      "grad_norm": 4.65146541595459,
      "learning_rate": 0.00019745773100606336,
      "loss": 0.2842,
      "step": 3196
    },
    {
      "epoch": 0.015038053755044828,
      "grad_norm": 3.4038188457489014,
      "learning_rate": 0.0001974567880279499,
      "loss": 1.0977,
      "step": 3197
    },
    {
      "epoch": 0.01504275755665729,
      "grad_norm": 3.23697566986084,
      "learning_rate": 0.00019745584504983642,
      "loss": 0.6589,
      "step": 3198
    },
    {
      "epoch": 0.015047461358269754,
      "grad_norm": 1.396203875541687,
      "learning_rate": 0.0001974549020717229,
      "loss": 0.1993,
      "step": 3199
    },
    {
      "epoch": 0.015052165159882216,
      "grad_norm": 3.0167236328125,
      "learning_rate": 0.00019745395909360943,
      "loss": 0.5821,
      "step": 3200
    },
    {
      "epoch": 0.01505686896149468,
      "grad_norm": 6.512544631958008,
      "learning_rate": 0.00019745301611549595,
      "loss": 0.3492,
      "step": 3201
    },
    {
      "epoch": 0.015061572763107144,
      "grad_norm": 1.6271601915359497,
      "learning_rate": 0.0001974520731373825,
      "loss": 0.0876,
      "step": 3202
    },
    {
      "epoch": 0.015066276564719606,
      "grad_norm": 1.5330654382705688,
      "learning_rate": 0.00019745113015926901,
      "loss": 0.1345,
      "step": 3203
    },
    {
      "epoch": 0.01507098036633207,
      "grad_norm": 0.6627981066703796,
      "learning_rate": 0.00019745018718115553,
      "loss": 0.066,
      "step": 3204
    },
    {
      "epoch": 0.015075684167944532,
      "grad_norm": 2.1943953037261963,
      "learning_rate": 0.00019744924420304205,
      "loss": 0.4563,
      "step": 3205
    },
    {
      "epoch": 0.015080387969556996,
      "grad_norm": 0.9904651045799255,
      "learning_rate": 0.0001974483012249286,
      "loss": 0.1036,
      "step": 3206
    },
    {
      "epoch": 0.01508509177116946,
      "grad_norm": 3.757298231124878,
      "learning_rate": 0.00019744735824681512,
      "loss": 0.4393,
      "step": 3207
    },
    {
      "epoch": 0.015089795572781922,
      "grad_norm": 0.5566444396972656,
      "learning_rate": 0.00019744641526870163,
      "loss": 0.0526,
      "step": 3208
    },
    {
      "epoch": 0.015094499374394386,
      "grad_norm": 2.158688545227051,
      "learning_rate": 0.00019744547229058815,
      "loss": 0.4016,
      "step": 3209
    },
    {
      "epoch": 0.015099203176006848,
      "grad_norm": 0.6300395727157593,
      "learning_rate": 0.00019744452931247464,
      "loss": 0.0991,
      "step": 3210
    },
    {
      "epoch": 0.015103906977619312,
      "grad_norm": 1.9928600788116455,
      "learning_rate": 0.0001974435863343612,
      "loss": 0.2171,
      "step": 3211
    },
    {
      "epoch": 0.015108610779231774,
      "grad_norm": 9.024728775024414,
      "learning_rate": 0.0001974426433562477,
      "loss": 1.1793,
      "step": 3212
    },
    {
      "epoch": 0.015113314580844238,
      "grad_norm": 2.6927921772003174,
      "learning_rate": 0.00019744170037813423,
      "loss": 0.2866,
      "step": 3213
    },
    {
      "epoch": 0.015118018382456702,
      "grad_norm": 1.2261631488800049,
      "learning_rate": 0.00019744075740002075,
      "loss": 0.0859,
      "step": 3214
    },
    {
      "epoch": 0.015122722184069164,
      "grad_norm": 1.2062187194824219,
      "learning_rate": 0.0001974398144219073,
      "loss": 0.1158,
      "step": 3215
    },
    {
      "epoch": 0.015127425985681628,
      "grad_norm": 3.057831287384033,
      "learning_rate": 0.0001974388714437938,
      "loss": 0.5732,
      "step": 3216
    },
    {
      "epoch": 0.01513212978729409,
      "grad_norm": 1.3960974216461182,
      "learning_rate": 0.00019743792846568033,
      "loss": 0.1316,
      "step": 3217
    },
    {
      "epoch": 0.015136833588906554,
      "grad_norm": 2.128882646560669,
      "learning_rate": 0.00019743698548756685,
      "loss": 0.1749,
      "step": 3218
    },
    {
      "epoch": 0.015141537390519018,
      "grad_norm": 0.7304826378822327,
      "learning_rate": 0.00019743604250945337,
      "loss": 0.055,
      "step": 3219
    },
    {
      "epoch": 0.01514624119213148,
      "grad_norm": 1.6603564023971558,
      "learning_rate": 0.00019743509953133989,
      "loss": 0.1343,
      "step": 3220
    },
    {
      "epoch": 0.015150944993743944,
      "grad_norm": 1.8081666231155396,
      "learning_rate": 0.0001974341565532264,
      "loss": 0.2299,
      "step": 3221
    },
    {
      "epoch": 0.015155648795356406,
      "grad_norm": 0.2623851001262665,
      "learning_rate": 0.00019743321357511292,
      "loss": 0.0194,
      "step": 3222
    },
    {
      "epoch": 0.01516035259696887,
      "grad_norm": 2.5798397064208984,
      "learning_rate": 0.00019743227059699944,
      "loss": 0.4057,
      "step": 3223
    },
    {
      "epoch": 0.015165056398581334,
      "grad_norm": 2.267091989517212,
      "learning_rate": 0.000197431327618886,
      "loss": 0.2691,
      "step": 3224
    },
    {
      "epoch": 0.015169760200193796,
      "grad_norm": 2.476876974105835,
      "learning_rate": 0.0001974303846407725,
      "loss": 0.4501,
      "step": 3225
    },
    {
      "epoch": 0.01517446400180626,
      "grad_norm": 5.022292137145996,
      "learning_rate": 0.00019742944166265902,
      "loss": 0.6857,
      "step": 3226
    },
    {
      "epoch": 0.015179167803418723,
      "grad_norm": 4.084447383880615,
      "learning_rate": 0.00019742849868454554,
      "loss": 0.6851,
      "step": 3227
    },
    {
      "epoch": 0.015183871605031186,
      "grad_norm": 0.6143414378166199,
      "learning_rate": 0.00019742755570643206,
      "loss": 0.0518,
      "step": 3228
    },
    {
      "epoch": 0.015188575406643649,
      "grad_norm": 3.582489013671875,
      "learning_rate": 0.0001974266127283186,
      "loss": 0.4703,
      "step": 3229
    },
    {
      "epoch": 0.015193279208256113,
      "grad_norm": 10.329062461853027,
      "learning_rate": 0.0001974256697502051,
      "loss": 0.3075,
      "step": 3230
    },
    {
      "epoch": 0.015197983009868576,
      "grad_norm": 2.3546977043151855,
      "learning_rate": 0.00019742472677209162,
      "loss": 0.3313,
      "step": 3231
    },
    {
      "epoch": 0.015202686811481039,
      "grad_norm": 2.811216115951538,
      "learning_rate": 0.00019742378379397814,
      "loss": 0.4341,
      "step": 3232
    },
    {
      "epoch": 0.015207390613093502,
      "grad_norm": 5.026741027832031,
      "learning_rate": 0.00019742284081586465,
      "loss": 0.5376,
      "step": 3233
    },
    {
      "epoch": 0.015212094414705965,
      "grad_norm": 1.1375494003295898,
      "learning_rate": 0.0001974218978377512,
      "loss": 0.1589,
      "step": 3234
    },
    {
      "epoch": 0.015216798216318429,
      "grad_norm": 2.9981648921966553,
      "learning_rate": 0.00019742095485963772,
      "loss": 0.3971,
      "step": 3235
    },
    {
      "epoch": 0.015221502017930892,
      "grad_norm": 0.5251962542533875,
      "learning_rate": 0.00019742001188152424,
      "loss": 0.0435,
      "step": 3236
    },
    {
      "epoch": 0.015226205819543355,
      "grad_norm": 3.4203531742095947,
      "learning_rate": 0.00019741906890341076,
      "loss": 0.4282,
      "step": 3237
    },
    {
      "epoch": 0.015230909621155819,
      "grad_norm": 1.6241297721862793,
      "learning_rate": 0.0001974181259252973,
      "loss": 0.2359,
      "step": 3238
    },
    {
      "epoch": 0.01523561342276828,
      "grad_norm": 1.2870738506317139,
      "learning_rate": 0.00019741718294718382,
      "loss": 0.0909,
      "step": 3239
    },
    {
      "epoch": 0.015240317224380745,
      "grad_norm": 1.29543936252594,
      "learning_rate": 0.00019741623996907034,
      "loss": 0.1477,
      "step": 3240
    },
    {
      "epoch": 0.015245021025993209,
      "grad_norm": 3.2894136905670166,
      "learning_rate": 0.00019741529699095683,
      "loss": 0.4386,
      "step": 3241
    },
    {
      "epoch": 0.01524972482760567,
      "grad_norm": 2.8346669673919678,
      "learning_rate": 0.00019741435401284335,
      "loss": 0.7221,
      "step": 3242
    },
    {
      "epoch": 0.015254428629218135,
      "grad_norm": 1.5599701404571533,
      "learning_rate": 0.0001974134110347299,
      "loss": 0.1751,
      "step": 3243
    },
    {
      "epoch": 0.015259132430830597,
      "grad_norm": 1.0432932376861572,
      "learning_rate": 0.00019741246805661641,
      "loss": 0.2164,
      "step": 3244
    },
    {
      "epoch": 0.01526383623244306,
      "grad_norm": 1.7499094009399414,
      "learning_rate": 0.00019741152507850293,
      "loss": 0.3301,
      "step": 3245
    },
    {
      "epoch": 0.015268540034055523,
      "grad_norm": 1.8254801034927368,
      "learning_rate": 0.00019741058210038945,
      "loss": 0.4578,
      "step": 3246
    },
    {
      "epoch": 0.015273243835667987,
      "grad_norm": 0.47971755266189575,
      "learning_rate": 0.000197409639122276,
      "loss": 0.0357,
      "step": 3247
    },
    {
      "epoch": 0.01527794763728045,
      "grad_norm": 1.322702407836914,
      "learning_rate": 0.00019740869614416252,
      "loss": 0.1394,
      "step": 3248
    },
    {
      "epoch": 0.015282651438892913,
      "grad_norm": 1.0122994184494019,
      "learning_rate": 0.00019740775316604903,
      "loss": 0.1779,
      "step": 3249
    },
    {
      "epoch": 0.015287355240505377,
      "grad_norm": 1.1844866275787354,
      "learning_rate": 0.00019740681018793555,
      "loss": 0.1982,
      "step": 3250
    },
    {
      "epoch": 0.015292059042117839,
      "grad_norm": 6.542663097381592,
      "learning_rate": 0.00019740586720982207,
      "loss": 0.5597,
      "step": 3251
    },
    {
      "epoch": 0.015296762843730303,
      "grad_norm": 3.4537501335144043,
      "learning_rate": 0.0001974049242317086,
      "loss": 0.2621,
      "step": 3252
    },
    {
      "epoch": 0.015301466645342767,
      "grad_norm": 1.8926494121551514,
      "learning_rate": 0.0001974039812535951,
      "loss": 0.1591,
      "step": 3253
    },
    {
      "epoch": 0.015306170446955229,
      "grad_norm": 0.6017530560493469,
      "learning_rate": 0.00019740303827548163,
      "loss": 0.0571,
      "step": 3254
    },
    {
      "epoch": 0.015310874248567693,
      "grad_norm": 1.2568786144256592,
      "learning_rate": 0.00019740209529736815,
      "loss": 0.1344,
      "step": 3255
    },
    {
      "epoch": 0.015315578050180155,
      "grad_norm": 1.0262882709503174,
      "learning_rate": 0.0001974011523192547,
      "loss": 0.1213,
      "step": 3256
    },
    {
      "epoch": 0.015320281851792619,
      "grad_norm": 0.41341903805732727,
      "learning_rate": 0.0001974002093411412,
      "loss": 0.0444,
      "step": 3257
    },
    {
      "epoch": 0.015324985653405083,
      "grad_norm": 3.0936994552612305,
      "learning_rate": 0.00019739926636302773,
      "loss": 0.652,
      "step": 3258
    },
    {
      "epoch": 0.015329689455017545,
      "grad_norm": 3.1672048568725586,
      "learning_rate": 0.00019739832338491425,
      "loss": 0.4151,
      "step": 3259
    },
    {
      "epoch": 0.015334393256630009,
      "grad_norm": 2.2067086696624756,
      "learning_rate": 0.00019739738040680077,
      "loss": 0.3152,
      "step": 3260
    },
    {
      "epoch": 0.015339097058242471,
      "grad_norm": 2.0743978023529053,
      "learning_rate": 0.00019739643742868729,
      "loss": 0.3045,
      "step": 3261
    },
    {
      "epoch": 0.015343800859854935,
      "grad_norm": 1.9534716606140137,
      "learning_rate": 0.0001973954944505738,
      "loss": 0.2988,
      "step": 3262
    },
    {
      "epoch": 0.015348504661467397,
      "grad_norm": 2.596696138381958,
      "learning_rate": 0.00019739455147246032,
      "loss": 0.4419,
      "step": 3263
    },
    {
      "epoch": 0.015353208463079861,
      "grad_norm": 3.303312301635742,
      "learning_rate": 0.00019739360849434684,
      "loss": 0.2983,
      "step": 3264
    },
    {
      "epoch": 0.015357912264692325,
      "grad_norm": 0.8577821850776672,
      "learning_rate": 0.0001973926655162334,
      "loss": 0.0547,
      "step": 3265
    },
    {
      "epoch": 0.015362616066304787,
      "grad_norm": 2.611835241317749,
      "learning_rate": 0.0001973917225381199,
      "loss": 0.4314,
      "step": 3266
    },
    {
      "epoch": 0.015367319867917251,
      "grad_norm": 6.435720443725586,
      "learning_rate": 0.00019739077956000642,
      "loss": 1.0304,
      "step": 3267
    },
    {
      "epoch": 0.015372023669529713,
      "grad_norm": 2.1861424446105957,
      "learning_rate": 0.00019738983658189294,
      "loss": 0.3461,
      "step": 3268
    },
    {
      "epoch": 0.015376727471142177,
      "grad_norm": 0.5695222616195679,
      "learning_rate": 0.00019738889360377946,
      "loss": 0.0399,
      "step": 3269
    },
    {
      "epoch": 0.015381431272754641,
      "grad_norm": 0.7799438238143921,
      "learning_rate": 0.000197387950625666,
      "loss": 0.0612,
      "step": 3270
    },
    {
      "epoch": 0.015386135074367103,
      "grad_norm": 0.9397897720336914,
      "learning_rate": 0.00019738700764755253,
      "loss": 0.1364,
      "step": 3271
    },
    {
      "epoch": 0.015390838875979567,
      "grad_norm": 3.9436020851135254,
      "learning_rate": 0.00019738606466943902,
      "loss": 0.8141,
      "step": 3272
    },
    {
      "epoch": 0.01539554267759203,
      "grad_norm": 1.26959228515625,
      "learning_rate": 0.00019738512169132554,
      "loss": 0.2575,
      "step": 3273
    },
    {
      "epoch": 0.015400246479204493,
      "grad_norm": 1.7898215055465698,
      "learning_rate": 0.00019738417871321208,
      "loss": 0.2391,
      "step": 3274
    },
    {
      "epoch": 0.015404950280816957,
      "grad_norm": 2.3051817417144775,
      "learning_rate": 0.0001973832357350986,
      "loss": 0.2859,
      "step": 3275
    },
    {
      "epoch": 0.01540965408242942,
      "grad_norm": 1.3868327140808105,
      "learning_rate": 0.00019738229275698512,
      "loss": 0.1281,
      "step": 3276
    },
    {
      "epoch": 0.015414357884041883,
      "grad_norm": 1.66635262966156,
      "learning_rate": 0.00019738134977887164,
      "loss": 0.1901,
      "step": 3277
    },
    {
      "epoch": 0.015419061685654345,
      "grad_norm": 2.2252769470214844,
      "learning_rate": 0.00019738040680075816,
      "loss": 0.2627,
      "step": 3278
    },
    {
      "epoch": 0.01542376548726681,
      "grad_norm": 5.5590314865112305,
      "learning_rate": 0.0001973794638226447,
      "loss": 1.3095,
      "step": 3279
    },
    {
      "epoch": 0.015428469288879271,
      "grad_norm": 1.4795012474060059,
      "learning_rate": 0.00019737852084453122,
      "loss": 0.3159,
      "step": 3280
    },
    {
      "epoch": 0.015433173090491735,
      "grad_norm": 2.4988064765930176,
      "learning_rate": 0.00019737757786641774,
      "loss": 0.3491,
      "step": 3281
    },
    {
      "epoch": 0.0154378768921042,
      "grad_norm": 1.3598946332931519,
      "learning_rate": 0.00019737663488830426,
      "loss": 0.136,
      "step": 3282
    },
    {
      "epoch": 0.015442580693716661,
      "grad_norm": 0.5530508160591125,
      "learning_rate": 0.00019737569191019075,
      "loss": 0.0674,
      "step": 3283
    },
    {
      "epoch": 0.015447284495329125,
      "grad_norm": 2.9146413803100586,
      "learning_rate": 0.0001973747489320773,
      "loss": 0.4202,
      "step": 3284
    },
    {
      "epoch": 0.015451988296941587,
      "grad_norm": 0.8669501543045044,
      "learning_rate": 0.00019737380595396381,
      "loss": 0.0759,
      "step": 3285
    },
    {
      "epoch": 0.015456692098554051,
      "grad_norm": 1.0704872608184814,
      "learning_rate": 0.00019737286297585033,
      "loss": 0.1002,
      "step": 3286
    },
    {
      "epoch": 0.015461395900166515,
      "grad_norm": 3.8943164348602295,
      "learning_rate": 0.00019737191999773685,
      "loss": 0.7258,
      "step": 3287
    },
    {
      "epoch": 0.015466099701778977,
      "grad_norm": 2.8189895153045654,
      "learning_rate": 0.0001973709770196234,
      "loss": 0.4675,
      "step": 3288
    },
    {
      "epoch": 0.015470803503391441,
      "grad_norm": 1.2137113809585571,
      "learning_rate": 0.00019737003404150992,
      "loss": 0.1209,
      "step": 3289
    },
    {
      "epoch": 0.015475507305003904,
      "grad_norm": 0.48195603489875793,
      "learning_rate": 0.00019736909106339643,
      "loss": 0.0544,
      "step": 3290
    },
    {
      "epoch": 0.015480211106616367,
      "grad_norm": 2.4071543216705322,
      "learning_rate": 0.00019736814808528295,
      "loss": 0.5778,
      "step": 3291
    },
    {
      "epoch": 0.015484914908228831,
      "grad_norm": 2.8052375316619873,
      "learning_rate": 0.00019736720510716947,
      "loss": 0.3091,
      "step": 3292
    },
    {
      "epoch": 0.015489618709841294,
      "grad_norm": 0.36104726791381836,
      "learning_rate": 0.000197366262129056,
      "loss": 0.0432,
      "step": 3293
    },
    {
      "epoch": 0.015494322511453757,
      "grad_norm": 3.214841604232788,
      "learning_rate": 0.0001973653191509425,
      "loss": 0.5469,
      "step": 3294
    },
    {
      "epoch": 0.01549902631306622,
      "grad_norm": 0.912553071975708,
      "learning_rate": 0.00019736437617282903,
      "loss": 0.0764,
      "step": 3295
    },
    {
      "epoch": 0.015503730114678684,
      "grad_norm": 8.835111618041992,
      "learning_rate": 0.00019736343319471555,
      "loss": 0.6215,
      "step": 3296
    },
    {
      "epoch": 0.015508433916291146,
      "grad_norm": 0.4414583444595337,
      "learning_rate": 0.0001973624902166021,
      "loss": 0.0296,
      "step": 3297
    },
    {
      "epoch": 0.01551313771790361,
      "grad_norm": 2.8901946544647217,
      "learning_rate": 0.0001973615472384886,
      "loss": 0.6462,
      "step": 3298
    },
    {
      "epoch": 0.015517841519516073,
      "grad_norm": 2.320767402648926,
      "learning_rate": 0.00019736060426037513,
      "loss": 0.2434,
      "step": 3299
    },
    {
      "epoch": 0.015522545321128536,
      "grad_norm": 3.6232352256774902,
      "learning_rate": 0.00019735966128226165,
      "loss": 0.6969,
      "step": 3300
    },
    {
      "epoch": 0.015527249122741,
      "grad_norm": 1.8179848194122314,
      "learning_rate": 0.00019735871830414817,
      "loss": 0.2231,
      "step": 3301
    },
    {
      "epoch": 0.015531952924353462,
      "grad_norm": 2.0814707279205322,
      "learning_rate": 0.0001973577753260347,
      "loss": 0.1655,
      "step": 3302
    },
    {
      "epoch": 0.015536656725965926,
      "grad_norm": 1.2203890085220337,
      "learning_rate": 0.0001973568323479212,
      "loss": 0.1059,
      "step": 3303
    },
    {
      "epoch": 0.01554136052757839,
      "grad_norm": 7.4139580726623535,
      "learning_rate": 0.00019735588936980772,
      "loss": 0.1329,
      "step": 3304
    },
    {
      "epoch": 0.015546064329190852,
      "grad_norm": 3.0700061321258545,
      "learning_rate": 0.00019735494639169424,
      "loss": 0.2294,
      "step": 3305
    },
    {
      "epoch": 0.015550768130803316,
      "grad_norm": 0.48271772265434265,
      "learning_rate": 0.0001973540034135808,
      "loss": 0.0173,
      "step": 3306
    },
    {
      "epoch": 0.015555471932415778,
      "grad_norm": 0.8663405776023865,
      "learning_rate": 0.0001973530604354673,
      "loss": 0.0874,
      "step": 3307
    },
    {
      "epoch": 0.015560175734028242,
      "grad_norm": 5.864316463470459,
      "learning_rate": 0.00019735211745735382,
      "loss": 0.9036,
      "step": 3308
    },
    {
      "epoch": 0.015564879535640706,
      "grad_norm": 1.656538963317871,
      "learning_rate": 0.00019735117447924034,
      "loss": 0.1998,
      "step": 3309
    },
    {
      "epoch": 0.015569583337253168,
      "grad_norm": 0.7764917612075806,
      "learning_rate": 0.00019735023150112686,
      "loss": 0.0815,
      "step": 3310
    },
    {
      "epoch": 0.015574287138865632,
      "grad_norm": 2.1772172451019287,
      "learning_rate": 0.0001973492885230134,
      "loss": 0.2005,
      "step": 3311
    },
    {
      "epoch": 0.015578990940478094,
      "grad_norm": 1.816495418548584,
      "learning_rate": 0.00019734834554489993,
      "loss": 0.1713,
      "step": 3312
    },
    {
      "epoch": 0.015583694742090558,
      "grad_norm": 0.42051154375076294,
      "learning_rate": 0.00019734740256678644,
      "loss": 0.0688,
      "step": 3313
    },
    {
      "epoch": 0.01558839854370302,
      "grad_norm": 4.5462141036987305,
      "learning_rate": 0.00019734645958867294,
      "loss": 0.5914,
      "step": 3314
    },
    {
      "epoch": 0.015593102345315484,
      "grad_norm": 1.37468683719635,
      "learning_rate": 0.00019734551661055948,
      "loss": 0.159,
      "step": 3315
    },
    {
      "epoch": 0.015597806146927948,
      "grad_norm": 1.294752597808838,
      "learning_rate": 0.000197344573632446,
      "loss": 0.131,
      "step": 3316
    },
    {
      "epoch": 0.01560250994854041,
      "grad_norm": 2.9180381298065186,
      "learning_rate": 0.00019734363065433252,
      "loss": 0.2599,
      "step": 3317
    },
    {
      "epoch": 0.015607213750152874,
      "grad_norm": 1.778507113456726,
      "learning_rate": 0.00019734268767621904,
      "loss": 0.2202,
      "step": 3318
    },
    {
      "epoch": 0.015611917551765336,
      "grad_norm": 4.966696739196777,
      "learning_rate": 0.00019734174469810556,
      "loss": 1.4522,
      "step": 3319
    },
    {
      "epoch": 0.0156166213533778,
      "grad_norm": 1.9898134469985962,
      "learning_rate": 0.0001973408017199921,
      "loss": 0.3804,
      "step": 3320
    },
    {
      "epoch": 0.015621325154990264,
      "grad_norm": 1.0762145519256592,
      "learning_rate": 0.00019733985874187862,
      "loss": 0.0724,
      "step": 3321
    },
    {
      "epoch": 0.015626028956602728,
      "grad_norm": 1.050793170928955,
      "learning_rate": 0.00019733891576376514,
      "loss": 0.07,
      "step": 3322
    },
    {
      "epoch": 0.01563073275821519,
      "grad_norm": 0.15949606895446777,
      "learning_rate": 0.00019733797278565166,
      "loss": 0.0119,
      "step": 3323
    },
    {
      "epoch": 0.015635436559827652,
      "grad_norm": 3.4603888988494873,
      "learning_rate": 0.00019733702980753818,
      "loss": 0.8631,
      "step": 3324
    },
    {
      "epoch": 0.015640140361440114,
      "grad_norm": 0.28614282608032227,
      "learning_rate": 0.0001973360868294247,
      "loss": 0.0315,
      "step": 3325
    },
    {
      "epoch": 0.01564484416305258,
      "grad_norm": 2.016608715057373,
      "learning_rate": 0.00019733514385131121,
      "loss": 0.198,
      "step": 3326
    },
    {
      "epoch": 0.015649547964665042,
      "grad_norm": 0.13991062343120575,
      "learning_rate": 0.00019733420087319773,
      "loss": 0.0095,
      "step": 3327
    },
    {
      "epoch": 0.015654251766277504,
      "grad_norm": 1.0717616081237793,
      "learning_rate": 0.00019733325789508425,
      "loss": 0.06,
      "step": 3328
    },
    {
      "epoch": 0.01565895556788997,
      "grad_norm": 2.8053629398345947,
      "learning_rate": 0.0001973323149169708,
      "loss": 0.3227,
      "step": 3329
    },
    {
      "epoch": 0.015663659369502432,
      "grad_norm": 2.42111873626709,
      "learning_rate": 0.00019733137193885732,
      "loss": 0.2094,
      "step": 3330
    },
    {
      "epoch": 0.015668363171114894,
      "grad_norm": 0.7890471816062927,
      "learning_rate": 0.00019733042896074383,
      "loss": 0.0624,
      "step": 3331
    },
    {
      "epoch": 0.01567306697272736,
      "grad_norm": 1.7197766304016113,
      "learning_rate": 0.00019732948598263035,
      "loss": 0.1578,
      "step": 3332
    },
    {
      "epoch": 0.015677770774339822,
      "grad_norm": 1.6647913455963135,
      "learning_rate": 0.00019732854300451687,
      "loss": 0.2682,
      "step": 3333
    },
    {
      "epoch": 0.015682474575952284,
      "grad_norm": 2.0338432788848877,
      "learning_rate": 0.0001973276000264034,
      "loss": 0.3561,
      "step": 3334
    },
    {
      "epoch": 0.015687178377564746,
      "grad_norm": 1.6276397705078125,
      "learning_rate": 0.0001973266570482899,
      "loss": 0.1445,
      "step": 3335
    },
    {
      "epoch": 0.015691882179177212,
      "grad_norm": 1.9893724918365479,
      "learning_rate": 0.00019732571407017643,
      "loss": 0.373,
      "step": 3336
    },
    {
      "epoch": 0.015696585980789674,
      "grad_norm": 2.057474374771118,
      "learning_rate": 0.00019732477109206295,
      "loss": 0.4438,
      "step": 3337
    },
    {
      "epoch": 0.015701289782402136,
      "grad_norm": 1.5325318574905396,
      "learning_rate": 0.0001973238281139495,
      "loss": 0.2374,
      "step": 3338
    },
    {
      "epoch": 0.015705993584014602,
      "grad_norm": 2.8413827419281006,
      "learning_rate": 0.000197322885135836,
      "loss": 0.529,
      "step": 3339
    },
    {
      "epoch": 0.015710697385627064,
      "grad_norm": 1.5603387355804443,
      "learning_rate": 0.00019732194215772253,
      "loss": 0.298,
      "step": 3340
    },
    {
      "epoch": 0.015715401187239526,
      "grad_norm": 1.3713151216506958,
      "learning_rate": 0.00019732099917960905,
      "loss": 0.1663,
      "step": 3341
    },
    {
      "epoch": 0.01572010498885199,
      "grad_norm": 2.2642767429351807,
      "learning_rate": 0.00019732005620149557,
      "loss": 0.294,
      "step": 3342
    },
    {
      "epoch": 0.015724808790464454,
      "grad_norm": 0.9111459255218506,
      "learning_rate": 0.0001973191132233821,
      "loss": 0.0881,
      "step": 3343
    },
    {
      "epoch": 0.015729512592076916,
      "grad_norm": 3.2170252799987793,
      "learning_rate": 0.00019731817024526863,
      "loss": 0.8547,
      "step": 3344
    },
    {
      "epoch": 0.01573421639368938,
      "grad_norm": 1.6236283779144287,
      "learning_rate": 0.00019731722726715512,
      "loss": 0.245,
      "step": 3345
    },
    {
      "epoch": 0.015738920195301844,
      "grad_norm": 3.810821056365967,
      "learning_rate": 0.00019731628428904164,
      "loss": 1.0319,
      "step": 3346
    },
    {
      "epoch": 0.015743623996914306,
      "grad_norm": 0.6174114346504211,
      "learning_rate": 0.0001973153413109282,
      "loss": 0.0571,
      "step": 3347
    },
    {
      "epoch": 0.01574832779852677,
      "grad_norm": 0.5483343005180359,
      "learning_rate": 0.0001973143983328147,
      "loss": 0.0943,
      "step": 3348
    },
    {
      "epoch": 0.015753031600139234,
      "grad_norm": 0.9331564903259277,
      "learning_rate": 0.00019731345535470122,
      "loss": 0.1995,
      "step": 3349
    },
    {
      "epoch": 0.015757735401751696,
      "grad_norm": 0.5552120804786682,
      "learning_rate": 0.00019731251237658774,
      "loss": 0.0539,
      "step": 3350
    },
    {
      "epoch": 0.01576243920336416,
      "grad_norm": 3.482908010482788,
      "learning_rate": 0.00019731156939847426,
      "loss": 0.2777,
      "step": 3351
    },
    {
      "epoch": 0.01576714300497662,
      "grad_norm": 2.914048194885254,
      "learning_rate": 0.0001973106264203608,
      "loss": 0.2431,
      "step": 3352
    },
    {
      "epoch": 0.015771846806589086,
      "grad_norm": 1.0963187217712402,
      "learning_rate": 0.00019730968344224733,
      "loss": 0.0867,
      "step": 3353
    },
    {
      "epoch": 0.01577655060820155,
      "grad_norm": 11.190572738647461,
      "learning_rate": 0.00019730874046413384,
      "loss": 0.4866,
      "step": 3354
    },
    {
      "epoch": 0.01578125440981401,
      "grad_norm": 0.9618772268295288,
      "learning_rate": 0.00019730779748602036,
      "loss": 0.1349,
      "step": 3355
    },
    {
      "epoch": 0.015785958211426476,
      "grad_norm": 1.1804749965667725,
      "learning_rate": 0.00019730685450790688,
      "loss": 0.0798,
      "step": 3356
    },
    {
      "epoch": 0.01579066201303894,
      "grad_norm": 1.1443824768066406,
      "learning_rate": 0.0001973059115297934,
      "loss": 0.0771,
      "step": 3357
    },
    {
      "epoch": 0.0157953658146514,
      "grad_norm": 6.132580757141113,
      "learning_rate": 0.00019730496855167992,
      "loss": 0.3783,
      "step": 3358
    },
    {
      "epoch": 0.015800069616263863,
      "grad_norm": 0.7664305567741394,
      "learning_rate": 0.00019730402557356644,
      "loss": 0.0774,
      "step": 3359
    },
    {
      "epoch": 0.01580477341787633,
      "grad_norm": 1.2471096515655518,
      "learning_rate": 0.00019730308259545296,
      "loss": 0.1206,
      "step": 3360
    },
    {
      "epoch": 0.01580947721948879,
      "grad_norm": 2.840252161026001,
      "learning_rate": 0.0001973021396173395,
      "loss": 0.2425,
      "step": 3361
    },
    {
      "epoch": 0.015814181021101253,
      "grad_norm": 1.5447663068771362,
      "learning_rate": 0.00019730119663922602,
      "loss": 0.0416,
      "step": 3362
    },
    {
      "epoch": 0.01581888482271372,
      "grad_norm": 0.7266577482223511,
      "learning_rate": 0.00019730025366111254,
      "loss": 0.0464,
      "step": 3363
    },
    {
      "epoch": 0.01582358862432618,
      "grad_norm": 5.053645610809326,
      "learning_rate": 0.00019729931068299906,
      "loss": 0.8097,
      "step": 3364
    },
    {
      "epoch": 0.015828292425938643,
      "grad_norm": 2.3974013328552246,
      "learning_rate": 0.00019729836770488558,
      "loss": 0.1631,
      "step": 3365
    },
    {
      "epoch": 0.01583299622755111,
      "grad_norm": 1.686331033706665,
      "learning_rate": 0.0001972974247267721,
      "loss": 0.0938,
      "step": 3366
    },
    {
      "epoch": 0.01583770002916357,
      "grad_norm": 2.6429250240325928,
      "learning_rate": 0.00019729648174865861,
      "loss": 0.2571,
      "step": 3367
    },
    {
      "epoch": 0.015842403830776033,
      "grad_norm": 2.3625035285949707,
      "learning_rate": 0.00019729553877054513,
      "loss": 0.2759,
      "step": 3368
    },
    {
      "epoch": 0.015847107632388495,
      "grad_norm": 3.217249870300293,
      "learning_rate": 0.00019729459579243165,
      "loss": 0.4623,
      "step": 3369
    },
    {
      "epoch": 0.01585181143400096,
      "grad_norm": 0.6666332483291626,
      "learning_rate": 0.0001972936528143182,
      "loss": 0.0179,
      "step": 3370
    },
    {
      "epoch": 0.015856515235613423,
      "grad_norm": 5.699955463409424,
      "learning_rate": 0.00019729270983620472,
      "loss": 0.659,
      "step": 3371
    },
    {
      "epoch": 0.015861219037225885,
      "grad_norm": 2.5387115478515625,
      "learning_rate": 0.00019729176685809123,
      "loss": 0.5143,
      "step": 3372
    },
    {
      "epoch": 0.01586592283883835,
      "grad_norm": 3.513455390930176,
      "learning_rate": 0.00019729082387997775,
      "loss": 0.1908,
      "step": 3373
    },
    {
      "epoch": 0.015870626640450813,
      "grad_norm": 2.4203014373779297,
      "learning_rate": 0.0001972898809018643,
      "loss": 0.2674,
      "step": 3374
    },
    {
      "epoch": 0.015875330442063275,
      "grad_norm": 1.5372354984283447,
      "learning_rate": 0.00019728893792375082,
      "loss": 0.2535,
      "step": 3375
    },
    {
      "epoch": 0.015880034243675737,
      "grad_norm": 0.8317320942878723,
      "learning_rate": 0.0001972879949456373,
      "loss": 0.0562,
      "step": 3376
    },
    {
      "epoch": 0.015884738045288203,
      "grad_norm": 4.178870677947998,
      "learning_rate": 0.00019728705196752383,
      "loss": 0.482,
      "step": 3377
    },
    {
      "epoch": 0.015889441846900665,
      "grad_norm": 1.566384196281433,
      "learning_rate": 0.00019728610898941035,
      "loss": 0.1275,
      "step": 3378
    },
    {
      "epoch": 0.015894145648513127,
      "grad_norm": 2.004394054412842,
      "learning_rate": 0.0001972851660112969,
      "loss": 0.1894,
      "step": 3379
    },
    {
      "epoch": 0.015898849450125593,
      "grad_norm": 1.6415317058563232,
      "learning_rate": 0.0001972842230331834,
      "loss": 0.2404,
      "step": 3380
    },
    {
      "epoch": 0.015903553251738055,
      "grad_norm": 0.6184366941452026,
      "learning_rate": 0.00019728328005506993,
      "loss": 0.0749,
      "step": 3381
    },
    {
      "epoch": 0.015908257053350517,
      "grad_norm": 1.8375805616378784,
      "learning_rate": 0.00019728233707695645,
      "loss": 0.2061,
      "step": 3382
    },
    {
      "epoch": 0.015912960854962983,
      "grad_norm": 2.0424270629882812,
      "learning_rate": 0.00019728139409884297,
      "loss": 0.5244,
      "step": 3383
    },
    {
      "epoch": 0.015917664656575445,
      "grad_norm": 1.4322563409805298,
      "learning_rate": 0.0001972804511207295,
      "loss": 0.1836,
      "step": 3384
    },
    {
      "epoch": 0.015922368458187907,
      "grad_norm": 0.7590155601501465,
      "learning_rate": 0.00019727950814261603,
      "loss": 0.1006,
      "step": 3385
    },
    {
      "epoch": 0.01592707225980037,
      "grad_norm": 3.001699686050415,
      "learning_rate": 0.00019727856516450255,
      "loss": 0.5382,
      "step": 3386
    },
    {
      "epoch": 0.015931776061412835,
      "grad_norm": 2.4070820808410645,
      "learning_rate": 0.00019727762218638907,
      "loss": 0.4259,
      "step": 3387
    },
    {
      "epoch": 0.015936479863025297,
      "grad_norm": 2.34639573097229,
      "learning_rate": 0.0001972766792082756,
      "loss": 0.3156,
      "step": 3388
    },
    {
      "epoch": 0.01594118366463776,
      "grad_norm": 2.7972826957702637,
      "learning_rate": 0.0001972757362301621,
      "loss": 0.3971,
      "step": 3389
    },
    {
      "epoch": 0.015945887466250225,
      "grad_norm": 1.6270596981048584,
      "learning_rate": 0.00019727479325204862,
      "loss": 0.1505,
      "step": 3390
    },
    {
      "epoch": 0.015950591267862687,
      "grad_norm": 0.33646413683891296,
      "learning_rate": 0.00019727385027393514,
      "loss": 0.0409,
      "step": 3391
    },
    {
      "epoch": 0.01595529506947515,
      "grad_norm": 3.305924892425537,
      "learning_rate": 0.00019727290729582166,
      "loss": 0.3759,
      "step": 3392
    },
    {
      "epoch": 0.01595999887108761,
      "grad_norm": 0.5241913795471191,
      "learning_rate": 0.0001972719643177082,
      "loss": 0.0513,
      "step": 3393
    },
    {
      "epoch": 0.015964702672700077,
      "grad_norm": 2.779956817626953,
      "learning_rate": 0.00019727102133959473,
      "loss": 0.5702,
      "step": 3394
    },
    {
      "epoch": 0.01596940647431254,
      "grad_norm": 0.6265453100204468,
      "learning_rate": 0.00019727007836148124,
      "loss": 0.064,
      "step": 3395
    },
    {
      "epoch": 0.015974110275925,
      "grad_norm": 0.39552220702171326,
      "learning_rate": 0.00019726913538336776,
      "loss": 0.0514,
      "step": 3396
    },
    {
      "epoch": 0.015978814077537467,
      "grad_norm": 1.5165016651153564,
      "learning_rate": 0.00019726819240525428,
      "loss": 0.1201,
      "step": 3397
    },
    {
      "epoch": 0.01598351787914993,
      "grad_norm": 2.248522996902466,
      "learning_rate": 0.0001972672494271408,
      "loss": 0.3719,
      "step": 3398
    },
    {
      "epoch": 0.01598822168076239,
      "grad_norm": 2.475695848464966,
      "learning_rate": 0.00019726630644902732,
      "loss": 0.372,
      "step": 3399
    },
    {
      "epoch": 0.015992925482374857,
      "grad_norm": 2.796400547027588,
      "learning_rate": 0.00019726536347091384,
      "loss": 0.4463,
      "step": 3400
    },
    {
      "epoch": 0.01599762928398732,
      "grad_norm": 0.4654727876186371,
      "learning_rate": 0.00019726442049280036,
      "loss": 0.0397,
      "step": 3401
    },
    {
      "epoch": 0.01600233308559978,
      "grad_norm": 2.553330659866333,
      "learning_rate": 0.0001972634775146869,
      "loss": 0.219,
      "step": 3402
    },
    {
      "epoch": 0.016007036887212243,
      "grad_norm": 2.7152445316314697,
      "learning_rate": 0.00019726253453657342,
      "loss": 0.2994,
      "step": 3403
    },
    {
      "epoch": 0.01601174068882471,
      "grad_norm": 0.4953206479549408,
      "learning_rate": 0.00019726159155845994,
      "loss": 0.0257,
      "step": 3404
    },
    {
      "epoch": 0.01601644449043717,
      "grad_norm": 1.2267823219299316,
      "learning_rate": 0.00019726064858034646,
      "loss": 0.0956,
      "step": 3405
    },
    {
      "epoch": 0.016021148292049633,
      "grad_norm": 3.816056728363037,
      "learning_rate": 0.000197259705602233,
      "loss": 0.3121,
      "step": 3406
    },
    {
      "epoch": 0.0160258520936621,
      "grad_norm": 6.914320468902588,
      "learning_rate": 0.0001972587626241195,
      "loss": 0.3779,
      "step": 3407
    },
    {
      "epoch": 0.01603055589527456,
      "grad_norm": 1.1052178144454956,
      "learning_rate": 0.00019725781964600601,
      "loss": 0.0966,
      "step": 3408
    },
    {
      "epoch": 0.016035259696887023,
      "grad_norm": 1.3628171682357788,
      "learning_rate": 0.00019725687666789253,
      "loss": 0.1011,
      "step": 3409
    },
    {
      "epoch": 0.016039963498499486,
      "grad_norm": 5.165470600128174,
      "learning_rate": 0.00019725593368977905,
      "loss": 0.3382,
      "step": 3410
    },
    {
      "epoch": 0.01604466730011195,
      "grad_norm": 1.5807619094848633,
      "learning_rate": 0.0001972549907116656,
      "loss": 0.1754,
      "step": 3411
    },
    {
      "epoch": 0.016049371101724413,
      "grad_norm": 2.003124952316284,
      "learning_rate": 0.00019725404773355212,
      "loss": 0.3999,
      "step": 3412
    },
    {
      "epoch": 0.016054074903336876,
      "grad_norm": 2.5091633796691895,
      "learning_rate": 0.00019725310475543863,
      "loss": 0.2374,
      "step": 3413
    },
    {
      "epoch": 0.01605877870494934,
      "grad_norm": 3.228919744491577,
      "learning_rate": 0.00019725216177732515,
      "loss": 0.2913,
      "step": 3414
    },
    {
      "epoch": 0.016063482506561803,
      "grad_norm": 3.4405059814453125,
      "learning_rate": 0.0001972512187992117,
      "loss": 0.4943,
      "step": 3415
    },
    {
      "epoch": 0.016068186308174266,
      "grad_norm": 0.4488454759120941,
      "learning_rate": 0.00019725027582109822,
      "loss": 0.0456,
      "step": 3416
    },
    {
      "epoch": 0.01607289010978673,
      "grad_norm": 3.740696907043457,
      "learning_rate": 0.00019724933284298474,
      "loss": 0.7093,
      "step": 3417
    },
    {
      "epoch": 0.016077593911399193,
      "grad_norm": 3.9819726943969727,
      "learning_rate": 0.00019724838986487125,
      "loss": 0.6686,
      "step": 3418
    },
    {
      "epoch": 0.016082297713011656,
      "grad_norm": 2.9595606327056885,
      "learning_rate": 0.00019724744688675775,
      "loss": 0.3278,
      "step": 3419
    },
    {
      "epoch": 0.016087001514624118,
      "grad_norm": 2.058253526687622,
      "learning_rate": 0.0001972465039086443,
      "loss": 0.2173,
      "step": 3420
    },
    {
      "epoch": 0.016091705316236583,
      "grad_norm": 2.4436464309692383,
      "learning_rate": 0.0001972455609305308,
      "loss": 0.244,
      "step": 3421
    },
    {
      "epoch": 0.016096409117849046,
      "grad_norm": 0.5146102905273438,
      "learning_rate": 0.00019724461795241733,
      "loss": 0.0696,
      "step": 3422
    },
    {
      "epoch": 0.016101112919461508,
      "grad_norm": 1.9560296535491943,
      "learning_rate": 0.00019724367497430385,
      "loss": 0.3043,
      "step": 3423
    },
    {
      "epoch": 0.016105816721073973,
      "grad_norm": 0.38819488883018494,
      "learning_rate": 0.0001972427319961904,
      "loss": 0.0329,
      "step": 3424
    },
    {
      "epoch": 0.016110520522686436,
      "grad_norm": 2.527592182159424,
      "learning_rate": 0.0001972417890180769,
      "loss": 0.2142,
      "step": 3425
    },
    {
      "epoch": 0.016115224324298898,
      "grad_norm": 0.8574690222740173,
      "learning_rate": 0.00019724084603996343,
      "loss": 0.1032,
      "step": 3426
    },
    {
      "epoch": 0.01611992812591136,
      "grad_norm": 0.6008712649345398,
      "learning_rate": 0.00019723990306184995,
      "loss": 0.0419,
      "step": 3427
    },
    {
      "epoch": 0.016124631927523825,
      "grad_norm": 0.7804669737815857,
      "learning_rate": 0.00019723896008373647,
      "loss": 0.0649,
      "step": 3428
    },
    {
      "epoch": 0.016129335729136288,
      "grad_norm": 1.9332300424575806,
      "learning_rate": 0.000197238017105623,
      "loss": 0.1612,
      "step": 3429
    },
    {
      "epoch": 0.01613403953074875,
      "grad_norm": 4.349641799926758,
      "learning_rate": 0.0001972370741275095,
      "loss": 0.9394,
      "step": 3430
    },
    {
      "epoch": 0.016138743332361215,
      "grad_norm": 2.6495535373687744,
      "learning_rate": 0.00019723613114939602,
      "loss": 0.2362,
      "step": 3431
    },
    {
      "epoch": 0.016143447133973678,
      "grad_norm": 2.133471965789795,
      "learning_rate": 0.00019723518817128254,
      "loss": 0.1348,
      "step": 3432
    },
    {
      "epoch": 0.01614815093558614,
      "grad_norm": 4.359438896179199,
      "learning_rate": 0.00019723424519316906,
      "loss": 0.7606,
      "step": 3433
    },
    {
      "epoch": 0.016152854737198605,
      "grad_norm": 5.664246082305908,
      "learning_rate": 0.0001972333022150556,
      "loss": 0.7941,
      "step": 3434
    },
    {
      "epoch": 0.016157558538811068,
      "grad_norm": 1.527618169784546,
      "learning_rate": 0.00019723235923694213,
      "loss": 0.1268,
      "step": 3435
    },
    {
      "epoch": 0.01616226234042353,
      "grad_norm": 2.1337623596191406,
      "learning_rate": 0.00019723141625882864,
      "loss": 0.1974,
      "step": 3436
    },
    {
      "epoch": 0.016166966142035992,
      "grad_norm": 3.2294132709503174,
      "learning_rate": 0.00019723047328071516,
      "loss": 0.2495,
      "step": 3437
    },
    {
      "epoch": 0.016171669943648458,
      "grad_norm": 1.4531080722808838,
      "learning_rate": 0.00019722953030260168,
      "loss": 0.1127,
      "step": 3438
    },
    {
      "epoch": 0.01617637374526092,
      "grad_norm": 2.297224521636963,
      "learning_rate": 0.0001972285873244882,
      "loss": 0.3158,
      "step": 3439
    },
    {
      "epoch": 0.016181077546873382,
      "grad_norm": 0.08365082740783691,
      "learning_rate": 0.00019722764434637472,
      "loss": 0.0067,
      "step": 3440
    },
    {
      "epoch": 0.016185781348485848,
      "grad_norm": 3.1682074069976807,
      "learning_rate": 0.00019722670136826124,
      "loss": 0.3938,
      "step": 3441
    },
    {
      "epoch": 0.01619048515009831,
      "grad_norm": 1.2248698472976685,
      "learning_rate": 0.00019722575839014776,
      "loss": 0.1615,
      "step": 3442
    },
    {
      "epoch": 0.016195188951710772,
      "grad_norm": 0.5316714644432068,
      "learning_rate": 0.0001972248154120343,
      "loss": 0.0325,
      "step": 3443
    },
    {
      "epoch": 0.016199892753323234,
      "grad_norm": 3.0410640239715576,
      "learning_rate": 0.00019722387243392082,
      "loss": 0.1169,
      "step": 3444
    },
    {
      "epoch": 0.0162045965549357,
      "grad_norm": 0.12683402001857758,
      "learning_rate": 0.00019722292945580734,
      "loss": 0.0063,
      "step": 3445
    },
    {
      "epoch": 0.016209300356548162,
      "grad_norm": 1.041824221611023,
      "learning_rate": 0.00019722198647769386,
      "loss": 0.0482,
      "step": 3446
    },
    {
      "epoch": 0.016214004158160624,
      "grad_norm": 0.128913015127182,
      "learning_rate": 0.0001972210434995804,
      "loss": 0.0058,
      "step": 3447
    },
    {
      "epoch": 0.01621870795977309,
      "grad_norm": 1.493908405303955,
      "learning_rate": 0.00019722010052146692,
      "loss": 0.1277,
      "step": 3448
    },
    {
      "epoch": 0.016223411761385552,
      "grad_norm": 3.4778642654418945,
      "learning_rate": 0.00019721915754335344,
      "loss": 0.7992,
      "step": 3449
    },
    {
      "epoch": 0.016228115562998014,
      "grad_norm": 2.2344563007354736,
      "learning_rate": 0.00019721821456523993,
      "loss": 0.1854,
      "step": 3450
    },
    {
      "epoch": 0.01623281936461048,
      "grad_norm": 1.5732959508895874,
      "learning_rate": 0.00019721727158712645,
      "loss": 0.0677,
      "step": 3451
    },
    {
      "epoch": 0.016237523166222942,
      "grad_norm": 2.583014726638794,
      "learning_rate": 0.000197216328609013,
      "loss": 0.1768,
      "step": 3452
    },
    {
      "epoch": 0.016242226967835404,
      "grad_norm": 0.9258081912994385,
      "learning_rate": 0.00019721538563089952,
      "loss": 0.0491,
      "step": 3453
    },
    {
      "epoch": 0.016246930769447866,
      "grad_norm": 2.1845884323120117,
      "learning_rate": 0.00019721444265278603,
      "loss": 0.0777,
      "step": 3454
    },
    {
      "epoch": 0.016251634571060332,
      "grad_norm": 1.0445587635040283,
      "learning_rate": 0.00019721349967467255,
      "loss": 0.0218,
      "step": 3455
    },
    {
      "epoch": 0.016256338372672794,
      "grad_norm": 7.165026664733887,
      "learning_rate": 0.0001972125566965591,
      "loss": 1.1446,
      "step": 3456
    },
    {
      "epoch": 0.016261042174285256,
      "grad_norm": 1.3091626167297363,
      "learning_rate": 0.00019721161371844562,
      "loss": 0.071,
      "step": 3457
    },
    {
      "epoch": 0.016265745975897722,
      "grad_norm": 3.4180240631103516,
      "learning_rate": 0.00019721067074033214,
      "loss": 0.2228,
      "step": 3458
    },
    {
      "epoch": 0.016270449777510184,
      "grad_norm": 8.132967948913574,
      "learning_rate": 0.00019720972776221865,
      "loss": 1.2653,
      "step": 3459
    },
    {
      "epoch": 0.016275153579122646,
      "grad_norm": 4.240304470062256,
      "learning_rate": 0.00019720878478410517,
      "loss": 0.2775,
      "step": 3460
    },
    {
      "epoch": 0.016279857380735112,
      "grad_norm": 1.642988920211792,
      "learning_rate": 0.0001972078418059917,
      "loss": 0.1157,
      "step": 3461
    },
    {
      "epoch": 0.016284561182347574,
      "grad_norm": 3.309264659881592,
      "learning_rate": 0.0001972068988278782,
      "loss": 0.2863,
      "step": 3462
    },
    {
      "epoch": 0.016289264983960036,
      "grad_norm": 5.395252704620361,
      "learning_rate": 0.00019720595584976473,
      "loss": 0.6495,
      "step": 3463
    },
    {
      "epoch": 0.0162939687855725,
      "grad_norm": 0.535448431968689,
      "learning_rate": 0.00019720501287165125,
      "loss": 0.025,
      "step": 3464
    },
    {
      "epoch": 0.016298672587184964,
      "grad_norm": 3.9217240810394287,
      "learning_rate": 0.0001972040698935378,
      "loss": 0.2857,
      "step": 3465
    },
    {
      "epoch": 0.016303376388797426,
      "grad_norm": 2.4743704795837402,
      "learning_rate": 0.0001972031269154243,
      "loss": 0.1807,
      "step": 3466
    },
    {
      "epoch": 0.01630808019040989,
      "grad_norm": 5.052432060241699,
      "learning_rate": 0.00019720218393731083,
      "loss": 1.0441,
      "step": 3467
    },
    {
      "epoch": 0.016312783992022354,
      "grad_norm": 4.015885829925537,
      "learning_rate": 0.00019720124095919735,
      "loss": 0.4457,
      "step": 3468
    },
    {
      "epoch": 0.016317487793634816,
      "grad_norm": 2.64345121383667,
      "learning_rate": 0.00019720029798108387,
      "loss": 0.5156,
      "step": 3469
    },
    {
      "epoch": 0.01632219159524728,
      "grad_norm": 3.328923463821411,
      "learning_rate": 0.0001971993550029704,
      "loss": 0.307,
      "step": 3470
    },
    {
      "epoch": 0.01632689539685974,
      "grad_norm": 3.8811569213867188,
      "learning_rate": 0.0001971984120248569,
      "loss": 0.5667,
      "step": 3471
    },
    {
      "epoch": 0.016331599198472206,
      "grad_norm": 2.1159021854400635,
      "learning_rate": 0.00019719746904674342,
      "loss": 0.2114,
      "step": 3472
    },
    {
      "epoch": 0.01633630300008467,
      "grad_norm": 2.7790486812591553,
      "learning_rate": 0.00019719652606862994,
      "loss": 0.3732,
      "step": 3473
    },
    {
      "epoch": 0.01634100680169713,
      "grad_norm": 0.27103501558303833,
      "learning_rate": 0.0001971955830905165,
      "loss": 0.0188,
      "step": 3474
    },
    {
      "epoch": 0.016345710603309596,
      "grad_norm": 3.468862771987915,
      "learning_rate": 0.000197194640112403,
      "loss": 0.6281,
      "step": 3475
    },
    {
      "epoch": 0.01635041440492206,
      "grad_norm": 2.21860408782959,
      "learning_rate": 0.00019719369713428953,
      "loss": 0.2267,
      "step": 3476
    },
    {
      "epoch": 0.01635511820653452,
      "grad_norm": 2.0876615047454834,
      "learning_rate": 0.00019719275415617604,
      "loss": 0.2663,
      "step": 3477
    },
    {
      "epoch": 0.016359822008146986,
      "grad_norm": 3.2046430110931396,
      "learning_rate": 0.00019719181117806256,
      "loss": 0.3764,
      "step": 3478
    },
    {
      "epoch": 0.01636452580975945,
      "grad_norm": 1.2559237480163574,
      "learning_rate": 0.0001971908681999491,
      "loss": 0.2055,
      "step": 3479
    },
    {
      "epoch": 0.01636922961137191,
      "grad_norm": 3.069631814956665,
      "learning_rate": 0.00019718992522183563,
      "loss": 0.7144,
      "step": 3480
    },
    {
      "epoch": 0.016373933412984373,
      "grad_norm": 0.7279776930809021,
      "learning_rate": 0.00019718898224372212,
      "loss": 0.1023,
      "step": 3481
    },
    {
      "epoch": 0.01637863721459684,
      "grad_norm": 1.9451907873153687,
      "learning_rate": 0.00019718803926560864,
      "loss": 0.4245,
      "step": 3482
    },
    {
      "epoch": 0.0163833410162093,
      "grad_norm": 1.8764816522598267,
      "learning_rate": 0.00019718709628749518,
      "loss": 0.4514,
      "step": 3483
    },
    {
      "epoch": 0.016388044817821763,
      "grad_norm": 3.7116587162017822,
      "learning_rate": 0.0001971861533093817,
      "loss": 0.5478,
      "step": 3484
    },
    {
      "epoch": 0.016392748619434228,
      "grad_norm": 1.6223052740097046,
      "learning_rate": 0.00019718521033126822,
      "loss": 0.2939,
      "step": 3485
    },
    {
      "epoch": 0.01639745242104669,
      "grad_norm": 1.1169008016586304,
      "learning_rate": 0.00019718426735315474,
      "loss": 0.1474,
      "step": 3486
    },
    {
      "epoch": 0.016402156222659153,
      "grad_norm": 0.39371535181999207,
      "learning_rate": 0.00019718332437504126,
      "loss": 0.0308,
      "step": 3487
    },
    {
      "epoch": 0.016406860024271615,
      "grad_norm": 1.0031698942184448,
      "learning_rate": 0.0001971823813969278,
      "loss": 0.1168,
      "step": 3488
    },
    {
      "epoch": 0.01641156382588408,
      "grad_norm": 1.2692919969558716,
      "learning_rate": 0.00019718143841881432,
      "loss": 0.1632,
      "step": 3489
    },
    {
      "epoch": 0.016416267627496543,
      "grad_norm": 1.694144368171692,
      "learning_rate": 0.00019718049544070084,
      "loss": 0.28,
      "step": 3490
    },
    {
      "epoch": 0.016420971429109005,
      "grad_norm": 3.2745039463043213,
      "learning_rate": 0.00019717955246258736,
      "loss": 0.7508,
      "step": 3491
    },
    {
      "epoch": 0.01642567523072147,
      "grad_norm": 2.1069259643554688,
      "learning_rate": 0.00019717860948447385,
      "loss": 0.5081,
      "step": 3492
    },
    {
      "epoch": 0.016430379032333933,
      "grad_norm": 2.1181464195251465,
      "learning_rate": 0.0001971776665063604,
      "loss": 0.4696,
      "step": 3493
    },
    {
      "epoch": 0.016435082833946395,
      "grad_norm": 1.77530837059021,
      "learning_rate": 0.00019717672352824692,
      "loss": 0.2786,
      "step": 3494
    },
    {
      "epoch": 0.01643978663555886,
      "grad_norm": 1.0454976558685303,
      "learning_rate": 0.00019717578055013343,
      "loss": 0.1254,
      "step": 3495
    },
    {
      "epoch": 0.016444490437171323,
      "grad_norm": 0.885767936706543,
      "learning_rate": 0.00019717483757201995,
      "loss": 0.0906,
      "step": 3496
    },
    {
      "epoch": 0.016449194238783785,
      "grad_norm": 2.6092848777770996,
      "learning_rate": 0.0001971738945939065,
      "loss": 0.2333,
      "step": 3497
    },
    {
      "epoch": 0.016453898040396247,
      "grad_norm": 1.6348057985305786,
      "learning_rate": 0.00019717295161579302,
      "loss": 0.2189,
      "step": 3498
    },
    {
      "epoch": 0.016458601842008713,
      "grad_norm": 2.7734813690185547,
      "learning_rate": 0.00019717200863767954,
      "loss": 0.4697,
      "step": 3499
    },
    {
      "epoch": 0.016463305643621175,
      "grad_norm": 2.017763137817383,
      "learning_rate": 0.00019717106565956605,
      "loss": 0.5301,
      "step": 3500
    },
    {
      "epoch": 0.016468009445233637,
      "grad_norm": 4.1484150886535645,
      "learning_rate": 0.00019717012268145257,
      "loss": 0.5304,
      "step": 3501
    },
    {
      "epoch": 0.016472713246846103,
      "grad_norm": 2.5699055194854736,
      "learning_rate": 0.0001971691797033391,
      "loss": 0.2856,
      "step": 3502
    },
    {
      "epoch": 0.016477417048458565,
      "grad_norm": 1.3414885997772217,
      "learning_rate": 0.0001971682367252256,
      "loss": 0.135,
      "step": 3503
    },
    {
      "epoch": 0.016482120850071027,
      "grad_norm": 1.4894579648971558,
      "learning_rate": 0.00019716729374711213,
      "loss": 0.2839,
      "step": 3504
    },
    {
      "epoch": 0.01648682465168349,
      "grad_norm": 0.9155077338218689,
      "learning_rate": 0.00019716635076899865,
      "loss": 0.0979,
      "step": 3505
    },
    {
      "epoch": 0.016491528453295955,
      "grad_norm": 2.397761106491089,
      "learning_rate": 0.0001971654077908852,
      "loss": 0.387,
      "step": 3506
    },
    {
      "epoch": 0.016496232254908417,
      "grad_norm": 2.0197718143463135,
      "learning_rate": 0.0001971644648127717,
      "loss": 0.252,
      "step": 3507
    },
    {
      "epoch": 0.01650093605652088,
      "grad_norm": 0.543716311454773,
      "learning_rate": 0.00019716352183465823,
      "loss": 0.0405,
      "step": 3508
    },
    {
      "epoch": 0.016505639858133345,
      "grad_norm": 2.439419746398926,
      "learning_rate": 0.00019716257885654475,
      "loss": 0.3118,
      "step": 3509
    },
    {
      "epoch": 0.016510343659745807,
      "grad_norm": 2.372974395751953,
      "learning_rate": 0.00019716163587843127,
      "loss": 0.3075,
      "step": 3510
    },
    {
      "epoch": 0.01651504746135827,
      "grad_norm": 1.320055365562439,
      "learning_rate": 0.00019716069290031781,
      "loss": 0.179,
      "step": 3511
    },
    {
      "epoch": 0.016519751262970735,
      "grad_norm": 3.1784756183624268,
      "learning_rate": 0.0001971597499222043,
      "loss": 0.5281,
      "step": 3512
    },
    {
      "epoch": 0.016524455064583197,
      "grad_norm": 4.3010993003845215,
      "learning_rate": 0.00019715880694409082,
      "loss": 0.7978,
      "step": 3513
    },
    {
      "epoch": 0.01652915886619566,
      "grad_norm": 1.0951616764068604,
      "learning_rate": 0.00019715786396597734,
      "loss": 0.2492,
      "step": 3514
    },
    {
      "epoch": 0.01653386266780812,
      "grad_norm": 1.544732689857483,
      "learning_rate": 0.0001971569209878639,
      "loss": 0.2253,
      "step": 3515
    },
    {
      "epoch": 0.016538566469420587,
      "grad_norm": 1.2324373722076416,
      "learning_rate": 0.0001971559780097504,
      "loss": 0.0961,
      "step": 3516
    },
    {
      "epoch": 0.01654327027103305,
      "grad_norm": 1.2242974042892456,
      "learning_rate": 0.00019715503503163693,
      "loss": 0.1172,
      "step": 3517
    },
    {
      "epoch": 0.01654797407264551,
      "grad_norm": 2.3236560821533203,
      "learning_rate": 0.00019715409205352344,
      "loss": 0.4115,
      "step": 3518
    },
    {
      "epoch": 0.016552677874257977,
      "grad_norm": 2.4716336727142334,
      "learning_rate": 0.00019715314907540996,
      "loss": 0.3125,
      "step": 3519
    },
    {
      "epoch": 0.01655738167587044,
      "grad_norm": 0.6395635604858398,
      "learning_rate": 0.0001971522060972965,
      "loss": 0.0485,
      "step": 3520
    },
    {
      "epoch": 0.0165620854774829,
      "grad_norm": 0.3491360545158386,
      "learning_rate": 0.00019715126311918303,
      "loss": 0.0266,
      "step": 3521
    },
    {
      "epoch": 0.016566789279095363,
      "grad_norm": 2.141554117202759,
      "learning_rate": 0.00019715032014106955,
      "loss": 0.3197,
      "step": 3522
    },
    {
      "epoch": 0.01657149308070783,
      "grad_norm": 1.6295440196990967,
      "learning_rate": 0.00019714937716295604,
      "loss": 0.2028,
      "step": 3523
    },
    {
      "epoch": 0.01657619688232029,
      "grad_norm": 4.54509162902832,
      "learning_rate": 0.00019714843418484258,
      "loss": 0.4456,
      "step": 3524
    },
    {
      "epoch": 0.016580900683932753,
      "grad_norm": 1.5991833209991455,
      "learning_rate": 0.0001971474912067291,
      "loss": 0.1843,
      "step": 3525
    },
    {
      "epoch": 0.01658560448554522,
      "grad_norm": 1.3134523630142212,
      "learning_rate": 0.00019714654822861562,
      "loss": 0.1419,
      "step": 3526
    },
    {
      "epoch": 0.01659030828715768,
      "grad_norm": 1.3431384563446045,
      "learning_rate": 0.00019714560525050214,
      "loss": 0.1903,
      "step": 3527
    },
    {
      "epoch": 0.016595012088770143,
      "grad_norm": 1.2467217445373535,
      "learning_rate": 0.00019714466227238866,
      "loss": 0.1919,
      "step": 3528
    },
    {
      "epoch": 0.01659971589038261,
      "grad_norm": 0.3973221480846405,
      "learning_rate": 0.0001971437192942752,
      "loss": 0.0466,
      "step": 3529
    },
    {
      "epoch": 0.01660441969199507,
      "grad_norm": 1.6399743556976318,
      "learning_rate": 0.00019714277631616172,
      "loss": 0.201,
      "step": 3530
    },
    {
      "epoch": 0.016609123493607533,
      "grad_norm": 1.9571728706359863,
      "learning_rate": 0.00019714183333804824,
      "loss": 0.4015,
      "step": 3531
    },
    {
      "epoch": 0.016613827295219995,
      "grad_norm": 0.33062559366226196,
      "learning_rate": 0.00019714089035993476,
      "loss": 0.027,
      "step": 3532
    },
    {
      "epoch": 0.01661853109683246,
      "grad_norm": 0.6843816637992859,
      "learning_rate": 0.00019713994738182128,
      "loss": 0.0629,
      "step": 3533
    },
    {
      "epoch": 0.016623234898444923,
      "grad_norm": 1.7571643590927124,
      "learning_rate": 0.0001971390044037078,
      "loss": 0.3093,
      "step": 3534
    },
    {
      "epoch": 0.016627938700057385,
      "grad_norm": 2.8013172149658203,
      "learning_rate": 0.00019713806142559432,
      "loss": 0.5882,
      "step": 3535
    },
    {
      "epoch": 0.01663264250166985,
      "grad_norm": 5.089735507965088,
      "learning_rate": 0.00019713711844748083,
      "loss": 0.5461,
      "step": 3536
    },
    {
      "epoch": 0.016637346303282313,
      "grad_norm": 0.9679855704307556,
      "learning_rate": 0.00019713617546936735,
      "loss": 0.1185,
      "step": 3537
    },
    {
      "epoch": 0.016642050104894775,
      "grad_norm": 2.5542190074920654,
      "learning_rate": 0.0001971352324912539,
      "loss": 0.3115,
      "step": 3538
    },
    {
      "epoch": 0.016646753906507238,
      "grad_norm": 1.7720224857330322,
      "learning_rate": 0.00019713428951314042,
      "loss": 0.26,
      "step": 3539
    },
    {
      "epoch": 0.016651457708119703,
      "grad_norm": 3.3848917484283447,
      "learning_rate": 0.00019713334653502694,
      "loss": 0.5402,
      "step": 3540
    },
    {
      "epoch": 0.016656161509732165,
      "grad_norm": 1.2282823324203491,
      "learning_rate": 0.00019713240355691345,
      "loss": 0.165,
      "step": 3541
    },
    {
      "epoch": 0.016660865311344628,
      "grad_norm": 1.704714059829712,
      "learning_rate": 0.00019713146057879997,
      "loss": 0.4724,
      "step": 3542
    },
    {
      "epoch": 0.016665569112957093,
      "grad_norm": 1.211280345916748,
      "learning_rate": 0.0001971305176006865,
      "loss": 0.1128,
      "step": 3543
    },
    {
      "epoch": 0.016670272914569555,
      "grad_norm": 0.8428826928138733,
      "learning_rate": 0.000197129574622573,
      "loss": 0.1174,
      "step": 3544
    },
    {
      "epoch": 0.016674976716182018,
      "grad_norm": 1.1484956741333008,
      "learning_rate": 0.00019712863164445953,
      "loss": 0.3036,
      "step": 3545
    },
    {
      "epoch": 0.016679680517794483,
      "grad_norm": 2.9548230171203613,
      "learning_rate": 0.00019712768866634605,
      "loss": 0.5137,
      "step": 3546
    },
    {
      "epoch": 0.016684384319406945,
      "grad_norm": 1.4376330375671387,
      "learning_rate": 0.0001971267456882326,
      "loss": 0.1939,
      "step": 3547
    },
    {
      "epoch": 0.016689088121019408,
      "grad_norm": 0.860629677772522,
      "learning_rate": 0.0001971258027101191,
      "loss": 0.1142,
      "step": 3548
    },
    {
      "epoch": 0.01669379192263187,
      "grad_norm": 1.054396629333496,
      "learning_rate": 0.00019712485973200563,
      "loss": 0.1358,
      "step": 3549
    },
    {
      "epoch": 0.016698495724244335,
      "grad_norm": 0.8264822363853455,
      "learning_rate": 0.00019712391675389215,
      "loss": 0.1294,
      "step": 3550
    },
    {
      "epoch": 0.016703199525856798,
      "grad_norm": 0.47018077969551086,
      "learning_rate": 0.00019712297377577867,
      "loss": 0.0294,
      "step": 3551
    },
    {
      "epoch": 0.01670790332746926,
      "grad_norm": 0.1280367374420166,
      "learning_rate": 0.00019712203079766521,
      "loss": 0.0067,
      "step": 3552
    },
    {
      "epoch": 0.016712607129081725,
      "grad_norm": 3.2092466354370117,
      "learning_rate": 0.00019712108781955173,
      "loss": 0.4848,
      "step": 3553
    },
    {
      "epoch": 0.016717310930694187,
      "grad_norm": 0.7965351343154907,
      "learning_rate": 0.00019712014484143822,
      "loss": 0.0862,
      "step": 3554
    },
    {
      "epoch": 0.01672201473230665,
      "grad_norm": 0.9334099888801575,
      "learning_rate": 0.00019711920186332474,
      "loss": 0.0967,
      "step": 3555
    },
    {
      "epoch": 0.016726718533919112,
      "grad_norm": 2.274336338043213,
      "learning_rate": 0.0001971182588852113,
      "loss": 0.3913,
      "step": 3556
    },
    {
      "epoch": 0.016731422335531577,
      "grad_norm": 3.4190406799316406,
      "learning_rate": 0.0001971173159070978,
      "loss": 0.3825,
      "step": 3557
    },
    {
      "epoch": 0.01673612613714404,
      "grad_norm": 2.696845769882202,
      "learning_rate": 0.00019711637292898433,
      "loss": 0.5269,
      "step": 3558
    },
    {
      "epoch": 0.016740829938756502,
      "grad_norm": 2.2786595821380615,
      "learning_rate": 0.00019711542995087084,
      "loss": 0.2628,
      "step": 3559
    },
    {
      "epoch": 0.016745533740368967,
      "grad_norm": 2.89017915725708,
      "learning_rate": 0.00019711448697275736,
      "loss": 0.3816,
      "step": 3560
    },
    {
      "epoch": 0.01675023754198143,
      "grad_norm": 1.046273112297058,
      "learning_rate": 0.0001971135439946439,
      "loss": 0.1082,
      "step": 3561
    },
    {
      "epoch": 0.016754941343593892,
      "grad_norm": 2.9493355751037598,
      "learning_rate": 0.00019711260101653043,
      "loss": 0.3225,
      "step": 3562
    },
    {
      "epoch": 0.016759645145206357,
      "grad_norm": 2.8940505981445312,
      "learning_rate": 0.00019711165803841695,
      "loss": 0.5132,
      "step": 3563
    },
    {
      "epoch": 0.01676434894681882,
      "grad_norm": 1.403325080871582,
      "learning_rate": 0.00019711071506030346,
      "loss": 0.1057,
      "step": 3564
    },
    {
      "epoch": 0.016769052748431282,
      "grad_norm": 4.4486284255981445,
      "learning_rate": 0.00019710977208218998,
      "loss": 0.1882,
      "step": 3565
    },
    {
      "epoch": 0.016773756550043744,
      "grad_norm": 0.2143268585205078,
      "learning_rate": 0.0001971088291040765,
      "loss": 0.0166,
      "step": 3566
    },
    {
      "epoch": 0.01677846035165621,
      "grad_norm": 4.947638988494873,
      "learning_rate": 0.00019710788612596302,
      "loss": 1.3701,
      "step": 3567
    },
    {
      "epoch": 0.016783164153268672,
      "grad_norm": 0.8676791191101074,
      "learning_rate": 0.00019710694314784954,
      "loss": 0.0963,
      "step": 3568
    },
    {
      "epoch": 0.016787867954881134,
      "grad_norm": 0.8315841555595398,
      "learning_rate": 0.00019710600016973606,
      "loss": 0.0929,
      "step": 3569
    },
    {
      "epoch": 0.0167925717564936,
      "grad_norm": 1.4605566263198853,
      "learning_rate": 0.0001971050571916226,
      "loss": 0.1379,
      "step": 3570
    },
    {
      "epoch": 0.016797275558106062,
      "grad_norm": 2.7022271156311035,
      "learning_rate": 0.00019710411421350912,
      "loss": 0.5879,
      "step": 3571
    },
    {
      "epoch": 0.016801979359718524,
      "grad_norm": 1.9081138372421265,
      "learning_rate": 0.00019710317123539564,
      "loss": 0.2778,
      "step": 3572
    },
    {
      "epoch": 0.016806683161330986,
      "grad_norm": 1.3726893663406372,
      "learning_rate": 0.00019710222825728216,
      "loss": 0.1444,
      "step": 3573
    },
    {
      "epoch": 0.016811386962943452,
      "grad_norm": 1.1057554483413696,
      "learning_rate": 0.00019710128527916868,
      "loss": 0.1324,
      "step": 3574
    },
    {
      "epoch": 0.016816090764555914,
      "grad_norm": 0.7432732582092285,
      "learning_rate": 0.0001971003423010552,
      "loss": 0.0532,
      "step": 3575
    },
    {
      "epoch": 0.016820794566168376,
      "grad_norm": 0.487371563911438,
      "learning_rate": 0.00019709939932294172,
      "loss": 0.0703,
      "step": 3576
    },
    {
      "epoch": 0.016825498367780842,
      "grad_norm": 2.801149606704712,
      "learning_rate": 0.00019709845634482823,
      "loss": 0.7148,
      "step": 3577
    },
    {
      "epoch": 0.016830202169393304,
      "grad_norm": 3.0470340251922607,
      "learning_rate": 0.00019709751336671475,
      "loss": 0.5015,
      "step": 3578
    },
    {
      "epoch": 0.016834905971005766,
      "grad_norm": 0.5824283361434937,
      "learning_rate": 0.0001970965703886013,
      "loss": 0.0949,
      "step": 3579
    },
    {
      "epoch": 0.01683960977261823,
      "grad_norm": 3.716141700744629,
      "learning_rate": 0.00019709562741048782,
      "loss": 0.5266,
      "step": 3580
    },
    {
      "epoch": 0.016844313574230694,
      "grad_norm": 1.2629510164260864,
      "learning_rate": 0.00019709468443237434,
      "loss": 0.1665,
      "step": 3581
    },
    {
      "epoch": 0.016849017375843156,
      "grad_norm": 0.788056492805481,
      "learning_rate": 0.00019709374145426085,
      "loss": 0.0663,
      "step": 3582
    },
    {
      "epoch": 0.016853721177455618,
      "grad_norm": 2.5404434204101562,
      "learning_rate": 0.0001970927984761474,
      "loss": 0.4675,
      "step": 3583
    },
    {
      "epoch": 0.016858424979068084,
      "grad_norm": 1.8986003398895264,
      "learning_rate": 0.00019709185549803392,
      "loss": 0.1541,
      "step": 3584
    },
    {
      "epoch": 0.016863128780680546,
      "grad_norm": 3.3678362369537354,
      "learning_rate": 0.0001970909125199204,
      "loss": 0.8849,
      "step": 3585
    },
    {
      "epoch": 0.016867832582293008,
      "grad_norm": 1.6056714057922363,
      "learning_rate": 0.00019708996954180693,
      "loss": 0.3521,
      "step": 3586
    },
    {
      "epoch": 0.016872536383905474,
      "grad_norm": 0.357321172952652,
      "learning_rate": 0.00019708902656369345,
      "loss": 0.0262,
      "step": 3587
    },
    {
      "epoch": 0.016877240185517936,
      "grad_norm": 1.853358507156372,
      "learning_rate": 0.00019708808358558,
      "loss": 0.3662,
      "step": 3588
    },
    {
      "epoch": 0.016881943987130398,
      "grad_norm": 1.3848669528961182,
      "learning_rate": 0.0001970871406074665,
      "loss": 0.1215,
      "step": 3589
    },
    {
      "epoch": 0.01688664778874286,
      "grad_norm": 1.7907953262329102,
      "learning_rate": 0.00019708619762935303,
      "loss": 0.1584,
      "step": 3590
    },
    {
      "epoch": 0.016891351590355326,
      "grad_norm": 2.0510661602020264,
      "learning_rate": 0.00019708525465123955,
      "loss": 0.1665,
      "step": 3591
    },
    {
      "epoch": 0.016896055391967788,
      "grad_norm": 1.155301809310913,
      "learning_rate": 0.00019708431167312607,
      "loss": 0.1415,
      "step": 3592
    },
    {
      "epoch": 0.01690075919358025,
      "grad_norm": 1.4038358926773071,
      "learning_rate": 0.00019708336869501261,
      "loss": 0.1263,
      "step": 3593
    },
    {
      "epoch": 0.016905462995192716,
      "grad_norm": 0.4532114267349243,
      "learning_rate": 0.00019708242571689913,
      "loss": 0.0624,
      "step": 3594
    },
    {
      "epoch": 0.016910166796805178,
      "grad_norm": 2.2954649925231934,
      "learning_rate": 0.00019708148273878565,
      "loss": 0.1829,
      "step": 3595
    },
    {
      "epoch": 0.01691487059841764,
      "grad_norm": 2.047724485397339,
      "learning_rate": 0.00019708053976067214,
      "loss": 0.2224,
      "step": 3596
    },
    {
      "epoch": 0.016919574400030106,
      "grad_norm": 2.154755115509033,
      "learning_rate": 0.0001970795967825587,
      "loss": 0.2376,
      "step": 3597
    },
    {
      "epoch": 0.016924278201642568,
      "grad_norm": 0.8235113024711609,
      "learning_rate": 0.0001970786538044452,
      "loss": 0.0917,
      "step": 3598
    },
    {
      "epoch": 0.01692898200325503,
      "grad_norm": 6.242901802062988,
      "learning_rate": 0.00019707771082633173,
      "loss": 0.273,
      "step": 3599
    },
    {
      "epoch": 0.016933685804867493,
      "grad_norm": 2.2801430225372314,
      "learning_rate": 0.00019707676784821824,
      "loss": 0.2086,
      "step": 3600
    },
    {
      "epoch": 0.016938389606479958,
      "grad_norm": 5.46804141998291,
      "learning_rate": 0.00019707582487010476,
      "loss": 0.5242,
      "step": 3601
    },
    {
      "epoch": 0.01694309340809242,
      "grad_norm": 8.279925346374512,
      "learning_rate": 0.0001970748818919913,
      "loss": 0.6189,
      "step": 3602
    },
    {
      "epoch": 0.016947797209704882,
      "grad_norm": 0.9532408714294434,
      "learning_rate": 0.00019707393891387783,
      "loss": 0.0406,
      "step": 3603
    },
    {
      "epoch": 0.016952501011317348,
      "grad_norm": 0.5224641561508179,
      "learning_rate": 0.00019707299593576435,
      "loss": 0.0234,
      "step": 3604
    },
    {
      "epoch": 0.01695720481292981,
      "grad_norm": 1.9028003215789795,
      "learning_rate": 0.00019707205295765086,
      "loss": 0.1453,
      "step": 3605
    },
    {
      "epoch": 0.016961908614542272,
      "grad_norm": 2.5581214427948,
      "learning_rate": 0.00019707110997953738,
      "loss": 0.1948,
      "step": 3606
    },
    {
      "epoch": 0.016966612416154735,
      "grad_norm": 0.2760333716869354,
      "learning_rate": 0.0001970701670014239,
      "loss": 0.0143,
      "step": 3607
    },
    {
      "epoch": 0.0169713162177672,
      "grad_norm": 0.4798816442489624,
      "learning_rate": 0.00019706922402331042,
      "loss": 0.0348,
      "step": 3608
    },
    {
      "epoch": 0.016976020019379662,
      "grad_norm": 0.2625824213027954,
      "learning_rate": 0.00019706828104519694,
      "loss": 0.0149,
      "step": 3609
    },
    {
      "epoch": 0.016980723820992125,
      "grad_norm": 3.63667368888855,
      "learning_rate": 0.00019706733806708346,
      "loss": 0.2549,
      "step": 3610
    },
    {
      "epoch": 0.01698542762260459,
      "grad_norm": 9.48281192779541,
      "learning_rate": 0.00019706639508897,
      "loss": 0.9779,
      "step": 3611
    },
    {
      "epoch": 0.016990131424217052,
      "grad_norm": 2.815265655517578,
      "learning_rate": 0.00019706545211085652,
      "loss": 0.6204,
      "step": 3612
    },
    {
      "epoch": 0.016994835225829515,
      "grad_norm": 5.338914394378662,
      "learning_rate": 0.00019706450913274304,
      "loss": 0.947,
      "step": 3613
    },
    {
      "epoch": 0.01699953902744198,
      "grad_norm": 1.8185871839523315,
      "learning_rate": 0.00019706356615462956,
      "loss": 0.0941,
      "step": 3614
    },
    {
      "epoch": 0.017004242829054442,
      "grad_norm": 2.8319356441497803,
      "learning_rate": 0.0001970626231765161,
      "loss": 0.4863,
      "step": 3615
    },
    {
      "epoch": 0.017008946630666905,
      "grad_norm": 3.6167609691619873,
      "learning_rate": 0.0001970616801984026,
      "loss": 0.2319,
      "step": 3616
    },
    {
      "epoch": 0.017013650432279367,
      "grad_norm": 2.721437454223633,
      "learning_rate": 0.00019706073722028912,
      "loss": 0.2714,
      "step": 3617
    },
    {
      "epoch": 0.017018354233891832,
      "grad_norm": 2.957231283187866,
      "learning_rate": 0.00019705979424217563,
      "loss": 0.6672,
      "step": 3618
    },
    {
      "epoch": 0.017023058035504295,
      "grad_norm": 2.9654953479766846,
      "learning_rate": 0.00019705885126406215,
      "loss": 0.1853,
      "step": 3619
    },
    {
      "epoch": 0.017027761837116757,
      "grad_norm": 1.9094184637069702,
      "learning_rate": 0.0001970579082859487,
      "loss": 0.1377,
      "step": 3620
    },
    {
      "epoch": 0.017032465638729222,
      "grad_norm": 3.6525981426239014,
      "learning_rate": 0.00019705696530783522,
      "loss": 0.4393,
      "step": 3621
    },
    {
      "epoch": 0.017037169440341685,
      "grad_norm": 1.981792688369751,
      "learning_rate": 0.00019705602232972174,
      "loss": 0.2972,
      "step": 3622
    },
    {
      "epoch": 0.017041873241954147,
      "grad_norm": 2.9038853645324707,
      "learning_rate": 0.00019705507935160825,
      "loss": 0.4089,
      "step": 3623
    },
    {
      "epoch": 0.01704657704356661,
      "grad_norm": 1.6743946075439453,
      "learning_rate": 0.0001970541363734948,
      "loss": 0.3903,
      "step": 3624
    },
    {
      "epoch": 0.017051280845179075,
      "grad_norm": 2.452738046646118,
      "learning_rate": 0.00019705319339538132,
      "loss": 0.3298,
      "step": 3625
    },
    {
      "epoch": 0.017055984646791537,
      "grad_norm": 3.834667682647705,
      "learning_rate": 0.00019705225041726784,
      "loss": 0.5479,
      "step": 3626
    },
    {
      "epoch": 0.017060688448404,
      "grad_norm": 1.1247485876083374,
      "learning_rate": 0.00019705130743915433,
      "loss": 0.1321,
      "step": 3627
    },
    {
      "epoch": 0.017065392250016465,
      "grad_norm": 2.2858493328094482,
      "learning_rate": 0.00019705036446104085,
      "loss": 0.2492,
      "step": 3628
    },
    {
      "epoch": 0.017070096051628927,
      "grad_norm": 1.2064446210861206,
      "learning_rate": 0.0001970494214829274,
      "loss": 0.3181,
      "step": 3629
    },
    {
      "epoch": 0.01707479985324139,
      "grad_norm": 3.078186273574829,
      "learning_rate": 0.0001970484785048139,
      "loss": 0.5314,
      "step": 3630
    },
    {
      "epoch": 0.017079503654853855,
      "grad_norm": 2.452733039855957,
      "learning_rate": 0.00019704753552670043,
      "loss": 0.1801,
      "step": 3631
    },
    {
      "epoch": 0.017084207456466317,
      "grad_norm": 2.1439003944396973,
      "learning_rate": 0.00019704659254858695,
      "loss": 0.3469,
      "step": 3632
    },
    {
      "epoch": 0.01708891125807878,
      "grad_norm": 1.2465968132019043,
      "learning_rate": 0.0001970456495704735,
      "loss": 0.195,
      "step": 3633
    },
    {
      "epoch": 0.01709361505969124,
      "grad_norm": 0.40158846974372864,
      "learning_rate": 0.00019704470659236001,
      "loss": 0.0593,
      "step": 3634
    },
    {
      "epoch": 0.017098318861303707,
      "grad_norm": 1.7956581115722656,
      "learning_rate": 0.00019704376361424653,
      "loss": 0.5282,
      "step": 3635
    },
    {
      "epoch": 0.01710302266291617,
      "grad_norm": 1.237852692604065,
      "learning_rate": 0.00019704282063613305,
      "loss": 0.164,
      "step": 3636
    },
    {
      "epoch": 0.01710772646452863,
      "grad_norm": 1.7508140802383423,
      "learning_rate": 0.00019704187765801957,
      "loss": 0.4336,
      "step": 3637
    },
    {
      "epoch": 0.017112430266141097,
      "grad_norm": 0.6308429837226868,
      "learning_rate": 0.0001970409346799061,
      "loss": 0.104,
      "step": 3638
    },
    {
      "epoch": 0.01711713406775356,
      "grad_norm": 1.0491894483566284,
      "learning_rate": 0.0001970399917017926,
      "loss": 0.2181,
      "step": 3639
    },
    {
      "epoch": 0.01712183786936602,
      "grad_norm": 0.9655614495277405,
      "learning_rate": 0.00019703904872367913,
      "loss": 0.1813,
      "step": 3640
    },
    {
      "epoch": 0.017126541670978483,
      "grad_norm": 1.1570929288864136,
      "learning_rate": 0.00019703810574556564,
      "loss": 0.2078,
      "step": 3641
    },
    {
      "epoch": 0.01713124547259095,
      "grad_norm": 0.8384228348731995,
      "learning_rate": 0.00019703716276745216,
      "loss": 0.1091,
      "step": 3642
    },
    {
      "epoch": 0.01713594927420341,
      "grad_norm": 0.534174919128418,
      "learning_rate": 0.0001970362197893387,
      "loss": 0.1095,
      "step": 3643
    },
    {
      "epoch": 0.017140653075815873,
      "grad_norm": 1.3226524591445923,
      "learning_rate": 0.00019703527681122523,
      "loss": 0.1475,
      "step": 3644
    },
    {
      "epoch": 0.01714535687742834,
      "grad_norm": 2.6615188121795654,
      "learning_rate": 0.00019703433383311175,
      "loss": 0.2066,
      "step": 3645
    },
    {
      "epoch": 0.0171500606790408,
      "grad_norm": 0.25166213512420654,
      "learning_rate": 0.00019703339085499826,
      "loss": 0.0223,
      "step": 3646
    },
    {
      "epoch": 0.017154764480653263,
      "grad_norm": 3.34622859954834,
      "learning_rate": 0.00019703244787688478,
      "loss": 0.4478,
      "step": 3647
    },
    {
      "epoch": 0.01715946828226573,
      "grad_norm": 0.6427475214004517,
      "learning_rate": 0.0001970315048987713,
      "loss": 0.0417,
      "step": 3648
    },
    {
      "epoch": 0.01716417208387819,
      "grad_norm": 1.4963030815124512,
      "learning_rate": 0.00019703056192065782,
      "loss": 0.3747,
      "step": 3649
    },
    {
      "epoch": 0.017168875885490653,
      "grad_norm": 1.1396191120147705,
      "learning_rate": 0.00019702961894254434,
      "loss": 0.1845,
      "step": 3650
    },
    {
      "epoch": 0.017173579687103115,
      "grad_norm": 3.140099048614502,
      "learning_rate": 0.00019702867596443086,
      "loss": 0.4967,
      "step": 3651
    },
    {
      "epoch": 0.01717828348871558,
      "grad_norm": 5.510948181152344,
      "learning_rate": 0.0001970277329863174,
      "loss": 0.4317,
      "step": 3652
    },
    {
      "epoch": 0.017182987290328043,
      "grad_norm": 1.2230876684188843,
      "learning_rate": 0.00019702679000820392,
      "loss": 0.1204,
      "step": 3653
    },
    {
      "epoch": 0.017187691091940505,
      "grad_norm": 3.0968568325042725,
      "learning_rate": 0.00019702584703009044,
      "loss": 0.3613,
      "step": 3654
    },
    {
      "epoch": 0.01719239489355297,
      "grad_norm": 1.2158128023147583,
      "learning_rate": 0.00019702490405197696,
      "loss": 0.1053,
      "step": 3655
    },
    {
      "epoch": 0.017197098695165433,
      "grad_norm": 2.230555295944214,
      "learning_rate": 0.0001970239610738635,
      "loss": 0.2951,
      "step": 3656
    },
    {
      "epoch": 0.017201802496777895,
      "grad_norm": 2.8180124759674072,
      "learning_rate": 0.00019702301809575002,
      "loss": 0.3718,
      "step": 3657
    },
    {
      "epoch": 0.017206506298390357,
      "grad_norm": 2.489267587661743,
      "learning_rate": 0.00019702207511763652,
      "loss": 0.2299,
      "step": 3658
    },
    {
      "epoch": 0.017211210100002823,
      "grad_norm": 1.8499970436096191,
      "learning_rate": 0.00019702113213952303,
      "loss": 0.1996,
      "step": 3659
    },
    {
      "epoch": 0.017215913901615285,
      "grad_norm": 3.1835567951202393,
      "learning_rate": 0.00019702018916140955,
      "loss": 0.4351,
      "step": 3660
    },
    {
      "epoch": 0.017220617703227747,
      "grad_norm": 3.5583362579345703,
      "learning_rate": 0.0001970192461832961,
      "loss": 0.4998,
      "step": 3661
    },
    {
      "epoch": 0.017225321504840213,
      "grad_norm": 2.507162570953369,
      "learning_rate": 0.00019701830320518262,
      "loss": 0.2598,
      "step": 3662
    },
    {
      "epoch": 0.017230025306452675,
      "grad_norm": 2.2535178661346436,
      "learning_rate": 0.00019701736022706914,
      "loss": 0.3757,
      "step": 3663
    },
    {
      "epoch": 0.017234729108065137,
      "grad_norm": 0.8353131413459778,
      "learning_rate": 0.00019701641724895565,
      "loss": 0.1037,
      "step": 3664
    },
    {
      "epoch": 0.017239432909677603,
      "grad_norm": 3.6338298320770264,
      "learning_rate": 0.0001970154742708422,
      "loss": 0.2204,
      "step": 3665
    },
    {
      "epoch": 0.017244136711290065,
      "grad_norm": 0.863305926322937,
      "learning_rate": 0.00019701453129272872,
      "loss": 0.1245,
      "step": 3666
    },
    {
      "epoch": 0.017248840512902527,
      "grad_norm": 1.8399266004562378,
      "learning_rate": 0.00019701358831461524,
      "loss": 0.2294,
      "step": 3667
    },
    {
      "epoch": 0.01725354431451499,
      "grad_norm": 1.609732747077942,
      "learning_rate": 0.00019701264533650176,
      "loss": 0.236,
      "step": 3668
    },
    {
      "epoch": 0.017258248116127455,
      "grad_norm": 3.3005735874176025,
      "learning_rate": 0.00019701170235838827,
      "loss": 0.5813,
      "step": 3669
    },
    {
      "epoch": 0.017262951917739917,
      "grad_norm": 2.109220027923584,
      "learning_rate": 0.0001970107593802748,
      "loss": 0.2674,
      "step": 3670
    },
    {
      "epoch": 0.01726765571935238,
      "grad_norm": 0.547401487827301,
      "learning_rate": 0.0001970098164021613,
      "loss": 0.0545,
      "step": 3671
    },
    {
      "epoch": 0.017272359520964845,
      "grad_norm": 2.516029119491577,
      "learning_rate": 0.00019700887342404783,
      "loss": 0.4666,
      "step": 3672
    },
    {
      "epoch": 0.017277063322577307,
      "grad_norm": 1.5678668022155762,
      "learning_rate": 0.00019700793044593435,
      "loss": 0.1268,
      "step": 3673
    },
    {
      "epoch": 0.01728176712418977,
      "grad_norm": 1.0765190124511719,
      "learning_rate": 0.0001970069874678209,
      "loss": 0.2455,
      "step": 3674
    },
    {
      "epoch": 0.01728647092580223,
      "grad_norm": 2.086754083633423,
      "learning_rate": 0.00019700604448970741,
      "loss": 0.179,
      "step": 3675
    },
    {
      "epoch": 0.017291174727414697,
      "grad_norm": 3.4823741912841797,
      "learning_rate": 0.00019700510151159393,
      "loss": 0.4266,
      "step": 3676
    },
    {
      "epoch": 0.01729587852902716,
      "grad_norm": 0.49371835589408875,
      "learning_rate": 0.00019700415853348045,
      "loss": 0.0543,
      "step": 3677
    },
    {
      "epoch": 0.01730058233063962,
      "grad_norm": 1.3895981311798096,
      "learning_rate": 0.00019700321555536697,
      "loss": 0.3671,
      "step": 3678
    },
    {
      "epoch": 0.017305286132252087,
      "grad_norm": 2.5993852615356445,
      "learning_rate": 0.0001970022725772535,
      "loss": 0.2769,
      "step": 3679
    },
    {
      "epoch": 0.01730998993386455,
      "grad_norm": 2.9197683334350586,
      "learning_rate": 0.00019700132959914,
      "loss": 0.4105,
      "step": 3680
    },
    {
      "epoch": 0.01731469373547701,
      "grad_norm": 2.9657044410705566,
      "learning_rate": 0.00019700038662102653,
      "loss": 0.288,
      "step": 3681
    },
    {
      "epoch": 0.017319397537089477,
      "grad_norm": 1.5069632530212402,
      "learning_rate": 0.00019699944364291304,
      "loss": 0.1721,
      "step": 3682
    },
    {
      "epoch": 0.01732410133870194,
      "grad_norm": 1.9745917320251465,
      "learning_rate": 0.0001969985006647996,
      "loss": 0.2517,
      "step": 3683
    },
    {
      "epoch": 0.0173288051403144,
      "grad_norm": 1.300781488418579,
      "learning_rate": 0.0001969975576866861,
      "loss": 0.2206,
      "step": 3684
    },
    {
      "epoch": 0.017333508941926864,
      "grad_norm": 1.6442207098007202,
      "learning_rate": 0.00019699661470857263,
      "loss": 0.3525,
      "step": 3685
    },
    {
      "epoch": 0.01733821274353933,
      "grad_norm": 2.281873941421509,
      "learning_rate": 0.00019699567173045915,
      "loss": 0.3515,
      "step": 3686
    },
    {
      "epoch": 0.01734291654515179,
      "grad_norm": 2.12321138381958,
      "learning_rate": 0.00019699472875234566,
      "loss": 0.4607,
      "step": 3687
    },
    {
      "epoch": 0.017347620346764254,
      "grad_norm": 2.6863152980804443,
      "learning_rate": 0.0001969937857742322,
      "loss": 0.7791,
      "step": 3688
    },
    {
      "epoch": 0.01735232414837672,
      "grad_norm": 2.608179807662964,
      "learning_rate": 0.0001969928427961187,
      "loss": 0.3383,
      "step": 3689
    },
    {
      "epoch": 0.01735702794998918,
      "grad_norm": 0.7987598776817322,
      "learning_rate": 0.00019699189981800522,
      "loss": 0.0985,
      "step": 3690
    },
    {
      "epoch": 0.017361731751601644,
      "grad_norm": 2.175180196762085,
      "learning_rate": 0.00019699095683989174,
      "loss": 0.2509,
      "step": 3691
    },
    {
      "epoch": 0.017366435553214106,
      "grad_norm": 0.5219361782073975,
      "learning_rate": 0.00019699001386177828,
      "loss": 0.0565,
      "step": 3692
    },
    {
      "epoch": 0.01737113935482657,
      "grad_norm": 1.6478817462921143,
      "learning_rate": 0.0001969890708836648,
      "loss": 0.28,
      "step": 3693
    },
    {
      "epoch": 0.017375843156439034,
      "grad_norm": 1.7191057205200195,
      "learning_rate": 0.00019698812790555132,
      "loss": 0.4818,
      "step": 3694
    },
    {
      "epoch": 0.017380546958051496,
      "grad_norm": 0.5243635773658752,
      "learning_rate": 0.00019698718492743784,
      "loss": 0.0649,
      "step": 3695
    },
    {
      "epoch": 0.01738525075966396,
      "grad_norm": 0.4458070695400238,
      "learning_rate": 0.00019698624194932436,
      "loss": 0.0381,
      "step": 3696
    },
    {
      "epoch": 0.017389954561276424,
      "grad_norm": 1.1331506967544556,
      "learning_rate": 0.0001969852989712109,
      "loss": 0.3738,
      "step": 3697
    },
    {
      "epoch": 0.017394658362888886,
      "grad_norm": 0.7059888243675232,
      "learning_rate": 0.00019698435599309742,
      "loss": 0.1814,
      "step": 3698
    },
    {
      "epoch": 0.01739936216450135,
      "grad_norm": 0.5044656991958618,
      "learning_rate": 0.00019698341301498394,
      "loss": 0.0555,
      "step": 3699
    },
    {
      "epoch": 0.017404065966113814,
      "grad_norm": 1.7924771308898926,
      "learning_rate": 0.00019698247003687046,
      "loss": 0.3075,
      "step": 3700
    },
    {
      "epoch": 0.017408769767726276,
      "grad_norm": 0.8289272785186768,
      "learning_rate": 0.00019698152705875695,
      "loss": 0.2235,
      "step": 3701
    },
    {
      "epoch": 0.017413473569338738,
      "grad_norm": 3.9810101985931396,
      "learning_rate": 0.0001969805840806435,
      "loss": 0.8232,
      "step": 3702
    },
    {
      "epoch": 0.017418177370951204,
      "grad_norm": 0.6291305422782898,
      "learning_rate": 0.00019697964110253002,
      "loss": 0.0962,
      "step": 3703
    },
    {
      "epoch": 0.017422881172563666,
      "grad_norm": 4.931605815887451,
      "learning_rate": 0.00019697869812441654,
      "loss": 0.6625,
      "step": 3704
    },
    {
      "epoch": 0.017427584974176128,
      "grad_norm": 0.26068538427352905,
      "learning_rate": 0.00019697775514630305,
      "loss": 0.0218,
      "step": 3705
    },
    {
      "epoch": 0.017432288775788594,
      "grad_norm": 1.029964804649353,
      "learning_rate": 0.0001969768121681896,
      "loss": 0.1137,
      "step": 3706
    },
    {
      "epoch": 0.017436992577401056,
      "grad_norm": 1.7969123125076294,
      "learning_rate": 0.00019697586919007612,
      "loss": 0.2588,
      "step": 3707
    },
    {
      "epoch": 0.017441696379013518,
      "grad_norm": 1.6993552446365356,
      "learning_rate": 0.00019697492621196264,
      "loss": 0.3009,
      "step": 3708
    },
    {
      "epoch": 0.01744640018062598,
      "grad_norm": 0.7148145437240601,
      "learning_rate": 0.00019697398323384916,
      "loss": 0.1474,
      "step": 3709
    },
    {
      "epoch": 0.017451103982238446,
      "grad_norm": 1.00485360622406,
      "learning_rate": 0.00019697304025573567,
      "loss": 0.1662,
      "step": 3710
    },
    {
      "epoch": 0.017455807783850908,
      "grad_norm": 1.406411051750183,
      "learning_rate": 0.0001969720972776222,
      "loss": 0.0993,
      "step": 3711
    },
    {
      "epoch": 0.01746051158546337,
      "grad_norm": 0.4611295759677887,
      "learning_rate": 0.0001969711542995087,
      "loss": 0.0621,
      "step": 3712
    },
    {
      "epoch": 0.017465215387075836,
      "grad_norm": 3.207455635070801,
      "learning_rate": 0.00019697021132139523,
      "loss": 0.4285,
      "step": 3713
    },
    {
      "epoch": 0.017469919188688298,
      "grad_norm": 2.0334043502807617,
      "learning_rate": 0.00019696926834328175,
      "loss": 0.2304,
      "step": 3714
    },
    {
      "epoch": 0.01747462299030076,
      "grad_norm": 1.3155814409255981,
      "learning_rate": 0.0001969683253651683,
      "loss": 0.1921,
      "step": 3715
    },
    {
      "epoch": 0.017479326791913226,
      "grad_norm": 2.6483635902404785,
      "learning_rate": 0.00019696738238705481,
      "loss": 0.3324,
      "step": 3716
    },
    {
      "epoch": 0.017484030593525688,
      "grad_norm": 0.6633105278015137,
      "learning_rate": 0.00019696643940894133,
      "loss": 0.0697,
      "step": 3717
    },
    {
      "epoch": 0.01748873439513815,
      "grad_norm": 2.929483652114868,
      "learning_rate": 0.00019696549643082785,
      "loss": 0.4668,
      "step": 3718
    },
    {
      "epoch": 0.017493438196750612,
      "grad_norm": 0.67685467004776,
      "learning_rate": 0.00019696455345271437,
      "loss": 0.0558,
      "step": 3719
    },
    {
      "epoch": 0.017498141998363078,
      "grad_norm": 1.3382099866867065,
      "learning_rate": 0.0001969636104746009,
      "loss": 0.1647,
      "step": 3720
    },
    {
      "epoch": 0.01750284579997554,
      "grad_norm": 1.4362337589263916,
      "learning_rate": 0.0001969626674964874,
      "loss": 0.1464,
      "step": 3721
    },
    {
      "epoch": 0.017507549601588002,
      "grad_norm": 1.671962857246399,
      "learning_rate": 0.00019696172451837393,
      "loss": 0.3528,
      "step": 3722
    },
    {
      "epoch": 0.017512253403200468,
      "grad_norm": 0.6895016431808472,
      "learning_rate": 0.00019696078154026044,
      "loss": 0.054,
      "step": 3723
    },
    {
      "epoch": 0.01751695720481293,
      "grad_norm": 1.4608396291732788,
      "learning_rate": 0.000196959838562147,
      "loss": 0.1618,
      "step": 3724
    },
    {
      "epoch": 0.017521661006425392,
      "grad_norm": 4.76951265335083,
      "learning_rate": 0.0001969588955840335,
      "loss": 0.9519,
      "step": 3725
    },
    {
      "epoch": 0.017526364808037855,
      "grad_norm": 0.43916749954223633,
      "learning_rate": 0.00019695795260592003,
      "loss": 0.054,
      "step": 3726
    },
    {
      "epoch": 0.01753106860965032,
      "grad_norm": 3.3195443153381348,
      "learning_rate": 0.00019695700962780655,
      "loss": 0.5397,
      "step": 3727
    },
    {
      "epoch": 0.017535772411262782,
      "grad_norm": 2.4901697635650635,
      "learning_rate": 0.00019695606664969306,
      "loss": 0.1854,
      "step": 3728
    },
    {
      "epoch": 0.017540476212875244,
      "grad_norm": 4.112648963928223,
      "learning_rate": 0.0001969551236715796,
      "loss": 0.5316,
      "step": 3729
    },
    {
      "epoch": 0.01754518001448771,
      "grad_norm": 0.493253618478775,
      "learning_rate": 0.00019695418069346613,
      "loss": 0.0513,
      "step": 3730
    },
    {
      "epoch": 0.017549883816100172,
      "grad_norm": 3.813680648803711,
      "learning_rate": 0.00019695323771535265,
      "loss": 0.5742,
      "step": 3731
    },
    {
      "epoch": 0.017554587617712634,
      "grad_norm": 1.6807444095611572,
      "learning_rate": 0.00019695229473723914,
      "loss": 0.1493,
      "step": 3732
    },
    {
      "epoch": 0.0175592914193251,
      "grad_norm": 1.1812694072723389,
      "learning_rate": 0.00019695135175912568,
      "loss": 0.0592,
      "step": 3733
    },
    {
      "epoch": 0.017563995220937562,
      "grad_norm": 0.2068316787481308,
      "learning_rate": 0.0001969504087810122,
      "loss": 0.0124,
      "step": 3734
    },
    {
      "epoch": 0.017568699022550024,
      "grad_norm": 1.1446558237075806,
      "learning_rate": 0.00019694946580289872,
      "loss": 0.0937,
      "step": 3735
    },
    {
      "epoch": 0.017573402824162487,
      "grad_norm": 3.9710066318511963,
      "learning_rate": 0.00019694852282478524,
      "loss": 0.7334,
      "step": 3736
    },
    {
      "epoch": 0.017578106625774952,
      "grad_norm": 1.205365538597107,
      "learning_rate": 0.00019694757984667176,
      "loss": 0.1421,
      "step": 3737
    },
    {
      "epoch": 0.017582810427387414,
      "grad_norm": 0.7833182215690613,
      "learning_rate": 0.0001969466368685583,
      "loss": 0.0658,
      "step": 3738
    },
    {
      "epoch": 0.017587514228999877,
      "grad_norm": 0.8659272193908691,
      "learning_rate": 0.00019694569389044482,
      "loss": 0.1014,
      "step": 3739
    },
    {
      "epoch": 0.017592218030612342,
      "grad_norm": 3.380343437194824,
      "learning_rate": 0.00019694475091233134,
      "loss": 0.5521,
      "step": 3740
    },
    {
      "epoch": 0.017596921832224804,
      "grad_norm": 2.9549520015716553,
      "learning_rate": 0.00019694380793421786,
      "loss": 0.3242,
      "step": 3741
    },
    {
      "epoch": 0.017601625633837267,
      "grad_norm": 2.44101619720459,
      "learning_rate": 0.00019694286495610438,
      "loss": 0.3964,
      "step": 3742
    },
    {
      "epoch": 0.01760632943544973,
      "grad_norm": 3.8309035301208496,
      "learning_rate": 0.0001969419219779909,
      "loss": 0.8801,
      "step": 3743
    },
    {
      "epoch": 0.017611033237062194,
      "grad_norm": 3.2754764556884766,
      "learning_rate": 0.00019694097899987742,
      "loss": 0.7842,
      "step": 3744
    },
    {
      "epoch": 0.017615737038674657,
      "grad_norm": 3.445002794265747,
      "learning_rate": 0.00019694003602176394,
      "loss": 0.4244,
      "step": 3745
    },
    {
      "epoch": 0.01762044084028712,
      "grad_norm": 0.41739967465400696,
      "learning_rate": 0.00019693909304365045,
      "loss": 0.0349,
      "step": 3746
    },
    {
      "epoch": 0.017625144641899584,
      "grad_norm": 4.428679943084717,
      "learning_rate": 0.000196938150065537,
      "loss": 1.0671,
      "step": 3747
    },
    {
      "epoch": 0.017629848443512047,
      "grad_norm": 1.645328164100647,
      "learning_rate": 0.00019693720708742352,
      "loss": 0.165,
      "step": 3748
    },
    {
      "epoch": 0.01763455224512451,
      "grad_norm": 2.271038770675659,
      "learning_rate": 0.00019693626410931004,
      "loss": 0.4963,
      "step": 3749
    },
    {
      "epoch": 0.017639256046736974,
      "grad_norm": 1.4349850416183472,
      "learning_rate": 0.00019693532113119656,
      "loss": 0.2928,
      "step": 3750
    },
    {
      "epoch": 0.017643959848349437,
      "grad_norm": 0.9651577472686768,
      "learning_rate": 0.00019693437815308307,
      "loss": 0.0586,
      "step": 3751
    },
    {
      "epoch": 0.0176486636499619,
      "grad_norm": 2.5348544120788574,
      "learning_rate": 0.0001969334351749696,
      "loss": 0.2777,
      "step": 3752
    },
    {
      "epoch": 0.01765336745157436,
      "grad_norm": 1.102596640586853,
      "learning_rate": 0.0001969324921968561,
      "loss": 0.1291,
      "step": 3753
    },
    {
      "epoch": 0.017658071253186827,
      "grad_norm": 0.537772536277771,
      "learning_rate": 0.00019693154921874263,
      "loss": 0.0631,
      "step": 3754
    },
    {
      "epoch": 0.01766277505479929,
      "grad_norm": 2.3654143810272217,
      "learning_rate": 0.00019693060624062915,
      "loss": 0.3074,
      "step": 3755
    },
    {
      "epoch": 0.01766747885641175,
      "grad_norm": 1.413703441619873,
      "learning_rate": 0.0001969296632625157,
      "loss": 0.2023,
      "step": 3756
    },
    {
      "epoch": 0.017672182658024217,
      "grad_norm": 2.0573294162750244,
      "learning_rate": 0.00019692872028440221,
      "loss": 0.2806,
      "step": 3757
    },
    {
      "epoch": 0.01767688645963668,
      "grad_norm": 0.619703471660614,
      "learning_rate": 0.00019692777730628873,
      "loss": 0.1295,
      "step": 3758
    },
    {
      "epoch": 0.01768159026124914,
      "grad_norm": 1.0814661979675293,
      "learning_rate": 0.00019692683432817525,
      "loss": 0.129,
      "step": 3759
    },
    {
      "epoch": 0.017686294062861603,
      "grad_norm": 1.009271502494812,
      "learning_rate": 0.00019692589135006177,
      "loss": 0.0968,
      "step": 3760
    },
    {
      "epoch": 0.01769099786447407,
      "grad_norm": 0.9179444313049316,
      "learning_rate": 0.00019692494837194832,
      "loss": 0.1129,
      "step": 3761
    },
    {
      "epoch": 0.01769570166608653,
      "grad_norm": 2.043524980545044,
      "learning_rate": 0.00019692400539383483,
      "loss": 0.2513,
      "step": 3762
    },
    {
      "epoch": 0.017700405467698993,
      "grad_norm": 4.034034729003906,
      "learning_rate": 0.00019692306241572133,
      "loss": 0.5447,
      "step": 3763
    },
    {
      "epoch": 0.01770510926931146,
      "grad_norm": 0.22075943648815155,
      "learning_rate": 0.00019692211943760784,
      "loss": 0.0308,
      "step": 3764
    },
    {
      "epoch": 0.01770981307092392,
      "grad_norm": 3.5128540992736816,
      "learning_rate": 0.0001969211764594944,
      "loss": 0.3099,
      "step": 3765
    },
    {
      "epoch": 0.017714516872536383,
      "grad_norm": 2.9459495544433594,
      "learning_rate": 0.0001969202334813809,
      "loss": 0.2969,
      "step": 3766
    },
    {
      "epoch": 0.01771922067414885,
      "grad_norm": 3.4009509086608887,
      "learning_rate": 0.00019691929050326743,
      "loss": 0.4238,
      "step": 3767
    },
    {
      "epoch": 0.01772392447576131,
      "grad_norm": 4.142199516296387,
      "learning_rate": 0.00019691834752515395,
      "loss": 0.4227,
      "step": 3768
    },
    {
      "epoch": 0.017728628277373773,
      "grad_norm": 3.3972065448760986,
      "learning_rate": 0.00019691740454704046,
      "loss": 0.4189,
      "step": 3769
    },
    {
      "epoch": 0.017733332078986235,
      "grad_norm": 2.9806485176086426,
      "learning_rate": 0.000196916461568927,
      "loss": 0.5026,
      "step": 3770
    },
    {
      "epoch": 0.0177380358805987,
      "grad_norm": 0.3319995105266571,
      "learning_rate": 0.00019691551859081353,
      "loss": 0.0359,
      "step": 3771
    },
    {
      "epoch": 0.017742739682211163,
      "grad_norm": 1.372253179550171,
      "learning_rate": 0.00019691457561270005,
      "loss": 0.1257,
      "step": 3772
    },
    {
      "epoch": 0.017747443483823625,
      "grad_norm": 4.526020526885986,
      "learning_rate": 0.00019691363263458657,
      "loss": 0.8969,
      "step": 3773
    },
    {
      "epoch": 0.01775214728543609,
      "grad_norm": 2.557589292526245,
      "learning_rate": 0.00019691268965647308,
      "loss": 0.5449,
      "step": 3774
    },
    {
      "epoch": 0.017756851087048553,
      "grad_norm": 4.937502861022949,
      "learning_rate": 0.0001969117466783596,
      "loss": 0.1705,
      "step": 3775
    },
    {
      "epoch": 0.017761554888661015,
      "grad_norm": 1.5948787927627563,
      "learning_rate": 0.00019691080370024612,
      "loss": 0.1584,
      "step": 3776
    },
    {
      "epoch": 0.017766258690273477,
      "grad_norm": 2.0311312675476074,
      "learning_rate": 0.00019690986072213264,
      "loss": 0.2983,
      "step": 3777
    },
    {
      "epoch": 0.017770962491885943,
      "grad_norm": 0.30304569005966187,
      "learning_rate": 0.00019690891774401916,
      "loss": 0.0296,
      "step": 3778
    },
    {
      "epoch": 0.017775666293498405,
      "grad_norm": 1.2073676586151123,
      "learning_rate": 0.0001969079747659057,
      "loss": 0.1463,
      "step": 3779
    },
    {
      "epoch": 0.017780370095110867,
      "grad_norm": 6.524633407592773,
      "learning_rate": 0.00019690703178779222,
      "loss": 0.5386,
      "step": 3780
    },
    {
      "epoch": 0.017785073896723333,
      "grad_norm": 2.9704017639160156,
      "learning_rate": 0.00019690608880967874,
      "loss": 0.6782,
      "step": 3781
    },
    {
      "epoch": 0.017789777698335795,
      "grad_norm": 2.632503032684326,
      "learning_rate": 0.00019690514583156526,
      "loss": 0.3711,
      "step": 3782
    },
    {
      "epoch": 0.017794481499948257,
      "grad_norm": 0.9511899948120117,
      "learning_rate": 0.00019690420285345178,
      "loss": 0.1415,
      "step": 3783
    },
    {
      "epoch": 0.017799185301560723,
      "grad_norm": 2.3510282039642334,
      "learning_rate": 0.0001969032598753383,
      "loss": 0.6693,
      "step": 3784
    },
    {
      "epoch": 0.017803889103173185,
      "grad_norm": 1.9260995388031006,
      "learning_rate": 0.00019690231689722482,
      "loss": 0.2233,
      "step": 3785
    },
    {
      "epoch": 0.017808592904785647,
      "grad_norm": 1.1392613649368286,
      "learning_rate": 0.00019690137391911134,
      "loss": 0.174,
      "step": 3786
    },
    {
      "epoch": 0.01781329670639811,
      "grad_norm": 1.5651373863220215,
      "learning_rate": 0.00019690043094099785,
      "loss": 0.1545,
      "step": 3787
    },
    {
      "epoch": 0.017818000508010575,
      "grad_norm": 2.240438222885132,
      "learning_rate": 0.0001968994879628844,
      "loss": 0.1851,
      "step": 3788
    },
    {
      "epoch": 0.017822704309623037,
      "grad_norm": 1.3453762531280518,
      "learning_rate": 0.00019689854498477092,
      "loss": 0.1255,
      "step": 3789
    },
    {
      "epoch": 0.0178274081112355,
      "grad_norm": 0.766476035118103,
      "learning_rate": 0.00019689760200665744,
      "loss": 0.0966,
      "step": 3790
    },
    {
      "epoch": 0.017832111912847965,
      "grad_norm": 0.6376662850379944,
      "learning_rate": 0.00019689665902854396,
      "loss": 0.0631,
      "step": 3791
    },
    {
      "epoch": 0.017836815714460427,
      "grad_norm": 0.8812822699546814,
      "learning_rate": 0.0001968957160504305,
      "loss": 0.1327,
      "step": 3792
    },
    {
      "epoch": 0.01784151951607289,
      "grad_norm": 2.3944661617279053,
      "learning_rate": 0.00019689477307231702,
      "loss": 0.1472,
      "step": 3793
    },
    {
      "epoch": 0.01784622331768535,
      "grad_norm": 0.3982601463794708,
      "learning_rate": 0.0001968938300942035,
      "loss": 0.0488,
      "step": 3794
    },
    {
      "epoch": 0.017850927119297817,
      "grad_norm": 2.984480381011963,
      "learning_rate": 0.00019689288711609003,
      "loss": 0.507,
      "step": 3795
    },
    {
      "epoch": 0.01785563092091028,
      "grad_norm": 0.5872991681098938,
      "learning_rate": 0.00019689194413797655,
      "loss": 0.0464,
      "step": 3796
    },
    {
      "epoch": 0.01786033472252274,
      "grad_norm": 2.2658865451812744,
      "learning_rate": 0.0001968910011598631,
      "loss": 0.4612,
      "step": 3797
    },
    {
      "epoch": 0.017865038524135207,
      "grad_norm": 2.9306185245513916,
      "learning_rate": 0.00019689005818174961,
      "loss": 0.3792,
      "step": 3798
    },
    {
      "epoch": 0.01786974232574767,
      "grad_norm": 2.3010830879211426,
      "learning_rate": 0.00019688911520363613,
      "loss": 0.1426,
      "step": 3799
    },
    {
      "epoch": 0.01787444612736013,
      "grad_norm": 3.8884119987487793,
      "learning_rate": 0.00019688817222552265,
      "loss": 0.9424,
      "step": 3800
    },
    {
      "epoch": 0.017879149928972597,
      "grad_norm": 0.9706506133079529,
      "learning_rate": 0.00019688722924740917,
      "loss": 0.0628,
      "step": 3801
    },
    {
      "epoch": 0.01788385373058506,
      "grad_norm": 4.880687713623047,
      "learning_rate": 0.00019688628626929572,
      "loss": 0.9016,
      "step": 3802
    },
    {
      "epoch": 0.01788855753219752,
      "grad_norm": 1.8489198684692383,
      "learning_rate": 0.00019688534329118223,
      "loss": 0.1021,
      "step": 3803
    },
    {
      "epoch": 0.017893261333809984,
      "grad_norm": 0.720050036907196,
      "learning_rate": 0.00019688440031306875,
      "loss": 0.0144,
      "step": 3804
    },
    {
      "epoch": 0.01789796513542245,
      "grad_norm": 3.1325275897979736,
      "learning_rate": 0.00019688345733495524,
      "loss": 0.3189,
      "step": 3805
    },
    {
      "epoch": 0.01790266893703491,
      "grad_norm": 1.3751760721206665,
      "learning_rate": 0.0001968825143568418,
      "loss": 0.119,
      "step": 3806
    },
    {
      "epoch": 0.017907372738647374,
      "grad_norm": 2.056185483932495,
      "learning_rate": 0.0001968815713787283,
      "loss": 0.2978,
      "step": 3807
    },
    {
      "epoch": 0.01791207654025984,
      "grad_norm": 9.075959205627441,
      "learning_rate": 0.00019688062840061483,
      "loss": 1.2109,
      "step": 3808
    },
    {
      "epoch": 0.0179167803418723,
      "grad_norm": 1.3406884670257568,
      "learning_rate": 0.00019687968542250135,
      "loss": 0.0791,
      "step": 3809
    },
    {
      "epoch": 0.017921484143484764,
      "grad_norm": 0.7827073931694031,
      "learning_rate": 0.00019687874244438786,
      "loss": 0.0856,
      "step": 3810
    },
    {
      "epoch": 0.017926187945097226,
      "grad_norm": 3.6456918716430664,
      "learning_rate": 0.0001968777994662744,
      "loss": 0.8806,
      "step": 3811
    },
    {
      "epoch": 0.01793089174670969,
      "grad_norm": 1.1705702543258667,
      "learning_rate": 0.00019687685648816093,
      "loss": 0.0484,
      "step": 3812
    },
    {
      "epoch": 0.017935595548322154,
      "grad_norm": 0.6902334094047546,
      "learning_rate": 0.00019687591351004745,
      "loss": 0.0493,
      "step": 3813
    },
    {
      "epoch": 0.017940299349934616,
      "grad_norm": 1.4061301946640015,
      "learning_rate": 0.00019687497053193397,
      "loss": 0.0875,
      "step": 3814
    },
    {
      "epoch": 0.01794500315154708,
      "grad_norm": 2.9068474769592285,
      "learning_rate": 0.00019687402755382048,
      "loss": 0.4274,
      "step": 3815
    },
    {
      "epoch": 0.017949706953159544,
      "grad_norm": 1.216145396232605,
      "learning_rate": 0.000196873084575707,
      "loss": 0.095,
      "step": 3816
    },
    {
      "epoch": 0.017954410754772006,
      "grad_norm": 1.3249449729919434,
      "learning_rate": 0.00019687214159759352,
      "loss": 0.111,
      "step": 3817
    },
    {
      "epoch": 0.01795911455638447,
      "grad_norm": 5.006459712982178,
      "learning_rate": 0.00019687119861948004,
      "loss": 0.2494,
      "step": 3818
    },
    {
      "epoch": 0.017963818357996934,
      "grad_norm": 0.47994157671928406,
      "learning_rate": 0.00019687025564136656,
      "loss": 0.0278,
      "step": 3819
    },
    {
      "epoch": 0.017968522159609396,
      "grad_norm": 0.24966591596603394,
      "learning_rate": 0.0001968693126632531,
      "loss": 0.0283,
      "step": 3820
    },
    {
      "epoch": 0.017973225961221858,
      "grad_norm": 2.954166889190674,
      "learning_rate": 0.00019686836968513962,
      "loss": 0.2202,
      "step": 3821
    },
    {
      "epoch": 0.017977929762834324,
      "grad_norm": 0.09702736884355545,
      "learning_rate": 0.00019686742670702614,
      "loss": 0.0057,
      "step": 3822
    },
    {
      "epoch": 0.017982633564446786,
      "grad_norm": 2.826557159423828,
      "learning_rate": 0.00019686648372891266,
      "loss": 0.2733,
      "step": 3823
    },
    {
      "epoch": 0.017987337366059248,
      "grad_norm": 5.038244247436523,
      "learning_rate": 0.0001968655407507992,
      "loss": 1.1586,
      "step": 3824
    },
    {
      "epoch": 0.017992041167671714,
      "grad_norm": 0.6315663456916809,
      "learning_rate": 0.0001968645977726857,
      "loss": 0.0416,
      "step": 3825
    },
    {
      "epoch": 0.017996744969284176,
      "grad_norm": 5.098273754119873,
      "learning_rate": 0.00019686365479457222,
      "loss": 1.0848,
      "step": 3826
    },
    {
      "epoch": 0.018001448770896638,
      "grad_norm": 3.708735704421997,
      "learning_rate": 0.00019686271181645874,
      "loss": 0.6004,
      "step": 3827
    },
    {
      "epoch": 0.018006152572509104,
      "grad_norm": 0.14713898301124573,
      "learning_rate": 0.00019686176883834525,
      "loss": 0.0093,
      "step": 3828
    },
    {
      "epoch": 0.018010856374121566,
      "grad_norm": 4.445476531982422,
      "learning_rate": 0.0001968608258602318,
      "loss": 1.1801,
      "step": 3829
    },
    {
      "epoch": 0.018015560175734028,
      "grad_norm": 0.31754615902900696,
      "learning_rate": 0.00019685988288211832,
      "loss": 0.0161,
      "step": 3830
    },
    {
      "epoch": 0.01802026397734649,
      "grad_norm": 1.5740714073181152,
      "learning_rate": 0.00019685893990400484,
      "loss": 0.0876,
      "step": 3831
    },
    {
      "epoch": 0.018024967778958956,
      "grad_norm": 2.8471317291259766,
      "learning_rate": 0.00019685799692589136,
      "loss": 0.3273,
      "step": 3832
    },
    {
      "epoch": 0.018029671580571418,
      "grad_norm": 3.898089647293091,
      "learning_rate": 0.0001968570539477779,
      "loss": 1.2589,
      "step": 3833
    },
    {
      "epoch": 0.01803437538218388,
      "grad_norm": 1.9204319715499878,
      "learning_rate": 0.00019685611096966442,
      "loss": 0.4869,
      "step": 3834
    },
    {
      "epoch": 0.018039079183796346,
      "grad_norm": 3.5149099826812744,
      "learning_rate": 0.00019685516799155094,
      "loss": 0.6616,
      "step": 3835
    },
    {
      "epoch": 0.018043782985408808,
      "grad_norm": 2.1344192028045654,
      "learning_rate": 0.00019685422501343743,
      "loss": 0.4422,
      "step": 3836
    },
    {
      "epoch": 0.01804848678702127,
      "grad_norm": 0.20279833674430847,
      "learning_rate": 0.00019685328203532395,
      "loss": 0.0172,
      "step": 3837
    },
    {
      "epoch": 0.018053190588633732,
      "grad_norm": 1.3848861455917358,
      "learning_rate": 0.0001968523390572105,
      "loss": 0.362,
      "step": 3838
    },
    {
      "epoch": 0.018057894390246198,
      "grad_norm": 4.7902398109436035,
      "learning_rate": 0.000196851396079097,
      "loss": 0.3901,
      "step": 3839
    },
    {
      "epoch": 0.01806259819185866,
      "grad_norm": 2.0858688354492188,
      "learning_rate": 0.00019685045310098353,
      "loss": 0.5398,
      "step": 3840
    },
    {
      "epoch": 0.018067301993471122,
      "grad_norm": 1.7394182682037354,
      "learning_rate": 0.00019684951012287005,
      "loss": 0.1938,
      "step": 3841
    },
    {
      "epoch": 0.018072005795083588,
      "grad_norm": 2.011596918106079,
      "learning_rate": 0.0001968485671447566,
      "loss": 0.2759,
      "step": 3842
    },
    {
      "epoch": 0.01807670959669605,
      "grad_norm": 1.4436417818069458,
      "learning_rate": 0.00019684762416664312,
      "loss": 0.3279,
      "step": 3843
    },
    {
      "epoch": 0.018081413398308512,
      "grad_norm": 2.0480666160583496,
      "learning_rate": 0.00019684668118852963,
      "loss": 0.4252,
      "step": 3844
    },
    {
      "epoch": 0.018086117199920978,
      "grad_norm": 2.612802505493164,
      "learning_rate": 0.00019684573821041615,
      "loss": 0.4325,
      "step": 3845
    },
    {
      "epoch": 0.01809082100153344,
      "grad_norm": 0.626986563205719,
      "learning_rate": 0.00019684479523230267,
      "loss": 0.2637,
      "step": 3846
    },
    {
      "epoch": 0.018095524803145902,
      "grad_norm": 1.1943080425262451,
      "learning_rate": 0.0001968438522541892,
      "loss": 0.2673,
      "step": 3847
    },
    {
      "epoch": 0.018100228604758364,
      "grad_norm": 1.9482674598693848,
      "learning_rate": 0.0001968429092760757,
      "loss": 0.3463,
      "step": 3848
    },
    {
      "epoch": 0.01810493240637083,
      "grad_norm": 1.4200955629348755,
      "learning_rate": 0.00019684196629796223,
      "loss": 0.2929,
      "step": 3849
    },
    {
      "epoch": 0.018109636207983292,
      "grad_norm": 1.2605342864990234,
      "learning_rate": 0.00019684102331984875,
      "loss": 0.3293,
      "step": 3850
    },
    {
      "epoch": 0.018114340009595754,
      "grad_norm": 7.8216094970703125,
      "learning_rate": 0.00019684008034173526,
      "loss": 0.8268,
      "step": 3851
    },
    {
      "epoch": 0.01811904381120822,
      "grad_norm": 1.4906225204467773,
      "learning_rate": 0.0001968391373636218,
      "loss": 0.2258,
      "step": 3852
    },
    {
      "epoch": 0.018123747612820682,
      "grad_norm": 1.8072744607925415,
      "learning_rate": 0.00019683819438550833,
      "loss": 0.2724,
      "step": 3853
    },
    {
      "epoch": 0.018128451414433144,
      "grad_norm": 2.477407932281494,
      "learning_rate": 0.00019683725140739485,
      "loss": 0.5294,
      "step": 3854
    },
    {
      "epoch": 0.018133155216045607,
      "grad_norm": 0.9200982451438904,
      "learning_rate": 0.00019683630842928137,
      "loss": 0.1542,
      "step": 3855
    },
    {
      "epoch": 0.018137859017658072,
      "grad_norm": 0.8311064839363098,
      "learning_rate": 0.00019683536545116788,
      "loss": 0.1167,
      "step": 3856
    },
    {
      "epoch": 0.018142562819270534,
      "grad_norm": 0.9509901404380798,
      "learning_rate": 0.0001968344224730544,
      "loss": 0.1857,
      "step": 3857
    },
    {
      "epoch": 0.018147266620882996,
      "grad_norm": 1.9078243970870972,
      "learning_rate": 0.00019683347949494092,
      "loss": 0.5974,
      "step": 3858
    },
    {
      "epoch": 0.018151970422495462,
      "grad_norm": 0.9588663578033447,
      "learning_rate": 0.00019683253651682744,
      "loss": 0.1189,
      "step": 3859
    },
    {
      "epoch": 0.018156674224107924,
      "grad_norm": 2.7845451831817627,
      "learning_rate": 0.00019683159353871396,
      "loss": 0.3464,
      "step": 3860
    },
    {
      "epoch": 0.018161378025720386,
      "grad_norm": 2.5546865463256836,
      "learning_rate": 0.0001968306505606005,
      "loss": 0.5261,
      "step": 3861
    },
    {
      "epoch": 0.018166081827332852,
      "grad_norm": 1.7331122159957886,
      "learning_rate": 0.00019682970758248702,
      "loss": 0.2908,
      "step": 3862
    },
    {
      "epoch": 0.018170785628945314,
      "grad_norm": 2.591609477996826,
      "learning_rate": 0.00019682876460437354,
      "loss": 0.5229,
      "step": 3863
    },
    {
      "epoch": 0.018175489430557776,
      "grad_norm": 1.5186741352081299,
      "learning_rate": 0.00019682782162626006,
      "loss": 0.3623,
      "step": 3864
    },
    {
      "epoch": 0.01818019323217024,
      "grad_norm": 0.97237628698349,
      "learning_rate": 0.0001968268786481466,
      "loss": 0.1612,
      "step": 3865
    },
    {
      "epoch": 0.018184897033782704,
      "grad_norm": 2.9871065616607666,
      "learning_rate": 0.00019682593567003313,
      "loss": 0.4533,
      "step": 3866
    },
    {
      "epoch": 0.018189600835395166,
      "grad_norm": 0.8048050999641418,
      "learning_rate": 0.00019682499269191962,
      "loss": 0.1545,
      "step": 3867
    },
    {
      "epoch": 0.01819430463700763,
      "grad_norm": 0.8794823288917542,
      "learning_rate": 0.00019682404971380614,
      "loss": 0.1389,
      "step": 3868
    },
    {
      "epoch": 0.018199008438620094,
      "grad_norm": 1.2408589124679565,
      "learning_rate": 0.00019682310673569265,
      "loss": 0.2393,
      "step": 3869
    },
    {
      "epoch": 0.018203712240232556,
      "grad_norm": 3.29496693611145,
      "learning_rate": 0.0001968221637575792,
      "loss": 0.741,
      "step": 3870
    },
    {
      "epoch": 0.01820841604184502,
      "grad_norm": 1.6436715126037598,
      "learning_rate": 0.00019682122077946572,
      "loss": 0.194,
      "step": 3871
    },
    {
      "epoch": 0.01821311984345748,
      "grad_norm": 1.7256319522857666,
      "learning_rate": 0.00019682027780135224,
      "loss": 0.375,
      "step": 3872
    },
    {
      "epoch": 0.018217823645069946,
      "grad_norm": 1.5254851579666138,
      "learning_rate": 0.00019681933482323876,
      "loss": 0.2842,
      "step": 3873
    },
    {
      "epoch": 0.01822252744668241,
      "grad_norm": 2.056330919265747,
      "learning_rate": 0.0001968183918451253,
      "loss": 0.2719,
      "step": 3874
    },
    {
      "epoch": 0.01822723124829487,
      "grad_norm": 1.6683008670806885,
      "learning_rate": 0.00019681744886701182,
      "loss": 0.1357,
      "step": 3875
    },
    {
      "epoch": 0.018231935049907336,
      "grad_norm": 2.242715835571289,
      "learning_rate": 0.00019681650588889834,
      "loss": 0.1849,
      "step": 3876
    },
    {
      "epoch": 0.0182366388515198,
      "grad_norm": 2.355858564376831,
      "learning_rate": 0.00019681556291078486,
      "loss": 0.4634,
      "step": 3877
    },
    {
      "epoch": 0.01824134265313226,
      "grad_norm": 2.524322032928467,
      "learning_rate": 0.00019681461993267135,
      "loss": 0.2518,
      "step": 3878
    },
    {
      "epoch": 0.018246046454744726,
      "grad_norm": 1.8276206254959106,
      "learning_rate": 0.0001968136769545579,
      "loss": 0.2952,
      "step": 3879
    },
    {
      "epoch": 0.01825075025635719,
      "grad_norm": 0.9884007573127747,
      "learning_rate": 0.0001968127339764444,
      "loss": 0.1033,
      "step": 3880
    },
    {
      "epoch": 0.01825545405796965,
      "grad_norm": 2.623478889465332,
      "learning_rate": 0.00019681179099833093,
      "loss": 0.4323,
      "step": 3881
    },
    {
      "epoch": 0.018260157859582113,
      "grad_norm": 1.12754487991333,
      "learning_rate": 0.00019681084802021745,
      "loss": 0.2615,
      "step": 3882
    },
    {
      "epoch": 0.01826486166119458,
      "grad_norm": 1.4517346620559692,
      "learning_rate": 0.000196809905042104,
      "loss": 0.259,
      "step": 3883
    },
    {
      "epoch": 0.01826956546280704,
      "grad_norm": 1.0998281240463257,
      "learning_rate": 0.00019680896206399052,
      "loss": 0.13,
      "step": 3884
    },
    {
      "epoch": 0.018274269264419503,
      "grad_norm": 0.7160497307777405,
      "learning_rate": 0.00019680801908587703,
      "loss": 0.0556,
      "step": 3885
    },
    {
      "epoch": 0.01827897306603197,
      "grad_norm": 2.883577346801758,
      "learning_rate": 0.00019680707610776355,
      "loss": 0.3279,
      "step": 3886
    },
    {
      "epoch": 0.01828367686764443,
      "grad_norm": 1.9789589643478394,
      "learning_rate": 0.00019680613312965007,
      "loss": 0.345,
      "step": 3887
    },
    {
      "epoch": 0.018288380669256893,
      "grad_norm": 1.7601197957992554,
      "learning_rate": 0.0001968051901515366,
      "loss": 0.128,
      "step": 3888
    },
    {
      "epoch": 0.018293084470869355,
      "grad_norm": 3.303061008453369,
      "learning_rate": 0.0001968042471734231,
      "loss": 0.3116,
      "step": 3889
    },
    {
      "epoch": 0.01829778827248182,
      "grad_norm": 2.2895803451538086,
      "learning_rate": 0.00019680330419530963,
      "loss": 0.2903,
      "step": 3890
    },
    {
      "epoch": 0.018302492074094283,
      "grad_norm": 2.5701401233673096,
      "learning_rate": 0.00019680236121719615,
      "loss": 0.2767,
      "step": 3891
    },
    {
      "epoch": 0.018307195875706745,
      "grad_norm": 2.384941339492798,
      "learning_rate": 0.0001968014182390827,
      "loss": 0.3127,
      "step": 3892
    },
    {
      "epoch": 0.01831189967731921,
      "grad_norm": 2.6657207012176514,
      "learning_rate": 0.0001968004752609692,
      "loss": 0.2564,
      "step": 3893
    },
    {
      "epoch": 0.018316603478931673,
      "grad_norm": 2.7423863410949707,
      "learning_rate": 0.00019679953228285573,
      "loss": 0.4512,
      "step": 3894
    },
    {
      "epoch": 0.018321307280544135,
      "grad_norm": 2.86318039894104,
      "learning_rate": 0.00019679858930474225,
      "loss": 0.2443,
      "step": 3895
    },
    {
      "epoch": 0.0183260110821566,
      "grad_norm": 1.38325834274292,
      "learning_rate": 0.00019679764632662877,
      "loss": 0.1997,
      "step": 3896
    },
    {
      "epoch": 0.018330714883769063,
      "grad_norm": 1.4035080671310425,
      "learning_rate": 0.0001967967033485153,
      "loss": 0.1772,
      "step": 3897
    },
    {
      "epoch": 0.018335418685381525,
      "grad_norm": 1.195183277130127,
      "learning_rate": 0.0001967957603704018,
      "loss": 0.2414,
      "step": 3898
    },
    {
      "epoch": 0.018340122486993987,
      "grad_norm": 2.2079718112945557,
      "learning_rate": 0.00019679481739228832,
      "loss": 0.1916,
      "step": 3899
    },
    {
      "epoch": 0.018344826288606453,
      "grad_norm": 1.7630863189697266,
      "learning_rate": 0.00019679387441417484,
      "loss": 0.3683,
      "step": 3900
    },
    {
      "epoch": 0.018349530090218915,
      "grad_norm": 3.7917394638061523,
      "learning_rate": 0.00019679293143606136,
      "loss": 0.4203,
      "step": 3901
    },
    {
      "epoch": 0.018354233891831377,
      "grad_norm": 4.674785614013672,
      "learning_rate": 0.0001967919884579479,
      "loss": 0.4269,
      "step": 3902
    },
    {
      "epoch": 0.018358937693443843,
      "grad_norm": 1.4592022895812988,
      "learning_rate": 0.00019679104547983442,
      "loss": 0.1879,
      "step": 3903
    },
    {
      "epoch": 0.018363641495056305,
      "grad_norm": 3.2947261333465576,
      "learning_rate": 0.00019679010250172094,
      "loss": 0.3708,
      "step": 3904
    },
    {
      "epoch": 0.018368345296668767,
      "grad_norm": 4.1413469314575195,
      "learning_rate": 0.00019678915952360746,
      "loss": 0.8617,
      "step": 3905
    },
    {
      "epoch": 0.01837304909828123,
      "grad_norm": 1.8022125959396362,
      "learning_rate": 0.000196788216545494,
      "loss": 0.1937,
      "step": 3906
    },
    {
      "epoch": 0.018377752899893695,
      "grad_norm": 3.245088577270508,
      "learning_rate": 0.00019678727356738053,
      "loss": 0.9534,
      "step": 3907
    },
    {
      "epoch": 0.018382456701506157,
      "grad_norm": 2.2778255939483643,
      "learning_rate": 0.00019678633058926704,
      "loss": 0.3623,
      "step": 3908
    },
    {
      "epoch": 0.01838716050311862,
      "grad_norm": 1.1318984031677246,
      "learning_rate": 0.00019678538761115354,
      "loss": 0.2539,
      "step": 3909
    },
    {
      "epoch": 0.018391864304731085,
      "grad_norm": 1.5361732244491577,
      "learning_rate": 0.00019678444463304005,
      "loss": 0.1879,
      "step": 3910
    },
    {
      "epoch": 0.018396568106343547,
      "grad_norm": 1.5670769214630127,
      "learning_rate": 0.0001967835016549266,
      "loss": 0.1473,
      "step": 3911
    },
    {
      "epoch": 0.01840127190795601,
      "grad_norm": 1.8637094497680664,
      "learning_rate": 0.00019678255867681312,
      "loss": 0.4121,
      "step": 3912
    },
    {
      "epoch": 0.018405975709568475,
      "grad_norm": 3.4560418128967285,
      "learning_rate": 0.00019678161569869964,
      "loss": 0.5881,
      "step": 3913
    },
    {
      "epoch": 0.018410679511180937,
      "grad_norm": 1.5040740966796875,
      "learning_rate": 0.00019678067272058616,
      "loss": 0.1535,
      "step": 3914
    },
    {
      "epoch": 0.0184153833127934,
      "grad_norm": 1.1553187370300293,
      "learning_rate": 0.0001967797297424727,
      "loss": 0.1751,
      "step": 3915
    },
    {
      "epoch": 0.01842008711440586,
      "grad_norm": 1.3556649684906006,
      "learning_rate": 0.00019677878676435922,
      "loss": 0.2303,
      "step": 3916
    },
    {
      "epoch": 0.018424790916018327,
      "grad_norm": 1.5810364484786987,
      "learning_rate": 0.00019677784378624574,
      "loss": 0.3855,
      "step": 3917
    },
    {
      "epoch": 0.01842949471763079,
      "grad_norm": 1.0234540700912476,
      "learning_rate": 0.00019677690080813226,
      "loss": 0.1455,
      "step": 3918
    },
    {
      "epoch": 0.01843419851924325,
      "grad_norm": 2.833613395690918,
      "learning_rate": 0.00019677595783001878,
      "loss": 0.3668,
      "step": 3919
    },
    {
      "epoch": 0.018438902320855717,
      "grad_norm": 1.5294088125228882,
      "learning_rate": 0.0001967750148519053,
      "loss": 0.3291,
      "step": 3920
    },
    {
      "epoch": 0.01844360612246818,
      "grad_norm": 0.687301754951477,
      "learning_rate": 0.0001967740718737918,
      "loss": 0.0758,
      "step": 3921
    },
    {
      "epoch": 0.01844830992408064,
      "grad_norm": 3.166391611099243,
      "learning_rate": 0.00019677312889567833,
      "loss": 0.4904,
      "step": 3922
    },
    {
      "epoch": 0.018453013725693104,
      "grad_norm": 1.7732841968536377,
      "learning_rate": 0.00019677218591756485,
      "loss": 0.2054,
      "step": 3923
    },
    {
      "epoch": 0.01845771752730557,
      "grad_norm": 3.387895345687866,
      "learning_rate": 0.0001967712429394514,
      "loss": 0.5258,
      "step": 3924
    },
    {
      "epoch": 0.01846242132891803,
      "grad_norm": 0.7881569266319275,
      "learning_rate": 0.00019677029996133792,
      "loss": 0.1515,
      "step": 3925
    },
    {
      "epoch": 0.018467125130530494,
      "grad_norm": 0.8862825036048889,
      "learning_rate": 0.00019676935698322443,
      "loss": 0.1176,
      "step": 3926
    },
    {
      "epoch": 0.01847182893214296,
      "grad_norm": 1.775105357170105,
      "learning_rate": 0.00019676841400511095,
      "loss": 0.4169,
      "step": 3927
    },
    {
      "epoch": 0.01847653273375542,
      "grad_norm": 2.141103506088257,
      "learning_rate": 0.00019676747102699747,
      "loss": 0.4161,
      "step": 3928
    },
    {
      "epoch": 0.018481236535367884,
      "grad_norm": 1.5808132886886597,
      "learning_rate": 0.000196766528048884,
      "loss": 0.2073,
      "step": 3929
    },
    {
      "epoch": 0.01848594033698035,
      "grad_norm": 1.0672800540924072,
      "learning_rate": 0.0001967655850707705,
      "loss": 0.0913,
      "step": 3930
    },
    {
      "epoch": 0.01849064413859281,
      "grad_norm": 1.3927559852600098,
      "learning_rate": 0.00019676464209265703,
      "loss": 0.1705,
      "step": 3931
    },
    {
      "epoch": 0.018495347940205274,
      "grad_norm": 1.5160263776779175,
      "learning_rate": 0.00019676369911454355,
      "loss": 0.3335,
      "step": 3932
    },
    {
      "epoch": 0.018500051741817736,
      "grad_norm": 2.4879801273345947,
      "learning_rate": 0.0001967627561364301,
      "loss": 0.4479,
      "step": 3933
    },
    {
      "epoch": 0.0185047555434302,
      "grad_norm": 0.47219228744506836,
      "learning_rate": 0.0001967618131583166,
      "loss": 0.0464,
      "step": 3934
    },
    {
      "epoch": 0.018509459345042664,
      "grad_norm": 0.4353538155555725,
      "learning_rate": 0.00019676087018020313,
      "loss": 0.0396,
      "step": 3935
    },
    {
      "epoch": 0.018514163146655126,
      "grad_norm": 3.4194388389587402,
      "learning_rate": 0.00019675992720208965,
      "loss": 0.8332,
      "step": 3936
    },
    {
      "epoch": 0.01851886694826759,
      "grad_norm": 3.0601720809936523,
      "learning_rate": 0.00019675898422397617,
      "loss": 0.537,
      "step": 3937
    },
    {
      "epoch": 0.018523570749880054,
      "grad_norm": 0.9064096212387085,
      "learning_rate": 0.0001967580412458627,
      "loss": 0.2262,
      "step": 3938
    },
    {
      "epoch": 0.018528274551492516,
      "grad_norm": 0.9200379252433777,
      "learning_rate": 0.00019675709826774923,
      "loss": 0.1717,
      "step": 3939
    },
    {
      "epoch": 0.018532978353104978,
      "grad_norm": 2.148188591003418,
      "learning_rate": 0.00019675615528963572,
      "loss": 0.3267,
      "step": 3940
    },
    {
      "epoch": 0.018537682154717443,
      "grad_norm": 1.1727993488311768,
      "learning_rate": 0.00019675521231152224,
      "loss": 0.1399,
      "step": 3941
    },
    {
      "epoch": 0.018542385956329906,
      "grad_norm": 0.48570382595062256,
      "learning_rate": 0.00019675426933340879,
      "loss": 0.0496,
      "step": 3942
    },
    {
      "epoch": 0.018547089757942368,
      "grad_norm": 0.48323747515678406,
      "learning_rate": 0.0001967533263552953,
      "loss": 0.0593,
      "step": 3943
    },
    {
      "epoch": 0.018551793559554833,
      "grad_norm": 0.546204149723053,
      "learning_rate": 0.00019675238337718182,
      "loss": 0.0845,
      "step": 3944
    },
    {
      "epoch": 0.018556497361167296,
      "grad_norm": 1.8956167697906494,
      "learning_rate": 0.00019675144039906834,
      "loss": 0.2641,
      "step": 3945
    },
    {
      "epoch": 0.018561201162779758,
      "grad_norm": 1.254059910774231,
      "learning_rate": 0.00019675049742095486,
      "loss": 0.3235,
      "step": 3946
    },
    {
      "epoch": 0.018565904964392223,
      "grad_norm": 0.874704897403717,
      "learning_rate": 0.0001967495544428414,
      "loss": 0.1618,
      "step": 3947
    },
    {
      "epoch": 0.018570608766004686,
      "grad_norm": 2.5152993202209473,
      "learning_rate": 0.00019674861146472793,
      "loss": 0.5051,
      "step": 3948
    },
    {
      "epoch": 0.018575312567617148,
      "grad_norm": 3.0349361896514893,
      "learning_rate": 0.00019674766848661444,
      "loss": 0.4191,
      "step": 3949
    },
    {
      "epoch": 0.01858001636922961,
      "grad_norm": 0.30990859866142273,
      "learning_rate": 0.00019674672550850096,
      "loss": 0.0324,
      "step": 3950
    },
    {
      "epoch": 0.018584720170842076,
      "grad_norm": 11.126672744750977,
      "learning_rate": 0.00019674578253038748,
      "loss": 0.4687,
      "step": 3951
    },
    {
      "epoch": 0.018589423972454538,
      "grad_norm": 4.392490386962891,
      "learning_rate": 0.000196744839552274,
      "loss": 0.1333,
      "step": 3952
    },
    {
      "epoch": 0.018594127774067,
      "grad_norm": 2.3222360610961914,
      "learning_rate": 0.00019674389657416052,
      "loss": 0.2673,
      "step": 3953
    },
    {
      "epoch": 0.018598831575679466,
      "grad_norm": 1.972172737121582,
      "learning_rate": 0.00019674295359604704,
      "loss": 0.109,
      "step": 3954
    },
    {
      "epoch": 0.018603535377291928,
      "grad_norm": 1.9768321514129639,
      "learning_rate": 0.00019674201061793356,
      "loss": 0.1973,
      "step": 3955
    },
    {
      "epoch": 0.01860823917890439,
      "grad_norm": 2.3389241695404053,
      "learning_rate": 0.0001967410676398201,
      "loss": 0.4563,
      "step": 3956
    },
    {
      "epoch": 0.018612942980516852,
      "grad_norm": 1.5808955430984497,
      "learning_rate": 0.00019674012466170662,
      "loss": 0.2972,
      "step": 3957
    },
    {
      "epoch": 0.018617646782129318,
      "grad_norm": 2.2689335346221924,
      "learning_rate": 0.00019673918168359314,
      "loss": 0.2683,
      "step": 3958
    },
    {
      "epoch": 0.01862235058374178,
      "grad_norm": 1.2344970703125,
      "learning_rate": 0.00019673823870547966,
      "loss": 0.2297,
      "step": 3959
    },
    {
      "epoch": 0.018627054385354242,
      "grad_norm": 1.1659709215164185,
      "learning_rate": 0.00019673729572736618,
      "loss": 0.1134,
      "step": 3960
    },
    {
      "epoch": 0.018631758186966708,
      "grad_norm": 1.8077571392059326,
      "learning_rate": 0.0001967363527492527,
      "loss": 0.1764,
      "step": 3961
    },
    {
      "epoch": 0.01863646198857917,
      "grad_norm": 0.9939180016517639,
      "learning_rate": 0.0001967354097711392,
      "loss": 0.1043,
      "step": 3962
    },
    {
      "epoch": 0.018641165790191632,
      "grad_norm": 1.3899133205413818,
      "learning_rate": 0.00019673446679302573,
      "loss": 0.1489,
      "step": 3963
    },
    {
      "epoch": 0.018645869591804098,
      "grad_norm": 4.801587104797363,
      "learning_rate": 0.00019673352381491225,
      "loss": 0.9924,
      "step": 3964
    },
    {
      "epoch": 0.01865057339341656,
      "grad_norm": 1.2834609746932983,
      "learning_rate": 0.0001967325808367988,
      "loss": 0.1108,
      "step": 3965
    },
    {
      "epoch": 0.018655277195029022,
      "grad_norm": 0.5894018411636353,
      "learning_rate": 0.00019673163785868532,
      "loss": 0.098,
      "step": 3966
    },
    {
      "epoch": 0.018659980996641484,
      "grad_norm": 1.3043996095657349,
      "learning_rate": 0.00019673069488057183,
      "loss": 0.1768,
      "step": 3967
    },
    {
      "epoch": 0.01866468479825395,
      "grad_norm": 1.9668225049972534,
      "learning_rate": 0.00019672975190245835,
      "loss": 0.1379,
      "step": 3968
    },
    {
      "epoch": 0.018669388599866412,
      "grad_norm": 2.3197813034057617,
      "learning_rate": 0.00019672880892434487,
      "loss": 0.2653,
      "step": 3969
    },
    {
      "epoch": 0.018674092401478874,
      "grad_norm": 1.987568974494934,
      "learning_rate": 0.00019672786594623142,
      "loss": 0.1911,
      "step": 3970
    },
    {
      "epoch": 0.01867879620309134,
      "grad_norm": 3.527021646499634,
      "learning_rate": 0.0001967269229681179,
      "loss": 0.5051,
      "step": 3971
    },
    {
      "epoch": 0.018683500004703802,
      "grad_norm": 3.6172282695770264,
      "learning_rate": 0.00019672597999000443,
      "loss": 0.6004,
      "step": 3972
    },
    {
      "epoch": 0.018688203806316264,
      "grad_norm": 3.1247079372406006,
      "learning_rate": 0.00019672503701189095,
      "loss": 0.3723,
      "step": 3973
    },
    {
      "epoch": 0.018692907607928726,
      "grad_norm": 3.749103546142578,
      "learning_rate": 0.0001967240940337775,
      "loss": 0.4355,
      "step": 3974
    },
    {
      "epoch": 0.018697611409541192,
      "grad_norm": 1.4545403718948364,
      "learning_rate": 0.000196723151055664,
      "loss": 0.1606,
      "step": 3975
    },
    {
      "epoch": 0.018702315211153654,
      "grad_norm": 2.5528433322906494,
      "learning_rate": 0.00019672220807755053,
      "loss": 0.1837,
      "step": 3976
    },
    {
      "epoch": 0.018707019012766116,
      "grad_norm": 1.6567600965499878,
      "learning_rate": 0.00019672126509943705,
      "loss": 0.1252,
      "step": 3977
    },
    {
      "epoch": 0.018711722814378582,
      "grad_norm": 1.839513897895813,
      "learning_rate": 0.00019672032212132357,
      "loss": 0.2954,
      "step": 3978
    },
    {
      "epoch": 0.018716426615991044,
      "grad_norm": 2.8609275817871094,
      "learning_rate": 0.0001967193791432101,
      "loss": 0.6522,
      "step": 3979
    },
    {
      "epoch": 0.018721130417603506,
      "grad_norm": 2.4280240535736084,
      "learning_rate": 0.00019671843616509663,
      "loss": 0.3073,
      "step": 3980
    },
    {
      "epoch": 0.018725834219215972,
      "grad_norm": 1.3117724657058716,
      "learning_rate": 0.00019671749318698315,
      "loss": 0.1856,
      "step": 3981
    },
    {
      "epoch": 0.018730538020828434,
      "grad_norm": 1.3106482028961182,
      "learning_rate": 0.00019671655020886967,
      "loss": 0.2693,
      "step": 3982
    },
    {
      "epoch": 0.018735241822440896,
      "grad_norm": 1.7105696201324463,
      "learning_rate": 0.00019671560723075619,
      "loss": 0.2624,
      "step": 3983
    },
    {
      "epoch": 0.01873994562405336,
      "grad_norm": 0.6385538578033447,
      "learning_rate": 0.0001967146642526427,
      "loss": 0.0867,
      "step": 3984
    },
    {
      "epoch": 0.018744649425665824,
      "grad_norm": 3.6406772136688232,
      "learning_rate": 0.00019671372127452922,
      "loss": 0.8335,
      "step": 3985
    },
    {
      "epoch": 0.018749353227278286,
      "grad_norm": 2.0040833950042725,
      "learning_rate": 0.00019671277829641574,
      "loss": 0.2803,
      "step": 3986
    },
    {
      "epoch": 0.01875405702889075,
      "grad_norm": 1.6454012393951416,
      "learning_rate": 0.00019671183531830226,
      "loss": 0.2546,
      "step": 3987
    },
    {
      "epoch": 0.018758760830503214,
      "grad_norm": 1.6862032413482666,
      "learning_rate": 0.0001967108923401888,
      "loss": 0.1924,
      "step": 3988
    },
    {
      "epoch": 0.018763464632115676,
      "grad_norm": 3.0718910694122314,
      "learning_rate": 0.00019670994936207533,
      "loss": 0.4689,
      "step": 3989
    },
    {
      "epoch": 0.01876816843372814,
      "grad_norm": 0.727399468421936,
      "learning_rate": 0.00019670900638396184,
      "loss": 0.0971,
      "step": 3990
    },
    {
      "epoch": 0.0187728722353406,
      "grad_norm": 0.6981673836708069,
      "learning_rate": 0.00019670806340584836,
      "loss": 0.0674,
      "step": 3991
    },
    {
      "epoch": 0.018777576036953066,
      "grad_norm": 1.8026281595230103,
      "learning_rate": 0.00019670712042773488,
      "loss": 0.2068,
      "step": 3992
    },
    {
      "epoch": 0.01878227983856553,
      "grad_norm": 1.9640578031539917,
      "learning_rate": 0.0001967061774496214,
      "loss": 0.2594,
      "step": 3993
    },
    {
      "epoch": 0.01878698364017799,
      "grad_norm": 4.4533467292785645,
      "learning_rate": 0.00019670523447150792,
      "loss": 1.4324,
      "step": 3994
    },
    {
      "epoch": 0.018791687441790456,
      "grad_norm": 1.7178419828414917,
      "learning_rate": 0.00019670429149339444,
      "loss": 0.2193,
      "step": 3995
    },
    {
      "epoch": 0.01879639124340292,
      "grad_norm": 1.7181456089019775,
      "learning_rate": 0.00019670334851528096,
      "loss": 0.3125,
      "step": 3996
    },
    {
      "epoch": 0.01880109504501538,
      "grad_norm": 0.6976529359817505,
      "learning_rate": 0.0001967024055371675,
      "loss": 0.0856,
      "step": 3997
    },
    {
      "epoch": 0.018805798846627846,
      "grad_norm": 1.973774790763855,
      "learning_rate": 0.00019670146255905402,
      "loss": 0.4435,
      "step": 3998
    },
    {
      "epoch": 0.01881050264824031,
      "grad_norm": 1.259909749031067,
      "learning_rate": 0.00019670051958094054,
      "loss": 0.2951,
      "step": 3999
    },
    {
      "epoch": 0.01881520644985277,
      "grad_norm": 1.9263854026794434,
      "learning_rate": 0.00019669957660282706,
      "loss": 0.347,
      "step": 4000
    },
    {
      "epoch": 0.018819910251465233,
      "grad_norm": 1.7278004884719849,
      "learning_rate": 0.0001966986336247136,
      "loss": 0.088,
      "step": 4001
    },
    {
      "epoch": 0.0188246140530777,
      "grad_norm": 2.435612678527832,
      "learning_rate": 0.0001966976906466001,
      "loss": 0.2132,
      "step": 4002
    },
    {
      "epoch": 0.01882931785469016,
      "grad_norm": 1.8555432558059692,
      "learning_rate": 0.0001966967476684866,
      "loss": 0.3535,
      "step": 4003
    },
    {
      "epoch": 0.018834021656302623,
      "grad_norm": 3.097245216369629,
      "learning_rate": 0.00019669580469037313,
      "loss": 0.2548,
      "step": 4004
    },
    {
      "epoch": 0.01883872545791509,
      "grad_norm": 0.3470856547355652,
      "learning_rate": 0.00019669486171225965,
      "loss": 0.0301,
      "step": 4005
    },
    {
      "epoch": 0.01884342925952755,
      "grad_norm": 2.8882925510406494,
      "learning_rate": 0.0001966939187341462,
      "loss": 0.3346,
      "step": 4006
    },
    {
      "epoch": 0.018848133061140013,
      "grad_norm": 0.5472223162651062,
      "learning_rate": 0.00019669297575603272,
      "loss": 0.0888,
      "step": 4007
    },
    {
      "epoch": 0.018852836862752475,
      "grad_norm": 1.9081059694290161,
      "learning_rate": 0.00019669203277791923,
      "loss": 0.2531,
      "step": 4008
    },
    {
      "epoch": 0.01885754066436494,
      "grad_norm": 0.9611683487892151,
      "learning_rate": 0.00019669108979980575,
      "loss": 0.1586,
      "step": 4009
    },
    {
      "epoch": 0.018862244465977403,
      "grad_norm": 1.2839378118515015,
      "learning_rate": 0.00019669014682169227,
      "loss": 0.2699,
      "step": 4010
    },
    {
      "epoch": 0.018866948267589865,
      "grad_norm": 4.146202087402344,
      "learning_rate": 0.00019668920384357882,
      "loss": 0.7558,
      "step": 4011
    },
    {
      "epoch": 0.01887165206920233,
      "grad_norm": 0.8171950578689575,
      "learning_rate": 0.00019668826086546534,
      "loss": 0.1442,
      "step": 4012
    },
    {
      "epoch": 0.018876355870814793,
      "grad_norm": 4.054571628570557,
      "learning_rate": 0.00019668731788735185,
      "loss": 0.4004,
      "step": 4013
    },
    {
      "epoch": 0.018881059672427255,
      "grad_norm": 2.784066915512085,
      "learning_rate": 0.00019668637490923835,
      "loss": 0.6028,
      "step": 4014
    },
    {
      "epoch": 0.01888576347403972,
      "grad_norm": 2.1353845596313477,
      "learning_rate": 0.0001966854319311249,
      "loss": 0.429,
      "step": 4015
    },
    {
      "epoch": 0.018890467275652183,
      "grad_norm": 1.1391123533248901,
      "learning_rate": 0.0001966844889530114,
      "loss": 0.1106,
      "step": 4016
    },
    {
      "epoch": 0.018895171077264645,
      "grad_norm": 1.659902572631836,
      "learning_rate": 0.00019668354597489793,
      "loss": 0.1483,
      "step": 4017
    },
    {
      "epoch": 0.018899874878877107,
      "grad_norm": 0.7203385829925537,
      "learning_rate": 0.00019668260299678445,
      "loss": 0.0992,
      "step": 4018
    },
    {
      "epoch": 0.018904578680489573,
      "grad_norm": 1.8876302242279053,
      "learning_rate": 0.00019668166001867097,
      "loss": 0.362,
      "step": 4019
    },
    {
      "epoch": 0.018909282482102035,
      "grad_norm": 2.6091911792755127,
      "learning_rate": 0.0001966807170405575,
      "loss": 0.4117,
      "step": 4020
    },
    {
      "epoch": 0.018913986283714497,
      "grad_norm": 0.8350380063056946,
      "learning_rate": 0.00019667977406244403,
      "loss": 0.1006,
      "step": 4021
    },
    {
      "epoch": 0.018918690085326963,
      "grad_norm": 1.6760284900665283,
      "learning_rate": 0.00019667883108433055,
      "loss": 0.3416,
      "step": 4022
    },
    {
      "epoch": 0.018923393886939425,
      "grad_norm": 2.625969409942627,
      "learning_rate": 0.00019667788810621707,
      "loss": 0.3273,
      "step": 4023
    },
    {
      "epoch": 0.018928097688551887,
      "grad_norm": 2.419400215148926,
      "learning_rate": 0.00019667694512810359,
      "loss": 0.6915,
      "step": 4024
    },
    {
      "epoch": 0.01893280149016435,
      "grad_norm": 0.3913060426712036,
      "learning_rate": 0.0001966760021499901,
      "loss": 0.0501,
      "step": 4025
    },
    {
      "epoch": 0.018937505291776815,
      "grad_norm": 1.4770259857177734,
      "learning_rate": 0.00019667505917187662,
      "loss": 0.165,
      "step": 4026
    },
    {
      "epoch": 0.018942209093389277,
      "grad_norm": 0.8287568688392639,
      "learning_rate": 0.00019667411619376314,
      "loss": 0.0981,
      "step": 4027
    },
    {
      "epoch": 0.01894691289500174,
      "grad_norm": 0.2648535668849945,
      "learning_rate": 0.00019667317321564966,
      "loss": 0.015,
      "step": 4028
    },
    {
      "epoch": 0.018951616696614205,
      "grad_norm": 1.618237018585205,
      "learning_rate": 0.0001966722302375362,
      "loss": 0.2783,
      "step": 4029
    },
    {
      "epoch": 0.018956320498226667,
      "grad_norm": 0.6535794138908386,
      "learning_rate": 0.00019667128725942273,
      "loss": 0.0576,
      "step": 4030
    },
    {
      "epoch": 0.01896102429983913,
      "grad_norm": 0.2533366084098816,
      "learning_rate": 0.00019667034428130924,
      "loss": 0.0246,
      "step": 4031
    },
    {
      "epoch": 0.018965728101451595,
      "grad_norm": 0.6775913238525391,
      "learning_rate": 0.00019666940130319576,
      "loss": 0.0471,
      "step": 4032
    },
    {
      "epoch": 0.018970431903064057,
      "grad_norm": 2.5608818531036377,
      "learning_rate": 0.00019666845832508228,
      "loss": 0.699,
      "step": 4033
    },
    {
      "epoch": 0.01897513570467652,
      "grad_norm": 1.912386417388916,
      "learning_rate": 0.0001966675153469688,
      "loss": 0.3652,
      "step": 4034
    },
    {
      "epoch": 0.01897983950628898,
      "grad_norm": 2.870889902114868,
      "learning_rate": 0.00019666657236885532,
      "loss": 0.3014,
      "step": 4035
    },
    {
      "epoch": 0.018984543307901447,
      "grad_norm": 3.521704912185669,
      "learning_rate": 0.00019666562939074184,
      "loss": 0.5894,
      "step": 4036
    },
    {
      "epoch": 0.01898924710951391,
      "grad_norm": 0.7051516175270081,
      "learning_rate": 0.00019666468641262836,
      "loss": 0.1291,
      "step": 4037
    },
    {
      "epoch": 0.01899395091112637,
      "grad_norm": 1.6465089321136475,
      "learning_rate": 0.0001966637434345149,
      "loss": 0.4516,
      "step": 4038
    },
    {
      "epoch": 0.018998654712738837,
      "grad_norm": 1.3885831832885742,
      "learning_rate": 0.00019666280045640142,
      "loss": 0.1985,
      "step": 4039
    },
    {
      "epoch": 0.0190033585143513,
      "grad_norm": 2.922349452972412,
      "learning_rate": 0.00019666185747828794,
      "loss": 0.6585,
      "step": 4040
    },
    {
      "epoch": 0.01900806231596376,
      "grad_norm": 2.2500362396240234,
      "learning_rate": 0.00019666091450017446,
      "loss": 0.363,
      "step": 4041
    },
    {
      "epoch": 0.019012766117576223,
      "grad_norm": 2.0402719974517822,
      "learning_rate": 0.000196659971522061,
      "loss": 0.2511,
      "step": 4042
    },
    {
      "epoch": 0.01901746991918869,
      "grad_norm": 2.1261911392211914,
      "learning_rate": 0.00019665902854394752,
      "loss": 0.301,
      "step": 4043
    },
    {
      "epoch": 0.01902217372080115,
      "grad_norm": 1.3728740215301514,
      "learning_rate": 0.00019665808556583404,
      "loss": 0.2938,
      "step": 4044
    },
    {
      "epoch": 0.019026877522413613,
      "grad_norm": 2.399181842803955,
      "learning_rate": 0.00019665714258772053,
      "loss": 0.3983,
      "step": 4045
    },
    {
      "epoch": 0.01903158132402608,
      "grad_norm": 1.2683871984481812,
      "learning_rate": 0.00019665619960960705,
      "loss": 0.269,
      "step": 4046
    },
    {
      "epoch": 0.01903628512563854,
      "grad_norm": 0.5033047199249268,
      "learning_rate": 0.0001966552566314936,
      "loss": 0.0511,
      "step": 4047
    },
    {
      "epoch": 0.019040988927251003,
      "grad_norm": 1.1306395530700684,
      "learning_rate": 0.00019665431365338012,
      "loss": 0.1324,
      "step": 4048
    },
    {
      "epoch": 0.01904569272886347,
      "grad_norm": 1.334867000579834,
      "learning_rate": 0.00019665337067526663,
      "loss": 0.1889,
      "step": 4049
    },
    {
      "epoch": 0.01905039653047593,
      "grad_norm": 1.945095419883728,
      "learning_rate": 0.00019665242769715315,
      "loss": 0.4573,
      "step": 4050
    },
    {
      "epoch": 0.019055100332088393,
      "grad_norm": 2.740758180618286,
      "learning_rate": 0.0001966514847190397,
      "loss": 0.4338,
      "step": 4051
    },
    {
      "epoch": 0.019059804133700856,
      "grad_norm": 1.4143248796463013,
      "learning_rate": 0.00019665054174092622,
      "loss": 0.2195,
      "step": 4052
    },
    {
      "epoch": 0.01906450793531332,
      "grad_norm": 1.7591103315353394,
      "learning_rate": 0.00019664959876281274,
      "loss": 0.2086,
      "step": 4053
    },
    {
      "epoch": 0.019069211736925783,
      "grad_norm": 1.6660372018814087,
      "learning_rate": 0.00019664865578469925,
      "loss": 0.1521,
      "step": 4054
    },
    {
      "epoch": 0.019073915538538246,
      "grad_norm": 1.0452325344085693,
      "learning_rate": 0.00019664771280658577,
      "loss": 0.0557,
      "step": 4055
    },
    {
      "epoch": 0.01907861934015071,
      "grad_norm": 0.5198490619659424,
      "learning_rate": 0.0001966467698284723,
      "loss": 0.0458,
      "step": 4056
    },
    {
      "epoch": 0.019083323141763173,
      "grad_norm": 2.8691439628601074,
      "learning_rate": 0.0001966458268503588,
      "loss": 0.2811,
      "step": 4057
    },
    {
      "epoch": 0.019088026943375636,
      "grad_norm": 1.885554313659668,
      "learning_rate": 0.00019664488387224533,
      "loss": 0.154,
      "step": 4058
    },
    {
      "epoch": 0.019092730744988098,
      "grad_norm": 1.6365487575531006,
      "learning_rate": 0.00019664394089413185,
      "loss": 0.301,
      "step": 4059
    },
    {
      "epoch": 0.019097434546600563,
      "grad_norm": 2.1905367374420166,
      "learning_rate": 0.00019664299791601837,
      "loss": 0.2294,
      "step": 4060
    },
    {
      "epoch": 0.019102138348213026,
      "grad_norm": 2.2731339931488037,
      "learning_rate": 0.0001966420549379049,
      "loss": 0.5114,
      "step": 4061
    },
    {
      "epoch": 0.019106842149825488,
      "grad_norm": 1.0715142488479614,
      "learning_rate": 0.00019664111195979143,
      "loss": 0.086,
      "step": 4062
    },
    {
      "epoch": 0.019111545951437953,
      "grad_norm": 2.5840747356414795,
      "learning_rate": 0.00019664016898167795,
      "loss": 0.4593,
      "step": 4063
    },
    {
      "epoch": 0.019116249753050416,
      "grad_norm": 3.086167335510254,
      "learning_rate": 0.00019663922600356447,
      "loss": 0.6205,
      "step": 4064
    },
    {
      "epoch": 0.019120953554662878,
      "grad_norm": 0.48068001866340637,
      "learning_rate": 0.00019663828302545099,
      "loss": 0.0337,
      "step": 4065
    },
    {
      "epoch": 0.019125657356275343,
      "grad_norm": 1.9859447479248047,
      "learning_rate": 0.0001966373400473375,
      "loss": 0.211,
      "step": 4066
    },
    {
      "epoch": 0.019130361157887806,
      "grad_norm": 1.367242693901062,
      "learning_rate": 0.00019663639706922402,
      "loss": 0.103,
      "step": 4067
    },
    {
      "epoch": 0.019135064959500268,
      "grad_norm": 3.333575963973999,
      "learning_rate": 0.00019663545409111054,
      "loss": 0.6153,
      "step": 4068
    },
    {
      "epoch": 0.01913976876111273,
      "grad_norm": 1.9086719751358032,
      "learning_rate": 0.00019663451111299706,
      "loss": 0.2143,
      "step": 4069
    },
    {
      "epoch": 0.019144472562725195,
      "grad_norm": 2.798577070236206,
      "learning_rate": 0.0001966335681348836,
      "loss": 0.7082,
      "step": 4070
    },
    {
      "epoch": 0.019149176364337658,
      "grad_norm": 1.4693394899368286,
      "learning_rate": 0.00019663262515677013,
      "loss": 0.1335,
      "step": 4071
    },
    {
      "epoch": 0.01915388016595012,
      "grad_norm": 2.377464532852173,
      "learning_rate": 0.00019663168217865664,
      "loss": 0.1547,
      "step": 4072
    },
    {
      "epoch": 0.019158583967562585,
      "grad_norm": 0.9073507785797119,
      "learning_rate": 0.00019663073920054316,
      "loss": 0.0679,
      "step": 4073
    },
    {
      "epoch": 0.019163287769175048,
      "grad_norm": 1.2380926609039307,
      "learning_rate": 0.0001966297962224297,
      "loss": 0.1226,
      "step": 4074
    },
    {
      "epoch": 0.01916799157078751,
      "grad_norm": 0.5014305710792542,
      "learning_rate": 0.00019662885324431623,
      "loss": 0.0631,
      "step": 4075
    },
    {
      "epoch": 0.019172695372399972,
      "grad_norm": 1.0519578456878662,
      "learning_rate": 0.00019662791026620272,
      "loss": 0.1293,
      "step": 4076
    },
    {
      "epoch": 0.019177399174012438,
      "grad_norm": 3.089721441268921,
      "learning_rate": 0.00019662696728808924,
      "loss": 0.5908,
      "step": 4077
    },
    {
      "epoch": 0.0191821029756249,
      "grad_norm": 1.0435603857040405,
      "learning_rate": 0.00019662602430997576,
      "loss": 0.1417,
      "step": 4078
    },
    {
      "epoch": 0.019186806777237362,
      "grad_norm": 0.15933500230312347,
      "learning_rate": 0.0001966250813318623,
      "loss": 0.0133,
      "step": 4079
    },
    {
      "epoch": 0.019191510578849828,
      "grad_norm": 1.6527351140975952,
      "learning_rate": 0.00019662413835374882,
      "loss": 0.4145,
      "step": 4080
    },
    {
      "epoch": 0.01919621438046229,
      "grad_norm": 2.3204963207244873,
      "learning_rate": 0.00019662319537563534,
      "loss": 0.5413,
      "step": 4081
    },
    {
      "epoch": 0.019200918182074752,
      "grad_norm": 3.2877206802368164,
      "learning_rate": 0.00019662225239752186,
      "loss": 0.3366,
      "step": 4082
    },
    {
      "epoch": 0.019205621983687218,
      "grad_norm": 2.23905348777771,
      "learning_rate": 0.0001966213094194084,
      "loss": 0.2657,
      "step": 4083
    },
    {
      "epoch": 0.01921032578529968,
      "grad_norm": 1.6578034162521362,
      "learning_rate": 0.00019662036644129492,
      "loss": 0.1464,
      "step": 4084
    },
    {
      "epoch": 0.019215029586912142,
      "grad_norm": 3.015486717224121,
      "learning_rate": 0.00019661942346318144,
      "loss": 0.501,
      "step": 4085
    },
    {
      "epoch": 0.019219733388524604,
      "grad_norm": 1.555452823638916,
      "learning_rate": 0.00019661848048506796,
      "loss": 0.1762,
      "step": 4086
    },
    {
      "epoch": 0.01922443719013707,
      "grad_norm": 2.078500747680664,
      "learning_rate": 0.00019661753750695445,
      "loss": 0.1458,
      "step": 4087
    },
    {
      "epoch": 0.019229140991749532,
      "grad_norm": 2.258543014526367,
      "learning_rate": 0.000196616594528841,
      "loss": 0.2715,
      "step": 4088
    },
    {
      "epoch": 0.019233844793361994,
      "grad_norm": 0.28848910331726074,
      "learning_rate": 0.00019661565155072752,
      "loss": 0.0351,
      "step": 4089
    },
    {
      "epoch": 0.01923854859497446,
      "grad_norm": 2.6722400188446045,
      "learning_rate": 0.00019661470857261403,
      "loss": 0.2893,
      "step": 4090
    },
    {
      "epoch": 0.019243252396586922,
      "grad_norm": 1.1637263298034668,
      "learning_rate": 0.00019661376559450055,
      "loss": 0.169,
      "step": 4091
    },
    {
      "epoch": 0.019247956198199384,
      "grad_norm": 1.676902413368225,
      "learning_rate": 0.0001966128226163871,
      "loss": 0.3632,
      "step": 4092
    },
    {
      "epoch": 0.019252659999811846,
      "grad_norm": 0.5198772549629211,
      "learning_rate": 0.00019661187963827362,
      "loss": 0.0459,
      "step": 4093
    },
    {
      "epoch": 0.019257363801424312,
      "grad_norm": 3.4600071907043457,
      "learning_rate": 0.00019661093666016014,
      "loss": 0.4237,
      "step": 4094
    },
    {
      "epoch": 0.019262067603036774,
      "grad_norm": 0.6571522355079651,
      "learning_rate": 0.00019660999368204665,
      "loss": 0.0773,
      "step": 4095
    },
    {
      "epoch": 0.019266771404649236,
      "grad_norm": 3.192875862121582,
      "learning_rate": 0.00019660905070393317,
      "loss": 0.3526,
      "step": 4096
    },
    {
      "epoch": 0.019271475206261702,
      "grad_norm": 2.880959987640381,
      "learning_rate": 0.0001966081077258197,
      "loss": 0.3402,
      "step": 4097
    },
    {
      "epoch": 0.019276179007874164,
      "grad_norm": 1.3869085311889648,
      "learning_rate": 0.0001966071647477062,
      "loss": 0.2607,
      "step": 4098
    },
    {
      "epoch": 0.019280882809486626,
      "grad_norm": 2.711057662963867,
      "learning_rate": 0.00019660622176959273,
      "loss": 0.4731,
      "step": 4099
    },
    {
      "epoch": 0.019285586611099092,
      "grad_norm": 2.9941494464874268,
      "learning_rate": 0.00019660527879147925,
      "loss": 0.6202,
      "step": 4100
    },
    {
      "epoch": 0.019290290412711554,
      "grad_norm": 4.356660842895508,
      "learning_rate": 0.0001966043358133658,
      "loss": 0.2238,
      "step": 4101
    },
    {
      "epoch": 0.019294994214324016,
      "grad_norm": 0.81688392162323,
      "learning_rate": 0.0001966033928352523,
      "loss": 0.0452,
      "step": 4102
    },
    {
      "epoch": 0.01929969801593648,
      "grad_norm": 0.5528756380081177,
      "learning_rate": 0.00019660244985713883,
      "loss": 0.0505,
      "step": 4103
    },
    {
      "epoch": 0.019304401817548944,
      "grad_norm": 3.6765191555023193,
      "learning_rate": 0.00019660150687902535,
      "loss": 0.4504,
      "step": 4104
    },
    {
      "epoch": 0.019309105619161406,
      "grad_norm": 3.1210007667541504,
      "learning_rate": 0.00019660056390091187,
      "loss": 0.1386,
      "step": 4105
    },
    {
      "epoch": 0.01931380942077387,
      "grad_norm": 2.8523037433624268,
      "learning_rate": 0.0001965996209227984,
      "loss": 0.3156,
      "step": 4106
    },
    {
      "epoch": 0.019318513222386334,
      "grad_norm": 1.6654245853424072,
      "learning_rate": 0.0001965986779446849,
      "loss": 0.1905,
      "step": 4107
    },
    {
      "epoch": 0.019323217023998796,
      "grad_norm": 2.228999137878418,
      "learning_rate": 0.00019659773496657142,
      "loss": 0.1872,
      "step": 4108
    },
    {
      "epoch": 0.01932792082561126,
      "grad_norm": 1.4194778203964233,
      "learning_rate": 0.00019659679198845794,
      "loss": 0.1414,
      "step": 4109
    },
    {
      "epoch": 0.01933262462722372,
      "grad_norm": 0.8721327781677246,
      "learning_rate": 0.00019659584901034446,
      "loss": 0.1291,
      "step": 4110
    },
    {
      "epoch": 0.019337328428836186,
      "grad_norm": 0.51724773645401,
      "learning_rate": 0.000196594906032231,
      "loss": 0.0635,
      "step": 4111
    },
    {
      "epoch": 0.01934203223044865,
      "grad_norm": 1.6526107788085938,
      "learning_rate": 0.00019659396305411753,
      "loss": 0.0947,
      "step": 4112
    },
    {
      "epoch": 0.01934673603206111,
      "grad_norm": 3.630603790283203,
      "learning_rate": 0.00019659302007600404,
      "loss": 0.2663,
      "step": 4113
    },
    {
      "epoch": 0.019351439833673576,
      "grad_norm": 1.2167645692825317,
      "learning_rate": 0.00019659207709789056,
      "loss": 0.0773,
      "step": 4114
    },
    {
      "epoch": 0.01935614363528604,
      "grad_norm": 2.927182674407959,
      "learning_rate": 0.0001965911341197771,
      "loss": 0.6397,
      "step": 4115
    },
    {
      "epoch": 0.0193608474368985,
      "grad_norm": 3.6232545375823975,
      "learning_rate": 0.00019659019114166363,
      "loss": 0.4617,
      "step": 4116
    },
    {
      "epoch": 0.019365551238510966,
      "grad_norm": 6.547072887420654,
      "learning_rate": 0.00019658924816355015,
      "loss": 0.4923,
      "step": 4117
    },
    {
      "epoch": 0.01937025504012343,
      "grad_norm": 0.2967657744884491,
      "learning_rate": 0.00019658830518543664,
      "loss": 0.0275,
      "step": 4118
    },
    {
      "epoch": 0.01937495884173589,
      "grad_norm": 1.9880287647247314,
      "learning_rate": 0.00019658736220732316,
      "loss": 0.1693,
      "step": 4119
    },
    {
      "epoch": 0.019379662643348353,
      "grad_norm": 3.233912229537964,
      "learning_rate": 0.0001965864192292097,
      "loss": 0.2969,
      "step": 4120
    },
    {
      "epoch": 0.01938436644496082,
      "grad_norm": 4.368069171905518,
      "learning_rate": 0.00019658547625109622,
      "loss": 0.9794,
      "step": 4121
    },
    {
      "epoch": 0.01938907024657328,
      "grad_norm": 2.4453279972076416,
      "learning_rate": 0.00019658453327298274,
      "loss": 0.2051,
      "step": 4122
    },
    {
      "epoch": 0.019393774048185743,
      "grad_norm": 4.240789413452148,
      "learning_rate": 0.00019658359029486926,
      "loss": 0.3379,
      "step": 4123
    },
    {
      "epoch": 0.01939847784979821,
      "grad_norm": 3.1842567920684814,
      "learning_rate": 0.0001965826473167558,
      "loss": 0.5004,
      "step": 4124
    },
    {
      "epoch": 0.01940318165141067,
      "grad_norm": 1.100324273109436,
      "learning_rate": 0.00019658170433864232,
      "loss": 0.112,
      "step": 4125
    },
    {
      "epoch": 0.019407885453023133,
      "grad_norm": 0.37695538997650146,
      "learning_rate": 0.00019658076136052884,
      "loss": 0.0423,
      "step": 4126
    },
    {
      "epoch": 0.019412589254635595,
      "grad_norm": 1.585581660270691,
      "learning_rate": 0.00019657981838241536,
      "loss": 0.1718,
      "step": 4127
    },
    {
      "epoch": 0.01941729305624806,
      "grad_norm": 3.0154213905334473,
      "learning_rate": 0.00019657887540430188,
      "loss": 0.5359,
      "step": 4128
    },
    {
      "epoch": 0.019421996857860523,
      "grad_norm": 4.9074296951293945,
      "learning_rate": 0.0001965779324261884,
      "loss": 1.2286,
      "step": 4129
    },
    {
      "epoch": 0.019426700659472985,
      "grad_norm": 2.336101770401001,
      "learning_rate": 0.00019657698944807491,
      "loss": 0.2125,
      "step": 4130
    },
    {
      "epoch": 0.01943140446108545,
      "grad_norm": 1.6417840719223022,
      "learning_rate": 0.00019657604646996143,
      "loss": 0.128,
      "step": 4131
    },
    {
      "epoch": 0.019436108262697913,
      "grad_norm": 1.6712727546691895,
      "learning_rate": 0.00019657510349184795,
      "loss": 0.4174,
      "step": 4132
    },
    {
      "epoch": 0.019440812064310375,
      "grad_norm": 0.6045161485671997,
      "learning_rate": 0.0001965741605137345,
      "loss": 0.1005,
      "step": 4133
    },
    {
      "epoch": 0.01944551586592284,
      "grad_norm": 1.1714041233062744,
      "learning_rate": 0.00019657321753562102,
      "loss": 0.2463,
      "step": 4134
    },
    {
      "epoch": 0.019450219667535303,
      "grad_norm": 2.5697498321533203,
      "learning_rate": 0.00019657227455750754,
      "loss": 0.4044,
      "step": 4135
    },
    {
      "epoch": 0.019454923469147765,
      "grad_norm": 2.6159768104553223,
      "learning_rate": 0.00019657133157939405,
      "loss": 0.3267,
      "step": 4136
    },
    {
      "epoch": 0.019459627270760227,
      "grad_norm": 4.066269397735596,
      "learning_rate": 0.00019657038860128057,
      "loss": 0.4304,
      "step": 4137
    },
    {
      "epoch": 0.019464331072372693,
      "grad_norm": 3.281019687652588,
      "learning_rate": 0.0001965694456231671,
      "loss": 0.4193,
      "step": 4138
    },
    {
      "epoch": 0.019469034873985155,
      "grad_norm": 2.1797165870666504,
      "learning_rate": 0.0001965685026450536,
      "loss": 0.5628,
      "step": 4139
    },
    {
      "epoch": 0.019473738675597617,
      "grad_norm": 2.0430126190185547,
      "learning_rate": 0.00019656755966694013,
      "loss": 0.4003,
      "step": 4140
    },
    {
      "epoch": 0.019478442477210083,
      "grad_norm": 1.1217293739318848,
      "learning_rate": 0.00019656661668882665,
      "loss": 0.1043,
      "step": 4141
    },
    {
      "epoch": 0.019483146278822545,
      "grad_norm": 2.366145372390747,
      "learning_rate": 0.0001965656737107132,
      "loss": 0.4332,
      "step": 4142
    },
    {
      "epoch": 0.019487850080435007,
      "grad_norm": 1.4911091327667236,
      "learning_rate": 0.0001965647307325997,
      "loss": 0.37,
      "step": 4143
    },
    {
      "epoch": 0.01949255388204747,
      "grad_norm": 1.4029783010482788,
      "learning_rate": 0.00019656378775448623,
      "loss": 0.3104,
      "step": 4144
    },
    {
      "epoch": 0.019497257683659935,
      "grad_norm": 0.5828604698181152,
      "learning_rate": 0.00019656284477637275,
      "loss": 0.0626,
      "step": 4145
    },
    {
      "epoch": 0.019501961485272397,
      "grad_norm": 1.5431654453277588,
      "learning_rate": 0.00019656190179825927,
      "loss": 0.4987,
      "step": 4146
    },
    {
      "epoch": 0.01950666528688486,
      "grad_norm": 1.7687183618545532,
      "learning_rate": 0.0001965609588201458,
      "loss": 0.4406,
      "step": 4147
    },
    {
      "epoch": 0.019511369088497325,
      "grad_norm": 0.989715576171875,
      "learning_rate": 0.00019656001584203233,
      "loss": 0.1428,
      "step": 4148
    },
    {
      "epoch": 0.019516072890109787,
      "grad_norm": 0.9162436723709106,
      "learning_rate": 0.00019655907286391882,
      "loss": 0.2384,
      "step": 4149
    },
    {
      "epoch": 0.01952077669172225,
      "grad_norm": 0.8030961155891418,
      "learning_rate": 0.00019655812988580534,
      "loss": 0.1238,
      "step": 4150
    },
    {
      "epoch": 0.019525480493334715,
      "grad_norm": 1.1766072511672974,
      "learning_rate": 0.0001965571869076919,
      "loss": 0.0997,
      "step": 4151
    },
    {
      "epoch": 0.019530184294947177,
      "grad_norm": 4.353530406951904,
      "learning_rate": 0.0001965562439295784,
      "loss": 0.609,
      "step": 4152
    },
    {
      "epoch": 0.01953488809655964,
      "grad_norm": 2.165323495864868,
      "learning_rate": 0.00019655530095146493,
      "loss": 0.3539,
      "step": 4153
    },
    {
      "epoch": 0.0195395918981721,
      "grad_norm": 1.457758903503418,
      "learning_rate": 0.00019655435797335144,
      "loss": 0.2209,
      "step": 4154
    },
    {
      "epoch": 0.019544295699784567,
      "grad_norm": 11.724705696105957,
      "learning_rate": 0.00019655341499523796,
      "loss": 1.0139,
      "step": 4155
    },
    {
      "epoch": 0.01954899950139703,
      "grad_norm": 3.1687567234039307,
      "learning_rate": 0.0001965524720171245,
      "loss": 0.3738,
      "step": 4156
    },
    {
      "epoch": 0.01955370330300949,
      "grad_norm": 1.8381528854370117,
      "learning_rate": 0.00019655152903901103,
      "loss": 0.2258,
      "step": 4157
    },
    {
      "epoch": 0.019558407104621957,
      "grad_norm": 1.9026976823806763,
      "learning_rate": 0.00019655058606089755,
      "loss": 0.3852,
      "step": 4158
    },
    {
      "epoch": 0.01956311090623442,
      "grad_norm": 2.9776947498321533,
      "learning_rate": 0.00019654964308278406,
      "loss": 0.5963,
      "step": 4159
    },
    {
      "epoch": 0.01956781470784688,
      "grad_norm": 3.988943576812744,
      "learning_rate": 0.00019654870010467056,
      "loss": 0.3177,
      "step": 4160
    },
    {
      "epoch": 0.019572518509459343,
      "grad_norm": 1.2867858409881592,
      "learning_rate": 0.0001965477571265571,
      "loss": 0.123,
      "step": 4161
    },
    {
      "epoch": 0.01957722231107181,
      "grad_norm": 1.0408228635787964,
      "learning_rate": 0.00019654681414844362,
      "loss": 0.2473,
      "step": 4162
    },
    {
      "epoch": 0.01958192611268427,
      "grad_norm": 1.5955411195755005,
      "learning_rate": 0.00019654587117033014,
      "loss": 0.261,
      "step": 4163
    },
    {
      "epoch": 0.019586629914296733,
      "grad_norm": 2.478774309158325,
      "learning_rate": 0.00019654492819221666,
      "loss": 0.4016,
      "step": 4164
    },
    {
      "epoch": 0.0195913337159092,
      "grad_norm": 1.913999319076538,
      "learning_rate": 0.0001965439852141032,
      "loss": 0.3444,
      "step": 4165
    },
    {
      "epoch": 0.01959603751752166,
      "grad_norm": 0.6555773615837097,
      "learning_rate": 0.00019654304223598972,
      "loss": 0.0693,
      "step": 4166
    },
    {
      "epoch": 0.019600741319134123,
      "grad_norm": 0.4984791576862335,
      "learning_rate": 0.00019654209925787624,
      "loss": 0.0405,
      "step": 4167
    },
    {
      "epoch": 0.01960544512074659,
      "grad_norm": 2.0147132873535156,
      "learning_rate": 0.00019654115627976276,
      "loss": 0.4245,
      "step": 4168
    },
    {
      "epoch": 0.01961014892235905,
      "grad_norm": 1.8895481824874878,
      "learning_rate": 0.00019654021330164928,
      "loss": 0.2517,
      "step": 4169
    },
    {
      "epoch": 0.019614852723971513,
      "grad_norm": 0.5117413401603699,
      "learning_rate": 0.0001965392703235358,
      "loss": 0.0684,
      "step": 4170
    },
    {
      "epoch": 0.019619556525583975,
      "grad_norm": 1.1357450485229492,
      "learning_rate": 0.00019653832734542231,
      "loss": 0.1207,
      "step": 4171
    },
    {
      "epoch": 0.01962426032719644,
      "grad_norm": 1.6315290927886963,
      "learning_rate": 0.00019653738436730883,
      "loss": 0.2323,
      "step": 4172
    },
    {
      "epoch": 0.019628964128808903,
      "grad_norm": 1.8914350271224976,
      "learning_rate": 0.00019653644138919535,
      "loss": 0.2692,
      "step": 4173
    },
    {
      "epoch": 0.019633667930421365,
      "grad_norm": 1.6013838052749634,
      "learning_rate": 0.0001965354984110819,
      "loss": 0.4065,
      "step": 4174
    },
    {
      "epoch": 0.01963837173203383,
      "grad_norm": 2.353180170059204,
      "learning_rate": 0.00019653455543296842,
      "loss": 0.4495,
      "step": 4175
    },
    {
      "epoch": 0.019643075533646293,
      "grad_norm": 0.5459936261177063,
      "learning_rate": 0.00019653361245485494,
      "loss": 0.0708,
      "step": 4176
    },
    {
      "epoch": 0.019647779335258755,
      "grad_norm": 4.108820915222168,
      "learning_rate": 0.00019653266947674145,
      "loss": 0.1551,
      "step": 4177
    },
    {
      "epoch": 0.019652483136871218,
      "grad_norm": 3.585832357406616,
      "learning_rate": 0.00019653172649862797,
      "loss": 0.2992,
      "step": 4178
    },
    {
      "epoch": 0.019657186938483683,
      "grad_norm": 0.1427927166223526,
      "learning_rate": 0.00019653078352051452,
      "loss": 0.0091,
      "step": 4179
    },
    {
      "epoch": 0.019661890740096145,
      "grad_norm": 2.390723943710327,
      "learning_rate": 0.000196529840542401,
      "loss": 0.2421,
      "step": 4180
    },
    {
      "epoch": 0.019666594541708608,
      "grad_norm": 2.503692150115967,
      "learning_rate": 0.00019652889756428753,
      "loss": 0.5049,
      "step": 4181
    },
    {
      "epoch": 0.019671298343321073,
      "grad_norm": 1.49452543258667,
      "learning_rate": 0.00019652795458617405,
      "loss": 0.1532,
      "step": 4182
    },
    {
      "epoch": 0.019676002144933535,
      "grad_norm": 3.1398510932922363,
      "learning_rate": 0.0001965270116080606,
      "loss": 0.2565,
      "step": 4183
    },
    {
      "epoch": 0.019680705946545998,
      "grad_norm": 0.597050666809082,
      "learning_rate": 0.0001965260686299471,
      "loss": 0.0485,
      "step": 4184
    },
    {
      "epoch": 0.019685409748158463,
      "grad_norm": 1.4964371919631958,
      "learning_rate": 0.00019652512565183363,
      "loss": 0.3674,
      "step": 4185
    },
    {
      "epoch": 0.019690113549770925,
      "grad_norm": 0.9306879639625549,
      "learning_rate": 0.00019652418267372015,
      "loss": 0.1255,
      "step": 4186
    },
    {
      "epoch": 0.019694817351383388,
      "grad_norm": 0.18507926166057587,
      "learning_rate": 0.00019652323969560667,
      "loss": 0.0141,
      "step": 4187
    },
    {
      "epoch": 0.01969952115299585,
      "grad_norm": 1.6299712657928467,
      "learning_rate": 0.0001965222967174932,
      "loss": 0.1356,
      "step": 4188
    },
    {
      "epoch": 0.019704224954608315,
      "grad_norm": 3.173229694366455,
      "learning_rate": 0.00019652135373937973,
      "loss": 0.3596,
      "step": 4189
    },
    {
      "epoch": 0.019708928756220778,
      "grad_norm": 1.7621850967407227,
      "learning_rate": 0.00019652041076126625,
      "loss": 0.212,
      "step": 4190
    },
    {
      "epoch": 0.01971363255783324,
      "grad_norm": 3.279790163040161,
      "learning_rate": 0.00019651946778315274,
      "loss": 0.952,
      "step": 4191
    },
    {
      "epoch": 0.019718336359445705,
      "grad_norm": 2.977883815765381,
      "learning_rate": 0.0001965185248050393,
      "loss": 0.441,
      "step": 4192
    },
    {
      "epoch": 0.019723040161058168,
      "grad_norm": 4.478198528289795,
      "learning_rate": 0.0001965175818269258,
      "loss": 0.2758,
      "step": 4193
    },
    {
      "epoch": 0.01972774396267063,
      "grad_norm": 0.5397735834121704,
      "learning_rate": 0.00019651663884881233,
      "loss": 0.0437,
      "step": 4194
    },
    {
      "epoch": 0.019732447764283095,
      "grad_norm": 0.9235547780990601,
      "learning_rate": 0.00019651569587069884,
      "loss": 0.0656,
      "step": 4195
    },
    {
      "epoch": 0.019737151565895557,
      "grad_norm": 5.005103588104248,
      "learning_rate": 0.00019651475289258536,
      "loss": 0.184,
      "step": 4196
    },
    {
      "epoch": 0.01974185536750802,
      "grad_norm": 0.8461281657218933,
      "learning_rate": 0.0001965138099144719,
      "loss": 0.0663,
      "step": 4197
    },
    {
      "epoch": 0.019746559169120482,
      "grad_norm": 1.4333416223526,
      "learning_rate": 0.00019651286693635843,
      "loss": 0.0837,
      "step": 4198
    },
    {
      "epoch": 0.019751262970732947,
      "grad_norm": 3.0703306198120117,
      "learning_rate": 0.00019651192395824495,
      "loss": 0.4724,
      "step": 4199
    },
    {
      "epoch": 0.01975596677234541,
      "grad_norm": 2.2572031021118164,
      "learning_rate": 0.00019651098098013146,
      "loss": 0.2038,
      "step": 4200
    },
    {
      "epoch": 0.019760670573957872,
      "grad_norm": 0.22643065452575684,
      "learning_rate": 0.00019651003800201798,
      "loss": 0.0101,
      "step": 4201
    },
    {
      "epoch": 0.019765374375570337,
      "grad_norm": 1.4528244733810425,
      "learning_rate": 0.0001965090950239045,
      "loss": 0.104,
      "step": 4202
    },
    {
      "epoch": 0.0197700781771828,
      "grad_norm": 4.386232852935791,
      "learning_rate": 0.00019650815204579102,
      "loss": 0.6505,
      "step": 4203
    },
    {
      "epoch": 0.019774781978795262,
      "grad_norm": 1.9799730777740479,
      "learning_rate": 0.00019650720906767754,
      "loss": 0.1834,
      "step": 4204
    },
    {
      "epoch": 0.019779485780407724,
      "grad_norm": 0.5877073407173157,
      "learning_rate": 0.00019650626608956406,
      "loss": 0.0403,
      "step": 4205
    },
    {
      "epoch": 0.01978418958202019,
      "grad_norm": 1.2978657484054565,
      "learning_rate": 0.0001965053231114506,
      "loss": 0.0982,
      "step": 4206
    },
    {
      "epoch": 0.019788893383632652,
      "grad_norm": 2.822984218597412,
      "learning_rate": 0.00019650438013333712,
      "loss": 0.2002,
      "step": 4207
    },
    {
      "epoch": 0.019793597185245114,
      "grad_norm": 1.0750489234924316,
      "learning_rate": 0.00019650343715522364,
      "loss": 0.0785,
      "step": 4208
    },
    {
      "epoch": 0.01979830098685758,
      "grad_norm": 1.2883107662200928,
      "learning_rate": 0.00019650249417711016,
      "loss": 0.0794,
      "step": 4209
    },
    {
      "epoch": 0.019803004788470042,
      "grad_norm": 4.065393924713135,
      "learning_rate": 0.0001965015511989967,
      "loss": 0.693,
      "step": 4210
    },
    {
      "epoch": 0.019807708590082504,
      "grad_norm": 2.624431610107422,
      "learning_rate": 0.0001965006082208832,
      "loss": 0.372,
      "step": 4211
    },
    {
      "epoch": 0.01981241239169497,
      "grad_norm": 3.235837936401367,
      "learning_rate": 0.00019649966524276971,
      "loss": 0.241,
      "step": 4212
    },
    {
      "epoch": 0.019817116193307432,
      "grad_norm": 0.9900696277618408,
      "learning_rate": 0.00019649872226465623,
      "loss": 0.0518,
      "step": 4213
    },
    {
      "epoch": 0.019821819994919894,
      "grad_norm": 1.6380692720413208,
      "learning_rate": 0.00019649777928654275,
      "loss": 0.1163,
      "step": 4214
    },
    {
      "epoch": 0.019826523796532356,
      "grad_norm": 4.515680313110352,
      "learning_rate": 0.0001964968363084293,
      "loss": 0.6804,
      "step": 4215
    },
    {
      "epoch": 0.019831227598144822,
      "grad_norm": 1.9828346967697144,
      "learning_rate": 0.00019649589333031582,
      "loss": 0.1578,
      "step": 4216
    },
    {
      "epoch": 0.019835931399757284,
      "grad_norm": 3.2751481533050537,
      "learning_rate": 0.00019649495035220234,
      "loss": 0.2979,
      "step": 4217
    },
    {
      "epoch": 0.019840635201369746,
      "grad_norm": 0.7631690502166748,
      "learning_rate": 0.00019649400737408885,
      "loss": 0.0544,
      "step": 4218
    },
    {
      "epoch": 0.01984533900298221,
      "grad_norm": 5.3863396644592285,
      "learning_rate": 0.00019649306439597537,
      "loss": 0.3801,
      "step": 4219
    },
    {
      "epoch": 0.019850042804594674,
      "grad_norm": 4.959956169128418,
      "learning_rate": 0.00019649212141786192,
      "loss": 1.0141,
      "step": 4220
    },
    {
      "epoch": 0.019854746606207136,
      "grad_norm": 0.7759138345718384,
      "learning_rate": 0.00019649117843974844,
      "loss": 0.0921,
      "step": 4221
    },
    {
      "epoch": 0.019859450407819598,
      "grad_norm": 2.477123260498047,
      "learning_rate": 0.00019649023546163493,
      "loss": 0.2474,
      "step": 4222
    },
    {
      "epoch": 0.019864154209432064,
      "grad_norm": 3.8171730041503906,
      "learning_rate": 0.00019648929248352145,
      "loss": 0.5367,
      "step": 4223
    },
    {
      "epoch": 0.019868858011044526,
      "grad_norm": 2.9515812397003174,
      "learning_rate": 0.000196488349505408,
      "loss": 0.6093,
      "step": 4224
    },
    {
      "epoch": 0.019873561812656988,
      "grad_norm": 4.188165664672852,
      "learning_rate": 0.0001964874065272945,
      "loss": 0.2914,
      "step": 4225
    },
    {
      "epoch": 0.019878265614269454,
      "grad_norm": 2.3617801666259766,
      "learning_rate": 0.00019648646354918103,
      "loss": 0.2105,
      "step": 4226
    },
    {
      "epoch": 0.019882969415881916,
      "grad_norm": 2.5133984088897705,
      "learning_rate": 0.00019648552057106755,
      "loss": 0.1921,
      "step": 4227
    },
    {
      "epoch": 0.019887673217494378,
      "grad_norm": 1.8029334545135498,
      "learning_rate": 0.00019648457759295407,
      "loss": 0.4423,
      "step": 4228
    },
    {
      "epoch": 0.019892377019106844,
      "grad_norm": 0.32501283288002014,
      "learning_rate": 0.0001964836346148406,
      "loss": 0.0289,
      "step": 4229
    },
    {
      "epoch": 0.019897080820719306,
      "grad_norm": 0.5218629240989685,
      "learning_rate": 0.00019648269163672713,
      "loss": 0.0654,
      "step": 4230
    },
    {
      "epoch": 0.019901784622331768,
      "grad_norm": 1.0780415534973145,
      "learning_rate": 0.00019648174865861365,
      "loss": 0.1029,
      "step": 4231
    },
    {
      "epoch": 0.01990648842394423,
      "grad_norm": 0.4122656285762787,
      "learning_rate": 0.00019648080568050017,
      "loss": 0.0508,
      "step": 4232
    },
    {
      "epoch": 0.019911192225556696,
      "grad_norm": 3.225499153137207,
      "learning_rate": 0.0001964798627023867,
      "loss": 0.678,
      "step": 4233
    },
    {
      "epoch": 0.019915896027169158,
      "grad_norm": 1.15659761428833,
      "learning_rate": 0.0001964789197242732,
      "loss": 0.1002,
      "step": 4234
    },
    {
      "epoch": 0.01992059982878162,
      "grad_norm": 2.142549991607666,
      "learning_rate": 0.00019647797674615973,
      "loss": 0.2875,
      "step": 4235
    },
    {
      "epoch": 0.019925303630394086,
      "grad_norm": 3.715543031692505,
      "learning_rate": 0.00019647703376804624,
      "loss": 0.6049,
      "step": 4236
    },
    {
      "epoch": 0.019930007432006548,
      "grad_norm": 1.263566255569458,
      "learning_rate": 0.00019647609078993276,
      "loss": 0.2361,
      "step": 4237
    },
    {
      "epoch": 0.01993471123361901,
      "grad_norm": 0.647264838218689,
      "learning_rate": 0.0001964751478118193,
      "loss": 0.0659,
      "step": 4238
    },
    {
      "epoch": 0.019939415035231473,
      "grad_norm": 1.086258053779602,
      "learning_rate": 0.00019647420483370583,
      "loss": 0.0975,
      "step": 4239
    },
    {
      "epoch": 0.019944118836843938,
      "grad_norm": 0.5585106015205383,
      "learning_rate": 0.00019647326185559235,
      "loss": 0.0659,
      "step": 4240
    },
    {
      "epoch": 0.0199488226384564,
      "grad_norm": 0.45826834440231323,
      "learning_rate": 0.00019647231887747886,
      "loss": 0.0605,
      "step": 4241
    },
    {
      "epoch": 0.019953526440068863,
      "grad_norm": 1.9312152862548828,
      "learning_rate": 0.00019647137589936538,
      "loss": 0.1714,
      "step": 4242
    },
    {
      "epoch": 0.019958230241681328,
      "grad_norm": 1.7914458513259888,
      "learning_rate": 0.0001964704329212519,
      "loss": 0.2199,
      "step": 4243
    },
    {
      "epoch": 0.01996293404329379,
      "grad_norm": 0.5593246221542358,
      "learning_rate": 0.00019646948994313842,
      "loss": 0.0398,
      "step": 4244
    },
    {
      "epoch": 0.019967637844906252,
      "grad_norm": 2.5873754024505615,
      "learning_rate": 0.00019646854696502494,
      "loss": 0.2981,
      "step": 4245
    },
    {
      "epoch": 0.019972341646518718,
      "grad_norm": 0.26001662015914917,
      "learning_rate": 0.00019646760398691146,
      "loss": 0.022,
      "step": 4246
    },
    {
      "epoch": 0.01997704544813118,
      "grad_norm": 3.527228832244873,
      "learning_rate": 0.000196466661008798,
      "loss": 0.6024,
      "step": 4247
    },
    {
      "epoch": 0.019981749249743642,
      "grad_norm": 4.737954139709473,
      "learning_rate": 0.00019646571803068452,
      "loss": 0.7794,
      "step": 4248
    },
    {
      "epoch": 0.019986453051356105,
      "grad_norm": 3.2352402210235596,
      "learning_rate": 0.00019646477505257104,
      "loss": 0.3203,
      "step": 4249
    },
    {
      "epoch": 0.01999115685296857,
      "grad_norm": 4.080683708190918,
      "learning_rate": 0.00019646383207445756,
      "loss": 0.4695,
      "step": 4250
    },
    {
      "epoch": 0.019995860654581032,
      "grad_norm": 1.1585602760314941,
      "learning_rate": 0.0001964628890963441,
      "loss": 0.0177,
      "step": 4251
    },
    {
      "epoch": 0.020000564456193495,
      "grad_norm": 5.7575907707214355,
      "learning_rate": 0.00019646194611823062,
      "loss": 1.5497,
      "step": 4252
    },
    {
      "epoch": 0.02000526825780596,
      "grad_norm": 3.3961434364318848,
      "learning_rate": 0.00019646100314011711,
      "loss": 0.0575,
      "step": 4253
    },
    {
      "epoch": 0.020009972059418422,
      "grad_norm": 2.96130633354187,
      "learning_rate": 0.00019646006016200363,
      "loss": 0.2392,
      "step": 4254
    },
    {
      "epoch": 0.020014675861030885,
      "grad_norm": 3.0179524421691895,
      "learning_rate": 0.00019645911718389015,
      "loss": 0.2288,
      "step": 4255
    },
    {
      "epoch": 0.020019379662643347,
      "grad_norm": 6.3236083984375,
      "learning_rate": 0.0001964581742057767,
      "loss": 1.1611,
      "step": 4256
    },
    {
      "epoch": 0.020024083464255812,
      "grad_norm": 1.6755061149597168,
      "learning_rate": 0.00019645723122766322,
      "loss": 0.2006,
      "step": 4257
    },
    {
      "epoch": 0.020028787265868275,
      "grad_norm": 2.101154088973999,
      "learning_rate": 0.00019645628824954974,
      "loss": 0.2801,
      "step": 4258
    },
    {
      "epoch": 0.020033491067480737,
      "grad_norm": 4.049069404602051,
      "learning_rate": 0.00019645534527143625,
      "loss": 0.4531,
      "step": 4259
    },
    {
      "epoch": 0.020038194869093202,
      "grad_norm": 3.919935703277588,
      "learning_rate": 0.0001964544022933228,
      "loss": 0.4788,
      "step": 4260
    },
    {
      "epoch": 0.020042898670705665,
      "grad_norm": 0.7197850942611694,
      "learning_rate": 0.00019645345931520932,
      "loss": 0.0746,
      "step": 4261
    },
    {
      "epoch": 0.020047602472318127,
      "grad_norm": 3.124124526977539,
      "learning_rate": 0.00019645251633709584,
      "loss": 0.559,
      "step": 4262
    },
    {
      "epoch": 0.020052306273930592,
      "grad_norm": 1.5272704362869263,
      "learning_rate": 0.00019645157335898236,
      "loss": 0.1769,
      "step": 4263
    },
    {
      "epoch": 0.020057010075543055,
      "grad_norm": 1.2321947813034058,
      "learning_rate": 0.00019645063038086887,
      "loss": 0.1529,
      "step": 4264
    },
    {
      "epoch": 0.020061713877155517,
      "grad_norm": 0.2556390166282654,
      "learning_rate": 0.0001964496874027554,
      "loss": 0.0241,
      "step": 4265
    },
    {
      "epoch": 0.02006641767876798,
      "grad_norm": 1.226952314376831,
      "learning_rate": 0.0001964487444246419,
      "loss": 0.1889,
      "step": 4266
    },
    {
      "epoch": 0.020071121480380445,
      "grad_norm": 2.8512094020843506,
      "learning_rate": 0.00019644780144652843,
      "loss": 0.3505,
      "step": 4267
    },
    {
      "epoch": 0.020075825281992907,
      "grad_norm": 0.9844933152198792,
      "learning_rate": 0.00019644685846841495,
      "loss": 0.0932,
      "step": 4268
    },
    {
      "epoch": 0.02008052908360537,
      "grad_norm": 3.0302956104278564,
      "learning_rate": 0.00019644591549030147,
      "loss": 0.7068,
      "step": 4269
    },
    {
      "epoch": 0.020085232885217835,
      "grad_norm": 12.580644607543945,
      "learning_rate": 0.000196444972512188,
      "loss": 1.0573,
      "step": 4270
    },
    {
      "epoch": 0.020089936686830297,
      "grad_norm": 2.1998772621154785,
      "learning_rate": 0.00019644402953407453,
      "loss": 0.3727,
      "step": 4271
    },
    {
      "epoch": 0.02009464048844276,
      "grad_norm": 4.163030624389648,
      "learning_rate": 0.00019644308655596105,
      "loss": 0.5998,
      "step": 4272
    },
    {
      "epoch": 0.02009934429005522,
      "grad_norm": 8.520561218261719,
      "learning_rate": 0.00019644214357784757,
      "loss": 0.9538,
      "step": 4273
    },
    {
      "epoch": 0.020104048091667687,
      "grad_norm": 2.8613109588623047,
      "learning_rate": 0.0001964412005997341,
      "loss": 0.3645,
      "step": 4274
    },
    {
      "epoch": 0.02010875189328015,
      "grad_norm": 1.3756848573684692,
      "learning_rate": 0.0001964402576216206,
      "loss": 0.1993,
      "step": 4275
    },
    {
      "epoch": 0.02011345569489261,
      "grad_norm": 1.1079930067062378,
      "learning_rate": 0.00019643931464350713,
      "loss": 0.0877,
      "step": 4276
    },
    {
      "epoch": 0.020118159496505077,
      "grad_norm": 1.1802723407745361,
      "learning_rate": 0.00019643837166539364,
      "loss": 0.1396,
      "step": 4277
    },
    {
      "epoch": 0.02012286329811754,
      "grad_norm": 1.2709139585494995,
      "learning_rate": 0.00019643742868728016,
      "loss": 0.0979,
      "step": 4278
    },
    {
      "epoch": 0.02012756709973,
      "grad_norm": 4.69108772277832,
      "learning_rate": 0.0001964364857091667,
      "loss": 1.0317,
      "step": 4279
    },
    {
      "epoch": 0.020132270901342467,
      "grad_norm": 6.461246967315674,
      "learning_rate": 0.00019643554273105323,
      "loss": 0.9149,
      "step": 4280
    },
    {
      "epoch": 0.02013697470295493,
      "grad_norm": 0.990847647190094,
      "learning_rate": 0.00019643459975293975,
      "loss": 0.1053,
      "step": 4281
    },
    {
      "epoch": 0.02014167850456739,
      "grad_norm": 1.635762333869934,
      "learning_rate": 0.00019643365677482626,
      "loss": 0.1869,
      "step": 4282
    },
    {
      "epoch": 0.020146382306179853,
      "grad_norm": 1.5646742582321167,
      "learning_rate": 0.0001964327137967128,
      "loss": 0.2624,
      "step": 4283
    },
    {
      "epoch": 0.02015108610779232,
      "grad_norm": 0.7579797506332397,
      "learning_rate": 0.0001964317708185993,
      "loss": 0.1135,
      "step": 4284
    },
    {
      "epoch": 0.02015578990940478,
      "grad_norm": 0.9319418668746948,
      "learning_rate": 0.00019643082784048582,
      "loss": 0.0878,
      "step": 4285
    },
    {
      "epoch": 0.020160493711017243,
      "grad_norm": 0.5216712355613708,
      "learning_rate": 0.00019642988486237234,
      "loss": 0.0507,
      "step": 4286
    },
    {
      "epoch": 0.02016519751262971,
      "grad_norm": 1.634726643562317,
      "learning_rate": 0.00019642894188425886,
      "loss": 0.1425,
      "step": 4287
    },
    {
      "epoch": 0.02016990131424217,
      "grad_norm": 0.8405049443244934,
      "learning_rate": 0.0001964279989061454,
      "loss": 0.0589,
      "step": 4288
    },
    {
      "epoch": 0.020174605115854633,
      "grad_norm": 1.2280830144882202,
      "learning_rate": 0.00019642705592803192,
      "loss": 0.0774,
      "step": 4289
    },
    {
      "epoch": 0.020179308917467095,
      "grad_norm": 0.5533272624015808,
      "learning_rate": 0.00019642611294991844,
      "loss": 0.0396,
      "step": 4290
    },
    {
      "epoch": 0.02018401271907956,
      "grad_norm": 0.11886067688465118,
      "learning_rate": 0.00019642516997180496,
      "loss": 0.0078,
      "step": 4291
    },
    {
      "epoch": 0.020188716520692023,
      "grad_norm": 4.0994486808776855,
      "learning_rate": 0.0001964242269936915,
      "loss": 1.0817,
      "step": 4292
    },
    {
      "epoch": 0.020193420322304485,
      "grad_norm": 1.3475993871688843,
      "learning_rate": 0.00019642328401557802,
      "loss": 0.1086,
      "step": 4293
    },
    {
      "epoch": 0.02019812412391695,
      "grad_norm": 0.36430951952934265,
      "learning_rate": 0.00019642234103746454,
      "loss": 0.0261,
      "step": 4294
    },
    {
      "epoch": 0.020202827925529413,
      "grad_norm": 2.21746826171875,
      "learning_rate": 0.00019642139805935106,
      "loss": 0.3329,
      "step": 4295
    },
    {
      "epoch": 0.020207531727141875,
      "grad_norm": 5.218186378479004,
      "learning_rate": 0.00019642045508123755,
      "loss": 0.8146,
      "step": 4296
    },
    {
      "epoch": 0.02021223552875434,
      "grad_norm": 2.62870192527771,
      "learning_rate": 0.0001964195121031241,
      "loss": 0.6095,
      "step": 4297
    },
    {
      "epoch": 0.020216939330366803,
      "grad_norm": 2.2978267669677734,
      "learning_rate": 0.00019641856912501062,
      "loss": 0.9574,
      "step": 4298
    },
    {
      "epoch": 0.020221643131979265,
      "grad_norm": 3.3355371952056885,
      "learning_rate": 0.00019641762614689714,
      "loss": 0.2947,
      "step": 4299
    },
    {
      "epoch": 0.020226346933591727,
      "grad_norm": 3.584181308746338,
      "learning_rate": 0.00019641668316878365,
      "loss": 0.7802,
      "step": 4300
    },
    {
      "epoch": 0.020231050735204193,
      "grad_norm": 2.627756357192993,
      "learning_rate": 0.0001964157401906702,
      "loss": 0.1407,
      "step": 4301
    },
    {
      "epoch": 0.020235754536816655,
      "grad_norm": 4.980571746826172,
      "learning_rate": 0.00019641479721255672,
      "loss": 0.4627,
      "step": 4302
    },
    {
      "epoch": 0.020240458338429117,
      "grad_norm": 3.9757936000823975,
      "learning_rate": 0.00019641385423444324,
      "loss": 0.5074,
      "step": 4303
    },
    {
      "epoch": 0.020245162140041583,
      "grad_norm": 5.996063709259033,
      "learning_rate": 0.00019641291125632976,
      "loss": 0.7726,
      "step": 4304
    },
    {
      "epoch": 0.020249865941654045,
      "grad_norm": 0.9632295966148376,
      "learning_rate": 0.00019641196827821627,
      "loss": 0.0879,
      "step": 4305
    },
    {
      "epoch": 0.020254569743266507,
      "grad_norm": 3.3496711254119873,
      "learning_rate": 0.0001964110253001028,
      "loss": 0.4478,
      "step": 4306
    },
    {
      "epoch": 0.02025927354487897,
      "grad_norm": 1.9077681303024292,
      "learning_rate": 0.0001964100823219893,
      "loss": 0.2122,
      "step": 4307
    },
    {
      "epoch": 0.020263977346491435,
      "grad_norm": 0.9905593395233154,
      "learning_rate": 0.00019640913934387583,
      "loss": 0.1312,
      "step": 4308
    },
    {
      "epoch": 0.020268681148103897,
      "grad_norm": 4.431159496307373,
      "learning_rate": 0.00019640819636576235,
      "loss": 0.5204,
      "step": 4309
    },
    {
      "epoch": 0.02027338494971636,
      "grad_norm": 4.023698806762695,
      "learning_rate": 0.0001964072533876489,
      "loss": 0.5257,
      "step": 4310
    },
    {
      "epoch": 0.020278088751328825,
      "grad_norm": 0.8646152019500732,
      "learning_rate": 0.0001964063104095354,
      "loss": 0.0729,
      "step": 4311
    },
    {
      "epoch": 0.020282792552941287,
      "grad_norm": 3.6640026569366455,
      "learning_rate": 0.00019640536743142193,
      "loss": 0.3856,
      "step": 4312
    },
    {
      "epoch": 0.02028749635455375,
      "grad_norm": 0.9571921825408936,
      "learning_rate": 0.00019640442445330845,
      "loss": 0.0788,
      "step": 4313
    },
    {
      "epoch": 0.020292200156166215,
      "grad_norm": 4.486917018890381,
      "learning_rate": 0.00019640348147519497,
      "loss": 0.3508,
      "step": 4314
    },
    {
      "epoch": 0.020296903957778677,
      "grad_norm": 2.129401445388794,
      "learning_rate": 0.0001964025384970815,
      "loss": 0.369,
      "step": 4315
    },
    {
      "epoch": 0.02030160775939114,
      "grad_norm": 1.5678478479385376,
      "learning_rate": 0.000196401595518968,
      "loss": 0.1995,
      "step": 4316
    },
    {
      "epoch": 0.0203063115610036,
      "grad_norm": 4.580954074859619,
      "learning_rate": 0.00019640065254085453,
      "loss": 0.5413,
      "step": 4317
    },
    {
      "epoch": 0.020311015362616067,
      "grad_norm": 2.820094585418701,
      "learning_rate": 0.00019639970956274104,
      "loss": 0.286,
      "step": 4318
    },
    {
      "epoch": 0.02031571916422853,
      "grad_norm": 3.182194232940674,
      "learning_rate": 0.00019639876658462756,
      "loss": 0.2976,
      "step": 4319
    },
    {
      "epoch": 0.02032042296584099,
      "grad_norm": 1.4451228380203247,
      "learning_rate": 0.0001963978236065141,
      "loss": 0.1952,
      "step": 4320
    },
    {
      "epoch": 0.020325126767453457,
      "grad_norm": 1.1863327026367188,
      "learning_rate": 0.00019639688062840063,
      "loss": 0.0983,
      "step": 4321
    },
    {
      "epoch": 0.02032983056906592,
      "grad_norm": 3.0342628955841064,
      "learning_rate": 0.00019639593765028715,
      "loss": 0.4807,
      "step": 4322
    },
    {
      "epoch": 0.02033453437067838,
      "grad_norm": 0.7244902849197388,
      "learning_rate": 0.00019639499467217366,
      "loss": 0.0596,
      "step": 4323
    },
    {
      "epoch": 0.020339238172290844,
      "grad_norm": 1.316955804824829,
      "learning_rate": 0.0001963940516940602,
      "loss": 0.2366,
      "step": 4324
    },
    {
      "epoch": 0.02034394197390331,
      "grad_norm": 1.4181981086730957,
      "learning_rate": 0.00019639310871594673,
      "loss": 0.1509,
      "step": 4325
    },
    {
      "epoch": 0.02034864577551577,
      "grad_norm": 0.9690862894058228,
      "learning_rate": 0.00019639216573783325,
      "loss": 0.1192,
      "step": 4326
    },
    {
      "epoch": 0.020353349577128234,
      "grad_norm": 2.5187766551971436,
      "learning_rate": 0.00019639122275971974,
      "loss": 0.3208,
      "step": 4327
    },
    {
      "epoch": 0.0203580533787407,
      "grad_norm": 0.4557802081108093,
      "learning_rate": 0.00019639027978160626,
      "loss": 0.0427,
      "step": 4328
    },
    {
      "epoch": 0.02036275718035316,
      "grad_norm": 2.2090978622436523,
      "learning_rate": 0.0001963893368034928,
      "loss": 0.4908,
      "step": 4329
    },
    {
      "epoch": 0.020367460981965624,
      "grad_norm": 0.2182426154613495,
      "learning_rate": 0.00019638839382537932,
      "loss": 0.0175,
      "step": 4330
    },
    {
      "epoch": 0.02037216478357809,
      "grad_norm": 2.3310375213623047,
      "learning_rate": 0.00019638745084726584,
      "loss": 0.1839,
      "step": 4331
    },
    {
      "epoch": 0.02037686858519055,
      "grad_norm": 4.025692939758301,
      "learning_rate": 0.00019638650786915236,
      "loss": 1.0056,
      "step": 4332
    },
    {
      "epoch": 0.020381572386803014,
      "grad_norm": 0.4559139907360077,
      "learning_rate": 0.0001963855648910389,
      "loss": 0.0376,
      "step": 4333
    },
    {
      "epoch": 0.020386276188415476,
      "grad_norm": 3.8718154430389404,
      "learning_rate": 0.00019638462191292542,
      "loss": 0.5483,
      "step": 4334
    },
    {
      "epoch": 0.02039097999002794,
      "grad_norm": 0.8890599608421326,
      "learning_rate": 0.00019638367893481194,
      "loss": 0.0755,
      "step": 4335
    },
    {
      "epoch": 0.020395683791640404,
      "grad_norm": 1.6783777475357056,
      "learning_rate": 0.00019638273595669846,
      "loss": 0.318,
      "step": 4336
    },
    {
      "epoch": 0.020400387593252866,
      "grad_norm": 0.5581516027450562,
      "learning_rate": 0.00019638179297858498,
      "loss": 0.0396,
      "step": 4337
    },
    {
      "epoch": 0.02040509139486533,
      "grad_norm": 2.7126317024230957,
      "learning_rate": 0.0001963808500004715,
      "loss": 0.8816,
      "step": 4338
    },
    {
      "epoch": 0.020409795196477794,
      "grad_norm": 6.029664039611816,
      "learning_rate": 0.00019637990702235802,
      "loss": 0.4639,
      "step": 4339
    },
    {
      "epoch": 0.020414498998090256,
      "grad_norm": 0.3086599111557007,
      "learning_rate": 0.00019637896404424454,
      "loss": 0.0234,
      "step": 4340
    },
    {
      "epoch": 0.020419202799702718,
      "grad_norm": 3.3416996002197266,
      "learning_rate": 0.00019637802106613105,
      "loss": 0.588,
      "step": 4341
    },
    {
      "epoch": 0.020423906601315184,
      "grad_norm": 0.7762408256530762,
      "learning_rate": 0.0001963770780880176,
      "loss": 0.0629,
      "step": 4342
    },
    {
      "epoch": 0.020428610402927646,
      "grad_norm": 2.280979871749878,
      "learning_rate": 0.00019637613510990412,
      "loss": 0.2675,
      "step": 4343
    },
    {
      "epoch": 0.020433314204540108,
      "grad_norm": 2.358855724334717,
      "learning_rate": 0.00019637519213179064,
      "loss": 0.2889,
      "step": 4344
    },
    {
      "epoch": 0.020438018006152574,
      "grad_norm": 1.5749163627624512,
      "learning_rate": 0.00019637424915367716,
      "loss": 0.2506,
      "step": 4345
    },
    {
      "epoch": 0.020442721807765036,
      "grad_norm": 1.4448872804641724,
      "learning_rate": 0.00019637330617556367,
      "loss": 0.1346,
      "step": 4346
    },
    {
      "epoch": 0.020447425609377498,
      "grad_norm": 3.3922500610351562,
      "learning_rate": 0.0001963723631974502,
      "loss": 0.2625,
      "step": 4347
    },
    {
      "epoch": 0.020452129410989964,
      "grad_norm": 1.4365936517715454,
      "learning_rate": 0.0001963714202193367,
      "loss": 0.1834,
      "step": 4348
    },
    {
      "epoch": 0.020456833212602426,
      "grad_norm": 1.4955894947052002,
      "learning_rate": 0.00019637047724122323,
      "loss": 0.2319,
      "step": 4349
    },
    {
      "epoch": 0.020461537014214888,
      "grad_norm": 1.276915192604065,
      "learning_rate": 0.00019636953426310975,
      "loss": 0.201,
      "step": 4350
    },
    {
      "epoch": 0.02046624081582735,
      "grad_norm": 0.3551810085773468,
      "learning_rate": 0.0001963685912849963,
      "loss": 0.0527,
      "step": 4351
    },
    {
      "epoch": 0.020470944617439816,
      "grad_norm": 4.128383159637451,
      "learning_rate": 0.0001963676483068828,
      "loss": 0.7046,
      "step": 4352
    },
    {
      "epoch": 0.020475648419052278,
      "grad_norm": 1.8298903703689575,
      "learning_rate": 0.00019636670532876933,
      "loss": 0.3038,
      "step": 4353
    },
    {
      "epoch": 0.02048035222066474,
      "grad_norm": 1.4102773666381836,
      "learning_rate": 0.00019636576235065585,
      "loss": 0.1119,
      "step": 4354
    },
    {
      "epoch": 0.020485056022277206,
      "grad_norm": 3.1483607292175293,
      "learning_rate": 0.00019636481937254237,
      "loss": 0.4596,
      "step": 4355
    },
    {
      "epoch": 0.020489759823889668,
      "grad_norm": 2.417299509048462,
      "learning_rate": 0.00019636387639442891,
      "loss": 0.2686,
      "step": 4356
    },
    {
      "epoch": 0.02049446362550213,
      "grad_norm": 0.4785434305667877,
      "learning_rate": 0.00019636293341631543,
      "loss": 0.0429,
      "step": 4357
    },
    {
      "epoch": 0.020499167427114592,
      "grad_norm": 2.341048240661621,
      "learning_rate": 0.00019636199043820192,
      "loss": 0.3476,
      "step": 4358
    },
    {
      "epoch": 0.020503871228727058,
      "grad_norm": 0.91031414270401,
      "learning_rate": 0.00019636104746008844,
      "loss": 0.1305,
      "step": 4359
    },
    {
      "epoch": 0.02050857503033952,
      "grad_norm": 2.4524037837982178,
      "learning_rate": 0.000196360104481975,
      "loss": 0.276,
      "step": 4360
    },
    {
      "epoch": 0.020513278831951982,
      "grad_norm": 1.5785853862762451,
      "learning_rate": 0.0001963591615038615,
      "loss": 0.1684,
      "step": 4361
    },
    {
      "epoch": 0.020517982633564448,
      "grad_norm": 3.2815418243408203,
      "learning_rate": 0.00019635821852574803,
      "loss": 0.4402,
      "step": 4362
    },
    {
      "epoch": 0.02052268643517691,
      "grad_norm": 1.8428277969360352,
      "learning_rate": 0.00019635727554763455,
      "loss": 0.1961,
      "step": 4363
    },
    {
      "epoch": 0.020527390236789372,
      "grad_norm": 1.123007893562317,
      "learning_rate": 0.00019635633256952106,
      "loss": 0.0565,
      "step": 4364
    },
    {
      "epoch": 0.020532094038401838,
      "grad_norm": 0.7502435445785522,
      "learning_rate": 0.0001963553895914076,
      "loss": 0.0772,
      "step": 4365
    },
    {
      "epoch": 0.0205367978400143,
      "grad_norm": 2.682567834854126,
      "learning_rate": 0.00019635444661329413,
      "loss": 0.3377,
      "step": 4366
    },
    {
      "epoch": 0.020541501641626762,
      "grad_norm": 1.9616830348968506,
      "learning_rate": 0.00019635350363518065,
      "loss": 0.1959,
      "step": 4367
    },
    {
      "epoch": 0.020546205443239225,
      "grad_norm": 0.3345967233181,
      "learning_rate": 0.00019635256065706717,
      "loss": 0.0422,
      "step": 4368
    },
    {
      "epoch": 0.02055090924485169,
      "grad_norm": 7.158527374267578,
      "learning_rate": 0.00019635161767895366,
      "loss": 0.9044,
      "step": 4369
    },
    {
      "epoch": 0.020555613046464152,
      "grad_norm": 3.809468984603882,
      "learning_rate": 0.0001963506747008402,
      "loss": 0.5791,
      "step": 4370
    },
    {
      "epoch": 0.020560316848076614,
      "grad_norm": 1.3559906482696533,
      "learning_rate": 0.00019634973172272672,
      "loss": 0.1108,
      "step": 4371
    },
    {
      "epoch": 0.02056502064968908,
      "grad_norm": 6.394443035125732,
      "learning_rate": 0.00019634878874461324,
      "loss": 1.3304,
      "step": 4372
    },
    {
      "epoch": 0.020569724451301542,
      "grad_norm": 4.464308738708496,
      "learning_rate": 0.00019634784576649976,
      "loss": 0.4967,
      "step": 4373
    },
    {
      "epoch": 0.020574428252914004,
      "grad_norm": 3.0108416080474854,
      "learning_rate": 0.0001963469027883863,
      "loss": 0.2426,
      "step": 4374
    },
    {
      "epoch": 0.020579132054526467,
      "grad_norm": 1.5654852390289307,
      "learning_rate": 0.00019634595981027282,
      "loss": 0.1927,
      "step": 4375
    },
    {
      "epoch": 0.020583835856138932,
      "grad_norm": 2.164231300354004,
      "learning_rate": 0.00019634501683215934,
      "loss": 0.2366,
      "step": 4376
    },
    {
      "epoch": 0.020588539657751394,
      "grad_norm": 3.5123658180236816,
      "learning_rate": 0.00019634407385404586,
      "loss": 0.4818,
      "step": 4377
    },
    {
      "epoch": 0.020593243459363857,
      "grad_norm": 0.6428961157798767,
      "learning_rate": 0.00019634313087593238,
      "loss": 0.075,
      "step": 4378
    },
    {
      "epoch": 0.020597947260976322,
      "grad_norm": 1.6473448276519775,
      "learning_rate": 0.0001963421878978189,
      "loss": 0.2501,
      "step": 4379
    },
    {
      "epoch": 0.020602651062588784,
      "grad_norm": 0.6513179540634155,
      "learning_rate": 0.00019634124491970542,
      "loss": 0.0381,
      "step": 4380
    },
    {
      "epoch": 0.020607354864201247,
      "grad_norm": 2.262324810028076,
      "learning_rate": 0.00019634030194159194,
      "loss": 0.3747,
      "step": 4381
    },
    {
      "epoch": 0.020612058665813712,
      "grad_norm": 1.9185380935668945,
      "learning_rate": 0.00019633935896347845,
      "loss": 0.4366,
      "step": 4382
    },
    {
      "epoch": 0.020616762467426174,
      "grad_norm": 2.7024903297424316,
      "learning_rate": 0.000196338415985365,
      "loss": 0.2281,
      "step": 4383
    },
    {
      "epoch": 0.020621466269038637,
      "grad_norm": 2.7356181144714355,
      "learning_rate": 0.00019633747300725152,
      "loss": 0.3909,
      "step": 4384
    },
    {
      "epoch": 0.0206261700706511,
      "grad_norm": 0.4324808120727539,
      "learning_rate": 0.00019633653002913804,
      "loss": 0.0414,
      "step": 4385
    },
    {
      "epoch": 0.020630873872263564,
      "grad_norm": 2.7999002933502197,
      "learning_rate": 0.00019633558705102456,
      "loss": 0.1934,
      "step": 4386
    },
    {
      "epoch": 0.020635577673876027,
      "grad_norm": 1.409603476524353,
      "learning_rate": 0.00019633464407291107,
      "loss": 0.2851,
      "step": 4387
    },
    {
      "epoch": 0.02064028147548849,
      "grad_norm": 0.9940833449363708,
      "learning_rate": 0.00019633370109479762,
      "loss": 0.1525,
      "step": 4388
    },
    {
      "epoch": 0.020644985277100954,
      "grad_norm": 1.7648311853408813,
      "learning_rate": 0.0001963327581166841,
      "loss": 0.244,
      "step": 4389
    },
    {
      "epoch": 0.020649689078713417,
      "grad_norm": 1.693056344985962,
      "learning_rate": 0.00019633181513857063,
      "loss": 0.4297,
      "step": 4390
    },
    {
      "epoch": 0.02065439288032588,
      "grad_norm": 1.4739625453948975,
      "learning_rate": 0.00019633087216045715,
      "loss": 0.1659,
      "step": 4391
    },
    {
      "epoch": 0.02065909668193834,
      "grad_norm": 2.521740198135376,
      "learning_rate": 0.0001963299291823437,
      "loss": 0.4197,
      "step": 4392
    },
    {
      "epoch": 0.020663800483550807,
      "grad_norm": 1.9394444227218628,
      "learning_rate": 0.0001963289862042302,
      "loss": 0.1626,
      "step": 4393
    },
    {
      "epoch": 0.02066850428516327,
      "grad_norm": 2.4433207511901855,
      "learning_rate": 0.00019632804322611673,
      "loss": 0.2396,
      "step": 4394
    },
    {
      "epoch": 0.02067320808677573,
      "grad_norm": 2.361887216567993,
      "learning_rate": 0.00019632710024800325,
      "loss": 0.176,
      "step": 4395
    },
    {
      "epoch": 0.020677911888388197,
      "grad_norm": 1.9140710830688477,
      "learning_rate": 0.00019632615726988977,
      "loss": 0.2759,
      "step": 4396
    },
    {
      "epoch": 0.02068261569000066,
      "grad_norm": 3.120635747909546,
      "learning_rate": 0.00019632521429177631,
      "loss": 0.6677,
      "step": 4397
    },
    {
      "epoch": 0.02068731949161312,
      "grad_norm": 2.7722647190093994,
      "learning_rate": 0.00019632427131366283,
      "loss": 0.5515,
      "step": 4398
    },
    {
      "epoch": 0.020692023293225587,
      "grad_norm": 4.074137210845947,
      "learning_rate": 0.00019632332833554935,
      "loss": 0.5983,
      "step": 4399
    },
    {
      "epoch": 0.02069672709483805,
      "grad_norm": 2.0233752727508545,
      "learning_rate": 0.00019632238535743584,
      "loss": 0.6817,
      "step": 4400
    },
    {
      "epoch": 0.02070143089645051,
      "grad_norm": 1.2979243993759155,
      "learning_rate": 0.0001963214423793224,
      "loss": 0.0987,
      "step": 4401
    },
    {
      "epoch": 0.020706134698062973,
      "grad_norm": 1.582177996635437,
      "learning_rate": 0.0001963204994012089,
      "loss": 0.1298,
      "step": 4402
    },
    {
      "epoch": 0.02071083849967544,
      "grad_norm": 4.254716396331787,
      "learning_rate": 0.00019631955642309543,
      "loss": 0.5235,
      "step": 4403
    },
    {
      "epoch": 0.0207155423012879,
      "grad_norm": 3.296135663986206,
      "learning_rate": 0.00019631861344498195,
      "loss": 0.2535,
      "step": 4404
    },
    {
      "epoch": 0.020720246102900363,
      "grad_norm": 4.475434303283691,
      "learning_rate": 0.00019631767046686846,
      "loss": 0.5544,
      "step": 4405
    },
    {
      "epoch": 0.02072494990451283,
      "grad_norm": 2.5861809253692627,
      "learning_rate": 0.000196316727488755,
      "loss": 0.0351,
      "step": 4406
    },
    {
      "epoch": 0.02072965370612529,
      "grad_norm": 1.123711347579956,
      "learning_rate": 0.00019631578451064153,
      "loss": 0.1887,
      "step": 4407
    },
    {
      "epoch": 0.020734357507737753,
      "grad_norm": 2.7874972820281982,
      "learning_rate": 0.00019631484153252805,
      "loss": 0.3569,
      "step": 4408
    },
    {
      "epoch": 0.020739061309350215,
      "grad_norm": 0.7433370351791382,
      "learning_rate": 0.00019631389855441457,
      "loss": 0.0782,
      "step": 4409
    },
    {
      "epoch": 0.02074376511096268,
      "grad_norm": 1.9568668603897095,
      "learning_rate": 0.00019631295557630108,
      "loss": 0.172,
      "step": 4410
    },
    {
      "epoch": 0.020748468912575143,
      "grad_norm": 0.613592803478241,
      "learning_rate": 0.0001963120125981876,
      "loss": 0.0555,
      "step": 4411
    },
    {
      "epoch": 0.020753172714187605,
      "grad_norm": 3.8674912452697754,
      "learning_rate": 0.00019631106962007412,
      "loss": 0.5176,
      "step": 4412
    },
    {
      "epoch": 0.02075787651580007,
      "grad_norm": 1.4078996181488037,
      "learning_rate": 0.00019631012664196064,
      "loss": 0.0895,
      "step": 4413
    },
    {
      "epoch": 0.020762580317412533,
      "grad_norm": 0.7020525932312012,
      "learning_rate": 0.00019630918366384716,
      "loss": 0.0719,
      "step": 4414
    },
    {
      "epoch": 0.020767284119024995,
      "grad_norm": 1.9292136430740356,
      "learning_rate": 0.0001963082406857337,
      "loss": 0.4324,
      "step": 4415
    },
    {
      "epoch": 0.02077198792063746,
      "grad_norm": 2.9117534160614014,
      "learning_rate": 0.00019630729770762022,
      "loss": 0.448,
      "step": 4416
    },
    {
      "epoch": 0.020776691722249923,
      "grad_norm": 2.7788119316101074,
      "learning_rate": 0.00019630635472950674,
      "loss": 0.3782,
      "step": 4417
    },
    {
      "epoch": 0.020781395523862385,
      "grad_norm": 0.8997746109962463,
      "learning_rate": 0.00019630541175139326,
      "loss": 0.0656,
      "step": 4418
    },
    {
      "epoch": 0.020786099325474847,
      "grad_norm": 1.5479912757873535,
      "learning_rate": 0.00019630446877327978,
      "loss": 0.3575,
      "step": 4419
    },
    {
      "epoch": 0.020790803127087313,
      "grad_norm": 2.85070538520813,
      "learning_rate": 0.0001963035257951663,
      "loss": 0.4929,
      "step": 4420
    },
    {
      "epoch": 0.020795506928699775,
      "grad_norm": 0.4510346055030823,
      "learning_rate": 0.00019630258281705282,
      "loss": 0.0537,
      "step": 4421
    },
    {
      "epoch": 0.020800210730312237,
      "grad_norm": 2.102341413497925,
      "learning_rate": 0.00019630163983893934,
      "loss": 0.2754,
      "step": 4422
    },
    {
      "epoch": 0.020804914531924703,
      "grad_norm": 2.2367866039276123,
      "learning_rate": 0.00019630069686082585,
      "loss": 0.3078,
      "step": 4423
    },
    {
      "epoch": 0.020809618333537165,
      "grad_norm": 2.168379068374634,
      "learning_rate": 0.0001962997538827124,
      "loss": 0.5618,
      "step": 4424
    },
    {
      "epoch": 0.020814322135149627,
      "grad_norm": 1.2299211025238037,
      "learning_rate": 0.00019629881090459892,
      "loss": 0.175,
      "step": 4425
    },
    {
      "epoch": 0.02081902593676209,
      "grad_norm": 1.3108030557632446,
      "learning_rate": 0.00019629786792648544,
      "loss": 0.1902,
      "step": 4426
    },
    {
      "epoch": 0.020823729738374555,
      "grad_norm": 1.509584665298462,
      "learning_rate": 0.00019629692494837196,
      "loss": 0.194,
      "step": 4427
    },
    {
      "epoch": 0.020828433539987017,
      "grad_norm": 1.3882390260696411,
      "learning_rate": 0.00019629598197025847,
      "loss": 0.1222,
      "step": 4428
    },
    {
      "epoch": 0.02083313734159948,
      "grad_norm": 1.5045877695083618,
      "learning_rate": 0.00019629503899214502,
      "loss": 0.1536,
      "step": 4429
    },
    {
      "epoch": 0.020837841143211945,
      "grad_norm": 0.45507219433784485,
      "learning_rate": 0.00019629409601403154,
      "loss": 0.04,
      "step": 4430
    },
    {
      "epoch": 0.020842544944824407,
      "grad_norm": 3.0610244274139404,
      "learning_rate": 0.00019629315303591803,
      "loss": 0.9179,
      "step": 4431
    },
    {
      "epoch": 0.02084724874643687,
      "grad_norm": 1.1731175184249878,
      "learning_rate": 0.00019629221005780455,
      "loss": 0.1746,
      "step": 4432
    },
    {
      "epoch": 0.020851952548049335,
      "grad_norm": 1.633523941040039,
      "learning_rate": 0.0001962912670796911,
      "loss": 0.1791,
      "step": 4433
    },
    {
      "epoch": 0.020856656349661797,
      "grad_norm": 1.9824237823486328,
      "learning_rate": 0.0001962903241015776,
      "loss": 0.5385,
      "step": 4434
    },
    {
      "epoch": 0.02086136015127426,
      "grad_norm": 3.645282745361328,
      "learning_rate": 0.00019628938112346413,
      "loss": 0.4278,
      "step": 4435
    },
    {
      "epoch": 0.02086606395288672,
      "grad_norm": 1.1366126537322998,
      "learning_rate": 0.00019628843814535065,
      "loss": 0.135,
      "step": 4436
    },
    {
      "epoch": 0.020870767754499187,
      "grad_norm": 0.23812514543533325,
      "learning_rate": 0.00019628749516723717,
      "loss": 0.0185,
      "step": 4437
    },
    {
      "epoch": 0.02087547155611165,
      "grad_norm": 0.7746367454528809,
      "learning_rate": 0.00019628655218912371,
      "loss": 0.1024,
      "step": 4438
    },
    {
      "epoch": 0.02088017535772411,
      "grad_norm": 11.023815155029297,
      "learning_rate": 0.00019628560921101023,
      "loss": 0.7026,
      "step": 4439
    },
    {
      "epoch": 0.020884879159336577,
      "grad_norm": 2.8475682735443115,
      "learning_rate": 0.00019628466623289675,
      "loss": 0.436,
      "step": 4440
    },
    {
      "epoch": 0.02088958296094904,
      "grad_norm": 1.051489233970642,
      "learning_rate": 0.00019628372325478327,
      "loss": 0.0696,
      "step": 4441
    },
    {
      "epoch": 0.0208942867625615,
      "grad_norm": 1.069342017173767,
      "learning_rate": 0.0001962827802766698,
      "loss": 0.1044,
      "step": 4442
    },
    {
      "epoch": 0.020898990564173964,
      "grad_norm": 0.8621953129768372,
      "learning_rate": 0.0001962818372985563,
      "loss": 0.0757,
      "step": 4443
    },
    {
      "epoch": 0.02090369436578643,
      "grad_norm": 1.8018951416015625,
      "learning_rate": 0.00019628089432044283,
      "loss": 0.2553,
      "step": 4444
    },
    {
      "epoch": 0.02090839816739889,
      "grad_norm": 0.80714350938797,
      "learning_rate": 0.00019627995134232935,
      "loss": 0.068,
      "step": 4445
    },
    {
      "epoch": 0.020913101969011354,
      "grad_norm": 1.8901814222335815,
      "learning_rate": 0.00019627900836421586,
      "loss": 0.2711,
      "step": 4446
    },
    {
      "epoch": 0.02091780577062382,
      "grad_norm": 1.134049892425537,
      "learning_rate": 0.0001962780653861024,
      "loss": 0.081,
      "step": 4447
    },
    {
      "epoch": 0.02092250957223628,
      "grad_norm": 1.0641692876815796,
      "learning_rate": 0.00019627712240798893,
      "loss": 0.0659,
      "step": 4448
    },
    {
      "epoch": 0.020927213373848744,
      "grad_norm": 0.6992411017417908,
      "learning_rate": 0.00019627617942987545,
      "loss": 0.0634,
      "step": 4449
    },
    {
      "epoch": 0.02093191717546121,
      "grad_norm": 2.7781476974487305,
      "learning_rate": 0.00019627523645176197,
      "loss": 0.3095,
      "step": 4450
    },
    {
      "epoch": 0.02093662097707367,
      "grad_norm": 6.84342098236084,
      "learning_rate": 0.00019627429347364848,
      "loss": 0.562,
      "step": 4451
    },
    {
      "epoch": 0.020941324778686134,
      "grad_norm": 0.6346099972724915,
      "learning_rate": 0.000196273350495535,
      "loss": 0.0222,
      "step": 4452
    },
    {
      "epoch": 0.020946028580298596,
      "grad_norm": 0.2851752042770386,
      "learning_rate": 0.00019627240751742152,
      "loss": 0.0152,
      "step": 4453
    },
    {
      "epoch": 0.02095073238191106,
      "grad_norm": 0.18940706551074982,
      "learning_rate": 0.00019627146453930804,
      "loss": 0.0126,
      "step": 4454
    },
    {
      "epoch": 0.020955436183523524,
      "grad_norm": 6.4786200523376465,
      "learning_rate": 0.00019627052156119456,
      "loss": 0.5366,
      "step": 4455
    },
    {
      "epoch": 0.020960139985135986,
      "grad_norm": 3.0835671424865723,
      "learning_rate": 0.0001962695785830811,
      "loss": 0.1846,
      "step": 4456
    },
    {
      "epoch": 0.02096484378674845,
      "grad_norm": 1.84814453125,
      "learning_rate": 0.00019626863560496762,
      "loss": 0.0716,
      "step": 4457
    },
    {
      "epoch": 0.020969547588360914,
      "grad_norm": 3.3012518882751465,
      "learning_rate": 0.00019626769262685414,
      "loss": 0.2419,
      "step": 4458
    },
    {
      "epoch": 0.020974251389973376,
      "grad_norm": 4.959433078765869,
      "learning_rate": 0.00019626674964874066,
      "loss": 0.2666,
      "step": 4459
    },
    {
      "epoch": 0.020978955191585838,
      "grad_norm": 4.483750343322754,
      "learning_rate": 0.0001962658066706272,
      "loss": 0.6891,
      "step": 4460
    },
    {
      "epoch": 0.020983658993198304,
      "grad_norm": 0.15944921970367432,
      "learning_rate": 0.00019626486369251372,
      "loss": 0.012,
      "step": 4461
    },
    {
      "epoch": 0.020988362794810766,
      "grad_norm": 2.18810772895813,
      "learning_rate": 0.00019626392071440022,
      "loss": 0.0841,
      "step": 4462
    },
    {
      "epoch": 0.020993066596423228,
      "grad_norm": 0.5308985114097595,
      "learning_rate": 0.00019626297773628674,
      "loss": 0.0308,
      "step": 4463
    },
    {
      "epoch": 0.020997770398035694,
      "grad_norm": 3.5217905044555664,
      "learning_rate": 0.00019626203475817325,
      "loss": 0.535,
      "step": 4464
    },
    {
      "epoch": 0.021002474199648156,
      "grad_norm": 5.060382843017578,
      "learning_rate": 0.0001962610917800598,
      "loss": 0.6255,
      "step": 4465
    },
    {
      "epoch": 0.021007178001260618,
      "grad_norm": 3.110945224761963,
      "learning_rate": 0.00019626014880194632,
      "loss": 0.1643,
      "step": 4466
    },
    {
      "epoch": 0.021011881802873084,
      "grad_norm": 3.1445071697235107,
      "learning_rate": 0.00019625920582383284,
      "loss": 0.3912,
      "step": 4467
    },
    {
      "epoch": 0.021016585604485546,
      "grad_norm": 1.3167531490325928,
      "learning_rate": 0.00019625826284571936,
      "loss": 0.1033,
      "step": 4468
    },
    {
      "epoch": 0.021021289406098008,
      "grad_norm": 2.4476253986358643,
      "learning_rate": 0.0001962573198676059,
      "loss": 0.382,
      "step": 4469
    },
    {
      "epoch": 0.02102599320771047,
      "grad_norm": 3.620309591293335,
      "learning_rate": 0.00019625637688949242,
      "loss": 0.2432,
      "step": 4470
    },
    {
      "epoch": 0.021030697009322936,
      "grad_norm": 3.3807854652404785,
      "learning_rate": 0.00019625543391137894,
      "loss": 0.1944,
      "step": 4471
    },
    {
      "epoch": 0.021035400810935398,
      "grad_norm": 3.6623928546905518,
      "learning_rate": 0.00019625449093326546,
      "loss": 0.2325,
      "step": 4472
    },
    {
      "epoch": 0.02104010461254786,
      "grad_norm": 2.79864501953125,
      "learning_rate": 0.00019625354795515195,
      "loss": 0.7016,
      "step": 4473
    },
    {
      "epoch": 0.021044808414160326,
      "grad_norm": 2.488577127456665,
      "learning_rate": 0.0001962526049770385,
      "loss": 0.2649,
      "step": 4474
    },
    {
      "epoch": 0.021049512215772788,
      "grad_norm": 1.0728362798690796,
      "learning_rate": 0.000196251661998925,
      "loss": 0.0967,
      "step": 4475
    },
    {
      "epoch": 0.02105421601738525,
      "grad_norm": 2.271468162536621,
      "learning_rate": 0.00019625071902081153,
      "loss": 0.2335,
      "step": 4476
    },
    {
      "epoch": 0.021058919818997712,
      "grad_norm": 3.1056904792785645,
      "learning_rate": 0.00019624977604269805,
      "loss": 0.8074,
      "step": 4477
    },
    {
      "epoch": 0.021063623620610178,
      "grad_norm": 0.3651047945022583,
      "learning_rate": 0.00019624883306458457,
      "loss": 0.0261,
      "step": 4478
    },
    {
      "epoch": 0.02106832742222264,
      "grad_norm": 2.510538101196289,
      "learning_rate": 0.00019624789008647111,
      "loss": 0.2767,
      "step": 4479
    },
    {
      "epoch": 0.021073031223835102,
      "grad_norm": 2.248988151550293,
      "learning_rate": 0.00019624694710835763,
      "loss": 0.1767,
      "step": 4480
    },
    {
      "epoch": 0.021077735025447568,
      "grad_norm": 1.1366671323776245,
      "learning_rate": 0.00019624600413024415,
      "loss": 0.2056,
      "step": 4481
    },
    {
      "epoch": 0.02108243882706003,
      "grad_norm": 4.105493068695068,
      "learning_rate": 0.00019624506115213067,
      "loss": 0.3906,
      "step": 4482
    },
    {
      "epoch": 0.021087142628672492,
      "grad_norm": 2.0076255798339844,
      "learning_rate": 0.0001962441181740172,
      "loss": 0.3709,
      "step": 4483
    },
    {
      "epoch": 0.021091846430284958,
      "grad_norm": 3.590902328491211,
      "learning_rate": 0.0001962431751959037,
      "loss": 0.6357,
      "step": 4484
    },
    {
      "epoch": 0.02109655023189742,
      "grad_norm": 1.7553915977478027,
      "learning_rate": 0.00019624223221779023,
      "loss": 0.3247,
      "step": 4485
    },
    {
      "epoch": 0.021101254033509882,
      "grad_norm": 3.4336094856262207,
      "learning_rate": 0.00019624128923967675,
      "loss": 0.4658,
      "step": 4486
    },
    {
      "epoch": 0.021105957835122344,
      "grad_norm": 5.121291160583496,
      "learning_rate": 0.00019624034626156326,
      "loss": 0.4833,
      "step": 4487
    },
    {
      "epoch": 0.02111066163673481,
      "grad_norm": 2.220888614654541,
      "learning_rate": 0.0001962394032834498,
      "loss": 0.3617,
      "step": 4488
    },
    {
      "epoch": 0.021115365438347272,
      "grad_norm": 3.2961225509643555,
      "learning_rate": 0.00019623846030533633,
      "loss": 0.5447,
      "step": 4489
    },
    {
      "epoch": 0.021120069239959734,
      "grad_norm": 6.1842827796936035,
      "learning_rate": 0.00019623751732722285,
      "loss": 0.3392,
      "step": 4490
    },
    {
      "epoch": 0.0211247730415722,
      "grad_norm": 0.32227975130081177,
      "learning_rate": 0.00019623657434910937,
      "loss": 0.0313,
      "step": 4491
    },
    {
      "epoch": 0.021129476843184662,
      "grad_norm": 0.6846120953559875,
      "learning_rate": 0.0001962356313709959,
      "loss": 0.0721,
      "step": 4492
    },
    {
      "epoch": 0.021134180644797124,
      "grad_norm": 1.1489875316619873,
      "learning_rate": 0.0001962346883928824,
      "loss": 0.1543,
      "step": 4493
    },
    {
      "epoch": 0.021138884446409587,
      "grad_norm": 1.8819589614868164,
      "learning_rate": 0.00019623374541476892,
      "loss": 0.383,
      "step": 4494
    },
    {
      "epoch": 0.021143588248022052,
      "grad_norm": 0.7272980809211731,
      "learning_rate": 0.00019623280243665544,
      "loss": 0.129,
      "step": 4495
    },
    {
      "epoch": 0.021148292049634514,
      "grad_norm": 0.6100497841835022,
      "learning_rate": 0.00019623185945854196,
      "loss": 0.0607,
      "step": 4496
    },
    {
      "epoch": 0.021152995851246977,
      "grad_norm": 4.706968307495117,
      "learning_rate": 0.0001962309164804285,
      "loss": 0.5459,
      "step": 4497
    },
    {
      "epoch": 0.021157699652859442,
      "grad_norm": 1.0362913608551025,
      "learning_rate": 0.00019622997350231502,
      "loss": 0.1563,
      "step": 4498
    },
    {
      "epoch": 0.021162403454471904,
      "grad_norm": 0.7906981706619263,
      "learning_rate": 0.00019622903052420154,
      "loss": 0.1169,
      "step": 4499
    },
    {
      "epoch": 0.021167107256084366,
      "grad_norm": 2.132072687149048,
      "learning_rate": 0.00019622808754608806,
      "loss": 0.2374,
      "step": 4500
    },
    {
      "epoch": 0.021171811057696832,
      "grad_norm": 3.346674919128418,
      "learning_rate": 0.0001962271445679746,
      "loss": 0.1506,
      "step": 4501
    },
    {
      "epoch": 0.021176514859309294,
      "grad_norm": 2.8975119590759277,
      "learning_rate": 0.00019622620158986112,
      "loss": 0.4589,
      "step": 4502
    },
    {
      "epoch": 0.021181218660921756,
      "grad_norm": 1.756274700164795,
      "learning_rate": 0.00019622525861174764,
      "loss": 0.186,
      "step": 4503
    },
    {
      "epoch": 0.02118592246253422,
      "grad_norm": 2.6376068592071533,
      "learning_rate": 0.00019622431563363414,
      "loss": 0.3742,
      "step": 4504
    },
    {
      "epoch": 0.021190626264146684,
      "grad_norm": 3.693586587905884,
      "learning_rate": 0.00019622337265552065,
      "loss": 0.4238,
      "step": 4505
    },
    {
      "epoch": 0.021195330065759146,
      "grad_norm": 0.3922056555747986,
      "learning_rate": 0.0001962224296774072,
      "loss": 0.049,
      "step": 4506
    },
    {
      "epoch": 0.02120003386737161,
      "grad_norm": 3.3756325244903564,
      "learning_rate": 0.00019622148669929372,
      "loss": 0.4852,
      "step": 4507
    },
    {
      "epoch": 0.021204737668984074,
      "grad_norm": 1.7900291681289673,
      "learning_rate": 0.00019622054372118024,
      "loss": 0.1826,
      "step": 4508
    },
    {
      "epoch": 0.021209441470596536,
      "grad_norm": 1.7328518629074097,
      "learning_rate": 0.00019621960074306676,
      "loss": 0.1442,
      "step": 4509
    },
    {
      "epoch": 0.021214145272209,
      "grad_norm": 0.9538343548774719,
      "learning_rate": 0.0001962186577649533,
      "loss": 0.0853,
      "step": 4510
    },
    {
      "epoch": 0.02121884907382146,
      "grad_norm": 1.401300072669983,
      "learning_rate": 0.00019621771478683982,
      "loss": 0.1699,
      "step": 4511
    },
    {
      "epoch": 0.021223552875433926,
      "grad_norm": 1.7626605033874512,
      "learning_rate": 0.00019621677180872634,
      "loss": 0.2482,
      "step": 4512
    },
    {
      "epoch": 0.02122825667704639,
      "grad_norm": 1.948533296585083,
      "learning_rate": 0.00019621582883061286,
      "loss": 0.1573,
      "step": 4513
    },
    {
      "epoch": 0.02123296047865885,
      "grad_norm": 0.47026288509368896,
      "learning_rate": 0.00019621488585249938,
      "loss": 0.0397,
      "step": 4514
    },
    {
      "epoch": 0.021237664280271316,
      "grad_norm": 0.33550557494163513,
      "learning_rate": 0.0001962139428743859,
      "loss": 0.0402,
      "step": 4515
    },
    {
      "epoch": 0.02124236808188378,
      "grad_norm": 3.316598653793335,
      "learning_rate": 0.0001962129998962724,
      "loss": 0.5753,
      "step": 4516
    },
    {
      "epoch": 0.02124707188349624,
      "grad_norm": 4.663318634033203,
      "learning_rate": 0.00019621205691815893,
      "loss": 0.5809,
      "step": 4517
    },
    {
      "epoch": 0.021251775685108706,
      "grad_norm": 3.840303659439087,
      "learning_rate": 0.00019621111394004545,
      "loss": 0.5839,
      "step": 4518
    },
    {
      "epoch": 0.02125647948672117,
      "grad_norm": 4.4796953201293945,
      "learning_rate": 0.000196210170961932,
      "loss": 0.8792,
      "step": 4519
    },
    {
      "epoch": 0.02126118328833363,
      "grad_norm": 1.1359939575195312,
      "learning_rate": 0.00019620922798381851,
      "loss": 0.0753,
      "step": 4520
    },
    {
      "epoch": 0.021265887089946093,
      "grad_norm": 0.3303976058959961,
      "learning_rate": 0.00019620828500570503,
      "loss": 0.0386,
      "step": 4521
    },
    {
      "epoch": 0.02127059089155856,
      "grad_norm": 1.0223909616470337,
      "learning_rate": 0.00019620734202759155,
      "loss": 0.1186,
      "step": 4522
    },
    {
      "epoch": 0.02127529469317102,
      "grad_norm": 4.459046840667725,
      "learning_rate": 0.00019620639904947807,
      "loss": 0.6104,
      "step": 4523
    },
    {
      "epoch": 0.021279998494783483,
      "grad_norm": 4.464503288269043,
      "learning_rate": 0.0001962054560713646,
      "loss": 0.5055,
      "step": 4524
    },
    {
      "epoch": 0.02128470229639595,
      "grad_norm": 0.27329137921333313,
      "learning_rate": 0.0001962045130932511,
      "loss": 0.0276,
      "step": 4525
    },
    {
      "epoch": 0.02128940609800841,
      "grad_norm": 1.142862319946289,
      "learning_rate": 0.00019620357011513763,
      "loss": 0.1345,
      "step": 4526
    },
    {
      "epoch": 0.021294109899620873,
      "grad_norm": 2.4097628593444824,
      "learning_rate": 0.00019620262713702415,
      "loss": 0.2976,
      "step": 4527
    },
    {
      "epoch": 0.021298813701233335,
      "grad_norm": 3.247720718383789,
      "learning_rate": 0.00019620168415891066,
      "loss": 0.3913,
      "step": 4528
    },
    {
      "epoch": 0.0213035175028458,
      "grad_norm": 3.518439292907715,
      "learning_rate": 0.0001962007411807972,
      "loss": 0.7211,
      "step": 4529
    },
    {
      "epoch": 0.021308221304458263,
      "grad_norm": 4.5636491775512695,
      "learning_rate": 0.00019619979820268373,
      "loss": 0.647,
      "step": 4530
    },
    {
      "epoch": 0.021312925106070725,
      "grad_norm": 0.5865355134010315,
      "learning_rate": 0.00019619885522457025,
      "loss": 0.0505,
      "step": 4531
    },
    {
      "epoch": 0.02131762890768319,
      "grad_norm": 1.8053407669067383,
      "learning_rate": 0.00019619791224645677,
      "loss": 0.1384,
      "step": 4532
    },
    {
      "epoch": 0.021322332709295653,
      "grad_norm": 1.1455360651016235,
      "learning_rate": 0.0001961969692683433,
      "loss": 0.0838,
      "step": 4533
    },
    {
      "epoch": 0.021327036510908115,
      "grad_norm": 2.27945613861084,
      "learning_rate": 0.00019619602629022983,
      "loss": 0.4259,
      "step": 4534
    },
    {
      "epoch": 0.02133174031252058,
      "grad_norm": 2.487373113632202,
      "learning_rate": 0.00019619508331211632,
      "loss": 0.3566,
      "step": 4535
    },
    {
      "epoch": 0.021336444114133043,
      "grad_norm": 4.879042148590088,
      "learning_rate": 0.00019619414033400284,
      "loss": 0.829,
      "step": 4536
    },
    {
      "epoch": 0.021341147915745505,
      "grad_norm": 2.1973860263824463,
      "learning_rate": 0.00019619319735588936,
      "loss": 0.3918,
      "step": 4537
    },
    {
      "epoch": 0.021345851717357967,
      "grad_norm": 2.3827240467071533,
      "learning_rate": 0.0001961922543777759,
      "loss": 0.2294,
      "step": 4538
    },
    {
      "epoch": 0.021350555518970433,
      "grad_norm": 0.27179813385009766,
      "learning_rate": 0.00019619131139966242,
      "loss": 0.0302,
      "step": 4539
    },
    {
      "epoch": 0.021355259320582895,
      "grad_norm": 0.7309192419052124,
      "learning_rate": 0.00019619036842154894,
      "loss": 0.0803,
      "step": 4540
    },
    {
      "epoch": 0.021359963122195357,
      "grad_norm": 1.5611214637756348,
      "learning_rate": 0.00019618942544343546,
      "loss": 0.1278,
      "step": 4541
    },
    {
      "epoch": 0.021364666923807823,
      "grad_norm": 1.7427157163619995,
      "learning_rate": 0.000196188482465322,
      "loss": 0.2689,
      "step": 4542
    },
    {
      "epoch": 0.021369370725420285,
      "grad_norm": 2.275043249130249,
      "learning_rate": 0.00019618753948720852,
      "loss": 0.1652,
      "step": 4543
    },
    {
      "epoch": 0.021374074527032747,
      "grad_norm": 0.4443422555923462,
      "learning_rate": 0.00019618659650909504,
      "loss": 0.0347,
      "step": 4544
    },
    {
      "epoch": 0.02137877832864521,
      "grad_norm": 1.6725049018859863,
      "learning_rate": 0.00019618565353098156,
      "loss": 0.317,
      "step": 4545
    },
    {
      "epoch": 0.021383482130257675,
      "grad_norm": 2.555896043777466,
      "learning_rate": 0.00019618471055286808,
      "loss": 0.191,
      "step": 4546
    },
    {
      "epoch": 0.021388185931870137,
      "grad_norm": 2.442521333694458,
      "learning_rate": 0.0001961837675747546,
      "loss": 0.637,
      "step": 4547
    },
    {
      "epoch": 0.0213928897334826,
      "grad_norm": 1.4891904592514038,
      "learning_rate": 0.00019618282459664112,
      "loss": 0.3921,
      "step": 4548
    },
    {
      "epoch": 0.021397593535095065,
      "grad_norm": 1.9957784414291382,
      "learning_rate": 0.00019618188161852764,
      "loss": 0.3927,
      "step": 4549
    },
    {
      "epoch": 0.021402297336707527,
      "grad_norm": 1.1068902015686035,
      "learning_rate": 0.00019618093864041416,
      "loss": 0.2239,
      "step": 4550
    },
    {
      "epoch": 0.02140700113831999,
      "grad_norm": 1.0810730457305908,
      "learning_rate": 0.0001961799956623007,
      "loss": 0.0669,
      "step": 4551
    },
    {
      "epoch": 0.021411704939932455,
      "grad_norm": 2.999788522720337,
      "learning_rate": 0.00019617905268418722,
      "loss": 0.2219,
      "step": 4552
    },
    {
      "epoch": 0.021416408741544917,
      "grad_norm": 1.9134032726287842,
      "learning_rate": 0.00019617810970607374,
      "loss": 0.1542,
      "step": 4553
    },
    {
      "epoch": 0.02142111254315738,
      "grad_norm": 1.165061354637146,
      "learning_rate": 0.00019617716672796026,
      "loss": 0.0695,
      "step": 4554
    },
    {
      "epoch": 0.02142581634476984,
      "grad_norm": 3.304461717605591,
      "learning_rate": 0.00019617622374984678,
      "loss": 0.3653,
      "step": 4555
    },
    {
      "epoch": 0.021430520146382307,
      "grad_norm": 0.8263144493103027,
      "learning_rate": 0.0001961752807717333,
      "loss": 0.0627,
      "step": 4556
    },
    {
      "epoch": 0.02143522394799477,
      "grad_norm": 1.636289358139038,
      "learning_rate": 0.0001961743377936198,
      "loss": 0.1041,
      "step": 4557
    },
    {
      "epoch": 0.02143992774960723,
      "grad_norm": 3.005772352218628,
      "learning_rate": 0.00019617339481550633,
      "loss": 0.2628,
      "step": 4558
    },
    {
      "epoch": 0.021444631551219697,
      "grad_norm": 1.5105611085891724,
      "learning_rate": 0.00019617245183739285,
      "loss": 0.0947,
      "step": 4559
    },
    {
      "epoch": 0.02144933535283216,
      "grad_norm": 5.626248836517334,
      "learning_rate": 0.0001961715088592794,
      "loss": 1.2398,
      "step": 4560
    },
    {
      "epoch": 0.02145403915444462,
      "grad_norm": 1.46407949924469,
      "learning_rate": 0.00019617056588116591,
      "loss": 0.0935,
      "step": 4561
    },
    {
      "epoch": 0.021458742956057087,
      "grad_norm": 0.5167157649993896,
      "learning_rate": 0.00019616962290305243,
      "loss": 0.0501,
      "step": 4562
    },
    {
      "epoch": 0.02146344675766955,
      "grad_norm": 3.8770177364349365,
      "learning_rate": 0.00019616867992493895,
      "loss": 0.5358,
      "step": 4563
    },
    {
      "epoch": 0.02146815055928201,
      "grad_norm": 1.645736813545227,
      "learning_rate": 0.00019616773694682547,
      "loss": 0.1557,
      "step": 4564
    },
    {
      "epoch": 0.021472854360894474,
      "grad_norm": 4.249980926513672,
      "learning_rate": 0.00019616679396871202,
      "loss": 0.4003,
      "step": 4565
    },
    {
      "epoch": 0.02147755816250694,
      "grad_norm": 1.9429290294647217,
      "learning_rate": 0.0001961658509905985,
      "loss": 0.1479,
      "step": 4566
    },
    {
      "epoch": 0.0214822619641194,
      "grad_norm": 4.843775749206543,
      "learning_rate": 0.00019616490801248503,
      "loss": 0.6736,
      "step": 4567
    },
    {
      "epoch": 0.021486965765731864,
      "grad_norm": 3.319066286087036,
      "learning_rate": 0.00019616396503437155,
      "loss": 0.4626,
      "step": 4568
    },
    {
      "epoch": 0.02149166956734433,
      "grad_norm": 0.2027023881673813,
      "learning_rate": 0.0001961630220562581,
      "loss": 0.0124,
      "step": 4569
    },
    {
      "epoch": 0.02149637336895679,
      "grad_norm": 0.16056399047374725,
      "learning_rate": 0.0001961620790781446,
      "loss": 0.0091,
      "step": 4570
    },
    {
      "epoch": 0.021501077170569254,
      "grad_norm": 4.587286472320557,
      "learning_rate": 0.00019616113610003113,
      "loss": 0.7324,
      "step": 4571
    },
    {
      "epoch": 0.021505780972181716,
      "grad_norm": 5.197255611419678,
      "learning_rate": 0.00019616019312191765,
      "loss": 0.4327,
      "step": 4572
    },
    {
      "epoch": 0.02151048477379418,
      "grad_norm": 3.072143793106079,
      "learning_rate": 0.00019615925014380417,
      "loss": 0.4451,
      "step": 4573
    },
    {
      "epoch": 0.021515188575406644,
      "grad_norm": 3.6070845127105713,
      "learning_rate": 0.0001961583071656907,
      "loss": 0.5827,
      "step": 4574
    },
    {
      "epoch": 0.021519892377019106,
      "grad_norm": 1.9113900661468506,
      "learning_rate": 0.00019615736418757723,
      "loss": 0.1809,
      "step": 4575
    },
    {
      "epoch": 0.02152459617863157,
      "grad_norm": 0.9167594313621521,
      "learning_rate": 0.00019615642120946375,
      "loss": 0.0874,
      "step": 4576
    },
    {
      "epoch": 0.021529299980244034,
      "grad_norm": 2.5315215587615967,
      "learning_rate": 0.00019615547823135027,
      "loss": 0.2123,
      "step": 4577
    },
    {
      "epoch": 0.021534003781856496,
      "grad_norm": 2.057894706726074,
      "learning_rate": 0.00019615453525323676,
      "loss": 0.1827,
      "step": 4578
    },
    {
      "epoch": 0.02153870758346896,
      "grad_norm": 1.2224595546722412,
      "learning_rate": 0.0001961535922751233,
      "loss": 0.1554,
      "step": 4579
    },
    {
      "epoch": 0.021543411385081424,
      "grad_norm": 0.242233544588089,
      "learning_rate": 0.00019615264929700982,
      "loss": 0.0189,
      "step": 4580
    },
    {
      "epoch": 0.021548115186693886,
      "grad_norm": 1.412903904914856,
      "learning_rate": 0.00019615170631889634,
      "loss": 0.1999,
      "step": 4581
    },
    {
      "epoch": 0.021552818988306348,
      "grad_norm": 0.527971088886261,
      "learning_rate": 0.00019615076334078286,
      "loss": 0.0712,
      "step": 4582
    },
    {
      "epoch": 0.021557522789918813,
      "grad_norm": 1.331570029258728,
      "learning_rate": 0.0001961498203626694,
      "loss": 0.1405,
      "step": 4583
    },
    {
      "epoch": 0.021562226591531276,
      "grad_norm": 0.6194980144500732,
      "learning_rate": 0.00019614887738455592,
      "loss": 0.0458,
      "step": 4584
    },
    {
      "epoch": 0.021566930393143738,
      "grad_norm": 0.7192552089691162,
      "learning_rate": 0.00019614793440644244,
      "loss": 0.067,
      "step": 4585
    },
    {
      "epoch": 0.021571634194756203,
      "grad_norm": 2.167605400085449,
      "learning_rate": 0.00019614699142832896,
      "loss": 0.2985,
      "step": 4586
    },
    {
      "epoch": 0.021576337996368666,
      "grad_norm": 0.5046941041946411,
      "learning_rate": 0.00019614604845021548,
      "loss": 0.055,
      "step": 4587
    },
    {
      "epoch": 0.021581041797981128,
      "grad_norm": 1.0101020336151123,
      "learning_rate": 0.000196145105472102,
      "loss": 0.0917,
      "step": 4588
    },
    {
      "epoch": 0.02158574559959359,
      "grad_norm": 1.9462627172470093,
      "learning_rate": 0.00019614416249398852,
      "loss": 0.3028,
      "step": 4589
    },
    {
      "epoch": 0.021590449401206056,
      "grad_norm": 0.8273681402206421,
      "learning_rate": 0.00019614321951587504,
      "loss": 0.0503,
      "step": 4590
    },
    {
      "epoch": 0.021595153202818518,
      "grad_norm": 1.4989241361618042,
      "learning_rate": 0.00019614227653776156,
      "loss": 0.1733,
      "step": 4591
    },
    {
      "epoch": 0.02159985700443098,
      "grad_norm": 3.016629457473755,
      "learning_rate": 0.0001961413335596481,
      "loss": 0.5195,
      "step": 4592
    },
    {
      "epoch": 0.021604560806043446,
      "grad_norm": 3.0079140663146973,
      "learning_rate": 0.00019614039058153462,
      "loss": 0.6664,
      "step": 4593
    },
    {
      "epoch": 0.021609264607655908,
      "grad_norm": 2.956928014755249,
      "learning_rate": 0.00019613944760342114,
      "loss": 0.2639,
      "step": 4594
    },
    {
      "epoch": 0.02161396840926837,
      "grad_norm": 0.7788374423980713,
      "learning_rate": 0.00019613850462530766,
      "loss": 0.067,
      "step": 4595
    },
    {
      "epoch": 0.021618672210880836,
      "grad_norm": 1.7935218811035156,
      "learning_rate": 0.00019613756164719418,
      "loss": 0.1128,
      "step": 4596
    },
    {
      "epoch": 0.021623376012493298,
      "grad_norm": 0.3992706835269928,
      "learning_rate": 0.0001961366186690807,
      "loss": 0.0282,
      "step": 4597
    },
    {
      "epoch": 0.02162807981410576,
      "grad_norm": 2.765158176422119,
      "learning_rate": 0.0001961356756909672,
      "loss": 0.4844,
      "step": 4598
    },
    {
      "epoch": 0.021632783615718222,
      "grad_norm": 4.933047294616699,
      "learning_rate": 0.00019613473271285373,
      "loss": 0.7337,
      "step": 4599
    },
    {
      "epoch": 0.021637487417330688,
      "grad_norm": 1.1676621437072754,
      "learning_rate": 0.00019613378973474025,
      "loss": 0.1452,
      "step": 4600
    },
    {
      "epoch": 0.02164219121894315,
      "grad_norm": 1.8819891214370728,
      "learning_rate": 0.0001961328467566268,
      "loss": 0.1205,
      "step": 4601
    },
    {
      "epoch": 0.021646895020555612,
      "grad_norm": 0.37361377477645874,
      "learning_rate": 0.00019613190377851331,
      "loss": 0.025,
      "step": 4602
    },
    {
      "epoch": 0.021651598822168078,
      "grad_norm": 0.9933614730834961,
      "learning_rate": 0.00019613096080039983,
      "loss": 0.0383,
      "step": 4603
    },
    {
      "epoch": 0.02165630262378054,
      "grad_norm": 0.5575709342956543,
      "learning_rate": 0.00019613001782228635,
      "loss": 0.0451,
      "step": 4604
    },
    {
      "epoch": 0.021661006425393002,
      "grad_norm": 2.359621047973633,
      "learning_rate": 0.00019612907484417287,
      "loss": 0.3989,
      "step": 4605
    },
    {
      "epoch": 0.021665710227005464,
      "grad_norm": 5.317811012268066,
      "learning_rate": 0.00019612813186605942,
      "loss": 0.8623,
      "step": 4606
    },
    {
      "epoch": 0.02167041402861793,
      "grad_norm": 1.9118168354034424,
      "learning_rate": 0.00019612718888794593,
      "loss": 0.1828,
      "step": 4607
    },
    {
      "epoch": 0.021675117830230392,
      "grad_norm": 2.6600968837738037,
      "learning_rate": 0.00019612624590983245,
      "loss": 0.1432,
      "step": 4608
    },
    {
      "epoch": 0.021679821631842854,
      "grad_norm": 0.5327757000923157,
      "learning_rate": 0.00019612530293171895,
      "loss": 0.0658,
      "step": 4609
    },
    {
      "epoch": 0.02168452543345532,
      "grad_norm": 1.6927673816680908,
      "learning_rate": 0.0001961243599536055,
      "loss": 0.3307,
      "step": 4610
    },
    {
      "epoch": 0.021689229235067782,
      "grad_norm": 5.03814172744751,
      "learning_rate": 0.000196123416975492,
      "loss": 0.7784,
      "step": 4611
    },
    {
      "epoch": 0.021693933036680244,
      "grad_norm": 1.7292894124984741,
      "learning_rate": 0.00019612247399737853,
      "loss": 0.4144,
      "step": 4612
    },
    {
      "epoch": 0.02169863683829271,
      "grad_norm": 1.9548122882843018,
      "learning_rate": 0.00019612153101926505,
      "loss": 0.2996,
      "step": 4613
    },
    {
      "epoch": 0.021703340639905172,
      "grad_norm": 1.1257882118225098,
      "learning_rate": 0.00019612058804115157,
      "loss": 0.0883,
      "step": 4614
    },
    {
      "epoch": 0.021708044441517634,
      "grad_norm": 1.8912529945373535,
      "learning_rate": 0.0001961196450630381,
      "loss": 0.4321,
      "step": 4615
    },
    {
      "epoch": 0.021712748243130096,
      "grad_norm": 1.0495221614837646,
      "learning_rate": 0.00019611870208492463,
      "loss": 0.1135,
      "step": 4616
    },
    {
      "epoch": 0.021717452044742562,
      "grad_norm": 3.1402626037597656,
      "learning_rate": 0.00019611775910681115,
      "loss": 0.369,
      "step": 4617
    },
    {
      "epoch": 0.021722155846355024,
      "grad_norm": 1.7580535411834717,
      "learning_rate": 0.00019611681612869767,
      "loss": 0.2591,
      "step": 4618
    },
    {
      "epoch": 0.021726859647967486,
      "grad_norm": 2.4319007396698,
      "learning_rate": 0.00019611587315058419,
      "loss": 0.2974,
      "step": 4619
    },
    {
      "epoch": 0.021731563449579952,
      "grad_norm": 3.140327215194702,
      "learning_rate": 0.0001961149301724707,
      "loss": 0.338,
      "step": 4620
    },
    {
      "epoch": 0.021736267251192414,
      "grad_norm": 1.45942223072052,
      "learning_rate": 0.00019611398719435722,
      "loss": 0.3088,
      "step": 4621
    },
    {
      "epoch": 0.021740971052804876,
      "grad_norm": 3.6527209281921387,
      "learning_rate": 0.00019611304421624374,
      "loss": 0.4655,
      "step": 4622
    },
    {
      "epoch": 0.02174567485441734,
      "grad_norm": 3.5841903686523438,
      "learning_rate": 0.00019611210123813026,
      "loss": 0.7667,
      "step": 4623
    },
    {
      "epoch": 0.021750378656029804,
      "grad_norm": 1.9604406356811523,
      "learning_rate": 0.0001961111582600168,
      "loss": 0.4182,
      "step": 4624
    },
    {
      "epoch": 0.021755082457642266,
      "grad_norm": 1.303673505783081,
      "learning_rate": 0.00019611021528190332,
      "loss": 0.1068,
      "step": 4625
    },
    {
      "epoch": 0.02175978625925473,
      "grad_norm": 2.996303081512451,
      "learning_rate": 0.00019610927230378984,
      "loss": 0.2283,
      "step": 4626
    },
    {
      "epoch": 0.021764490060867194,
      "grad_norm": 3.820852518081665,
      "learning_rate": 0.00019610832932567636,
      "loss": 0.4668,
      "step": 4627
    },
    {
      "epoch": 0.021769193862479656,
      "grad_norm": 1.028053879737854,
      "learning_rate": 0.00019610738634756288,
      "loss": 0.121,
      "step": 4628
    },
    {
      "epoch": 0.02177389766409212,
      "grad_norm": 2.2358412742614746,
      "learning_rate": 0.0001961064433694494,
      "loss": 0.2068,
      "step": 4629
    },
    {
      "epoch": 0.021778601465704584,
      "grad_norm": 2.9516634941101074,
      "learning_rate": 0.00019610550039133592,
      "loss": 0.6357,
      "step": 4630
    },
    {
      "epoch": 0.021783305267317046,
      "grad_norm": 1.3039698600769043,
      "learning_rate": 0.00019610455741322244,
      "loss": 0.1052,
      "step": 4631
    },
    {
      "epoch": 0.02178800906892951,
      "grad_norm": 2.5563089847564697,
      "learning_rate": 0.00019610361443510896,
      "loss": 0.4938,
      "step": 4632
    },
    {
      "epoch": 0.02179271287054197,
      "grad_norm": 1.6753265857696533,
      "learning_rate": 0.0001961026714569955,
      "loss": 0.2112,
      "step": 4633
    },
    {
      "epoch": 0.021797416672154436,
      "grad_norm": 2.9489705562591553,
      "learning_rate": 0.00019610172847888202,
      "loss": 0.6969,
      "step": 4634
    },
    {
      "epoch": 0.0218021204737669,
      "grad_norm": 1.4725186824798584,
      "learning_rate": 0.00019610078550076854,
      "loss": 0.1772,
      "step": 4635
    },
    {
      "epoch": 0.02180682427537936,
      "grad_norm": 1.8109649419784546,
      "learning_rate": 0.00019609984252265506,
      "loss": 0.3523,
      "step": 4636
    },
    {
      "epoch": 0.021811528076991826,
      "grad_norm": 0.503947913646698,
      "learning_rate": 0.00019609889954454158,
      "loss": 0.0433,
      "step": 4637
    },
    {
      "epoch": 0.02181623187860429,
      "grad_norm": 3.2286839485168457,
      "learning_rate": 0.00019609795656642812,
      "loss": 0.4223,
      "step": 4638
    },
    {
      "epoch": 0.02182093568021675,
      "grad_norm": 0.6410857439041138,
      "learning_rate": 0.00019609701358831464,
      "loss": 0.0917,
      "step": 4639
    },
    {
      "epoch": 0.021825639481829213,
      "grad_norm": 2.2891829013824463,
      "learning_rate": 0.00019609607061020113,
      "loss": 0.1918,
      "step": 4640
    },
    {
      "epoch": 0.02183034328344168,
      "grad_norm": 1.0525599718093872,
      "learning_rate": 0.00019609512763208765,
      "loss": 0.1678,
      "step": 4641
    },
    {
      "epoch": 0.02183504708505414,
      "grad_norm": 1.886164665222168,
      "learning_rate": 0.0001960941846539742,
      "loss": 0.3585,
      "step": 4642
    },
    {
      "epoch": 0.021839750886666603,
      "grad_norm": 2.4244227409362793,
      "learning_rate": 0.00019609324167586071,
      "loss": 0.5742,
      "step": 4643
    },
    {
      "epoch": 0.02184445468827907,
      "grad_norm": 1.6878223419189453,
      "learning_rate": 0.00019609229869774723,
      "loss": 0.4091,
      "step": 4644
    },
    {
      "epoch": 0.02184915848989153,
      "grad_norm": 0.7665025591850281,
      "learning_rate": 0.00019609135571963375,
      "loss": 0.0645,
      "step": 4645
    },
    {
      "epoch": 0.021853862291503993,
      "grad_norm": 2.189903736114502,
      "learning_rate": 0.00019609041274152027,
      "loss": 0.2797,
      "step": 4646
    },
    {
      "epoch": 0.02185856609311646,
      "grad_norm": 1.237982153892517,
      "learning_rate": 0.00019608946976340682,
      "loss": 0.2412,
      "step": 4647
    },
    {
      "epoch": 0.02186326989472892,
      "grad_norm": 0.5734562873840332,
      "learning_rate": 0.00019608852678529333,
      "loss": 0.0608,
      "step": 4648
    },
    {
      "epoch": 0.021867973696341383,
      "grad_norm": 1.8882367610931396,
      "learning_rate": 0.00019608758380717985,
      "loss": 0.4348,
      "step": 4649
    },
    {
      "epoch": 0.021872677497953845,
      "grad_norm": 2.0378870964050293,
      "learning_rate": 0.00019608664082906637,
      "loss": 0.1127,
      "step": 4650
    },
    {
      "epoch": 0.02187738129956631,
      "grad_norm": 2.9009597301483154,
      "learning_rate": 0.0001960856978509529,
      "loss": 0.2946,
      "step": 4651
    },
    {
      "epoch": 0.021882085101178773,
      "grad_norm": 3.853041648864746,
      "learning_rate": 0.0001960847548728394,
      "loss": 0.3587,
      "step": 4652
    },
    {
      "epoch": 0.021886788902791235,
      "grad_norm": 1.2732607126235962,
      "learning_rate": 0.00019608381189472593,
      "loss": 0.2277,
      "step": 4653
    },
    {
      "epoch": 0.0218914927044037,
      "grad_norm": 1.090680480003357,
      "learning_rate": 0.00019608286891661245,
      "loss": 0.1491,
      "step": 4654
    },
    {
      "epoch": 0.021896196506016163,
      "grad_norm": 1.1503242254257202,
      "learning_rate": 0.00019608192593849897,
      "loss": 0.1293,
      "step": 4655
    },
    {
      "epoch": 0.021900900307628625,
      "grad_norm": 1.4509837627410889,
      "learning_rate": 0.0001960809829603855,
      "loss": 0.1422,
      "step": 4656
    },
    {
      "epoch": 0.021905604109241087,
      "grad_norm": 2.8995988368988037,
      "learning_rate": 0.00019608003998227203,
      "loss": 0.3196,
      "step": 4657
    },
    {
      "epoch": 0.021910307910853553,
      "grad_norm": 0.7171849608421326,
      "learning_rate": 0.00019607909700415855,
      "loss": 0.101,
      "step": 4658
    },
    {
      "epoch": 0.021915011712466015,
      "grad_norm": 4.05487060546875,
      "learning_rate": 0.00019607815402604507,
      "loss": 0.658,
      "step": 4659
    },
    {
      "epoch": 0.021919715514078477,
      "grad_norm": 3.276277780532837,
      "learning_rate": 0.00019607721104793159,
      "loss": 0.4437,
      "step": 4660
    },
    {
      "epoch": 0.021924419315690943,
      "grad_norm": 1.917933464050293,
      "learning_rate": 0.0001960762680698181,
      "loss": 0.2276,
      "step": 4661
    },
    {
      "epoch": 0.021929123117303405,
      "grad_norm": 0.3968122601509094,
      "learning_rate": 0.00019607532509170462,
      "loss": 0.0366,
      "step": 4662
    },
    {
      "epoch": 0.021933826918915867,
      "grad_norm": 1.6553148031234741,
      "learning_rate": 0.00019607438211359114,
      "loss": 0.3954,
      "step": 4663
    },
    {
      "epoch": 0.021938530720528333,
      "grad_norm": 3.5597615242004395,
      "learning_rate": 0.00019607343913547766,
      "loss": 0.4053,
      "step": 4664
    },
    {
      "epoch": 0.021943234522140795,
      "grad_norm": 0.6245070099830627,
      "learning_rate": 0.0001960724961573642,
      "loss": 0.063,
      "step": 4665
    },
    {
      "epoch": 0.021947938323753257,
      "grad_norm": 1.1542128324508667,
      "learning_rate": 0.00019607155317925072,
      "loss": 0.3326,
      "step": 4666
    },
    {
      "epoch": 0.02195264212536572,
      "grad_norm": 2.4180092811584473,
      "learning_rate": 0.00019607061020113724,
      "loss": 0.627,
      "step": 4667
    },
    {
      "epoch": 0.021957345926978185,
      "grad_norm": 0.8776414394378662,
      "learning_rate": 0.00019606966722302376,
      "loss": 0.1418,
      "step": 4668
    },
    {
      "epoch": 0.021962049728590647,
      "grad_norm": 1.6642320156097412,
      "learning_rate": 0.0001960687242449103,
      "loss": 0.2733,
      "step": 4669
    },
    {
      "epoch": 0.02196675353020311,
      "grad_norm": 1.7839155197143555,
      "learning_rate": 0.0001960677812667968,
      "loss": 0.2567,
      "step": 4670
    },
    {
      "epoch": 0.021971457331815575,
      "grad_norm": 1.7706952095031738,
      "learning_rate": 0.00019606683828868332,
      "loss": 0.4134,
      "step": 4671
    },
    {
      "epoch": 0.021976161133428037,
      "grad_norm": 0.6682544946670532,
      "learning_rate": 0.00019606589531056984,
      "loss": 0.1121,
      "step": 4672
    },
    {
      "epoch": 0.0219808649350405,
      "grad_norm": 2.061899423599243,
      "learning_rate": 0.00019606495233245636,
      "loss": 0.35,
      "step": 4673
    },
    {
      "epoch": 0.02198556873665296,
      "grad_norm": 1.0103111267089844,
      "learning_rate": 0.0001960640093543429,
      "loss": 0.1573,
      "step": 4674
    },
    {
      "epoch": 0.021990272538265427,
      "grad_norm": 1.4531594514846802,
      "learning_rate": 0.00019606306637622942,
      "loss": 0.2005,
      "step": 4675
    },
    {
      "epoch": 0.02199497633987789,
      "grad_norm": 0.8906956315040588,
      "learning_rate": 0.00019606212339811594,
      "loss": 0.2379,
      "step": 4676
    },
    {
      "epoch": 0.02199968014149035,
      "grad_norm": 1.7074724435806274,
      "learning_rate": 0.00019606118042000246,
      "loss": 0.3728,
      "step": 4677
    },
    {
      "epoch": 0.022004383943102817,
      "grad_norm": 1.8004698753356934,
      "learning_rate": 0.000196060237441889,
      "loss": 0.2585,
      "step": 4678
    },
    {
      "epoch": 0.02200908774471528,
      "grad_norm": 1.4284123182296753,
      "learning_rate": 0.00019605929446377552,
      "loss": 0.1627,
      "step": 4679
    },
    {
      "epoch": 0.02201379154632774,
      "grad_norm": 1.304442286491394,
      "learning_rate": 0.00019605835148566204,
      "loss": 0.137,
      "step": 4680
    },
    {
      "epoch": 0.022018495347940207,
      "grad_norm": 1.258650302886963,
      "learning_rate": 0.00019605740850754856,
      "loss": 0.1601,
      "step": 4681
    },
    {
      "epoch": 0.02202319914955267,
      "grad_norm": 3.610466480255127,
      "learning_rate": 0.00019605646552943505,
      "loss": 0.4844,
      "step": 4682
    },
    {
      "epoch": 0.02202790295116513,
      "grad_norm": 2.2297685146331787,
      "learning_rate": 0.0001960555225513216,
      "loss": 0.4271,
      "step": 4683
    },
    {
      "epoch": 0.022032606752777593,
      "grad_norm": 1.519112467765808,
      "learning_rate": 0.00019605457957320811,
      "loss": 0.5091,
      "step": 4684
    },
    {
      "epoch": 0.02203731055439006,
      "grad_norm": 2.2835707664489746,
      "learning_rate": 0.00019605363659509463,
      "loss": 0.3594,
      "step": 4685
    },
    {
      "epoch": 0.02204201435600252,
      "grad_norm": 2.1105005741119385,
      "learning_rate": 0.00019605269361698115,
      "loss": 0.4264,
      "step": 4686
    },
    {
      "epoch": 0.022046718157614983,
      "grad_norm": 0.7190556526184082,
      "learning_rate": 0.00019605175063886767,
      "loss": 0.0577,
      "step": 4687
    },
    {
      "epoch": 0.02205142195922745,
      "grad_norm": 2.043576717376709,
      "learning_rate": 0.00019605080766075422,
      "loss": 0.2287,
      "step": 4688
    },
    {
      "epoch": 0.02205612576083991,
      "grad_norm": 1.1136622428894043,
      "learning_rate": 0.00019604986468264073,
      "loss": 0.1125,
      "step": 4689
    },
    {
      "epoch": 0.022060829562452373,
      "grad_norm": 1.1272741556167603,
      "learning_rate": 0.00019604892170452725,
      "loss": 0.1901,
      "step": 4690
    },
    {
      "epoch": 0.022065533364064836,
      "grad_norm": 3.9346978664398193,
      "learning_rate": 0.00019604797872641377,
      "loss": 0.2639,
      "step": 4691
    },
    {
      "epoch": 0.0220702371656773,
      "grad_norm": 1.8385192155838013,
      "learning_rate": 0.0001960470357483003,
      "loss": 0.2757,
      "step": 4692
    },
    {
      "epoch": 0.022074940967289763,
      "grad_norm": 2.7787625789642334,
      "learning_rate": 0.0001960460927701868,
      "loss": 0.2709,
      "step": 4693
    },
    {
      "epoch": 0.022079644768902226,
      "grad_norm": 1.1896328926086426,
      "learning_rate": 0.00019604514979207333,
      "loss": 0.123,
      "step": 4694
    },
    {
      "epoch": 0.02208434857051469,
      "grad_norm": 1.2215968370437622,
      "learning_rate": 0.00019604420681395985,
      "loss": 0.1473,
      "step": 4695
    },
    {
      "epoch": 0.022089052372127153,
      "grad_norm": 3.0692057609558105,
      "learning_rate": 0.00019604326383584637,
      "loss": 0.8659,
      "step": 4696
    },
    {
      "epoch": 0.022093756173739616,
      "grad_norm": 2.4339849948883057,
      "learning_rate": 0.0001960423208577329,
      "loss": 0.1506,
      "step": 4697
    },
    {
      "epoch": 0.02209845997535208,
      "grad_norm": 0.20571069419384003,
      "learning_rate": 0.00019604137787961943,
      "loss": 0.0146,
      "step": 4698
    },
    {
      "epoch": 0.022103163776964543,
      "grad_norm": 1.5731449127197266,
      "learning_rate": 0.00019604043490150595,
      "loss": 0.3597,
      "step": 4699
    },
    {
      "epoch": 0.022107867578577006,
      "grad_norm": 1.2954976558685303,
      "learning_rate": 0.00019603949192339247,
      "loss": 0.0867,
      "step": 4700
    },
    {
      "epoch": 0.022112571380189468,
      "grad_norm": 4.037081241607666,
      "learning_rate": 0.00019603854894527899,
      "loss": 0.3457,
      "step": 4701
    },
    {
      "epoch": 0.022117275181801933,
      "grad_norm": 3.254161834716797,
      "learning_rate": 0.0001960376059671655,
      "loss": 0.3545,
      "step": 4702
    },
    {
      "epoch": 0.022121978983414396,
      "grad_norm": 2.281527280807495,
      "learning_rate": 0.00019603666298905202,
      "loss": 0.2437,
      "step": 4703
    },
    {
      "epoch": 0.022126682785026858,
      "grad_norm": 1.6985998153686523,
      "learning_rate": 0.00019603572001093854,
      "loss": 0.2892,
      "step": 4704
    },
    {
      "epoch": 0.022131386586639323,
      "grad_norm": 1.2862567901611328,
      "learning_rate": 0.00019603477703282506,
      "loss": 0.1524,
      "step": 4705
    },
    {
      "epoch": 0.022136090388251786,
      "grad_norm": 3.7671265602111816,
      "learning_rate": 0.0001960338340547116,
      "loss": 0.6664,
      "step": 4706
    },
    {
      "epoch": 0.022140794189864248,
      "grad_norm": 0.40194404125213623,
      "learning_rate": 0.00019603289107659812,
      "loss": 0.024,
      "step": 4707
    },
    {
      "epoch": 0.02214549799147671,
      "grad_norm": 4.009317398071289,
      "learning_rate": 0.00019603194809848464,
      "loss": 0.8967,
      "step": 4708
    },
    {
      "epoch": 0.022150201793089176,
      "grad_norm": 0.13916347920894623,
      "learning_rate": 0.00019603100512037116,
      "loss": 0.009,
      "step": 4709
    },
    {
      "epoch": 0.022154905594701638,
      "grad_norm": 0.546043336391449,
      "learning_rate": 0.0001960300621422577,
      "loss": 0.042,
      "step": 4710
    },
    {
      "epoch": 0.0221596093963141,
      "grad_norm": 4.012077808380127,
      "learning_rate": 0.00019602911916414423,
      "loss": 0.5893,
      "step": 4711
    },
    {
      "epoch": 0.022164313197926565,
      "grad_norm": 1.1388263702392578,
      "learning_rate": 0.00019602817618603075,
      "loss": 0.0914,
      "step": 4712
    },
    {
      "epoch": 0.022169016999539028,
      "grad_norm": 1.7997009754180908,
      "learning_rate": 0.00019602723320791724,
      "loss": 0.1813,
      "step": 4713
    },
    {
      "epoch": 0.02217372080115149,
      "grad_norm": 0.6562750339508057,
      "learning_rate": 0.00019602629022980376,
      "loss": 0.0688,
      "step": 4714
    },
    {
      "epoch": 0.022178424602763955,
      "grad_norm": 0.5680360794067383,
      "learning_rate": 0.0001960253472516903,
      "loss": 0.0682,
      "step": 4715
    },
    {
      "epoch": 0.022183128404376418,
      "grad_norm": 1.783735990524292,
      "learning_rate": 0.00019602440427357682,
      "loss": 0.2882,
      "step": 4716
    },
    {
      "epoch": 0.02218783220598888,
      "grad_norm": 1.1432346105575562,
      "learning_rate": 0.00019602346129546334,
      "loss": 0.1427,
      "step": 4717
    },
    {
      "epoch": 0.022192536007601342,
      "grad_norm": 1.9978411197662354,
      "learning_rate": 0.00019602251831734986,
      "loss": 0.1674,
      "step": 4718
    },
    {
      "epoch": 0.022197239809213808,
      "grad_norm": 4.013365268707275,
      "learning_rate": 0.0001960215753392364,
      "loss": 0.6035,
      "step": 4719
    },
    {
      "epoch": 0.02220194361082627,
      "grad_norm": 0.28475242853164673,
      "learning_rate": 0.00019602063236112292,
      "loss": 0.028,
      "step": 4720
    },
    {
      "epoch": 0.022206647412438732,
      "grad_norm": 0.6308900713920593,
      "learning_rate": 0.00019601968938300944,
      "loss": 0.0444,
      "step": 4721
    },
    {
      "epoch": 0.022211351214051198,
      "grad_norm": 1.5055534839630127,
      "learning_rate": 0.00019601874640489596,
      "loss": 0.2052,
      "step": 4722
    },
    {
      "epoch": 0.02221605501566366,
      "grad_norm": 0.7984216809272766,
      "learning_rate": 0.00019601780342678248,
      "loss": 0.0824,
      "step": 4723
    },
    {
      "epoch": 0.022220758817276122,
      "grad_norm": 0.5220149159431458,
      "learning_rate": 0.000196016860448669,
      "loss": 0.0452,
      "step": 4724
    },
    {
      "epoch": 0.022225462618888584,
      "grad_norm": 1.8240395784378052,
      "learning_rate": 0.00019601591747055551,
      "loss": 0.2992,
      "step": 4725
    },
    {
      "epoch": 0.02223016642050105,
      "grad_norm": 2.0522379875183105,
      "learning_rate": 0.00019601497449244203,
      "loss": 0.3087,
      "step": 4726
    },
    {
      "epoch": 0.022234870222113512,
      "grad_norm": 2.8414981365203857,
      "learning_rate": 0.00019601403151432855,
      "loss": 0.5188,
      "step": 4727
    },
    {
      "epoch": 0.022239574023725974,
      "grad_norm": 2.5652918815612793,
      "learning_rate": 0.0001960130885362151,
      "loss": 0.6067,
      "step": 4728
    },
    {
      "epoch": 0.02224427782533844,
      "grad_norm": 0.8629608154296875,
      "learning_rate": 0.00019601214555810162,
      "loss": 0.1022,
      "step": 4729
    },
    {
      "epoch": 0.022248981626950902,
      "grad_norm": 0.4375329911708832,
      "learning_rate": 0.00019601120257998813,
      "loss": 0.0352,
      "step": 4730
    },
    {
      "epoch": 0.022253685428563364,
      "grad_norm": 2.6869988441467285,
      "learning_rate": 0.00019601025960187465,
      "loss": 0.2087,
      "step": 4731
    },
    {
      "epoch": 0.02225838923017583,
      "grad_norm": 4.997509479522705,
      "learning_rate": 0.00019600931662376117,
      "loss": 0.7621,
      "step": 4732
    },
    {
      "epoch": 0.022263093031788292,
      "grad_norm": 2.7470057010650635,
      "learning_rate": 0.0001960083736456477,
      "loss": 0.2777,
      "step": 4733
    },
    {
      "epoch": 0.022267796833400754,
      "grad_norm": 1.4147106409072876,
      "learning_rate": 0.0001960074306675342,
      "loss": 0.104,
      "step": 4734
    },
    {
      "epoch": 0.022272500635013216,
      "grad_norm": 2.6162290573120117,
      "learning_rate": 0.00019600648768942073,
      "loss": 0.4194,
      "step": 4735
    },
    {
      "epoch": 0.022277204436625682,
      "grad_norm": 0.2665351629257202,
      "learning_rate": 0.00019600554471130725,
      "loss": 0.0247,
      "step": 4736
    },
    {
      "epoch": 0.022281908238238144,
      "grad_norm": 0.36827054619789124,
      "learning_rate": 0.00019600460173319377,
      "loss": 0.032,
      "step": 4737
    },
    {
      "epoch": 0.022286612039850606,
      "grad_norm": 2.69150447845459,
      "learning_rate": 0.0001960036587550803,
      "loss": 0.3035,
      "step": 4738
    },
    {
      "epoch": 0.022291315841463072,
      "grad_norm": 0.1856757253408432,
      "learning_rate": 0.00019600271577696683,
      "loss": 0.0148,
      "step": 4739
    },
    {
      "epoch": 0.022296019643075534,
      "grad_norm": 1.0364235639572144,
      "learning_rate": 0.00019600177279885335,
      "loss": 0.0994,
      "step": 4740
    },
    {
      "epoch": 0.022300723444687996,
      "grad_norm": 4.102396488189697,
      "learning_rate": 0.00019600082982073987,
      "loss": 0.4939,
      "step": 4741
    },
    {
      "epoch": 0.02230542724630046,
      "grad_norm": 1.083367943763733,
      "learning_rate": 0.0001959998868426264,
      "loss": 0.0824,
      "step": 4742
    },
    {
      "epoch": 0.022310131047912924,
      "grad_norm": 3.667893409729004,
      "learning_rate": 0.00019599894386451293,
      "loss": 0.4687,
      "step": 4743
    },
    {
      "epoch": 0.022314834849525386,
      "grad_norm": 1.819682002067566,
      "learning_rate": 0.00019599800088639942,
      "loss": 0.202,
      "step": 4744
    },
    {
      "epoch": 0.02231953865113785,
      "grad_norm": 4.3501458168029785,
      "learning_rate": 0.00019599705790828594,
      "loss": 0.5918,
      "step": 4745
    },
    {
      "epoch": 0.022324242452750314,
      "grad_norm": 0.4595370888710022,
      "learning_rate": 0.00019599611493017246,
      "loss": 0.0601,
      "step": 4746
    },
    {
      "epoch": 0.022328946254362776,
      "grad_norm": 0.3709793984889984,
      "learning_rate": 0.000195995171952059,
      "loss": 0.029,
      "step": 4747
    },
    {
      "epoch": 0.02233365005597524,
      "grad_norm": 2.201885461807251,
      "learning_rate": 0.00019599422897394552,
      "loss": 0.3544,
      "step": 4748
    },
    {
      "epoch": 0.022338353857587704,
      "grad_norm": 0.29252126812934875,
      "learning_rate": 0.00019599328599583204,
      "loss": 0.0307,
      "step": 4749
    },
    {
      "epoch": 0.022343057659200166,
      "grad_norm": 0.723869264125824,
      "learning_rate": 0.00019599234301771856,
      "loss": 0.0614,
      "step": 4750
    },
    {
      "epoch": 0.02234776146081263,
      "grad_norm": 4.139683246612549,
      "learning_rate": 0.0001959914000396051,
      "loss": 0.0907,
      "step": 4751
    },
    {
      "epoch": 0.02235246526242509,
      "grad_norm": 3.356008529663086,
      "learning_rate": 0.00019599045706149163,
      "loss": 0.6614,
      "step": 4752
    },
    {
      "epoch": 0.022357169064037556,
      "grad_norm": 3.030977964401245,
      "learning_rate": 0.00019598951408337814,
      "loss": 0.408,
      "step": 4753
    },
    {
      "epoch": 0.02236187286565002,
      "grad_norm": 3.48715877532959,
      "learning_rate": 0.00019598857110526466,
      "loss": 0.2197,
      "step": 4754
    },
    {
      "epoch": 0.02236657666726248,
      "grad_norm": 3.3736422061920166,
      "learning_rate": 0.00019598762812715116,
      "loss": 0.7131,
      "step": 4755
    },
    {
      "epoch": 0.022371280468874946,
      "grad_norm": 3.368884325027466,
      "learning_rate": 0.0001959866851490377,
      "loss": 0.5553,
      "step": 4756
    },
    {
      "epoch": 0.02237598427048741,
      "grad_norm": 2.000051736831665,
      "learning_rate": 0.00019598574217092422,
      "loss": 0.1596,
      "step": 4757
    },
    {
      "epoch": 0.02238068807209987,
      "grad_norm": 3.1571898460388184,
      "learning_rate": 0.00019598479919281074,
      "loss": 0.6142,
      "step": 4758
    },
    {
      "epoch": 0.022385391873712333,
      "grad_norm": 1.4906755685806274,
      "learning_rate": 0.00019598385621469726,
      "loss": 0.1194,
      "step": 4759
    },
    {
      "epoch": 0.0223900956753248,
      "grad_norm": 2.2718851566314697,
      "learning_rate": 0.0001959829132365838,
      "loss": 0.2358,
      "step": 4760
    },
    {
      "epoch": 0.02239479947693726,
      "grad_norm": 2.4588284492492676,
      "learning_rate": 0.00019598197025847032,
      "loss": 0.5608,
      "step": 4761
    },
    {
      "epoch": 0.022399503278549723,
      "grad_norm": 0.10593587160110474,
      "learning_rate": 0.00019598102728035684,
      "loss": 0.0073,
      "step": 4762
    },
    {
      "epoch": 0.02240420708016219,
      "grad_norm": 1.813628911972046,
      "learning_rate": 0.00019598008430224336,
      "loss": 0.195,
      "step": 4763
    },
    {
      "epoch": 0.02240891088177465,
      "grad_norm": 2.527256965637207,
      "learning_rate": 0.00019597914132412988,
      "loss": 0.5516,
      "step": 4764
    },
    {
      "epoch": 0.022413614683387113,
      "grad_norm": 5.163635730743408,
      "learning_rate": 0.0001959781983460164,
      "loss": 1.3624,
      "step": 4765
    },
    {
      "epoch": 0.02241831848499958,
      "grad_norm": 3.3970868587493896,
      "learning_rate": 0.00019597725536790291,
      "loss": 0.6251,
      "step": 4766
    },
    {
      "epoch": 0.02242302228661204,
      "grad_norm": 2.4943368434906006,
      "learning_rate": 0.00019597631238978943,
      "loss": 0.258,
      "step": 4767
    },
    {
      "epoch": 0.022427726088224503,
      "grad_norm": 2.543713331222534,
      "learning_rate": 0.00019597536941167595,
      "loss": 0.5773,
      "step": 4768
    },
    {
      "epoch": 0.022432429889836965,
      "grad_norm": 2.2597415447235107,
      "learning_rate": 0.0001959744264335625,
      "loss": 0.4323,
      "step": 4769
    },
    {
      "epoch": 0.02243713369144943,
      "grad_norm": 0.5371257066726685,
      "learning_rate": 0.00019597348345544902,
      "loss": 0.0409,
      "step": 4770
    },
    {
      "epoch": 0.022441837493061893,
      "grad_norm": 2.7161874771118164,
      "learning_rate": 0.00019597254047733553,
      "loss": 0.5143,
      "step": 4771
    },
    {
      "epoch": 0.022446541294674355,
      "grad_norm": 1.2954767942428589,
      "learning_rate": 0.00019597159749922205,
      "loss": 0.1167,
      "step": 4772
    },
    {
      "epoch": 0.02245124509628682,
      "grad_norm": 1.9386241436004639,
      "learning_rate": 0.00019597065452110857,
      "loss": 0.238,
      "step": 4773
    },
    {
      "epoch": 0.022455948897899283,
      "grad_norm": 1.5792028903961182,
      "learning_rate": 0.00019596971154299512,
      "loss": 0.1586,
      "step": 4774
    },
    {
      "epoch": 0.022460652699511745,
      "grad_norm": 0.8962587118148804,
      "learning_rate": 0.0001959687685648816,
      "loss": 0.1303,
      "step": 4775
    },
    {
      "epoch": 0.022465356501124207,
      "grad_norm": 1.8705147504806519,
      "learning_rate": 0.00019596782558676813,
      "loss": 0.3097,
      "step": 4776
    },
    {
      "epoch": 0.022470060302736673,
      "grad_norm": 4.524742126464844,
      "learning_rate": 0.00019596688260865465,
      "loss": 0.6184,
      "step": 4777
    },
    {
      "epoch": 0.022474764104349135,
      "grad_norm": 2.3253872394561768,
      "learning_rate": 0.0001959659396305412,
      "loss": 0.3964,
      "step": 4778
    },
    {
      "epoch": 0.022479467905961597,
      "grad_norm": 7.812188148498535,
      "learning_rate": 0.0001959649966524277,
      "loss": 0.2803,
      "step": 4779
    },
    {
      "epoch": 0.022484171707574063,
      "grad_norm": 2.663069725036621,
      "learning_rate": 0.00019596405367431423,
      "loss": 0.5594,
      "step": 4780
    },
    {
      "epoch": 0.022488875509186525,
      "grad_norm": 1.5367118120193481,
      "learning_rate": 0.00019596311069620075,
      "loss": 0.3831,
      "step": 4781
    },
    {
      "epoch": 0.022493579310798987,
      "grad_norm": 2.147190570831299,
      "learning_rate": 0.00019596216771808727,
      "loss": 0.5746,
      "step": 4782
    },
    {
      "epoch": 0.022498283112411453,
      "grad_norm": 2.342475175857544,
      "learning_rate": 0.0001959612247399738,
      "loss": 0.456,
      "step": 4783
    },
    {
      "epoch": 0.022502986914023915,
      "grad_norm": 1.3073441982269287,
      "learning_rate": 0.00019596028176186033,
      "loss": 0.2577,
      "step": 4784
    },
    {
      "epoch": 0.022507690715636377,
      "grad_norm": 1.9521949291229248,
      "learning_rate": 0.00019595933878374685,
      "loss": 0.3055,
      "step": 4785
    },
    {
      "epoch": 0.02251239451724884,
      "grad_norm": 1.371563196182251,
      "learning_rate": 0.00019595839580563334,
      "loss": 0.194,
      "step": 4786
    },
    {
      "epoch": 0.022517098318861305,
      "grad_norm": 1.1347182989120483,
      "learning_rate": 0.00019595745282751986,
      "loss": 0.0964,
      "step": 4787
    },
    {
      "epoch": 0.022521802120473767,
      "grad_norm": 1.297135591506958,
      "learning_rate": 0.0001959565098494064,
      "loss": 0.3005,
      "step": 4788
    },
    {
      "epoch": 0.02252650592208623,
      "grad_norm": 2.637422800064087,
      "learning_rate": 0.00019595556687129292,
      "loss": 0.5577,
      "step": 4789
    },
    {
      "epoch": 0.022531209723698695,
      "grad_norm": 0.3384736478328705,
      "learning_rate": 0.00019595462389317944,
      "loss": 0.0296,
      "step": 4790
    },
    {
      "epoch": 0.022535913525311157,
      "grad_norm": 0.9563606381416321,
      "learning_rate": 0.00019595368091506596,
      "loss": 0.1671,
      "step": 4791
    },
    {
      "epoch": 0.02254061732692362,
      "grad_norm": 0.5560228228569031,
      "learning_rate": 0.0001959527379369525,
      "loss": 0.089,
      "step": 4792
    },
    {
      "epoch": 0.02254532112853608,
      "grad_norm": 1.6021414995193481,
      "learning_rate": 0.00019595179495883903,
      "loss": 0.2287,
      "step": 4793
    },
    {
      "epoch": 0.022550024930148547,
      "grad_norm": 0.8938919901847839,
      "learning_rate": 0.00019595085198072554,
      "loss": 0.1836,
      "step": 4794
    },
    {
      "epoch": 0.02255472873176101,
      "grad_norm": 0.8260307908058167,
      "learning_rate": 0.00019594990900261206,
      "loss": 0.1068,
      "step": 4795
    },
    {
      "epoch": 0.02255943253337347,
      "grad_norm": 2.6137845516204834,
      "learning_rate": 0.00019594896602449858,
      "loss": 0.5256,
      "step": 4796
    },
    {
      "epoch": 0.022564136334985937,
      "grad_norm": 1.3734937906265259,
      "learning_rate": 0.0001959480230463851,
      "loss": 0.1775,
      "step": 4797
    },
    {
      "epoch": 0.0225688401365984,
      "grad_norm": 0.6143976449966431,
      "learning_rate": 0.00019594708006827162,
      "loss": 0.1191,
      "step": 4798
    },
    {
      "epoch": 0.02257354393821086,
      "grad_norm": 2.2410130500793457,
      "learning_rate": 0.00019594613709015814,
      "loss": 0.6605,
      "step": 4799
    },
    {
      "epoch": 0.022578247739823327,
      "grad_norm": 2.4699203968048096,
      "learning_rate": 0.00019594519411204466,
      "loss": 0.5998,
      "step": 4800
    },
    {
      "epoch": 0.02258295154143579,
      "grad_norm": 1.6264842748641968,
      "learning_rate": 0.0001959442511339312,
      "loss": 0.124,
      "step": 4801
    },
    {
      "epoch": 0.02258765534304825,
      "grad_norm": 0.6966007947921753,
      "learning_rate": 0.00019594330815581772,
      "loss": 0.0586,
      "step": 4802
    },
    {
      "epoch": 0.022592359144660713,
      "grad_norm": 1.91286301612854,
      "learning_rate": 0.00019594236517770424,
      "loss": 0.2225,
      "step": 4803
    },
    {
      "epoch": 0.02259706294627318,
      "grad_norm": 1.812055230140686,
      "learning_rate": 0.00019594142219959076,
      "loss": 0.2162,
      "step": 4804
    },
    {
      "epoch": 0.02260176674788564,
      "grad_norm": 2.170454978942871,
      "learning_rate": 0.00019594047922147728,
      "loss": 0.3244,
      "step": 4805
    },
    {
      "epoch": 0.022606470549498103,
      "grad_norm": 0.4328044056892395,
      "learning_rate": 0.0001959395362433638,
      "loss": 0.0688,
      "step": 4806
    },
    {
      "epoch": 0.02261117435111057,
      "grad_norm": 0.9004313945770264,
      "learning_rate": 0.00019593859326525031,
      "loss": 0.1218,
      "step": 4807
    },
    {
      "epoch": 0.02261587815272303,
      "grad_norm": 0.7120288610458374,
      "learning_rate": 0.00019593765028713683,
      "loss": 0.1229,
      "step": 4808
    },
    {
      "epoch": 0.022620581954335493,
      "grad_norm": 2.9819610118865967,
      "learning_rate": 0.00019593670730902335,
      "loss": 0.7244,
      "step": 4809
    },
    {
      "epoch": 0.022625285755947955,
      "grad_norm": 0.7157456874847412,
      "learning_rate": 0.0001959357643309099,
      "loss": 0.0806,
      "step": 4810
    },
    {
      "epoch": 0.02262998955756042,
      "grad_norm": 2.7432570457458496,
      "learning_rate": 0.00019593482135279642,
      "loss": 0.252,
      "step": 4811
    },
    {
      "epoch": 0.022634693359172883,
      "grad_norm": 5.647603988647461,
      "learning_rate": 0.00019593387837468293,
      "loss": 0.7558,
      "step": 4812
    },
    {
      "epoch": 0.022639397160785345,
      "grad_norm": 4.259737491607666,
      "learning_rate": 0.00019593293539656945,
      "loss": 0.9993,
      "step": 4813
    },
    {
      "epoch": 0.02264410096239781,
      "grad_norm": 1.8127609491348267,
      "learning_rate": 0.00019593199241845597,
      "loss": 0.2456,
      "step": 4814
    },
    {
      "epoch": 0.022648804764010273,
      "grad_norm": 2.065391778945923,
      "learning_rate": 0.00019593104944034252,
      "loss": 0.2801,
      "step": 4815
    },
    {
      "epoch": 0.022653508565622735,
      "grad_norm": 0.782175600528717,
      "learning_rate": 0.00019593010646222904,
      "loss": 0.0737,
      "step": 4816
    },
    {
      "epoch": 0.0226582123672352,
      "grad_norm": 0.811227023601532,
      "learning_rate": 0.00019592916348411553,
      "loss": 0.1357,
      "step": 4817
    },
    {
      "epoch": 0.022662916168847663,
      "grad_norm": 2.6957502365112305,
      "learning_rate": 0.00019592822050600205,
      "loss": 0.4282,
      "step": 4818
    },
    {
      "epoch": 0.022667619970460125,
      "grad_norm": 0.35136619210243225,
      "learning_rate": 0.0001959272775278886,
      "loss": 0.0245,
      "step": 4819
    },
    {
      "epoch": 0.022672323772072588,
      "grad_norm": 1.8031398057937622,
      "learning_rate": 0.0001959263345497751,
      "loss": 0.1924,
      "step": 4820
    },
    {
      "epoch": 0.022677027573685053,
      "grad_norm": 2.2397539615631104,
      "learning_rate": 0.00019592539157166163,
      "loss": 0.1666,
      "step": 4821
    },
    {
      "epoch": 0.022681731375297515,
      "grad_norm": 1.094818115234375,
      "learning_rate": 0.00019592444859354815,
      "loss": 0.1146,
      "step": 4822
    },
    {
      "epoch": 0.022686435176909978,
      "grad_norm": 3.1531002521514893,
      "learning_rate": 0.00019592350561543467,
      "loss": 0.5626,
      "step": 4823
    },
    {
      "epoch": 0.022691138978522443,
      "grad_norm": 1.8887056112289429,
      "learning_rate": 0.0001959225626373212,
      "loss": 0.1682,
      "step": 4824
    },
    {
      "epoch": 0.022695842780134905,
      "grad_norm": 1.0514732599258423,
      "learning_rate": 0.00019592161965920773,
      "loss": 0.2039,
      "step": 4825
    },
    {
      "epoch": 0.022700546581747368,
      "grad_norm": 1.5122582912445068,
      "learning_rate": 0.00019592067668109425,
      "loss": 0.2903,
      "step": 4826
    },
    {
      "epoch": 0.02270525038335983,
      "grad_norm": 0.806331217288971,
      "learning_rate": 0.00019591973370298077,
      "loss": 0.1808,
      "step": 4827
    },
    {
      "epoch": 0.022709954184972295,
      "grad_norm": 1.5225028991699219,
      "learning_rate": 0.0001959187907248673,
      "loss": 0.1442,
      "step": 4828
    },
    {
      "epoch": 0.022714657986584758,
      "grad_norm": 8.258415222167969,
      "learning_rate": 0.0001959178477467538,
      "loss": 0.406,
      "step": 4829
    },
    {
      "epoch": 0.02271936178819722,
      "grad_norm": 2.354685068130493,
      "learning_rate": 0.00019591690476864032,
      "loss": 0.3168,
      "step": 4830
    },
    {
      "epoch": 0.022724065589809685,
      "grad_norm": 0.9078541994094849,
      "learning_rate": 0.00019591596179052684,
      "loss": 0.1018,
      "step": 4831
    },
    {
      "epoch": 0.022728769391422148,
      "grad_norm": 0.3459915220737457,
      "learning_rate": 0.00019591501881241336,
      "loss": 0.0413,
      "step": 4832
    },
    {
      "epoch": 0.02273347319303461,
      "grad_norm": 0.4613634943962097,
      "learning_rate": 0.0001959140758342999,
      "loss": 0.05,
      "step": 4833
    },
    {
      "epoch": 0.022738176994647075,
      "grad_norm": 2.7981598377227783,
      "learning_rate": 0.00019591313285618643,
      "loss": 0.5414,
      "step": 4834
    },
    {
      "epoch": 0.022742880796259538,
      "grad_norm": 0.7020918726921082,
      "learning_rate": 0.00019591218987807294,
      "loss": 0.0982,
      "step": 4835
    },
    {
      "epoch": 0.022747584597872,
      "grad_norm": 1.8295880556106567,
      "learning_rate": 0.00019591124689995946,
      "loss": 0.2218,
      "step": 4836
    },
    {
      "epoch": 0.022752288399484462,
      "grad_norm": 1.729764461517334,
      "learning_rate": 0.00019591030392184598,
      "loss": 0.1367,
      "step": 4837
    },
    {
      "epoch": 0.022756992201096927,
      "grad_norm": 3.9183497428894043,
      "learning_rate": 0.0001959093609437325,
      "loss": 0.4209,
      "step": 4838
    },
    {
      "epoch": 0.02276169600270939,
      "grad_norm": 1.9824614524841309,
      "learning_rate": 0.00019590841796561902,
      "loss": 0.2849,
      "step": 4839
    },
    {
      "epoch": 0.022766399804321852,
      "grad_norm": 1.4366741180419922,
      "learning_rate": 0.00019590747498750554,
      "loss": 0.1093,
      "step": 4840
    },
    {
      "epoch": 0.022771103605934317,
      "grad_norm": 2.30608868598938,
      "learning_rate": 0.00019590653200939206,
      "loss": 0.5808,
      "step": 4841
    },
    {
      "epoch": 0.02277580740754678,
      "grad_norm": 2.1954944133758545,
      "learning_rate": 0.0001959055890312786,
      "loss": 0.1777,
      "step": 4842
    },
    {
      "epoch": 0.022780511209159242,
      "grad_norm": 1.5445680618286133,
      "learning_rate": 0.00019590464605316512,
      "loss": 0.1141,
      "step": 4843
    },
    {
      "epoch": 0.022785215010771704,
      "grad_norm": 2.375392436981201,
      "learning_rate": 0.00019590370307505164,
      "loss": 0.2641,
      "step": 4844
    },
    {
      "epoch": 0.02278991881238417,
      "grad_norm": 2.8254504203796387,
      "learning_rate": 0.00019590276009693816,
      "loss": 0.5309,
      "step": 4845
    },
    {
      "epoch": 0.022794622613996632,
      "grad_norm": 0.11891540884971619,
      "learning_rate": 0.00019590181711882468,
      "loss": 0.0075,
      "step": 4846
    },
    {
      "epoch": 0.022799326415609094,
      "grad_norm": 1.857711672782898,
      "learning_rate": 0.00019590087414071122,
      "loss": 0.2117,
      "step": 4847
    },
    {
      "epoch": 0.02280403021722156,
      "grad_norm": 1.2444210052490234,
      "learning_rate": 0.00019589993116259771,
      "loss": 0.1062,
      "step": 4848
    },
    {
      "epoch": 0.022808734018834022,
      "grad_norm": 1.0989203453063965,
      "learning_rate": 0.00019589898818448423,
      "loss": 0.2478,
      "step": 4849
    },
    {
      "epoch": 0.022813437820446484,
      "grad_norm": 2.552887439727783,
      "learning_rate": 0.00019589804520637075,
      "loss": 0.3884,
      "step": 4850
    },
    {
      "epoch": 0.02281814162205895,
      "grad_norm": 0.7097412943840027,
      "learning_rate": 0.0001958971022282573,
      "loss": 0.0648,
      "step": 4851
    },
    {
      "epoch": 0.022822845423671412,
      "grad_norm": 2.8864758014678955,
      "learning_rate": 0.00019589615925014382,
      "loss": 0.2689,
      "step": 4852
    },
    {
      "epoch": 0.022827549225283874,
      "grad_norm": 3.521357774734497,
      "learning_rate": 0.00019589521627203033,
      "loss": 0.2058,
      "step": 4853
    },
    {
      "epoch": 0.022832253026896336,
      "grad_norm": 2.5935051441192627,
      "learning_rate": 0.00019589427329391685,
      "loss": 0.4353,
      "step": 4854
    },
    {
      "epoch": 0.022836956828508802,
      "grad_norm": 3.6022891998291016,
      "learning_rate": 0.00019589333031580337,
      "loss": 0.2824,
      "step": 4855
    },
    {
      "epoch": 0.022841660630121264,
      "grad_norm": 5.2699055671691895,
      "learning_rate": 0.00019589238733768992,
      "loss": 0.6768,
      "step": 4856
    },
    {
      "epoch": 0.022846364431733726,
      "grad_norm": 2.056497573852539,
      "learning_rate": 0.00019589144435957644,
      "loss": 0.263,
      "step": 4857
    },
    {
      "epoch": 0.022851068233346192,
      "grad_norm": 3.5605034828186035,
      "learning_rate": 0.00019589050138146296,
      "loss": 0.5142,
      "step": 4858
    },
    {
      "epoch": 0.022855772034958654,
      "grad_norm": 1.0003629922866821,
      "learning_rate": 0.00019588955840334947,
      "loss": 0.0777,
      "step": 4859
    },
    {
      "epoch": 0.022860475836571116,
      "grad_norm": 2.1571147441864014,
      "learning_rate": 0.000195888615425236,
      "loss": 0.2123,
      "step": 4860
    },
    {
      "epoch": 0.022865179638183578,
      "grad_norm": 2.019315719604492,
      "learning_rate": 0.0001958876724471225,
      "loss": 0.2112,
      "step": 4861
    },
    {
      "epoch": 0.022869883439796044,
      "grad_norm": 4.140025615692139,
      "learning_rate": 0.00019588672946900903,
      "loss": 0.5857,
      "step": 4862
    },
    {
      "epoch": 0.022874587241408506,
      "grad_norm": 1.4434531927108765,
      "learning_rate": 0.00019588578649089555,
      "loss": 0.2198,
      "step": 4863
    },
    {
      "epoch": 0.022879291043020968,
      "grad_norm": 2.952800989151001,
      "learning_rate": 0.00019588484351278207,
      "loss": 0.3849,
      "step": 4864
    },
    {
      "epoch": 0.022883994844633434,
      "grad_norm": 1.4645124673843384,
      "learning_rate": 0.0001958839005346686,
      "loss": 0.3184,
      "step": 4865
    },
    {
      "epoch": 0.022888698646245896,
      "grad_norm": 3.843690872192383,
      "learning_rate": 0.00019588295755655513,
      "loss": 0.5182,
      "step": 4866
    },
    {
      "epoch": 0.022893402447858358,
      "grad_norm": 1.1213738918304443,
      "learning_rate": 0.00019588201457844165,
      "loss": 0.1437,
      "step": 4867
    },
    {
      "epoch": 0.022898106249470824,
      "grad_norm": 0.4694024920463562,
      "learning_rate": 0.00019588107160032817,
      "loss": 0.0818,
      "step": 4868
    },
    {
      "epoch": 0.022902810051083286,
      "grad_norm": 1.1776436567306519,
      "learning_rate": 0.0001958801286222147,
      "loss": 0.2924,
      "step": 4869
    },
    {
      "epoch": 0.022907513852695748,
      "grad_norm": 2.1082003116607666,
      "learning_rate": 0.0001958791856441012,
      "loss": 0.3756,
      "step": 4870
    },
    {
      "epoch": 0.02291221765430821,
      "grad_norm": 1.959258794784546,
      "learning_rate": 0.00019587824266598772,
      "loss": 0.3911,
      "step": 4871
    },
    {
      "epoch": 0.022916921455920676,
      "grad_norm": 2.0982038974761963,
      "learning_rate": 0.00019587729968787424,
      "loss": 0.4122,
      "step": 4872
    },
    {
      "epoch": 0.022921625257533138,
      "grad_norm": 0.523954451084137,
      "learning_rate": 0.00019587635670976076,
      "loss": 0.0671,
      "step": 4873
    },
    {
      "epoch": 0.0229263290591456,
      "grad_norm": 1.0156668424606323,
      "learning_rate": 0.0001958754137316473,
      "loss": 0.124,
      "step": 4874
    },
    {
      "epoch": 0.022931032860758066,
      "grad_norm": 0.334473580121994,
      "learning_rate": 0.00019587447075353383,
      "loss": 0.0332,
      "step": 4875
    },
    {
      "epoch": 0.022935736662370528,
      "grad_norm": 3.067735433578491,
      "learning_rate": 0.00019587352777542034,
      "loss": 0.353,
      "step": 4876
    },
    {
      "epoch": 0.02294044046398299,
      "grad_norm": 0.8550761342048645,
      "learning_rate": 0.00019587258479730686,
      "loss": 0.1173,
      "step": 4877
    },
    {
      "epoch": 0.022945144265595453,
      "grad_norm": 3.445387601852417,
      "learning_rate": 0.0001958716418191934,
      "loss": 0.5116,
      "step": 4878
    },
    {
      "epoch": 0.022949848067207918,
      "grad_norm": 0.5611037015914917,
      "learning_rate": 0.0001958706988410799,
      "loss": 0.0668,
      "step": 4879
    },
    {
      "epoch": 0.02295455186882038,
      "grad_norm": 2.0405774116516113,
      "learning_rate": 0.00019586975586296642,
      "loss": 0.3842,
      "step": 4880
    },
    {
      "epoch": 0.022959255670432843,
      "grad_norm": 1.2157342433929443,
      "learning_rate": 0.00019586881288485294,
      "loss": 0.1555,
      "step": 4881
    },
    {
      "epoch": 0.022963959472045308,
      "grad_norm": 1.2395330667495728,
      "learning_rate": 0.00019586786990673946,
      "loss": 0.1672,
      "step": 4882
    },
    {
      "epoch": 0.02296866327365777,
      "grad_norm": 3.3954453468322754,
      "learning_rate": 0.000195866926928626,
      "loss": 0.54,
      "step": 4883
    },
    {
      "epoch": 0.022973367075270233,
      "grad_norm": 1.819750428199768,
      "learning_rate": 0.00019586598395051252,
      "loss": 0.2331,
      "step": 4884
    },
    {
      "epoch": 0.022978070876882698,
      "grad_norm": 2.7607839107513428,
      "learning_rate": 0.00019586504097239904,
      "loss": 0.4305,
      "step": 4885
    },
    {
      "epoch": 0.02298277467849516,
      "grad_norm": 3.749924898147583,
      "learning_rate": 0.00019586409799428556,
      "loss": 0.3587,
      "step": 4886
    },
    {
      "epoch": 0.022987478480107622,
      "grad_norm": 1.7768205404281616,
      "learning_rate": 0.00019586315501617208,
      "loss": 0.2566,
      "step": 4887
    },
    {
      "epoch": 0.022992182281720085,
      "grad_norm": 1.5299259424209595,
      "learning_rate": 0.00019586221203805862,
      "loss": 0.2779,
      "step": 4888
    },
    {
      "epoch": 0.02299688608333255,
      "grad_norm": 4.366508483886719,
      "learning_rate": 0.00019586126905994514,
      "loss": 0.6873,
      "step": 4889
    },
    {
      "epoch": 0.023001589884945012,
      "grad_norm": 2.2429065704345703,
      "learning_rate": 0.00019586032608183163,
      "loss": 0.3792,
      "step": 4890
    },
    {
      "epoch": 0.023006293686557475,
      "grad_norm": 1.6365280151367188,
      "learning_rate": 0.00019585938310371815,
      "loss": 0.4623,
      "step": 4891
    },
    {
      "epoch": 0.02301099748816994,
      "grad_norm": 1.358696460723877,
      "learning_rate": 0.0001958584401256047,
      "loss": 0.2924,
      "step": 4892
    },
    {
      "epoch": 0.023015701289782402,
      "grad_norm": 2.154477119445801,
      "learning_rate": 0.00019585749714749122,
      "loss": 0.2091,
      "step": 4893
    },
    {
      "epoch": 0.023020405091394865,
      "grad_norm": 2.477799654006958,
      "learning_rate": 0.00019585655416937773,
      "loss": 0.8684,
      "step": 4894
    },
    {
      "epoch": 0.023025108893007327,
      "grad_norm": 3.2435781955718994,
      "learning_rate": 0.00019585561119126425,
      "loss": 0.853,
      "step": 4895
    },
    {
      "epoch": 0.023029812694619792,
      "grad_norm": 1.7219489812850952,
      "learning_rate": 0.00019585466821315077,
      "loss": 0.2664,
      "step": 4896
    },
    {
      "epoch": 0.023034516496232255,
      "grad_norm": 2.0049691200256348,
      "learning_rate": 0.00019585372523503732,
      "loss": 0.4858,
      "step": 4897
    },
    {
      "epoch": 0.023039220297844717,
      "grad_norm": 2.7103049755096436,
      "learning_rate": 0.00019585278225692384,
      "loss": 0.3569,
      "step": 4898
    },
    {
      "epoch": 0.023043924099457182,
      "grad_norm": 1.6841051578521729,
      "learning_rate": 0.00019585183927881036,
      "loss": 0.4062,
      "step": 4899
    },
    {
      "epoch": 0.023048627901069645,
      "grad_norm": 0.9411169290542603,
      "learning_rate": 0.00019585089630069687,
      "loss": 0.2143,
      "step": 4900
    },
    {
      "epoch": 0.023053331702682107,
      "grad_norm": 2.256716728210449,
      "learning_rate": 0.0001958499533225834,
      "loss": 0.2984,
      "step": 4901
    },
    {
      "epoch": 0.023058035504294572,
      "grad_norm": 1.9893555641174316,
      "learning_rate": 0.0001958490103444699,
      "loss": 0.3799,
      "step": 4902
    },
    {
      "epoch": 0.023062739305907035,
      "grad_norm": 0.3342714309692383,
      "learning_rate": 0.00019584806736635643,
      "loss": 0.0253,
      "step": 4903
    },
    {
      "epoch": 0.023067443107519497,
      "grad_norm": 4.984592914581299,
      "learning_rate": 0.00019584712438824295,
      "loss": 0.5926,
      "step": 4904
    },
    {
      "epoch": 0.02307214690913196,
      "grad_norm": 0.9932512640953064,
      "learning_rate": 0.00019584618141012947,
      "loss": 0.2172,
      "step": 4905
    },
    {
      "epoch": 0.023076850710744425,
      "grad_norm": 1.0192254781723022,
      "learning_rate": 0.000195845238432016,
      "loss": 0.1847,
      "step": 4906
    },
    {
      "epoch": 0.023081554512356887,
      "grad_norm": 2.785944938659668,
      "learning_rate": 0.00019584429545390253,
      "loss": 0.7257,
      "step": 4907
    },
    {
      "epoch": 0.02308625831396935,
      "grad_norm": 3.551424026489258,
      "learning_rate": 0.00019584335247578905,
      "loss": 0.8024,
      "step": 4908
    },
    {
      "epoch": 0.023090962115581815,
      "grad_norm": 1.7680891752243042,
      "learning_rate": 0.00019584240949767557,
      "loss": 0.3942,
      "step": 4909
    },
    {
      "epoch": 0.023095665917194277,
      "grad_norm": 2.1763756275177,
      "learning_rate": 0.0001958414665195621,
      "loss": 0.344,
      "step": 4910
    },
    {
      "epoch": 0.02310036971880674,
      "grad_norm": 2.093693494796753,
      "learning_rate": 0.0001958405235414486,
      "loss": 0.4631,
      "step": 4911
    },
    {
      "epoch": 0.0231050735204192,
      "grad_norm": 0.8785091042518616,
      "learning_rate": 0.00019583958056333512,
      "loss": 0.1027,
      "step": 4912
    },
    {
      "epoch": 0.023109777322031667,
      "grad_norm": 3.138505458831787,
      "learning_rate": 0.00019583863758522164,
      "loss": 0.5351,
      "step": 4913
    },
    {
      "epoch": 0.02311448112364413,
      "grad_norm": 0.440544068813324,
      "learning_rate": 0.00019583769460710816,
      "loss": 0.0676,
      "step": 4914
    },
    {
      "epoch": 0.02311918492525659,
      "grad_norm": 3.0783979892730713,
      "learning_rate": 0.0001958367516289947,
      "loss": 0.6619,
      "step": 4915
    },
    {
      "epoch": 0.023123888726869057,
      "grad_norm": 1.8985217809677124,
      "learning_rate": 0.00019583580865088123,
      "loss": 0.3931,
      "step": 4916
    },
    {
      "epoch": 0.02312859252848152,
      "grad_norm": 1.669616937637329,
      "learning_rate": 0.00019583486567276774,
      "loss": 0.4326,
      "step": 4917
    },
    {
      "epoch": 0.02313329633009398,
      "grad_norm": 1.6041866540908813,
      "learning_rate": 0.00019583392269465426,
      "loss": 0.3498,
      "step": 4918
    },
    {
      "epoch": 0.023138000131706447,
      "grad_norm": 1.1395303010940552,
      "learning_rate": 0.0001958329797165408,
      "loss": 0.177,
      "step": 4919
    },
    {
      "epoch": 0.02314270393331891,
      "grad_norm": 2.1298303604125977,
      "learning_rate": 0.00019583203673842733,
      "loss": 0.5016,
      "step": 4920
    },
    {
      "epoch": 0.02314740773493137,
      "grad_norm": 1.4660789966583252,
      "learning_rate": 0.00019583109376031382,
      "loss": 0.1971,
      "step": 4921
    },
    {
      "epoch": 0.023152111536543833,
      "grad_norm": 1.43989896774292,
      "learning_rate": 0.00019583015078220034,
      "loss": 0.359,
      "step": 4922
    },
    {
      "epoch": 0.0231568153381563,
      "grad_norm": 1.660132884979248,
      "learning_rate": 0.00019582920780408686,
      "loss": 0.3442,
      "step": 4923
    },
    {
      "epoch": 0.02316151913976876,
      "grad_norm": 0.7113617062568665,
      "learning_rate": 0.0001958282648259734,
      "loss": 0.1506,
      "step": 4924
    },
    {
      "epoch": 0.023166222941381223,
      "grad_norm": 1.3156074285507202,
      "learning_rate": 0.00019582732184785992,
      "loss": 0.4142,
      "step": 4925
    },
    {
      "epoch": 0.02317092674299369,
      "grad_norm": 1.291894793510437,
      "learning_rate": 0.00019582637886974644,
      "loss": 0.3724,
      "step": 4926
    },
    {
      "epoch": 0.02317563054460615,
      "grad_norm": 0.6361099481582642,
      "learning_rate": 0.00019582543589163296,
      "loss": 0.2315,
      "step": 4927
    },
    {
      "epoch": 0.023180334346218613,
      "grad_norm": 1.6814194917678833,
      "learning_rate": 0.0001958244929135195,
      "loss": 0.3129,
      "step": 4928
    },
    {
      "epoch": 0.02318503814783108,
      "grad_norm": 0.9520938992500305,
      "learning_rate": 0.00019582354993540602,
      "loss": 0.1631,
      "step": 4929
    },
    {
      "epoch": 0.02318974194944354,
      "grad_norm": 1.5496563911437988,
      "learning_rate": 0.00019582260695729254,
      "loss": 0.409,
      "step": 4930
    },
    {
      "epoch": 0.023194445751056003,
      "grad_norm": 1.0668001174926758,
      "learning_rate": 0.00019582166397917906,
      "loss": 0.2806,
      "step": 4931
    },
    {
      "epoch": 0.023199149552668465,
      "grad_norm": 0.5401169657707214,
      "learning_rate": 0.00019582072100106558,
      "loss": 0.0847,
      "step": 4932
    },
    {
      "epoch": 0.02320385335428093,
      "grad_norm": 0.9641879200935364,
      "learning_rate": 0.0001958197780229521,
      "loss": 0.1278,
      "step": 4933
    },
    {
      "epoch": 0.023208557155893393,
      "grad_norm": 0.7566136717796326,
      "learning_rate": 0.00019581883504483862,
      "loss": 0.1688,
      "step": 4934
    },
    {
      "epoch": 0.023213260957505855,
      "grad_norm": 0.7822988629341125,
      "learning_rate": 0.00019581789206672513,
      "loss": 0.0801,
      "step": 4935
    },
    {
      "epoch": 0.02321796475911832,
      "grad_norm": 2.045341968536377,
      "learning_rate": 0.00019581694908861165,
      "loss": 0.3298,
      "step": 4936
    },
    {
      "epoch": 0.023222668560730783,
      "grad_norm": 1.5697276592254639,
      "learning_rate": 0.0001958160061104982,
      "loss": 0.3185,
      "step": 4937
    },
    {
      "epoch": 0.023227372362343245,
      "grad_norm": 2.197011709213257,
      "learning_rate": 0.00019581506313238472,
      "loss": 0.1928,
      "step": 4938
    },
    {
      "epoch": 0.023232076163955707,
      "grad_norm": 0.9483405351638794,
      "learning_rate": 0.00019581412015427124,
      "loss": 0.0965,
      "step": 4939
    },
    {
      "epoch": 0.023236779965568173,
      "grad_norm": 3.09954571723938,
      "learning_rate": 0.00019581317717615775,
      "loss": 0.4913,
      "step": 4940
    },
    {
      "epoch": 0.023241483767180635,
      "grad_norm": 1.729782223701477,
      "learning_rate": 0.00019581223419804427,
      "loss": 0.2494,
      "step": 4941
    },
    {
      "epoch": 0.023246187568793097,
      "grad_norm": 0.9412187337875366,
      "learning_rate": 0.0001958112912199308,
      "loss": 0.2295,
      "step": 4942
    },
    {
      "epoch": 0.023250891370405563,
      "grad_norm": 1.7999097108840942,
      "learning_rate": 0.0001958103482418173,
      "loss": 0.1579,
      "step": 4943
    },
    {
      "epoch": 0.023255595172018025,
      "grad_norm": 1.9804049730300903,
      "learning_rate": 0.00019580940526370383,
      "loss": 0.1551,
      "step": 4944
    },
    {
      "epoch": 0.023260298973630487,
      "grad_norm": 2.3488175868988037,
      "learning_rate": 0.00019580846228559035,
      "loss": 0.3996,
      "step": 4945
    },
    {
      "epoch": 0.023265002775242953,
      "grad_norm": 1.0780589580535889,
      "learning_rate": 0.00019580751930747687,
      "loss": 0.1411,
      "step": 4946
    },
    {
      "epoch": 0.023269706576855415,
      "grad_norm": 2.679724931716919,
      "learning_rate": 0.0001958065763293634,
      "loss": 0.6041,
      "step": 4947
    },
    {
      "epoch": 0.023274410378467877,
      "grad_norm": 1.3298512697219849,
      "learning_rate": 0.00019580563335124993,
      "loss": 0.1427,
      "step": 4948
    },
    {
      "epoch": 0.02327911418008034,
      "grad_norm": 2.1587038040161133,
      "learning_rate": 0.00019580469037313645,
      "loss": 0.1386,
      "step": 4949
    },
    {
      "epoch": 0.023283817981692805,
      "grad_norm": 0.500649094581604,
      "learning_rate": 0.00019580374739502297,
      "loss": 0.042,
      "step": 4950
    },
    {
      "epoch": 0.023288521783305267,
      "grad_norm": 9.432027816772461,
      "learning_rate": 0.00019580280441690951,
      "loss": 0.1735,
      "step": 4951
    },
    {
      "epoch": 0.02329322558491773,
      "grad_norm": 0.2906748950481415,
      "learning_rate": 0.000195801861438796,
      "loss": 0.0185,
      "step": 4952
    },
    {
      "epoch": 0.023297929386530195,
      "grad_norm": 2.6420812606811523,
      "learning_rate": 0.00019580091846068252,
      "loss": 0.166,
      "step": 4953
    },
    {
      "epoch": 0.023302633188142657,
      "grad_norm": 1.710063099861145,
      "learning_rate": 0.00019579997548256904,
      "loss": 0.2204,
      "step": 4954
    },
    {
      "epoch": 0.02330733698975512,
      "grad_norm": 0.7913016080856323,
      "learning_rate": 0.00019579903250445556,
      "loss": 0.0698,
      "step": 4955
    },
    {
      "epoch": 0.02331204079136758,
      "grad_norm": 4.708489418029785,
      "learning_rate": 0.0001957980895263421,
      "loss": 1.1546,
      "step": 4956
    },
    {
      "epoch": 0.023316744592980047,
      "grad_norm": 3.0440664291381836,
      "learning_rate": 0.00019579714654822863,
      "loss": 0.4195,
      "step": 4957
    },
    {
      "epoch": 0.02332144839459251,
      "grad_norm": 4.8351945877075195,
      "learning_rate": 0.00019579620357011514,
      "loss": 0.5371,
      "step": 4958
    },
    {
      "epoch": 0.02332615219620497,
      "grad_norm": 3.324441909790039,
      "learning_rate": 0.00019579526059200166,
      "loss": 0.3213,
      "step": 4959
    },
    {
      "epoch": 0.023330855997817437,
      "grad_norm": 8.415428161621094,
      "learning_rate": 0.0001957943176138882,
      "loss": 0.2843,
      "step": 4960
    },
    {
      "epoch": 0.0233355597994299,
      "grad_norm": 3.2482478618621826,
      "learning_rate": 0.00019579337463577473,
      "loss": 0.3714,
      "step": 4961
    },
    {
      "epoch": 0.02334026360104236,
      "grad_norm": 1.5179262161254883,
      "learning_rate": 0.00019579243165766125,
      "loss": 0.2026,
      "step": 4962
    },
    {
      "epoch": 0.023344967402654827,
      "grad_norm": 0.2278747260570526,
      "learning_rate": 0.00019579148867954777,
      "loss": 0.0114,
      "step": 4963
    },
    {
      "epoch": 0.02334967120426729,
      "grad_norm": 7.114585876464844,
      "learning_rate": 0.00019579054570143426,
      "loss": 0.2514,
      "step": 4964
    },
    {
      "epoch": 0.02335437500587975,
      "grad_norm": 0.2868928015232086,
      "learning_rate": 0.0001957896027233208,
      "loss": 0.0158,
      "step": 4965
    },
    {
      "epoch": 0.023359078807492214,
      "grad_norm": 4.578009128570557,
      "learning_rate": 0.00019578865974520732,
      "loss": 0.824,
      "step": 4966
    },
    {
      "epoch": 0.02336378260910468,
      "grad_norm": 2.886604070663452,
      "learning_rate": 0.00019578771676709384,
      "loss": 0.3779,
      "step": 4967
    },
    {
      "epoch": 0.02336848641071714,
      "grad_norm": 1.3180416822433472,
      "learning_rate": 0.00019578677378898036,
      "loss": 0.1076,
      "step": 4968
    },
    {
      "epoch": 0.023373190212329604,
      "grad_norm": 1.7807695865631104,
      "learning_rate": 0.0001957858308108669,
      "loss": 0.211,
      "step": 4969
    },
    {
      "epoch": 0.02337789401394207,
      "grad_norm": 2.3409924507141113,
      "learning_rate": 0.00019578488783275342,
      "loss": 0.2811,
      "step": 4970
    },
    {
      "epoch": 0.02338259781555453,
      "grad_norm": 4.673250198364258,
      "learning_rate": 0.00019578394485463994,
      "loss": 1.1987,
      "step": 4971
    },
    {
      "epoch": 0.023387301617166994,
      "grad_norm": 2.144507884979248,
      "learning_rate": 0.00019578300187652646,
      "loss": 0.5031,
      "step": 4972
    },
    {
      "epoch": 0.023392005418779456,
      "grad_norm": 2.5160913467407227,
      "learning_rate": 0.00019578205889841298,
      "loss": 0.3619,
      "step": 4973
    },
    {
      "epoch": 0.02339670922039192,
      "grad_norm": 2.0744431018829346,
      "learning_rate": 0.0001957811159202995,
      "loss": 0.3495,
      "step": 4974
    },
    {
      "epoch": 0.023401413022004384,
      "grad_norm": 1.3349705934524536,
      "learning_rate": 0.00019578017294218602,
      "loss": 0.128,
      "step": 4975
    },
    {
      "epoch": 0.023406116823616846,
      "grad_norm": 4.168778419494629,
      "learning_rate": 0.00019577922996407253,
      "loss": 0.6851,
      "step": 4976
    },
    {
      "epoch": 0.02341082062522931,
      "grad_norm": 1.6408387422561646,
      "learning_rate": 0.00019577828698595905,
      "loss": 0.2018,
      "step": 4977
    },
    {
      "epoch": 0.023415524426841774,
      "grad_norm": 2.687741994857788,
      "learning_rate": 0.0001957773440078456,
      "loss": 0.5962,
      "step": 4978
    },
    {
      "epoch": 0.023420228228454236,
      "grad_norm": 2.0259623527526855,
      "learning_rate": 0.00019577640102973212,
      "loss": 0.3907,
      "step": 4979
    },
    {
      "epoch": 0.0234249320300667,
      "grad_norm": 1.7141387462615967,
      "learning_rate": 0.00019577545805161864,
      "loss": 0.4656,
      "step": 4980
    },
    {
      "epoch": 0.023429635831679164,
      "grad_norm": 1.7602053880691528,
      "learning_rate": 0.00019577451507350515,
      "loss": 0.3575,
      "step": 4981
    },
    {
      "epoch": 0.023434339633291626,
      "grad_norm": 1.584256887435913,
      "learning_rate": 0.00019577357209539167,
      "loss": 0.3794,
      "step": 4982
    },
    {
      "epoch": 0.023439043434904088,
      "grad_norm": 2.9260523319244385,
      "learning_rate": 0.0001957726291172782,
      "loss": 0.339,
      "step": 4983
    },
    {
      "epoch": 0.023443747236516554,
      "grad_norm": 1.7318487167358398,
      "learning_rate": 0.0001957716861391647,
      "loss": 0.3728,
      "step": 4984
    },
    {
      "epoch": 0.023448451038129016,
      "grad_norm": 3.152301788330078,
      "learning_rate": 0.00019577074316105123,
      "loss": 0.5521,
      "step": 4985
    },
    {
      "epoch": 0.023453154839741478,
      "grad_norm": 0.7172564268112183,
      "learning_rate": 0.00019576980018293775,
      "loss": 0.1665,
      "step": 4986
    },
    {
      "epoch": 0.023457858641353944,
      "grad_norm": 2.3333303928375244,
      "learning_rate": 0.0001957688572048243,
      "loss": 0.4959,
      "step": 4987
    },
    {
      "epoch": 0.023462562442966406,
      "grad_norm": 0.8287210464477539,
      "learning_rate": 0.0001957679142267108,
      "loss": 0.0975,
      "step": 4988
    },
    {
      "epoch": 0.023467266244578868,
      "grad_norm": 1.8453145027160645,
      "learning_rate": 0.00019576697124859733,
      "loss": 0.5669,
      "step": 4989
    },
    {
      "epoch": 0.02347197004619133,
      "grad_norm": 0.7479321360588074,
      "learning_rate": 0.00019576602827048385,
      "loss": 0.1325,
      "step": 4990
    },
    {
      "epoch": 0.023476673847803796,
      "grad_norm": 0.9789323806762695,
      "learning_rate": 0.00019576508529237037,
      "loss": 0.1313,
      "step": 4991
    },
    {
      "epoch": 0.023481377649416258,
      "grad_norm": 1.1227612495422363,
      "learning_rate": 0.00019576414231425691,
      "loss": 0.1628,
      "step": 4992
    },
    {
      "epoch": 0.02348608145102872,
      "grad_norm": 1.6736408472061157,
      "learning_rate": 0.00019576319933614343,
      "loss": 0.2017,
      "step": 4993
    },
    {
      "epoch": 0.023490785252641186,
      "grad_norm": 0.83321613073349,
      "learning_rate": 0.00019576225635802995,
      "loss": 0.2383,
      "step": 4994
    },
    {
      "epoch": 0.023495489054253648,
      "grad_norm": 1.9693890810012817,
      "learning_rate": 0.00019576131337991644,
      "loss": 0.3035,
      "step": 4995
    },
    {
      "epoch": 0.02350019285586611,
      "grad_norm": 1.1260777711868286,
      "learning_rate": 0.00019576037040180296,
      "loss": 0.1963,
      "step": 4996
    },
    {
      "epoch": 0.023504896657478576,
      "grad_norm": 2.803110361099243,
      "learning_rate": 0.0001957594274236895,
      "loss": 0.3686,
      "step": 4997
    },
    {
      "epoch": 0.023509600459091038,
      "grad_norm": 2.1724796295166016,
      "learning_rate": 0.00019575848444557603,
      "loss": 0.4449,
      "step": 4998
    },
    {
      "epoch": 0.0235143042607035,
      "grad_norm": 1.553391456604004,
      "learning_rate": 0.00019575754146746254,
      "loss": 0.2098,
      "step": 4999
    },
    {
      "epoch": 0.023519008062315962,
      "grad_norm": 0.32297176122665405,
      "learning_rate": 0.00019575659848934906,
      "loss": 0.0318,
      "step": 5000
    },
    {
      "epoch": 0.023523711863928428,
      "grad_norm": 3.6743502616882324,
      "learning_rate": 0.0001957556555112356,
      "loss": 0.4371,
      "step": 5001
    },
    {
      "epoch": 0.02352841566554089,
      "grad_norm": 4.522095680236816,
      "learning_rate": 0.00019575471253312213,
      "loss": 0.6737,
      "step": 5002
    },
    {
      "epoch": 0.023533119467153352,
      "grad_norm": 1.6519495248794556,
      "learning_rate": 0.00019575376955500865,
      "loss": 0.2617,
      "step": 5003
    },
    {
      "epoch": 0.023537823268765818,
      "grad_norm": 3.2248051166534424,
      "learning_rate": 0.00019575282657689517,
      "loss": 0.4968,
      "step": 5004
    },
    {
      "epoch": 0.02354252707037828,
      "grad_norm": 1.1410794258117676,
      "learning_rate": 0.00019575188359878168,
      "loss": 0.2817,
      "step": 5005
    },
    {
      "epoch": 0.023547230871990742,
      "grad_norm": 2.9430220127105713,
      "learning_rate": 0.0001957509406206682,
      "loss": 0.7603,
      "step": 5006
    },
    {
      "epoch": 0.023551934673603205,
      "grad_norm": 1.795211911201477,
      "learning_rate": 0.00019574999764255472,
      "loss": 0.4729,
      "step": 5007
    },
    {
      "epoch": 0.02355663847521567,
      "grad_norm": 0.30815184116363525,
      "learning_rate": 0.00019574905466444124,
      "loss": 0.0546,
      "step": 5008
    },
    {
      "epoch": 0.023561342276828132,
      "grad_norm": 14.943511962890625,
      "learning_rate": 0.00019574811168632776,
      "loss": 0.3116,
      "step": 5009
    },
    {
      "epoch": 0.023566046078440595,
      "grad_norm": 1.2902058362960815,
      "learning_rate": 0.0001957471687082143,
      "loss": 0.217,
      "step": 5010
    },
    {
      "epoch": 0.02357074988005306,
      "grad_norm": 1.586822271347046,
      "learning_rate": 0.00019574622573010082,
      "loss": 0.2164,
      "step": 5011
    },
    {
      "epoch": 0.023575453681665522,
      "grad_norm": 0.8363018035888672,
      "learning_rate": 0.00019574528275198734,
      "loss": 0.1611,
      "step": 5012
    },
    {
      "epoch": 0.023580157483277984,
      "grad_norm": 2.558014392852783,
      "learning_rate": 0.00019574433977387386,
      "loss": 0.6763,
      "step": 5013
    },
    {
      "epoch": 0.02358486128489045,
      "grad_norm": 4.045482158660889,
      "learning_rate": 0.00019574339679576038,
      "loss": 0.5321,
      "step": 5014
    },
    {
      "epoch": 0.023589565086502912,
      "grad_norm": 1.1906425952911377,
      "learning_rate": 0.0001957424538176469,
      "loss": 0.2042,
      "step": 5015
    },
    {
      "epoch": 0.023594268888115374,
      "grad_norm": 3.5106093883514404,
      "learning_rate": 0.00019574151083953342,
      "loss": 1.2734,
      "step": 5016
    },
    {
      "epoch": 0.023598972689727837,
      "grad_norm": 6.842299461364746,
      "learning_rate": 0.00019574056786141993,
      "loss": 0.7997,
      "step": 5017
    },
    {
      "epoch": 0.023603676491340302,
      "grad_norm": 1.3108266592025757,
      "learning_rate": 0.00019573962488330645,
      "loss": 0.2578,
      "step": 5018
    },
    {
      "epoch": 0.023608380292952764,
      "grad_norm": 0.367944598197937,
      "learning_rate": 0.000195738681905193,
      "loss": 0.0442,
      "step": 5019
    },
    {
      "epoch": 0.023613084094565227,
      "grad_norm": 3.329369306564331,
      "learning_rate": 0.00019573773892707952,
      "loss": 0.5937,
      "step": 5020
    },
    {
      "epoch": 0.023617787896177692,
      "grad_norm": 1.4401158094406128,
      "learning_rate": 0.00019573679594896604,
      "loss": 0.3914,
      "step": 5021
    },
    {
      "epoch": 0.023622491697790154,
      "grad_norm": 1.3076657056808472,
      "learning_rate": 0.00019573585297085255,
      "loss": 0.243,
      "step": 5022
    },
    {
      "epoch": 0.023627195499402617,
      "grad_norm": 2.7690646648406982,
      "learning_rate": 0.00019573490999273907,
      "loss": 0.7206,
      "step": 5023
    },
    {
      "epoch": 0.02363189930101508,
      "grad_norm": 1.6232020854949951,
      "learning_rate": 0.00019573396701462562,
      "loss": 0.2884,
      "step": 5024
    },
    {
      "epoch": 0.023636603102627544,
      "grad_norm": 1.1908793449401855,
      "learning_rate": 0.00019573302403651214,
      "loss": 0.1771,
      "step": 5025
    },
    {
      "epoch": 0.023641306904240007,
      "grad_norm": 2.1062088012695312,
      "learning_rate": 0.00019573208105839863,
      "loss": 0.3939,
      "step": 5026
    },
    {
      "epoch": 0.02364601070585247,
      "grad_norm": 1.9343667030334473,
      "learning_rate": 0.00019573113808028515,
      "loss": 0.3873,
      "step": 5027
    },
    {
      "epoch": 0.023650714507464934,
      "grad_norm": 1.1388394832611084,
      "learning_rate": 0.0001957301951021717,
      "loss": 0.1701,
      "step": 5028
    },
    {
      "epoch": 0.023655418309077397,
      "grad_norm": 1.6110645532608032,
      "learning_rate": 0.0001957292521240582,
      "loss": 0.1914,
      "step": 5029
    },
    {
      "epoch": 0.02366012211068986,
      "grad_norm": 1.6550612449645996,
      "learning_rate": 0.00019572830914594473,
      "loss": 0.2503,
      "step": 5030
    },
    {
      "epoch": 0.023664825912302324,
      "grad_norm": 0.7114784121513367,
      "learning_rate": 0.00019572736616783125,
      "loss": 0.0724,
      "step": 5031
    },
    {
      "epoch": 0.023669529713914787,
      "grad_norm": 0.7878510355949402,
      "learning_rate": 0.00019572642318971777,
      "loss": 0.076,
      "step": 5032
    },
    {
      "epoch": 0.02367423351552725,
      "grad_norm": 1.2158185243606567,
      "learning_rate": 0.00019572548021160431,
      "loss": 0.13,
      "step": 5033
    },
    {
      "epoch": 0.02367893731713971,
      "grad_norm": 1.3552169799804688,
      "learning_rate": 0.00019572453723349083,
      "loss": 0.1759,
      "step": 5034
    },
    {
      "epoch": 0.023683641118752177,
      "grad_norm": 0.4237730801105499,
      "learning_rate": 0.00019572359425537735,
      "loss": 0.0729,
      "step": 5035
    },
    {
      "epoch": 0.02368834492036464,
      "grad_norm": 0.7731040716171265,
      "learning_rate": 0.00019572265127726387,
      "loss": 0.0997,
      "step": 5036
    },
    {
      "epoch": 0.0236930487219771,
      "grad_norm": 1.431809425354004,
      "learning_rate": 0.0001957217082991504,
      "loss": 0.2682,
      "step": 5037
    },
    {
      "epoch": 0.023697752523589567,
      "grad_norm": 1.1302390098571777,
      "learning_rate": 0.0001957207653210369,
      "loss": 0.1273,
      "step": 5038
    },
    {
      "epoch": 0.02370245632520203,
      "grad_norm": 1.0042881965637207,
      "learning_rate": 0.00019571982234292343,
      "loss": 0.2172,
      "step": 5039
    },
    {
      "epoch": 0.02370716012681449,
      "grad_norm": 2.155913829803467,
      "learning_rate": 0.00019571887936480994,
      "loss": 0.2791,
      "step": 5040
    },
    {
      "epoch": 0.023711863928426953,
      "grad_norm": 1.621728539466858,
      "learning_rate": 0.00019571793638669646,
      "loss": 0.2334,
      "step": 5041
    },
    {
      "epoch": 0.02371656773003942,
      "grad_norm": 0.63458251953125,
      "learning_rate": 0.000195716993408583,
      "loss": 0.0668,
      "step": 5042
    },
    {
      "epoch": 0.02372127153165188,
      "grad_norm": 1.1961326599121094,
      "learning_rate": 0.00019571605043046953,
      "loss": 0.1953,
      "step": 5043
    },
    {
      "epoch": 0.023725975333264343,
      "grad_norm": 2.9849960803985596,
      "learning_rate": 0.00019571510745235605,
      "loss": 0.6693,
      "step": 5044
    },
    {
      "epoch": 0.02373067913487681,
      "grad_norm": 1.958693027496338,
      "learning_rate": 0.00019571416447424257,
      "loss": 0.3872,
      "step": 5045
    },
    {
      "epoch": 0.02373538293648927,
      "grad_norm": 4.808949947357178,
      "learning_rate": 0.00019571322149612908,
      "loss": 0.444,
      "step": 5046
    },
    {
      "epoch": 0.023740086738101733,
      "grad_norm": 0.2253102958202362,
      "learning_rate": 0.0001957122785180156,
      "loss": 0.0187,
      "step": 5047
    },
    {
      "epoch": 0.0237447905397142,
      "grad_norm": 1.3865978717803955,
      "learning_rate": 0.00019571133553990212,
      "loss": 0.2814,
      "step": 5048
    },
    {
      "epoch": 0.02374949434132666,
      "grad_norm": 0.29155951738357544,
      "learning_rate": 0.00019571039256178864,
      "loss": 0.0242,
      "step": 5049
    },
    {
      "epoch": 0.023754198142939123,
      "grad_norm": 1.3404630422592163,
      "learning_rate": 0.00019570944958367516,
      "loss": 0.14,
      "step": 5050
    },
    {
      "epoch": 0.023758901944551585,
      "grad_norm": 4.175556182861328,
      "learning_rate": 0.0001957085066055617,
      "loss": 0.6759,
      "step": 5051
    },
    {
      "epoch": 0.02376360574616405,
      "grad_norm": 1.869273066520691,
      "learning_rate": 0.00019570756362744822,
      "loss": 0.3875,
      "step": 5052
    },
    {
      "epoch": 0.023768309547776513,
      "grad_norm": 2.050686836242676,
      "learning_rate": 0.00019570662064933474,
      "loss": 0.2017,
      "step": 5053
    },
    {
      "epoch": 0.023773013349388975,
      "grad_norm": 1.0344332456588745,
      "learning_rate": 0.00019570567767122126,
      "loss": 0.0981,
      "step": 5054
    },
    {
      "epoch": 0.02377771715100144,
      "grad_norm": 1.3676961660385132,
      "learning_rate": 0.00019570473469310778,
      "loss": 0.339,
      "step": 5055
    },
    {
      "epoch": 0.023782420952613903,
      "grad_norm": 0.7331574559211731,
      "learning_rate": 0.00019570379171499432,
      "loss": 0.102,
      "step": 5056
    },
    {
      "epoch": 0.023787124754226365,
      "grad_norm": 1.7017937898635864,
      "learning_rate": 0.00019570284873688082,
      "loss": 0.3058,
      "step": 5057
    },
    {
      "epoch": 0.023791828555838827,
      "grad_norm": 0.7765277028083801,
      "learning_rate": 0.00019570190575876733,
      "loss": 0.1296,
      "step": 5058
    },
    {
      "epoch": 0.023796532357451293,
      "grad_norm": 0.6564870476722717,
      "learning_rate": 0.00019570096278065385,
      "loss": 0.0862,
      "step": 5059
    },
    {
      "epoch": 0.023801236159063755,
      "grad_norm": 1.5058180093765259,
      "learning_rate": 0.0001957000198025404,
      "loss": 0.2403,
      "step": 5060
    },
    {
      "epoch": 0.023805939960676217,
      "grad_norm": 2.087327241897583,
      "learning_rate": 0.00019569907682442692,
      "loss": 0.1743,
      "step": 5061
    },
    {
      "epoch": 0.023810643762288683,
      "grad_norm": 0.27726441621780396,
      "learning_rate": 0.00019569813384631344,
      "loss": 0.0278,
      "step": 5062
    },
    {
      "epoch": 0.023815347563901145,
      "grad_norm": 1.871226191520691,
      "learning_rate": 0.00019569719086819995,
      "loss": 0.2258,
      "step": 5063
    },
    {
      "epoch": 0.023820051365513607,
      "grad_norm": 0.26543402671813965,
      "learning_rate": 0.00019569624789008647,
      "loss": 0.0218,
      "step": 5064
    },
    {
      "epoch": 0.023824755167126073,
      "grad_norm": 0.5521519184112549,
      "learning_rate": 0.00019569530491197302,
      "loss": 0.0407,
      "step": 5065
    },
    {
      "epoch": 0.023829458968738535,
      "grad_norm": 2.339081287384033,
      "learning_rate": 0.00019569436193385954,
      "loss": 0.5706,
      "step": 5066
    },
    {
      "epoch": 0.023834162770350997,
      "grad_norm": 1.1993299722671509,
      "learning_rate": 0.00019569341895574606,
      "loss": 0.1238,
      "step": 5067
    },
    {
      "epoch": 0.02383886657196346,
      "grad_norm": 1.6385955810546875,
      "learning_rate": 0.00019569247597763255,
      "loss": 0.1964,
      "step": 5068
    },
    {
      "epoch": 0.023843570373575925,
      "grad_norm": 3.478447198867798,
      "learning_rate": 0.0001956915329995191,
      "loss": 0.4495,
      "step": 5069
    },
    {
      "epoch": 0.023848274175188387,
      "grad_norm": 23.921165466308594,
      "learning_rate": 0.0001956905900214056,
      "loss": 0.5122,
      "step": 5070
    },
    {
      "epoch": 0.02385297797680085,
      "grad_norm": 0.5387915968894958,
      "learning_rate": 0.00019568964704329213,
      "loss": 0.0522,
      "step": 5071
    },
    {
      "epoch": 0.023857681778413315,
      "grad_norm": 0.3544189929962158,
      "learning_rate": 0.00019568870406517865,
      "loss": 0.0231,
      "step": 5072
    },
    {
      "epoch": 0.023862385580025777,
      "grad_norm": 8.242426872253418,
      "learning_rate": 0.00019568776108706517,
      "loss": 1.1104,
      "step": 5073
    },
    {
      "epoch": 0.02386708938163824,
      "grad_norm": 0.9852491021156311,
      "learning_rate": 0.00019568681810895171,
      "loss": 0.077,
      "step": 5074
    },
    {
      "epoch": 0.0238717931832507,
      "grad_norm": 1.0073310136795044,
      "learning_rate": 0.00019568587513083823,
      "loss": 0.0795,
      "step": 5075
    },
    {
      "epoch": 0.023876496984863167,
      "grad_norm": 0.36431753635406494,
      "learning_rate": 0.00019568493215272475,
      "loss": 0.0194,
      "step": 5076
    },
    {
      "epoch": 0.02388120078647563,
      "grad_norm": 3.238939046859741,
      "learning_rate": 0.00019568398917461127,
      "loss": 0.4612,
      "step": 5077
    },
    {
      "epoch": 0.02388590458808809,
      "grad_norm": 3.5868406295776367,
      "learning_rate": 0.0001956830461964978,
      "loss": 0.423,
      "step": 5078
    },
    {
      "epoch": 0.023890608389700557,
      "grad_norm": 4.35233736038208,
      "learning_rate": 0.0001956821032183843,
      "loss": 0.8879,
      "step": 5079
    },
    {
      "epoch": 0.02389531219131302,
      "grad_norm": 2.242055654525757,
      "learning_rate": 0.00019568116024027083,
      "loss": 0.4,
      "step": 5080
    },
    {
      "epoch": 0.02390001599292548,
      "grad_norm": 0.9204925894737244,
      "learning_rate": 0.00019568021726215734,
      "loss": 0.0914,
      "step": 5081
    },
    {
      "epoch": 0.023904719794537947,
      "grad_norm": 1.5585317611694336,
      "learning_rate": 0.00019567927428404386,
      "loss": 0.2915,
      "step": 5082
    },
    {
      "epoch": 0.02390942359615041,
      "grad_norm": 1.965334415435791,
      "learning_rate": 0.0001956783313059304,
      "loss": 0.3499,
      "step": 5083
    },
    {
      "epoch": 0.02391412739776287,
      "grad_norm": 0.6348370909690857,
      "learning_rate": 0.00019567738832781693,
      "loss": 0.038,
      "step": 5084
    },
    {
      "epoch": 0.023918831199375334,
      "grad_norm": 1.873757004737854,
      "learning_rate": 0.00019567644534970345,
      "loss": 0.3479,
      "step": 5085
    },
    {
      "epoch": 0.0239235350009878,
      "grad_norm": 2.6841139793395996,
      "learning_rate": 0.00019567550237158997,
      "loss": 0.2855,
      "step": 5086
    },
    {
      "epoch": 0.02392823880260026,
      "grad_norm": 0.6788502931594849,
      "learning_rate": 0.0001956745593934765,
      "loss": 0.1252,
      "step": 5087
    },
    {
      "epoch": 0.023932942604212724,
      "grad_norm": 2.0238146781921387,
      "learning_rate": 0.000195673616415363,
      "loss": 0.1648,
      "step": 5088
    },
    {
      "epoch": 0.02393764640582519,
      "grad_norm": 2.107164144515991,
      "learning_rate": 0.00019567267343724952,
      "loss": 0.2959,
      "step": 5089
    },
    {
      "epoch": 0.02394235020743765,
      "grad_norm": 7.728880405426025,
      "learning_rate": 0.00019567173045913604,
      "loss": 0.4372,
      "step": 5090
    },
    {
      "epoch": 0.023947054009050114,
      "grad_norm": 1.5230931043624878,
      "learning_rate": 0.00019567078748102256,
      "loss": 0.1921,
      "step": 5091
    },
    {
      "epoch": 0.023951757810662576,
      "grad_norm": 0.5849301218986511,
      "learning_rate": 0.0001956698445029091,
      "loss": 0.0453,
      "step": 5092
    },
    {
      "epoch": 0.02395646161227504,
      "grad_norm": 2.748142719268799,
      "learning_rate": 0.00019566890152479562,
      "loss": 0.3627,
      "step": 5093
    },
    {
      "epoch": 0.023961165413887504,
      "grad_norm": 1.4213184118270874,
      "learning_rate": 0.00019566795854668214,
      "loss": 0.2682,
      "step": 5094
    },
    {
      "epoch": 0.023965869215499966,
      "grad_norm": 3.191805839538574,
      "learning_rate": 0.00019566701556856866,
      "loss": 0.5185,
      "step": 5095
    },
    {
      "epoch": 0.02397057301711243,
      "grad_norm": 0.7785848379135132,
      "learning_rate": 0.00019566607259045518,
      "loss": 0.0606,
      "step": 5096
    },
    {
      "epoch": 0.023975276818724894,
      "grad_norm": 0.26975274085998535,
      "learning_rate": 0.00019566512961234172,
      "loss": 0.0178,
      "step": 5097
    },
    {
      "epoch": 0.023979980620337356,
      "grad_norm": 2.6012394428253174,
      "learning_rate": 0.00019566418663422824,
      "loss": 0.1528,
      "step": 5098
    },
    {
      "epoch": 0.02398468442194982,
      "grad_norm": 0.34915703535079956,
      "learning_rate": 0.00019566324365611473,
      "loss": 0.0356,
      "step": 5099
    },
    {
      "epoch": 0.023989388223562284,
      "grad_norm": 15.012855529785156,
      "learning_rate": 0.00019566230067800125,
      "loss": 0.639,
      "step": 5100
    },
    {
      "epoch": 0.023994092025174746,
      "grad_norm": 2.7689907550811768,
      "learning_rate": 0.0001956613576998878,
      "loss": 0.5458,
      "step": 5101
    },
    {
      "epoch": 0.023998795826787208,
      "grad_norm": 1.1029706001281738,
      "learning_rate": 0.00019566041472177432,
      "loss": 0.0636,
      "step": 5102
    },
    {
      "epoch": 0.024003499628399674,
      "grad_norm": 0.29882264137268066,
      "learning_rate": 0.00019565947174366084,
      "loss": 0.0204,
      "step": 5103
    },
    {
      "epoch": 0.024008203430012136,
      "grad_norm": 4.843481063842773,
      "learning_rate": 0.00019565852876554735,
      "loss": 0.9167,
      "step": 5104
    },
    {
      "epoch": 0.024012907231624598,
      "grad_norm": 0.3404085040092468,
      "learning_rate": 0.00019565758578743387,
      "loss": 0.0351,
      "step": 5105
    },
    {
      "epoch": 0.024017611033237064,
      "grad_norm": 3.257253646850586,
      "learning_rate": 0.00019565664280932042,
      "loss": 0.4214,
      "step": 5106
    },
    {
      "epoch": 0.024022314834849526,
      "grad_norm": 5.165491104125977,
      "learning_rate": 0.00019565569983120694,
      "loss": 0.9519,
      "step": 5107
    },
    {
      "epoch": 0.024027018636461988,
      "grad_norm": 2.7822225093841553,
      "learning_rate": 0.00019565475685309346,
      "loss": 0.4551,
      "step": 5108
    },
    {
      "epoch": 0.02403172243807445,
      "grad_norm": 5.173708915710449,
      "learning_rate": 0.00019565381387497998,
      "loss": 0.9199,
      "step": 5109
    },
    {
      "epoch": 0.024036426239686916,
      "grad_norm": 3.1875159740448,
      "learning_rate": 0.0001956528708968665,
      "loss": 0.2061,
      "step": 5110
    },
    {
      "epoch": 0.024041130041299378,
      "grad_norm": 3.8330330848693848,
      "learning_rate": 0.000195651927918753,
      "loss": 0.3834,
      "step": 5111
    },
    {
      "epoch": 0.02404583384291184,
      "grad_norm": 0.8103556632995605,
      "learning_rate": 0.00019565098494063953,
      "loss": 0.0702,
      "step": 5112
    },
    {
      "epoch": 0.024050537644524306,
      "grad_norm": 3.0160622596740723,
      "learning_rate": 0.00019565004196252605,
      "loss": 0.1428,
      "step": 5113
    },
    {
      "epoch": 0.024055241446136768,
      "grad_norm": 2.6093037128448486,
      "learning_rate": 0.00019564909898441257,
      "loss": 0.5131,
      "step": 5114
    },
    {
      "epoch": 0.02405994524774923,
      "grad_norm": 2.642054319381714,
      "learning_rate": 0.00019564815600629911,
      "loss": 0.265,
      "step": 5115
    },
    {
      "epoch": 0.024064649049361696,
      "grad_norm": 2.3624517917633057,
      "learning_rate": 0.00019564721302818563,
      "loss": 0.3778,
      "step": 5116
    },
    {
      "epoch": 0.024069352850974158,
      "grad_norm": 2.7483699321746826,
      "learning_rate": 0.00019564627005007215,
      "loss": 0.2056,
      "step": 5117
    },
    {
      "epoch": 0.02407405665258662,
      "grad_norm": 1.7553025484085083,
      "learning_rate": 0.00019564532707195867,
      "loss": 0.2688,
      "step": 5118
    },
    {
      "epoch": 0.024078760454199082,
      "grad_norm": 1.798526644706726,
      "learning_rate": 0.0001956443840938452,
      "loss": 0.2375,
      "step": 5119
    },
    {
      "epoch": 0.024083464255811548,
      "grad_norm": 1.3869680166244507,
      "learning_rate": 0.0001956434411157317,
      "loss": 0.1342,
      "step": 5120
    },
    {
      "epoch": 0.02408816805742401,
      "grad_norm": 3.0994086265563965,
      "learning_rate": 0.00019564249813761823,
      "loss": 0.5316,
      "step": 5121
    },
    {
      "epoch": 0.024092871859036472,
      "grad_norm": 2.055485963821411,
      "learning_rate": 0.00019564155515950474,
      "loss": 0.3813,
      "step": 5122
    },
    {
      "epoch": 0.024097575660648938,
      "grad_norm": 1.4871927499771118,
      "learning_rate": 0.00019564061218139126,
      "loss": 0.2738,
      "step": 5123
    },
    {
      "epoch": 0.0241022794622614,
      "grad_norm": 3.001391887664795,
      "learning_rate": 0.0001956396692032778,
      "loss": 0.5992,
      "step": 5124
    },
    {
      "epoch": 0.024106983263873862,
      "grad_norm": 2.1889212131500244,
      "learning_rate": 0.00019563872622516433,
      "loss": 0.3371,
      "step": 5125
    },
    {
      "epoch": 0.024111687065486324,
      "grad_norm": 2.8506791591644287,
      "learning_rate": 0.00019563778324705085,
      "loss": 0.6379,
      "step": 5126
    },
    {
      "epoch": 0.02411639086709879,
      "grad_norm": 2.9280548095703125,
      "learning_rate": 0.00019563684026893737,
      "loss": 0.3802,
      "step": 5127
    },
    {
      "epoch": 0.024121094668711252,
      "grad_norm": 1.4324089288711548,
      "learning_rate": 0.0001956358972908239,
      "loss": 0.2096,
      "step": 5128
    },
    {
      "epoch": 0.024125798470323714,
      "grad_norm": 1.6736912727355957,
      "learning_rate": 0.00019563495431271043,
      "loss": 0.1557,
      "step": 5129
    },
    {
      "epoch": 0.02413050227193618,
      "grad_norm": 0.25984591245651245,
      "learning_rate": 0.00019563401133459692,
      "loss": 0.0215,
      "step": 5130
    },
    {
      "epoch": 0.024135206073548642,
      "grad_norm": 0.7504438757896423,
      "learning_rate": 0.00019563306835648344,
      "loss": 0.0796,
      "step": 5131
    },
    {
      "epoch": 0.024139909875161104,
      "grad_norm": 1.5257762670516968,
      "learning_rate": 0.00019563212537836996,
      "loss": 0.2187,
      "step": 5132
    },
    {
      "epoch": 0.02414461367677357,
      "grad_norm": 1.5301567316055298,
      "learning_rate": 0.0001956311824002565,
      "loss": 0.1776,
      "step": 5133
    },
    {
      "epoch": 0.024149317478386032,
      "grad_norm": 2.8844549655914307,
      "learning_rate": 0.00019563023942214302,
      "loss": 0.8004,
      "step": 5134
    },
    {
      "epoch": 0.024154021279998494,
      "grad_norm": 1.049207091331482,
      "learning_rate": 0.00019562929644402954,
      "loss": 0.1918,
      "step": 5135
    },
    {
      "epoch": 0.024158725081610957,
      "grad_norm": 1.9223119020462036,
      "learning_rate": 0.00019562835346591606,
      "loss": 0.3168,
      "step": 5136
    },
    {
      "epoch": 0.024163428883223422,
      "grad_norm": 2.46566104888916,
      "learning_rate": 0.0001956274104878026,
      "loss": 0.2791,
      "step": 5137
    },
    {
      "epoch": 0.024168132684835884,
      "grad_norm": 1.896462321281433,
      "learning_rate": 0.00019562646750968912,
      "loss": 0.2423,
      "step": 5138
    },
    {
      "epoch": 0.024172836486448347,
      "grad_norm": 1.8193968534469604,
      "learning_rate": 0.00019562552453157564,
      "loss": 0.483,
      "step": 5139
    },
    {
      "epoch": 0.024177540288060812,
      "grad_norm": 1.35125732421875,
      "learning_rate": 0.00019562458155346216,
      "loss": 0.1424,
      "step": 5140
    },
    {
      "epoch": 0.024182244089673274,
      "grad_norm": 2.2280967235565186,
      "learning_rate": 0.00019562363857534865,
      "loss": 0.4218,
      "step": 5141
    },
    {
      "epoch": 0.024186947891285736,
      "grad_norm": 2.0475566387176514,
      "learning_rate": 0.0001956226955972352,
      "loss": 0.3752,
      "step": 5142
    },
    {
      "epoch": 0.0241916516928982,
      "grad_norm": 1.7436219453811646,
      "learning_rate": 0.00019562175261912172,
      "loss": 0.2453,
      "step": 5143
    },
    {
      "epoch": 0.024196355494510664,
      "grad_norm": 1.6762579679489136,
      "learning_rate": 0.00019562080964100824,
      "loss": 0.2809,
      "step": 5144
    },
    {
      "epoch": 0.024201059296123126,
      "grad_norm": 0.2710908055305481,
      "learning_rate": 0.00019561986666289475,
      "loss": 0.0215,
      "step": 5145
    },
    {
      "epoch": 0.02420576309773559,
      "grad_norm": 3.09584903717041,
      "learning_rate": 0.00019561892368478127,
      "loss": 0.6199,
      "step": 5146
    },
    {
      "epoch": 0.024210466899348054,
      "grad_norm": 0.6866841316223145,
      "learning_rate": 0.00019561798070666782,
      "loss": 0.0755,
      "step": 5147
    },
    {
      "epoch": 0.024215170700960516,
      "grad_norm": 1.2522584199905396,
      "learning_rate": 0.00019561703772855434,
      "loss": 0.3362,
      "step": 5148
    },
    {
      "epoch": 0.02421987450257298,
      "grad_norm": 2.672877073287964,
      "learning_rate": 0.00019561609475044086,
      "loss": 0.7402,
      "step": 5149
    },
    {
      "epoch": 0.024224578304185444,
      "grad_norm": 1.1368943452835083,
      "learning_rate": 0.00019561515177232738,
      "loss": 0.1372,
      "step": 5150
    },
    {
      "epoch": 0.024229282105797906,
      "grad_norm": 2.3504233360290527,
      "learning_rate": 0.0001956142087942139,
      "loss": 0.3555,
      "step": 5151
    },
    {
      "epoch": 0.02423398590741037,
      "grad_norm": 4.960940361022949,
      "learning_rate": 0.0001956132658161004,
      "loss": 0.9048,
      "step": 5152
    },
    {
      "epoch": 0.02423868970902283,
      "grad_norm": 0.6873422265052795,
      "learning_rate": 0.00019561232283798693,
      "loss": 0.0523,
      "step": 5153
    },
    {
      "epoch": 0.024243393510635296,
      "grad_norm": 0.9692957401275635,
      "learning_rate": 0.00019561137985987345,
      "loss": 0.1403,
      "step": 5154
    },
    {
      "epoch": 0.02424809731224776,
      "grad_norm": 1.622478723526001,
      "learning_rate": 0.00019561043688175997,
      "loss": 0.2148,
      "step": 5155
    },
    {
      "epoch": 0.02425280111386022,
      "grad_norm": 2.7409510612487793,
      "learning_rate": 0.00019560949390364651,
      "loss": 0.3774,
      "step": 5156
    },
    {
      "epoch": 0.024257504915472686,
      "grad_norm": 2.0928845405578613,
      "learning_rate": 0.00019560855092553303,
      "loss": 0.29,
      "step": 5157
    },
    {
      "epoch": 0.02426220871708515,
      "grad_norm": 1.8279428482055664,
      "learning_rate": 0.00019560760794741955,
      "loss": 0.1979,
      "step": 5158
    },
    {
      "epoch": 0.02426691251869761,
      "grad_norm": 2.3869686126708984,
      "learning_rate": 0.00019560666496930607,
      "loss": 0.3017,
      "step": 5159
    },
    {
      "epoch": 0.024271616320310073,
      "grad_norm": 1.5895497798919678,
      "learning_rate": 0.00019560572199119262,
      "loss": 0.1971,
      "step": 5160
    },
    {
      "epoch": 0.02427632012192254,
      "grad_norm": 1.1255931854248047,
      "learning_rate": 0.0001956047790130791,
      "loss": 0.1431,
      "step": 5161
    },
    {
      "epoch": 0.024281023923535,
      "grad_norm": 1.7428627014160156,
      "learning_rate": 0.00019560383603496563,
      "loss": 0.294,
      "step": 5162
    },
    {
      "epoch": 0.024285727725147463,
      "grad_norm": 2.071442127227783,
      "learning_rate": 0.00019560289305685214,
      "loss": 0.3038,
      "step": 5163
    },
    {
      "epoch": 0.02429043152675993,
      "grad_norm": 0.7607004046440125,
      "learning_rate": 0.00019560195007873866,
      "loss": 0.1172,
      "step": 5164
    },
    {
      "epoch": 0.02429513532837239,
      "grad_norm": 2.281209945678711,
      "learning_rate": 0.0001956010071006252,
      "loss": 0.3969,
      "step": 5165
    },
    {
      "epoch": 0.024299839129984853,
      "grad_norm": 3.656958818435669,
      "learning_rate": 0.00019560006412251173,
      "loss": 0.7572,
      "step": 5166
    },
    {
      "epoch": 0.02430454293159732,
      "grad_norm": 2.858649253845215,
      "learning_rate": 0.00019559912114439825,
      "loss": 0.347,
      "step": 5167
    },
    {
      "epoch": 0.02430924673320978,
      "grad_norm": 3.419290542602539,
      "learning_rate": 0.00019559817816628476,
      "loss": 0.375,
      "step": 5168
    },
    {
      "epoch": 0.024313950534822243,
      "grad_norm": 0.8692753911018372,
      "learning_rate": 0.0001955972351881713,
      "loss": 0.0979,
      "step": 5169
    },
    {
      "epoch": 0.024318654336434705,
      "grad_norm": 0.8815743327140808,
      "learning_rate": 0.00019559629221005783,
      "loss": 0.0951,
      "step": 5170
    },
    {
      "epoch": 0.02432335813804717,
      "grad_norm": 2.5287516117095947,
      "learning_rate": 0.00019559534923194435,
      "loss": 0.2564,
      "step": 5171
    },
    {
      "epoch": 0.024328061939659633,
      "grad_norm": 1.5843565464019775,
      "learning_rate": 0.00019559440625383084,
      "loss": 0.1459,
      "step": 5172
    },
    {
      "epoch": 0.024332765741272095,
      "grad_norm": 1.473812460899353,
      "learning_rate": 0.00019559346327571736,
      "loss": 0.1662,
      "step": 5173
    },
    {
      "epoch": 0.02433746954288456,
      "grad_norm": 2.1993825435638428,
      "learning_rate": 0.0001955925202976039,
      "loss": 0.2575,
      "step": 5174
    },
    {
      "epoch": 0.024342173344497023,
      "grad_norm": 0.9969603419303894,
      "learning_rate": 0.00019559157731949042,
      "loss": 0.1046,
      "step": 5175
    },
    {
      "epoch": 0.024346877146109485,
      "grad_norm": 2.852937936782837,
      "learning_rate": 0.00019559063434137694,
      "loss": 0.5956,
      "step": 5176
    },
    {
      "epoch": 0.024351580947721947,
      "grad_norm": 0.18529675900936127,
      "learning_rate": 0.00019558969136326346,
      "loss": 0.0091,
      "step": 5177
    },
    {
      "epoch": 0.024356284749334413,
      "grad_norm": 1.2229971885681152,
      "learning_rate": 0.00019558874838515,
      "loss": 0.0706,
      "step": 5178
    },
    {
      "epoch": 0.024360988550946875,
      "grad_norm": 4.414968013763428,
      "learning_rate": 0.00019558780540703652,
      "loss": 1.3245,
      "step": 5179
    },
    {
      "epoch": 0.024365692352559337,
      "grad_norm": 1.4637718200683594,
      "learning_rate": 0.00019558686242892304,
      "loss": 0.214,
      "step": 5180
    },
    {
      "epoch": 0.024370396154171803,
      "grad_norm": 1.7244433164596558,
      "learning_rate": 0.00019558591945080956,
      "loss": 0.2263,
      "step": 5181
    },
    {
      "epoch": 0.024375099955784265,
      "grad_norm": 2.523338556289673,
      "learning_rate": 0.00019558497647269608,
      "loss": 0.4022,
      "step": 5182
    },
    {
      "epoch": 0.024379803757396727,
      "grad_norm": 0.3805040717124939,
      "learning_rate": 0.0001955840334945826,
      "loss": 0.0363,
      "step": 5183
    },
    {
      "epoch": 0.024384507559009193,
      "grad_norm": 2.002739429473877,
      "learning_rate": 0.00019558309051646912,
      "loss": 0.4058,
      "step": 5184
    },
    {
      "epoch": 0.024389211360621655,
      "grad_norm": 1.0111899375915527,
      "learning_rate": 0.00019558214753835564,
      "loss": 0.1317,
      "step": 5185
    },
    {
      "epoch": 0.024393915162234117,
      "grad_norm": 2.9585070610046387,
      "learning_rate": 0.00019558120456024215,
      "loss": 0.5144,
      "step": 5186
    },
    {
      "epoch": 0.02439861896384658,
      "grad_norm": 0.6040731072425842,
      "learning_rate": 0.0001955802615821287,
      "loss": 0.0485,
      "step": 5187
    },
    {
      "epoch": 0.024403322765459045,
      "grad_norm": 0.49638667702674866,
      "learning_rate": 0.00019557931860401522,
      "loss": 0.0397,
      "step": 5188
    },
    {
      "epoch": 0.024408026567071507,
      "grad_norm": 0.3995474576950073,
      "learning_rate": 0.00019557837562590174,
      "loss": 0.0328,
      "step": 5189
    },
    {
      "epoch": 0.02441273036868397,
      "grad_norm": 2.32140851020813,
      "learning_rate": 0.00019557743264778826,
      "loss": 0.1471,
      "step": 5190
    },
    {
      "epoch": 0.024417434170296435,
      "grad_norm": 3.80983829498291,
      "learning_rate": 0.00019557648966967478,
      "loss": 0.4573,
      "step": 5191
    },
    {
      "epoch": 0.024422137971908897,
      "grad_norm": 2.8365228176116943,
      "learning_rate": 0.0001955755466915613,
      "loss": 0.6269,
      "step": 5192
    },
    {
      "epoch": 0.02442684177352136,
      "grad_norm": 1.5276882648468018,
      "learning_rate": 0.0001955746037134478,
      "loss": 0.1127,
      "step": 5193
    },
    {
      "epoch": 0.02443154557513382,
      "grad_norm": 2.386237621307373,
      "learning_rate": 0.00019557366073533433,
      "loss": 0.3228,
      "step": 5194
    },
    {
      "epoch": 0.024436249376746287,
      "grad_norm": 2.2625112533569336,
      "learning_rate": 0.00019557271775722085,
      "loss": 0.2985,
      "step": 5195
    },
    {
      "epoch": 0.02444095317835875,
      "grad_norm": 2.149637460708618,
      "learning_rate": 0.0001955717747791074,
      "loss": 0.3157,
      "step": 5196
    },
    {
      "epoch": 0.02444565697997121,
      "grad_norm": 0.32968419790267944,
      "learning_rate": 0.00019557083180099391,
      "loss": 0.0261,
      "step": 5197
    },
    {
      "epoch": 0.024450360781583677,
      "grad_norm": 2.4550461769104004,
      "learning_rate": 0.00019556988882288043,
      "loss": 0.4793,
      "step": 5198
    },
    {
      "epoch": 0.02445506458319614,
      "grad_norm": 2.2560431957244873,
      "learning_rate": 0.00019556894584476695,
      "loss": 0.4015,
      "step": 5199
    },
    {
      "epoch": 0.0244597683848086,
      "grad_norm": 6.376296043395996,
      "learning_rate": 0.00019556800286665347,
      "loss": 1.3951,
      "step": 5200
    },
    {
      "epoch": 0.024464472186421067,
      "grad_norm": 0.5784335136413574,
      "learning_rate": 0.00019556705988854002,
      "loss": 0.0416,
      "step": 5201
    },
    {
      "epoch": 0.02446917598803353,
      "grad_norm": 1.8893671035766602,
      "learning_rate": 0.00019556611691042653,
      "loss": 0.1376,
      "step": 5202
    },
    {
      "epoch": 0.02447387978964599,
      "grad_norm": 0.40035709738731384,
      "learning_rate": 0.00019556517393231303,
      "loss": 0.0256,
      "step": 5203
    },
    {
      "epoch": 0.024478583591258454,
      "grad_norm": 1.1744464635849,
      "learning_rate": 0.00019556423095419954,
      "loss": 0.1033,
      "step": 5204
    },
    {
      "epoch": 0.02448328739287092,
      "grad_norm": 0.803149938583374,
      "learning_rate": 0.00019556328797608606,
      "loss": 0.0837,
      "step": 5205
    },
    {
      "epoch": 0.02448799119448338,
      "grad_norm": 0.7109699249267578,
      "learning_rate": 0.0001955623449979726,
      "loss": 0.0552,
      "step": 5206
    },
    {
      "epoch": 0.024492694996095844,
      "grad_norm": 0.5128365755081177,
      "learning_rate": 0.00019556140201985913,
      "loss": 0.0594,
      "step": 5207
    },
    {
      "epoch": 0.02449739879770831,
      "grad_norm": 2.8431503772735596,
      "learning_rate": 0.00019556045904174565,
      "loss": 0.2416,
      "step": 5208
    },
    {
      "epoch": 0.02450210259932077,
      "grad_norm": 1.1933313608169556,
      "learning_rate": 0.00019555951606363216,
      "loss": 0.1002,
      "step": 5209
    },
    {
      "epoch": 0.024506806400933234,
      "grad_norm": 2.3291139602661133,
      "learning_rate": 0.0001955585730855187,
      "loss": 0.368,
      "step": 5210
    },
    {
      "epoch": 0.024511510202545696,
      "grad_norm": 3.632761001586914,
      "learning_rate": 0.00019555763010740523,
      "loss": 0.5848,
      "step": 5211
    },
    {
      "epoch": 0.02451621400415816,
      "grad_norm": 3.695941686630249,
      "learning_rate": 0.00019555668712929175,
      "loss": 0.4712,
      "step": 5212
    },
    {
      "epoch": 0.024520917805770624,
      "grad_norm": 7.046163082122803,
      "learning_rate": 0.00019555574415117827,
      "loss": 1.0985,
      "step": 5213
    },
    {
      "epoch": 0.024525621607383086,
      "grad_norm": 3.1591951847076416,
      "learning_rate": 0.00019555480117306479,
      "loss": 0.3107,
      "step": 5214
    },
    {
      "epoch": 0.02453032540899555,
      "grad_norm": 1.4011372327804565,
      "learning_rate": 0.0001955538581949513,
      "loss": 0.1774,
      "step": 5215
    },
    {
      "epoch": 0.024535029210608014,
      "grad_norm": 1.0171958208084106,
      "learning_rate": 0.00019555291521683782,
      "loss": 0.1919,
      "step": 5216
    },
    {
      "epoch": 0.024539733012220476,
      "grad_norm": 4.904646873474121,
      "learning_rate": 0.00019555197223872434,
      "loss": 0.5436,
      "step": 5217
    },
    {
      "epoch": 0.02454443681383294,
      "grad_norm": 3.740006446838379,
      "learning_rate": 0.00019555102926061086,
      "loss": 1.1915,
      "step": 5218
    },
    {
      "epoch": 0.024549140615445404,
      "grad_norm": 7.837240219116211,
      "learning_rate": 0.0001955500862824974,
      "loss": 0.8356,
      "step": 5219
    },
    {
      "epoch": 0.024553844417057866,
      "grad_norm": 0.933681845664978,
      "learning_rate": 0.00019554914330438392,
      "loss": 0.104,
      "step": 5220
    },
    {
      "epoch": 0.024558548218670328,
      "grad_norm": 2.5852179527282715,
      "learning_rate": 0.00019554820032627044,
      "loss": 0.4004,
      "step": 5221
    },
    {
      "epoch": 0.024563252020282794,
      "grad_norm": 0.647734522819519,
      "learning_rate": 0.00019554725734815696,
      "loss": 0.093,
      "step": 5222
    },
    {
      "epoch": 0.024567955821895256,
      "grad_norm": 2.9074628353118896,
      "learning_rate": 0.00019554631437004348,
      "loss": 0.5333,
      "step": 5223
    },
    {
      "epoch": 0.024572659623507718,
      "grad_norm": 1.0157817602157593,
      "learning_rate": 0.00019554537139193,
      "loss": 0.0715,
      "step": 5224
    },
    {
      "epoch": 0.024577363425120183,
      "grad_norm": 1.4639229774475098,
      "learning_rate": 0.00019554442841381652,
      "loss": 0.2647,
      "step": 5225
    },
    {
      "epoch": 0.024582067226732646,
      "grad_norm": 0.9188462495803833,
      "learning_rate": 0.00019554348543570304,
      "loss": 0.0774,
      "step": 5226
    },
    {
      "epoch": 0.024586771028345108,
      "grad_norm": 3.060669422149658,
      "learning_rate": 0.00019554254245758955,
      "loss": 0.5117,
      "step": 5227
    },
    {
      "epoch": 0.02459147482995757,
      "grad_norm": 1.7760553359985352,
      "learning_rate": 0.0001955415994794761,
      "loss": 0.129,
      "step": 5228
    },
    {
      "epoch": 0.024596178631570036,
      "grad_norm": 1.2731422185897827,
      "learning_rate": 0.00019554065650136262,
      "loss": 0.1735,
      "step": 5229
    },
    {
      "epoch": 0.024600882433182498,
      "grad_norm": 2.212517261505127,
      "learning_rate": 0.00019553971352324914,
      "loss": 0.6176,
      "step": 5230
    },
    {
      "epoch": 0.02460558623479496,
      "grad_norm": 1.6262952089309692,
      "learning_rate": 0.00019553877054513566,
      "loss": 0.3254,
      "step": 5231
    },
    {
      "epoch": 0.024610290036407426,
      "grad_norm": 0.9514100551605225,
      "learning_rate": 0.00019553782756702218,
      "loss": 0.1399,
      "step": 5232
    },
    {
      "epoch": 0.024614993838019888,
      "grad_norm": 2.239774465560913,
      "learning_rate": 0.00019553688458890872,
      "loss": 0.4851,
      "step": 5233
    },
    {
      "epoch": 0.02461969763963235,
      "grad_norm": 2.3072054386138916,
      "learning_rate": 0.0001955359416107952,
      "loss": 0.2302,
      "step": 5234
    },
    {
      "epoch": 0.024624401441244816,
      "grad_norm": 2.8869566917419434,
      "learning_rate": 0.00019553499863268173,
      "loss": 0.5533,
      "step": 5235
    },
    {
      "epoch": 0.024629105242857278,
      "grad_norm": 0.9457080960273743,
      "learning_rate": 0.00019553405565456825,
      "loss": 0.187,
      "step": 5236
    },
    {
      "epoch": 0.02463380904446974,
      "grad_norm": 0.583501935005188,
      "learning_rate": 0.0001955331126764548,
      "loss": 0.0323,
      "step": 5237
    },
    {
      "epoch": 0.024638512846082202,
      "grad_norm": 2.8711090087890625,
      "learning_rate": 0.00019553216969834131,
      "loss": 0.3472,
      "step": 5238
    },
    {
      "epoch": 0.024643216647694668,
      "grad_norm": 3.2961933612823486,
      "learning_rate": 0.00019553122672022783,
      "loss": 0.6751,
      "step": 5239
    },
    {
      "epoch": 0.02464792044930713,
      "grad_norm": 2.1114842891693115,
      "learning_rate": 0.00019553028374211435,
      "loss": 0.2296,
      "step": 5240
    },
    {
      "epoch": 0.024652624250919592,
      "grad_norm": 1.0858728885650635,
      "learning_rate": 0.00019552934076400087,
      "loss": 0.1303,
      "step": 5241
    },
    {
      "epoch": 0.024657328052532058,
      "grad_norm": 2.024768829345703,
      "learning_rate": 0.00019552839778588742,
      "loss": 0.277,
      "step": 5242
    },
    {
      "epoch": 0.02466203185414452,
      "grad_norm": 0.9526063799858093,
      "learning_rate": 0.00019552745480777393,
      "loss": 0.1471,
      "step": 5243
    },
    {
      "epoch": 0.024666735655756982,
      "grad_norm": 1.0283153057098389,
      "learning_rate": 0.00019552651182966045,
      "loss": 0.0895,
      "step": 5244
    },
    {
      "epoch": 0.024671439457369444,
      "grad_norm": 3.4144489765167236,
      "learning_rate": 0.00019552556885154697,
      "loss": 0.7102,
      "step": 5245
    },
    {
      "epoch": 0.02467614325898191,
      "grad_norm": 2.600036144256592,
      "learning_rate": 0.0001955246258734335,
      "loss": 0.5116,
      "step": 5246
    },
    {
      "epoch": 0.024680847060594372,
      "grad_norm": 1.188309669494629,
      "learning_rate": 0.00019552368289532,
      "loss": 0.3086,
      "step": 5247
    },
    {
      "epoch": 0.024685550862206834,
      "grad_norm": 1.4755895137786865,
      "learning_rate": 0.00019552273991720653,
      "loss": 0.2902,
      "step": 5248
    },
    {
      "epoch": 0.0246902546638193,
      "grad_norm": 1.30482816696167,
      "learning_rate": 0.00019552179693909305,
      "loss": 0.1313,
      "step": 5249
    },
    {
      "epoch": 0.024694958465431762,
      "grad_norm": 1.0944932699203491,
      "learning_rate": 0.00019552085396097956,
      "loss": 0.2215,
      "step": 5250
    },
    {
      "epoch": 0.024699662267044224,
      "grad_norm": 5.7687273025512695,
      "learning_rate": 0.0001955199109828661,
      "loss": 0.4903,
      "step": 5251
    },
    {
      "epoch": 0.02470436606865669,
      "grad_norm": 2.238713502883911,
      "learning_rate": 0.00019551896800475263,
      "loss": 0.2288,
      "step": 5252
    },
    {
      "epoch": 0.024709069870269152,
      "grad_norm": 3.0633485317230225,
      "learning_rate": 0.00019551802502663915,
      "loss": 0.2863,
      "step": 5253
    },
    {
      "epoch": 0.024713773671881614,
      "grad_norm": 5.621810436248779,
      "learning_rate": 0.00019551708204852567,
      "loss": 0.4422,
      "step": 5254
    },
    {
      "epoch": 0.024718477473494076,
      "grad_norm": 2.1228229999542236,
      "learning_rate": 0.00019551613907041219,
      "loss": 0.3758,
      "step": 5255
    },
    {
      "epoch": 0.024723181275106542,
      "grad_norm": 2.0616776943206787,
      "learning_rate": 0.0001955151960922987,
      "loss": 0.3019,
      "step": 5256
    },
    {
      "epoch": 0.024727885076719004,
      "grad_norm": 0.7758424282073975,
      "learning_rate": 0.00019551425311418522,
      "loss": 0.0784,
      "step": 5257
    },
    {
      "epoch": 0.024732588878331466,
      "grad_norm": 0.3806631565093994,
      "learning_rate": 0.00019551331013607174,
      "loss": 0.0263,
      "step": 5258
    },
    {
      "epoch": 0.024737292679943932,
      "grad_norm": 2.3477227687835693,
      "learning_rate": 0.00019551236715795826,
      "loss": 0.6457,
      "step": 5259
    },
    {
      "epoch": 0.024741996481556394,
      "grad_norm": 2.3751473426818848,
      "learning_rate": 0.0001955114241798448,
      "loss": 0.4445,
      "step": 5260
    },
    {
      "epoch": 0.024746700283168856,
      "grad_norm": 1.7772235870361328,
      "learning_rate": 0.00019551048120173132,
      "loss": 0.1421,
      "step": 5261
    },
    {
      "epoch": 0.02475140408478132,
      "grad_norm": 1.5974493026733398,
      "learning_rate": 0.00019550953822361784,
      "loss": 0.2059,
      "step": 5262
    },
    {
      "epoch": 0.024756107886393784,
      "grad_norm": 2.7537801265716553,
      "learning_rate": 0.00019550859524550436,
      "loss": 0.4915,
      "step": 5263
    },
    {
      "epoch": 0.024760811688006246,
      "grad_norm": 0.719146192073822,
      "learning_rate": 0.00019550765226739088,
      "loss": 0.1461,
      "step": 5264
    },
    {
      "epoch": 0.02476551548961871,
      "grad_norm": 2.529883623123169,
      "learning_rate": 0.0001955067092892774,
      "loss": 0.1631,
      "step": 5265
    },
    {
      "epoch": 0.024770219291231174,
      "grad_norm": 2.891676187515259,
      "learning_rate": 0.00019550576631116392,
      "loss": 0.456,
      "step": 5266
    },
    {
      "epoch": 0.024774923092843636,
      "grad_norm": 3.246830701828003,
      "learning_rate": 0.00019550482333305044,
      "loss": 0.7445,
      "step": 5267
    },
    {
      "epoch": 0.0247796268944561,
      "grad_norm": 1.0032914876937866,
      "learning_rate": 0.00019550388035493695,
      "loss": 0.11,
      "step": 5268
    },
    {
      "epoch": 0.024784330696068564,
      "grad_norm": 2.554823398590088,
      "learning_rate": 0.0001955029373768235,
      "loss": 0.3492,
      "step": 5269
    },
    {
      "epoch": 0.024789034497681026,
      "grad_norm": 1.9166193008422852,
      "learning_rate": 0.00019550199439871002,
      "loss": 0.2728,
      "step": 5270
    },
    {
      "epoch": 0.02479373829929349,
      "grad_norm": 1.4553866386413574,
      "learning_rate": 0.00019550105142059654,
      "loss": 0.3062,
      "step": 5271
    },
    {
      "epoch": 0.02479844210090595,
      "grad_norm": 1.1651462316513062,
      "learning_rate": 0.00019550010844248306,
      "loss": 0.2271,
      "step": 5272
    },
    {
      "epoch": 0.024803145902518416,
      "grad_norm": 3.255345106124878,
      "learning_rate": 0.00019549916546436958,
      "loss": 0.549,
      "step": 5273
    },
    {
      "epoch": 0.02480784970413088,
      "grad_norm": 0.6521837115287781,
      "learning_rate": 0.00019549822248625612,
      "loss": 0.0334,
      "step": 5274
    },
    {
      "epoch": 0.02481255350574334,
      "grad_norm": 3.7802255153656006,
      "learning_rate": 0.00019549727950814264,
      "loss": 0.8727,
      "step": 5275
    },
    {
      "epoch": 0.024817257307355806,
      "grad_norm": 0.5734700560569763,
      "learning_rate": 0.00019549633653002916,
      "loss": 0.0521,
      "step": 5276
    },
    {
      "epoch": 0.02482196110896827,
      "grad_norm": 0.4175224006175995,
      "learning_rate": 0.00019549539355191565,
      "loss": 0.0435,
      "step": 5277
    },
    {
      "epoch": 0.02482666491058073,
      "grad_norm": 1.5501904487609863,
      "learning_rate": 0.0001954944505738022,
      "loss": 0.1934,
      "step": 5278
    },
    {
      "epoch": 0.024831368712193196,
      "grad_norm": 0.888124942779541,
      "learning_rate": 0.00019549350759568871,
      "loss": 0.1209,
      "step": 5279
    },
    {
      "epoch": 0.02483607251380566,
      "grad_norm": 0.6025219559669495,
      "learning_rate": 0.00019549256461757523,
      "loss": 0.0698,
      "step": 5280
    },
    {
      "epoch": 0.02484077631541812,
      "grad_norm": 2.338144540786743,
      "learning_rate": 0.00019549162163946175,
      "loss": 0.6371,
      "step": 5281
    },
    {
      "epoch": 0.024845480117030583,
      "grad_norm": 1.2782819271087646,
      "learning_rate": 0.00019549067866134827,
      "loss": 0.1965,
      "step": 5282
    },
    {
      "epoch": 0.02485018391864305,
      "grad_norm": 0.91256183385849,
      "learning_rate": 0.00019548973568323482,
      "loss": 0.1746,
      "step": 5283
    },
    {
      "epoch": 0.02485488772025551,
      "grad_norm": 0.4153386652469635,
      "learning_rate": 0.00019548879270512133,
      "loss": 0.0699,
      "step": 5284
    },
    {
      "epoch": 0.024859591521867973,
      "grad_norm": 1.249468207359314,
      "learning_rate": 0.00019548784972700785,
      "loss": 0.1397,
      "step": 5285
    },
    {
      "epoch": 0.02486429532348044,
      "grad_norm": 0.09315814077854156,
      "learning_rate": 0.00019548690674889437,
      "loss": 0.0061,
      "step": 5286
    },
    {
      "epoch": 0.0248689991250929,
      "grad_norm": 3.353015184402466,
      "learning_rate": 0.0001954859637707809,
      "loss": 0.8841,
      "step": 5287
    },
    {
      "epoch": 0.024873702926705363,
      "grad_norm": 1.2184820175170898,
      "learning_rate": 0.0001954850207926674,
      "loss": 0.1463,
      "step": 5288
    },
    {
      "epoch": 0.024878406728317825,
      "grad_norm": 1.4150115251541138,
      "learning_rate": 0.00019548407781455393,
      "loss": 0.1551,
      "step": 5289
    },
    {
      "epoch": 0.02488311052993029,
      "grad_norm": 1.177539348602295,
      "learning_rate": 0.00019548313483644045,
      "loss": 0.1557,
      "step": 5290
    },
    {
      "epoch": 0.024887814331542753,
      "grad_norm": 0.8541215062141418,
      "learning_rate": 0.00019548219185832696,
      "loss": 0.0745,
      "step": 5291
    },
    {
      "epoch": 0.024892518133155215,
      "grad_norm": 0.7661207914352417,
      "learning_rate": 0.0001954812488802135,
      "loss": 0.1454,
      "step": 5292
    },
    {
      "epoch": 0.02489722193476768,
      "grad_norm": 0.8582977056503296,
      "learning_rate": 0.00019548030590210003,
      "loss": 0.099,
      "step": 5293
    },
    {
      "epoch": 0.024901925736380143,
      "grad_norm": 2.8898611068725586,
      "learning_rate": 0.00019547936292398655,
      "loss": 0.5567,
      "step": 5294
    },
    {
      "epoch": 0.024906629537992605,
      "grad_norm": 1.1798454523086548,
      "learning_rate": 0.00019547841994587307,
      "loss": 0.2738,
      "step": 5295
    },
    {
      "epoch": 0.02491133333960507,
      "grad_norm": 1.0312355756759644,
      "learning_rate": 0.00019547747696775959,
      "loss": 0.1066,
      "step": 5296
    },
    {
      "epoch": 0.024916037141217533,
      "grad_norm": 3.0721518993377686,
      "learning_rate": 0.0001954765339896461,
      "loss": 0.3222,
      "step": 5297
    },
    {
      "epoch": 0.024920740942829995,
      "grad_norm": 2.058950185775757,
      "learning_rate": 0.00019547559101153262,
      "loss": 0.3882,
      "step": 5298
    },
    {
      "epoch": 0.024925444744442457,
      "grad_norm": 2.8747670650482178,
      "learning_rate": 0.00019547464803341914,
      "loss": 0.347,
      "step": 5299
    },
    {
      "epoch": 0.024930148546054923,
      "grad_norm": 1.217754602432251,
      "learning_rate": 0.00019547370505530566,
      "loss": 0.1333,
      "step": 5300
    },
    {
      "epoch": 0.024934852347667385,
      "grad_norm": 5.976075649261475,
      "learning_rate": 0.0001954727620771922,
      "loss": 0.4056,
      "step": 5301
    },
    {
      "epoch": 0.024939556149279847,
      "grad_norm": 0.21937240660190582,
      "learning_rate": 0.00019547181909907872,
      "loss": 0.0284,
      "step": 5302
    },
    {
      "epoch": 0.024944259950892313,
      "grad_norm": 5.743014335632324,
      "learning_rate": 0.00019547087612096524,
      "loss": 0.1992,
      "step": 5303
    },
    {
      "epoch": 0.024948963752504775,
      "grad_norm": 1.5414323806762695,
      "learning_rate": 0.00019546993314285176,
      "loss": 0.1114,
      "step": 5304
    },
    {
      "epoch": 0.024953667554117237,
      "grad_norm": 2.0692856311798096,
      "learning_rate": 0.00019546899016473828,
      "loss": 0.3166,
      "step": 5305
    },
    {
      "epoch": 0.0249583713557297,
      "grad_norm": 1.0275533199310303,
      "learning_rate": 0.00019546804718662483,
      "loss": 0.1424,
      "step": 5306
    },
    {
      "epoch": 0.024963075157342165,
      "grad_norm": 2.435723066329956,
      "learning_rate": 0.00019546710420851134,
      "loss": 0.269,
      "step": 5307
    },
    {
      "epoch": 0.024967778958954627,
      "grad_norm": 1.3103532791137695,
      "learning_rate": 0.00019546616123039784,
      "loss": 0.0635,
      "step": 5308
    },
    {
      "epoch": 0.02497248276056709,
      "grad_norm": 0.37634775042533875,
      "learning_rate": 0.00019546521825228435,
      "loss": 0.0531,
      "step": 5309
    },
    {
      "epoch": 0.024977186562179555,
      "grad_norm": 2.4879138469696045,
      "learning_rate": 0.0001954642752741709,
      "loss": 0.2117,
      "step": 5310
    },
    {
      "epoch": 0.024981890363792017,
      "grad_norm": 2.046757698059082,
      "learning_rate": 0.00019546333229605742,
      "loss": 0.1735,
      "step": 5311
    },
    {
      "epoch": 0.02498659416540448,
      "grad_norm": 2.3814587593078613,
      "learning_rate": 0.00019546238931794394,
      "loss": 0.2473,
      "step": 5312
    },
    {
      "epoch": 0.024991297967016945,
      "grad_norm": 3.4066171646118164,
      "learning_rate": 0.00019546144633983046,
      "loss": 0.405,
      "step": 5313
    },
    {
      "epoch": 0.024996001768629407,
      "grad_norm": 2.930842399597168,
      "learning_rate": 0.00019546050336171698,
      "loss": 0.6585,
      "step": 5314
    },
    {
      "epoch": 0.02500070557024187,
      "grad_norm": 2.067654848098755,
      "learning_rate": 0.00019545956038360352,
      "loss": 0.3653,
      "step": 5315
    },
    {
      "epoch": 0.02500540937185433,
      "grad_norm": 1.384158730506897,
      "learning_rate": 0.00019545861740549004,
      "loss": 0.1652,
      "step": 5316
    },
    {
      "epoch": 0.025010113173466797,
      "grad_norm": 2.818134307861328,
      "learning_rate": 0.00019545767442737656,
      "loss": 0.2621,
      "step": 5317
    },
    {
      "epoch": 0.02501481697507926,
      "grad_norm": 5.5209197998046875,
      "learning_rate": 0.00019545673144926308,
      "loss": 0.6997,
      "step": 5318
    },
    {
      "epoch": 0.02501952077669172,
      "grad_norm": 1.9439629316329956,
      "learning_rate": 0.0001954557884711496,
      "loss": 0.1333,
      "step": 5319
    },
    {
      "epoch": 0.025024224578304187,
      "grad_norm": 2.1378839015960693,
      "learning_rate": 0.00019545484549303611,
      "loss": 0.1486,
      "step": 5320
    },
    {
      "epoch": 0.02502892837991665,
      "grad_norm": 1.713485598564148,
      "learning_rate": 0.00019545390251492263,
      "loss": 0.3193,
      "step": 5321
    },
    {
      "epoch": 0.02503363218152911,
      "grad_norm": 1.981866717338562,
      "learning_rate": 0.00019545295953680915,
      "loss": 0.3897,
      "step": 5322
    },
    {
      "epoch": 0.025038335983141573,
      "grad_norm": 2.2369730472564697,
      "learning_rate": 0.00019545201655869567,
      "loss": 0.4141,
      "step": 5323
    },
    {
      "epoch": 0.02504303978475404,
      "grad_norm": 1.0711538791656494,
      "learning_rate": 0.00019545107358058222,
      "loss": 0.1058,
      "step": 5324
    },
    {
      "epoch": 0.0250477435863665,
      "grad_norm": 2.0618810653686523,
      "learning_rate": 0.00019545013060246873,
      "loss": 0.2418,
      "step": 5325
    },
    {
      "epoch": 0.025052447387978963,
      "grad_norm": 0.7795113325119019,
      "learning_rate": 0.00019544918762435525,
      "loss": 0.0725,
      "step": 5326
    },
    {
      "epoch": 0.02505715118959143,
      "grad_norm": 2.7404141426086426,
      "learning_rate": 0.00019544824464624177,
      "loss": 0.3186,
      "step": 5327
    },
    {
      "epoch": 0.02506185499120389,
      "grad_norm": 1.1293773651123047,
      "learning_rate": 0.0001954473016681283,
      "loss": 0.1704,
      "step": 5328
    },
    {
      "epoch": 0.025066558792816353,
      "grad_norm": 1.925986647605896,
      "learning_rate": 0.0001954463586900148,
      "loss": 0.1737,
      "step": 5329
    },
    {
      "epoch": 0.02507126259442882,
      "grad_norm": 0.42273998260498047,
      "learning_rate": 0.00019544541571190133,
      "loss": 0.0327,
      "step": 5330
    },
    {
      "epoch": 0.02507596639604128,
      "grad_norm": 1.7660472393035889,
      "learning_rate": 0.00019544447273378785,
      "loss": 0.244,
      "step": 5331
    },
    {
      "epoch": 0.025080670197653743,
      "grad_norm": 2.21445631980896,
      "learning_rate": 0.00019544352975567436,
      "loss": 0.3219,
      "step": 5332
    },
    {
      "epoch": 0.025085373999266206,
      "grad_norm": 3.495948314666748,
      "learning_rate": 0.0001954425867775609,
      "loss": 0.6292,
      "step": 5333
    },
    {
      "epoch": 0.02509007780087867,
      "grad_norm": 0.7022237777709961,
      "learning_rate": 0.00019544164379944743,
      "loss": 0.0491,
      "step": 5334
    },
    {
      "epoch": 0.025094781602491133,
      "grad_norm": 2.138878345489502,
      "learning_rate": 0.00019544070082133395,
      "loss": 0.5215,
      "step": 5335
    },
    {
      "epoch": 0.025099485404103596,
      "grad_norm": 0.12896376848220825,
      "learning_rate": 0.00019543975784322047,
      "loss": 0.0083,
      "step": 5336
    },
    {
      "epoch": 0.02510418920571606,
      "grad_norm": 1.2023807764053345,
      "learning_rate": 0.000195438814865107,
      "loss": 0.099,
      "step": 5337
    },
    {
      "epoch": 0.025108893007328523,
      "grad_norm": 1.2347869873046875,
      "learning_rate": 0.00019543787188699353,
      "loss": 0.1967,
      "step": 5338
    },
    {
      "epoch": 0.025113596808940986,
      "grad_norm": 0.7643174529075623,
      "learning_rate": 0.00019543692890888002,
      "loss": 0.0681,
      "step": 5339
    },
    {
      "epoch": 0.025118300610553448,
      "grad_norm": 1.0360757112503052,
      "learning_rate": 0.00019543598593076654,
      "loss": 0.1668,
      "step": 5340
    },
    {
      "epoch": 0.025123004412165913,
      "grad_norm": 0.5793231725692749,
      "learning_rate": 0.00019543504295265306,
      "loss": 0.0601,
      "step": 5341
    },
    {
      "epoch": 0.025127708213778376,
      "grad_norm": 3.6540186405181885,
      "learning_rate": 0.0001954340999745396,
      "loss": 0.4588,
      "step": 5342
    },
    {
      "epoch": 0.025132412015390838,
      "grad_norm": 1.4113428592681885,
      "learning_rate": 0.00019543315699642612,
      "loss": 0.1687,
      "step": 5343
    },
    {
      "epoch": 0.025137115817003303,
      "grad_norm": 1.4827184677124023,
      "learning_rate": 0.00019543221401831264,
      "loss": 0.2916,
      "step": 5344
    },
    {
      "epoch": 0.025141819618615766,
      "grad_norm": 1.3593250513076782,
      "learning_rate": 0.00019543127104019916,
      "loss": 0.1084,
      "step": 5345
    },
    {
      "epoch": 0.025146523420228228,
      "grad_norm": 1.8748670816421509,
      "learning_rate": 0.0001954303280620857,
      "loss": 0.1861,
      "step": 5346
    },
    {
      "epoch": 0.025151227221840693,
      "grad_norm": 12.296977996826172,
      "learning_rate": 0.00019542938508397223,
      "loss": 0.8928,
      "step": 5347
    },
    {
      "epoch": 0.025155931023453156,
      "grad_norm": 1.5502032041549683,
      "learning_rate": 0.00019542844210585874,
      "loss": 0.1156,
      "step": 5348
    },
    {
      "epoch": 0.025160634825065618,
      "grad_norm": 2.7025063037872314,
      "learning_rate": 0.00019542749912774526,
      "loss": 0.2493,
      "step": 5349
    },
    {
      "epoch": 0.02516533862667808,
      "grad_norm": 1.6985368728637695,
      "learning_rate": 0.00019542655614963175,
      "loss": 0.311,
      "step": 5350
    },
    {
      "epoch": 0.025170042428290545,
      "grad_norm": 4.622432708740234,
      "learning_rate": 0.0001954256131715183,
      "loss": 0.7263,
      "step": 5351
    },
    {
      "epoch": 0.025174746229903008,
      "grad_norm": 8.943052291870117,
      "learning_rate": 0.00019542467019340482,
      "loss": 1.366,
      "step": 5352
    },
    {
      "epoch": 0.02517945003151547,
      "grad_norm": 1.9185295104980469,
      "learning_rate": 0.00019542372721529134,
      "loss": 0.2238,
      "step": 5353
    },
    {
      "epoch": 0.025184153833127935,
      "grad_norm": 0.870029866695404,
      "learning_rate": 0.00019542278423717786,
      "loss": 0.0934,
      "step": 5354
    },
    {
      "epoch": 0.025188857634740398,
      "grad_norm": 3.3486952781677246,
      "learning_rate": 0.00019542184125906437,
      "loss": 0.3514,
      "step": 5355
    },
    {
      "epoch": 0.02519356143635286,
      "grad_norm": 2.491309642791748,
      "learning_rate": 0.00019542089828095092,
      "loss": 0.2125,
      "step": 5356
    },
    {
      "epoch": 0.025198265237965322,
      "grad_norm": 2.5146405696868896,
      "learning_rate": 0.00019541995530283744,
      "loss": 0.3896,
      "step": 5357
    },
    {
      "epoch": 0.025202969039577788,
      "grad_norm": 2.587298631668091,
      "learning_rate": 0.00019541901232472396,
      "loss": 0.2522,
      "step": 5358
    },
    {
      "epoch": 0.02520767284119025,
      "grad_norm": 3.4996955394744873,
      "learning_rate": 0.00019541806934661048,
      "loss": 0.3814,
      "step": 5359
    },
    {
      "epoch": 0.025212376642802712,
      "grad_norm": 2.522789478302002,
      "learning_rate": 0.000195417126368497,
      "loss": 0.4928,
      "step": 5360
    },
    {
      "epoch": 0.025217080444415178,
      "grad_norm": 3.920538902282715,
      "learning_rate": 0.00019541618339038351,
      "loss": 0.6636,
      "step": 5361
    },
    {
      "epoch": 0.02522178424602764,
      "grad_norm": 1.301255226135254,
      "learning_rate": 0.00019541524041227003,
      "loss": 0.1059,
      "step": 5362
    },
    {
      "epoch": 0.025226488047640102,
      "grad_norm": 3.840873956680298,
      "learning_rate": 0.00019541429743415655,
      "loss": 0.8364,
      "step": 5363
    },
    {
      "epoch": 0.025231191849252568,
      "grad_norm": 0.49810606241226196,
      "learning_rate": 0.00019541335445604307,
      "loss": 0.0726,
      "step": 5364
    },
    {
      "epoch": 0.02523589565086503,
      "grad_norm": 2.804075241088867,
      "learning_rate": 0.00019541241147792962,
      "loss": 0.2183,
      "step": 5365
    },
    {
      "epoch": 0.025240599452477492,
      "grad_norm": 1.2776247262954712,
      "learning_rate": 0.00019541146849981613,
      "loss": 0.2217,
      "step": 5366
    },
    {
      "epoch": 0.025245303254089954,
      "grad_norm": 1.312789797782898,
      "learning_rate": 0.00019541052552170265,
      "loss": 0.1928,
      "step": 5367
    },
    {
      "epoch": 0.02525000705570242,
      "grad_norm": 1.4590704441070557,
      "learning_rate": 0.00019540958254358917,
      "loss": 0.283,
      "step": 5368
    },
    {
      "epoch": 0.025254710857314882,
      "grad_norm": 3.05143666267395,
      "learning_rate": 0.00019540863956547572,
      "loss": 0.6512,
      "step": 5369
    },
    {
      "epoch": 0.025259414658927344,
      "grad_norm": 3.3653957843780518,
      "learning_rate": 0.0001954076965873622,
      "loss": 0.7152,
      "step": 5370
    },
    {
      "epoch": 0.02526411846053981,
      "grad_norm": 6.07867431640625,
      "learning_rate": 0.00019540675360924873,
      "loss": 0.95,
      "step": 5371
    },
    {
      "epoch": 0.025268822262152272,
      "grad_norm": 0.8120195865631104,
      "learning_rate": 0.00019540581063113525,
      "loss": 0.1038,
      "step": 5372
    },
    {
      "epoch": 0.025273526063764734,
      "grad_norm": 2.0824995040893555,
      "learning_rate": 0.00019540486765302176,
      "loss": 0.2942,
      "step": 5373
    },
    {
      "epoch": 0.025278229865377196,
      "grad_norm": 2.2321786880493164,
      "learning_rate": 0.0001954039246749083,
      "loss": 0.3312,
      "step": 5374
    },
    {
      "epoch": 0.025282933666989662,
      "grad_norm": 1.7711641788482666,
      "learning_rate": 0.00019540298169679483,
      "loss": 0.3689,
      "step": 5375
    },
    {
      "epoch": 0.025287637468602124,
      "grad_norm": 2.123055934906006,
      "learning_rate": 0.00019540203871868135,
      "loss": 0.2888,
      "step": 5376
    },
    {
      "epoch": 0.025292341270214586,
      "grad_norm": 1.5415135622024536,
      "learning_rate": 0.00019540109574056787,
      "loss": 0.1528,
      "step": 5377
    },
    {
      "epoch": 0.025297045071827052,
      "grad_norm": 0.19573189318180084,
      "learning_rate": 0.0001954001527624544,
      "loss": 0.0128,
      "step": 5378
    },
    {
      "epoch": 0.025301748873439514,
      "grad_norm": 2.808507204055786,
      "learning_rate": 0.00019539920978434093,
      "loss": 0.273,
      "step": 5379
    },
    {
      "epoch": 0.025306452675051976,
      "grad_norm": 1.963866949081421,
      "learning_rate": 0.00019539826680622745,
      "loss": 0.4956,
      "step": 5380
    },
    {
      "epoch": 0.025311156476664442,
      "grad_norm": 2.2552428245544434,
      "learning_rate": 0.00019539732382811394,
      "loss": 0.1414,
      "step": 5381
    },
    {
      "epoch": 0.025315860278276904,
      "grad_norm": 0.9795222282409668,
      "learning_rate": 0.00019539638085000046,
      "loss": 0.1434,
      "step": 5382
    },
    {
      "epoch": 0.025320564079889366,
      "grad_norm": 1.1456633806228638,
      "learning_rate": 0.000195395437871887,
      "loss": 0.2208,
      "step": 5383
    },
    {
      "epoch": 0.02532526788150183,
      "grad_norm": 1.1325548887252808,
      "learning_rate": 0.00019539449489377352,
      "loss": 0.321,
      "step": 5384
    },
    {
      "epoch": 0.025329971683114294,
      "grad_norm": 1.872533917427063,
      "learning_rate": 0.00019539355191566004,
      "loss": 0.5544,
      "step": 5385
    },
    {
      "epoch": 0.025334675484726756,
      "grad_norm": 0.8573015332221985,
      "learning_rate": 0.00019539260893754656,
      "loss": 0.1055,
      "step": 5386
    },
    {
      "epoch": 0.02533937928633922,
      "grad_norm": 0.7497062087059021,
      "learning_rate": 0.0001953916659594331,
      "loss": 0.1042,
      "step": 5387
    },
    {
      "epoch": 0.025344083087951684,
      "grad_norm": 4.058112621307373,
      "learning_rate": 0.00019539072298131963,
      "loss": 0.8962,
      "step": 5388
    },
    {
      "epoch": 0.025348786889564146,
      "grad_norm": 0.7208545207977295,
      "learning_rate": 0.00019538978000320614,
      "loss": 0.1377,
      "step": 5389
    },
    {
      "epoch": 0.02535349069117661,
      "grad_norm": 0.5912036299705505,
      "learning_rate": 0.00019538883702509266,
      "loss": 0.0606,
      "step": 5390
    },
    {
      "epoch": 0.02535819449278907,
      "grad_norm": 2.590036153793335,
      "learning_rate": 0.00019538789404697918,
      "loss": 0.2045,
      "step": 5391
    },
    {
      "epoch": 0.025362898294401536,
      "grad_norm": 1.0792264938354492,
      "learning_rate": 0.0001953869510688657,
      "loss": 0.194,
      "step": 5392
    },
    {
      "epoch": 0.025367602096014,
      "grad_norm": 1.3427045345306396,
      "learning_rate": 0.00019538600809075222,
      "loss": 0.1757,
      "step": 5393
    },
    {
      "epoch": 0.02537230589762646,
      "grad_norm": 2.450860023498535,
      "learning_rate": 0.00019538506511263874,
      "loss": 0.3059,
      "step": 5394
    },
    {
      "epoch": 0.025377009699238926,
      "grad_norm": 1.673415184020996,
      "learning_rate": 0.00019538412213452526,
      "loss": 0.441,
      "step": 5395
    },
    {
      "epoch": 0.02538171350085139,
      "grad_norm": 0.6746134161949158,
      "learning_rate": 0.0001953831791564118,
      "loss": 0.0858,
      "step": 5396
    },
    {
      "epoch": 0.02538641730246385,
      "grad_norm": 1.2646843194961548,
      "learning_rate": 0.00019538223617829832,
      "loss": 0.137,
      "step": 5397
    },
    {
      "epoch": 0.025391121104076316,
      "grad_norm": 2.7451400756835938,
      "learning_rate": 0.00019538129320018484,
      "loss": 0.6441,
      "step": 5398
    },
    {
      "epoch": 0.02539582490568878,
      "grad_norm": 0.5052376985549927,
      "learning_rate": 0.00019538035022207136,
      "loss": 0.0631,
      "step": 5399
    },
    {
      "epoch": 0.02540052870730124,
      "grad_norm": 1.1349846124649048,
      "learning_rate": 0.00019537940724395788,
      "loss": 0.1103,
      "step": 5400
    },
    {
      "epoch": 0.025405232508913703,
      "grad_norm": 1.9111535549163818,
      "learning_rate": 0.0001953784642658444,
      "loss": 0.2178,
      "step": 5401
    },
    {
      "epoch": 0.02540993631052617,
      "grad_norm": 5.141432285308838,
      "learning_rate": 0.00019537752128773091,
      "loss": 0.7799,
      "step": 5402
    },
    {
      "epoch": 0.02541464011213863,
      "grad_norm": 1.6046509742736816,
      "learning_rate": 0.00019537657830961743,
      "loss": 0.1974,
      "step": 5403
    },
    {
      "epoch": 0.025419343913751093,
      "grad_norm": 0.6656987071037292,
      "learning_rate": 0.00019537563533150395,
      "loss": 0.1161,
      "step": 5404
    },
    {
      "epoch": 0.02542404771536356,
      "grad_norm": 0.28456681966781616,
      "learning_rate": 0.0001953746923533905,
      "loss": 0.023,
      "step": 5405
    },
    {
      "epoch": 0.02542875151697602,
      "grad_norm": 2.0086545944213867,
      "learning_rate": 0.00019537374937527702,
      "loss": 0.282,
      "step": 5406
    },
    {
      "epoch": 0.025433455318588483,
      "grad_norm": 2.4474079608917236,
      "learning_rate": 0.00019537280639716353,
      "loss": 0.3854,
      "step": 5407
    },
    {
      "epoch": 0.025438159120200945,
      "grad_norm": 4.160293102264404,
      "learning_rate": 0.00019537186341905005,
      "loss": 0.8721,
      "step": 5408
    },
    {
      "epoch": 0.02544286292181341,
      "grad_norm": 2.2872815132141113,
      "learning_rate": 0.00019537092044093657,
      "loss": 0.3493,
      "step": 5409
    },
    {
      "epoch": 0.025447566723425873,
      "grad_norm": 0.4493792653083801,
      "learning_rate": 0.00019536997746282312,
      "loss": 0.065,
      "step": 5410
    },
    {
      "epoch": 0.025452270525038335,
      "grad_norm": 1.3741947412490845,
      "learning_rate": 0.00019536903448470964,
      "loss": 0.1808,
      "step": 5411
    },
    {
      "epoch": 0.0254569743266508,
      "grad_norm": 1.0001163482666016,
      "learning_rate": 0.00019536809150659613,
      "loss": 0.148,
      "step": 5412
    },
    {
      "epoch": 0.025461678128263263,
      "grad_norm": 2.7965805530548096,
      "learning_rate": 0.00019536714852848265,
      "loss": 0.3962,
      "step": 5413
    },
    {
      "epoch": 0.025466381929875725,
      "grad_norm": 2.5633509159088135,
      "learning_rate": 0.00019536620555036916,
      "loss": 0.4254,
      "step": 5414
    },
    {
      "epoch": 0.02547108573148819,
      "grad_norm": 2.6292617321014404,
      "learning_rate": 0.0001953652625722557,
      "loss": 0.4134,
      "step": 5415
    },
    {
      "epoch": 0.025475789533100653,
      "grad_norm": 2.1477692127227783,
      "learning_rate": 0.00019536431959414223,
      "loss": 0.3733,
      "step": 5416
    },
    {
      "epoch": 0.025480493334713115,
      "grad_norm": 1.9877197742462158,
      "learning_rate": 0.00019536337661602875,
      "loss": 0.2221,
      "step": 5417
    },
    {
      "epoch": 0.025485197136325577,
      "grad_norm": 1.065267562866211,
      "learning_rate": 0.00019536243363791527,
      "loss": 0.1624,
      "step": 5418
    },
    {
      "epoch": 0.025489900937938043,
      "grad_norm": 0.7562480568885803,
      "learning_rate": 0.0001953614906598018,
      "loss": 0.0964,
      "step": 5419
    },
    {
      "epoch": 0.025494604739550505,
      "grad_norm": 1.7680773735046387,
      "learning_rate": 0.00019536054768168833,
      "loss": 0.4377,
      "step": 5420
    },
    {
      "epoch": 0.025499308541162967,
      "grad_norm": 2.5411479473114014,
      "learning_rate": 0.00019535960470357485,
      "loss": 0.3517,
      "step": 5421
    },
    {
      "epoch": 0.025504012342775433,
      "grad_norm": 2.308394432067871,
      "learning_rate": 0.00019535866172546137,
      "loss": 0.2687,
      "step": 5422
    },
    {
      "epoch": 0.025508716144387895,
      "grad_norm": 1.7559926509857178,
      "learning_rate": 0.00019535771874734786,
      "loss": 0.2801,
      "step": 5423
    },
    {
      "epoch": 0.025513419946000357,
      "grad_norm": 0.6019519567489624,
      "learning_rate": 0.0001953567757692344,
      "loss": 0.0735,
      "step": 5424
    },
    {
      "epoch": 0.02551812374761282,
      "grad_norm": 1.6913875341415405,
      "learning_rate": 0.00019535583279112092,
      "loss": 0.2203,
      "step": 5425
    },
    {
      "epoch": 0.025522827549225285,
      "grad_norm": 2.480104684829712,
      "learning_rate": 0.00019535488981300744,
      "loss": 0.2821,
      "step": 5426
    },
    {
      "epoch": 0.025527531350837747,
      "grad_norm": 1.9566950798034668,
      "learning_rate": 0.00019535394683489396,
      "loss": 0.2356,
      "step": 5427
    },
    {
      "epoch": 0.02553223515245021,
      "grad_norm": 0.5307504534721375,
      "learning_rate": 0.0001953530038567805,
      "loss": 0.0568,
      "step": 5428
    },
    {
      "epoch": 0.025536938954062675,
      "grad_norm": 1.8753650188446045,
      "learning_rate": 0.00019535206087866703,
      "loss": 0.3628,
      "step": 5429
    },
    {
      "epoch": 0.025541642755675137,
      "grad_norm": 0.6779794096946716,
      "learning_rate": 0.00019535111790055354,
      "loss": 0.0734,
      "step": 5430
    },
    {
      "epoch": 0.0255463465572876,
      "grad_norm": 2.3299756050109863,
      "learning_rate": 0.00019535017492244006,
      "loss": 0.5614,
      "step": 5431
    },
    {
      "epoch": 0.025551050358900065,
      "grad_norm": 0.32522085309028625,
      "learning_rate": 0.00019534923194432658,
      "loss": 0.0305,
      "step": 5432
    },
    {
      "epoch": 0.025555754160512527,
      "grad_norm": 2.6571614742279053,
      "learning_rate": 0.0001953482889662131,
      "loss": 0.4414,
      "step": 5433
    },
    {
      "epoch": 0.02556045796212499,
      "grad_norm": 2.8406097888946533,
      "learning_rate": 0.00019534734598809962,
      "loss": 0.3408,
      "step": 5434
    },
    {
      "epoch": 0.02556516176373745,
      "grad_norm": 1.6862636804580688,
      "learning_rate": 0.00019534640300998614,
      "loss": 0.2057,
      "step": 5435
    },
    {
      "epoch": 0.025569865565349917,
      "grad_norm": 4.094875335693359,
      "learning_rate": 0.00019534546003187266,
      "loss": 0.5021,
      "step": 5436
    },
    {
      "epoch": 0.02557456936696238,
      "grad_norm": 0.9203320145606995,
      "learning_rate": 0.0001953445170537592,
      "loss": 0.1345,
      "step": 5437
    },
    {
      "epoch": 0.02557927316857484,
      "grad_norm": 1.2443211078643799,
      "learning_rate": 0.00019534357407564572,
      "loss": 0.1326,
      "step": 5438
    },
    {
      "epoch": 0.025583976970187307,
      "grad_norm": 0.5501699447631836,
      "learning_rate": 0.00019534263109753224,
      "loss": 0.0575,
      "step": 5439
    },
    {
      "epoch": 0.02558868077179977,
      "grad_norm": 3.966269016265869,
      "learning_rate": 0.00019534168811941876,
      "loss": 0.5018,
      "step": 5440
    },
    {
      "epoch": 0.02559338457341223,
      "grad_norm": 3.3462538719177246,
      "learning_rate": 0.00019534074514130528,
      "loss": 0.187,
      "step": 5441
    },
    {
      "epoch": 0.025598088375024693,
      "grad_norm": 3.52284836769104,
      "learning_rate": 0.00019533980216319182,
      "loss": 0.3629,
      "step": 5442
    },
    {
      "epoch": 0.02560279217663716,
      "grad_norm": 1.1396368741989136,
      "learning_rate": 0.00019533885918507831,
      "loss": 0.0828,
      "step": 5443
    },
    {
      "epoch": 0.02560749597824962,
      "grad_norm": 1.3860435485839844,
      "learning_rate": 0.00019533791620696483,
      "loss": 0.1606,
      "step": 5444
    },
    {
      "epoch": 0.025612199779862083,
      "grad_norm": 2.1576366424560547,
      "learning_rate": 0.00019533697322885135,
      "loss": 0.1824,
      "step": 5445
    },
    {
      "epoch": 0.02561690358147455,
      "grad_norm": 3.258344888687134,
      "learning_rate": 0.0001953360302507379,
      "loss": 0.3624,
      "step": 5446
    },
    {
      "epoch": 0.02562160738308701,
      "grad_norm": 0.47601932287216187,
      "learning_rate": 0.00019533508727262442,
      "loss": 0.0321,
      "step": 5447
    },
    {
      "epoch": 0.025626311184699473,
      "grad_norm": 4.654374122619629,
      "learning_rate": 0.00019533414429451093,
      "loss": 1.0879,
      "step": 5448
    },
    {
      "epoch": 0.02563101498631194,
      "grad_norm": 2.6331217288970947,
      "learning_rate": 0.00019533320131639745,
      "loss": 0.443,
      "step": 5449
    },
    {
      "epoch": 0.0256357187879244,
      "grad_norm": 1.3690650463104248,
      "learning_rate": 0.00019533225833828397,
      "loss": 0.143,
      "step": 5450
    },
    {
      "epoch": 0.025640422589536863,
      "grad_norm": 3.4125895500183105,
      "learning_rate": 0.00019533131536017052,
      "loss": 0.2628,
      "step": 5451
    },
    {
      "epoch": 0.025645126391149325,
      "grad_norm": 3.093723773956299,
      "learning_rate": 0.00019533037238205704,
      "loss": 0.4945,
      "step": 5452
    },
    {
      "epoch": 0.02564983019276179,
      "grad_norm": 0.47775784134864807,
      "learning_rate": 0.00019532942940394355,
      "loss": 0.0304,
      "step": 5453
    },
    {
      "epoch": 0.025654533994374253,
      "grad_norm": 2.7116429805755615,
      "learning_rate": 0.00019532848642583005,
      "loss": 0.397,
      "step": 5454
    },
    {
      "epoch": 0.025659237795986715,
      "grad_norm": 2.5608558654785156,
      "learning_rate": 0.0001953275434477166,
      "loss": 0.1064,
      "step": 5455
    },
    {
      "epoch": 0.02566394159759918,
      "grad_norm": 1.489345908164978,
      "learning_rate": 0.0001953266004696031,
      "loss": 0.1071,
      "step": 5456
    },
    {
      "epoch": 0.025668645399211643,
      "grad_norm": 3.4085330963134766,
      "learning_rate": 0.00019532565749148963,
      "loss": 0.3969,
      "step": 5457
    },
    {
      "epoch": 0.025673349200824105,
      "grad_norm": 0.3424905836582184,
      "learning_rate": 0.00019532471451337615,
      "loss": 0.0401,
      "step": 5458
    },
    {
      "epoch": 0.025678053002436568,
      "grad_norm": 3.6384434700012207,
      "learning_rate": 0.00019532377153526267,
      "loss": 0.6861,
      "step": 5459
    },
    {
      "epoch": 0.025682756804049033,
      "grad_norm": 2.3778443336486816,
      "learning_rate": 0.0001953228285571492,
      "loss": 0.2896,
      "step": 5460
    },
    {
      "epoch": 0.025687460605661495,
      "grad_norm": 3.496333599090576,
      "learning_rate": 0.00019532188557903573,
      "loss": 0.7251,
      "step": 5461
    },
    {
      "epoch": 0.025692164407273958,
      "grad_norm": 1.9436616897583008,
      "learning_rate": 0.00019532094260092225,
      "loss": 0.1159,
      "step": 5462
    },
    {
      "epoch": 0.025696868208886423,
      "grad_norm": 0.8719086050987244,
      "learning_rate": 0.00019531999962280877,
      "loss": 0.112,
      "step": 5463
    },
    {
      "epoch": 0.025701572010498885,
      "grad_norm": 2.138359785079956,
      "learning_rate": 0.0001953190566446953,
      "loss": 0.4258,
      "step": 5464
    },
    {
      "epoch": 0.025706275812111348,
      "grad_norm": 0.7120166420936584,
      "learning_rate": 0.0001953181136665818,
      "loss": 0.1,
      "step": 5465
    },
    {
      "epoch": 0.025710979613723813,
      "grad_norm": 0.8208585977554321,
      "learning_rate": 0.00019531717068846832,
      "loss": 0.0743,
      "step": 5466
    },
    {
      "epoch": 0.025715683415336275,
      "grad_norm": 0.2890869379043579,
      "learning_rate": 0.00019531622771035484,
      "loss": 0.0176,
      "step": 5467
    },
    {
      "epoch": 0.025720387216948738,
      "grad_norm": 2.634349822998047,
      "learning_rate": 0.00019531528473224136,
      "loss": 0.3927,
      "step": 5468
    },
    {
      "epoch": 0.0257250910185612,
      "grad_norm": 1.3913849592208862,
      "learning_rate": 0.0001953143417541279,
      "loss": 0.1806,
      "step": 5469
    },
    {
      "epoch": 0.025729794820173665,
      "grad_norm": 2.1424615383148193,
      "learning_rate": 0.00019531339877601443,
      "loss": 0.3283,
      "step": 5470
    },
    {
      "epoch": 0.025734498621786128,
      "grad_norm": 1.9820334911346436,
      "learning_rate": 0.00019531245579790094,
      "loss": 0.2021,
      "step": 5471
    },
    {
      "epoch": 0.02573920242339859,
      "grad_norm": 1.6376906633377075,
      "learning_rate": 0.00019531151281978746,
      "loss": 0.2087,
      "step": 5472
    },
    {
      "epoch": 0.025743906225011055,
      "grad_norm": 0.2857755422592163,
      "learning_rate": 0.00019531056984167398,
      "loss": 0.0246,
      "step": 5473
    },
    {
      "epoch": 0.025748610026623518,
      "grad_norm": 2.305427312850952,
      "learning_rate": 0.0001953096268635605,
      "loss": 0.4784,
      "step": 5474
    },
    {
      "epoch": 0.02575331382823598,
      "grad_norm": 4.889462471008301,
      "learning_rate": 0.00019530868388544702,
      "loss": 0.5907,
      "step": 5475
    },
    {
      "epoch": 0.025758017629848442,
      "grad_norm": 1.0412535667419434,
      "learning_rate": 0.00019530774090733354,
      "loss": 0.1386,
      "step": 5476
    },
    {
      "epoch": 0.025762721431460908,
      "grad_norm": 3.35959792137146,
      "learning_rate": 0.00019530679792922006,
      "loss": 0.6251,
      "step": 5477
    },
    {
      "epoch": 0.02576742523307337,
      "grad_norm": 3.0072176456451416,
      "learning_rate": 0.0001953058549511066,
      "loss": 0.5939,
      "step": 5478
    },
    {
      "epoch": 0.025772129034685832,
      "grad_norm": 1.1034080982208252,
      "learning_rate": 0.00019530491197299312,
      "loss": 0.1034,
      "step": 5479
    },
    {
      "epoch": 0.025776832836298297,
      "grad_norm": 1.8458442687988281,
      "learning_rate": 0.00019530396899487964,
      "loss": 0.2131,
      "step": 5480
    },
    {
      "epoch": 0.02578153663791076,
      "grad_norm": 3.1850688457489014,
      "learning_rate": 0.00019530302601676616,
      "loss": 0.2947,
      "step": 5481
    },
    {
      "epoch": 0.025786240439523222,
      "grad_norm": 1.2144191265106201,
      "learning_rate": 0.00019530208303865268,
      "loss": 0.1253,
      "step": 5482
    },
    {
      "epoch": 0.025790944241135687,
      "grad_norm": 3.239246129989624,
      "learning_rate": 0.00019530114006053922,
      "loss": 0.9122,
      "step": 5483
    },
    {
      "epoch": 0.02579564804274815,
      "grad_norm": 1.853234052658081,
      "learning_rate": 0.00019530019708242574,
      "loss": 0.1926,
      "step": 5484
    },
    {
      "epoch": 0.025800351844360612,
      "grad_norm": 1.2374986410140991,
      "learning_rate": 0.00019529925410431223,
      "loss": 0.1126,
      "step": 5485
    },
    {
      "epoch": 0.025805055645973074,
      "grad_norm": 0.9335140585899353,
      "learning_rate": 0.00019529831112619875,
      "loss": 0.0901,
      "step": 5486
    },
    {
      "epoch": 0.02580975944758554,
      "grad_norm": 0.5505406856536865,
      "learning_rate": 0.0001952973681480853,
      "loss": 0.0638,
      "step": 5487
    },
    {
      "epoch": 0.025814463249198002,
      "grad_norm": 2.6272029876708984,
      "learning_rate": 0.00019529642516997182,
      "loss": 0.4657,
      "step": 5488
    },
    {
      "epoch": 0.025819167050810464,
      "grad_norm": 3.161648750305176,
      "learning_rate": 0.00019529548219185833,
      "loss": 0.214,
      "step": 5489
    },
    {
      "epoch": 0.02582387085242293,
      "grad_norm": 4.113336563110352,
      "learning_rate": 0.00019529453921374485,
      "loss": 0.8746,
      "step": 5490
    },
    {
      "epoch": 0.025828574654035392,
      "grad_norm": 0.7055491209030151,
      "learning_rate": 0.00019529359623563137,
      "loss": 0.0639,
      "step": 5491
    },
    {
      "epoch": 0.025833278455647854,
      "grad_norm": 0.9767218232154846,
      "learning_rate": 0.00019529265325751792,
      "loss": 0.0585,
      "step": 5492
    },
    {
      "epoch": 0.025837982257260316,
      "grad_norm": 2.6800169944763184,
      "learning_rate": 0.00019529171027940444,
      "loss": 0.3874,
      "step": 5493
    },
    {
      "epoch": 0.025842686058872782,
      "grad_norm": 2.966902256011963,
      "learning_rate": 0.00019529076730129095,
      "loss": 0.6628,
      "step": 5494
    },
    {
      "epoch": 0.025847389860485244,
      "grad_norm": 3.0567471981048584,
      "learning_rate": 0.00019528982432317747,
      "loss": 0.6721,
      "step": 5495
    },
    {
      "epoch": 0.025852093662097706,
      "grad_norm": 1.0938823223114014,
      "learning_rate": 0.000195288881345064,
      "loss": 0.1702,
      "step": 5496
    },
    {
      "epoch": 0.025856797463710172,
      "grad_norm": 2.3351118564605713,
      "learning_rate": 0.0001952879383669505,
      "loss": 0.2118,
      "step": 5497
    },
    {
      "epoch": 0.025861501265322634,
      "grad_norm": 1.5084335803985596,
      "learning_rate": 0.00019528699538883703,
      "loss": 0.0867,
      "step": 5498
    },
    {
      "epoch": 0.025866205066935096,
      "grad_norm": 1.7292606830596924,
      "learning_rate": 0.00019528605241072355,
      "loss": 0.2903,
      "step": 5499
    },
    {
      "epoch": 0.025870908868547562,
      "grad_norm": 2.2271809577941895,
      "learning_rate": 0.00019528510943261007,
      "loss": 0.3169,
      "step": 5500
    },
    {
      "epoch": 0.025875612670160024,
      "grad_norm": 2.1239869594573975,
      "learning_rate": 0.0001952841664544966,
      "loss": 0.247,
      "step": 5501
    },
    {
      "epoch": 0.025880316471772486,
      "grad_norm": 1.8103644847869873,
      "learning_rate": 0.00019528322347638313,
      "loss": 0.2623,
      "step": 5502
    },
    {
      "epoch": 0.025885020273384948,
      "grad_norm": 2.5476043224334717,
      "learning_rate": 0.00019528228049826965,
      "loss": 0.3262,
      "step": 5503
    },
    {
      "epoch": 0.025889724074997414,
      "grad_norm": 2.1865081787109375,
      "learning_rate": 0.00019528133752015617,
      "loss": 0.3417,
      "step": 5504
    },
    {
      "epoch": 0.025894427876609876,
      "grad_norm": 0.5291217565536499,
      "learning_rate": 0.0001952803945420427,
      "loss": 0.0459,
      "step": 5505
    },
    {
      "epoch": 0.025899131678222338,
      "grad_norm": 1.1486624479293823,
      "learning_rate": 0.0001952794515639292,
      "loss": 0.168,
      "step": 5506
    },
    {
      "epoch": 0.025903835479834804,
      "grad_norm": 4.992517948150635,
      "learning_rate": 0.00019527850858581572,
      "loss": 0.8542,
      "step": 5507
    },
    {
      "epoch": 0.025908539281447266,
      "grad_norm": 1.865932583808899,
      "learning_rate": 0.00019527756560770224,
      "loss": 0.3346,
      "step": 5508
    },
    {
      "epoch": 0.025913243083059728,
      "grad_norm": 3.390371084213257,
      "learning_rate": 0.00019527662262958876,
      "loss": 0.5133,
      "step": 5509
    },
    {
      "epoch": 0.02591794688467219,
      "grad_norm": 0.8183834552764893,
      "learning_rate": 0.0001952756796514753,
      "loss": 0.0994,
      "step": 5510
    },
    {
      "epoch": 0.025922650686284656,
      "grad_norm": 3.254293918609619,
      "learning_rate": 0.00019527473667336183,
      "loss": 0.2025,
      "step": 5511
    },
    {
      "epoch": 0.025927354487897118,
      "grad_norm": 0.641550600528717,
      "learning_rate": 0.00019527379369524834,
      "loss": 0.091,
      "step": 5512
    },
    {
      "epoch": 0.02593205828950958,
      "grad_norm": 0.8694524765014648,
      "learning_rate": 0.00019527285071713486,
      "loss": 0.2226,
      "step": 5513
    },
    {
      "epoch": 0.025936762091122046,
      "grad_norm": 0.5347316861152649,
      "learning_rate": 0.00019527190773902138,
      "loss": 0.0511,
      "step": 5514
    },
    {
      "epoch": 0.025941465892734508,
      "grad_norm": 2.0508925914764404,
      "learning_rate": 0.00019527096476090793,
      "loss": 0.5152,
      "step": 5515
    },
    {
      "epoch": 0.02594616969434697,
      "grad_norm": 2.94477915763855,
      "learning_rate": 0.00019527002178279442,
      "loss": 0.3596,
      "step": 5516
    },
    {
      "epoch": 0.025950873495959436,
      "grad_norm": 1.930657148361206,
      "learning_rate": 0.00019526907880468094,
      "loss": 0.3119,
      "step": 5517
    },
    {
      "epoch": 0.025955577297571898,
      "grad_norm": 2.5363383293151855,
      "learning_rate": 0.00019526813582656746,
      "loss": 0.4611,
      "step": 5518
    },
    {
      "epoch": 0.02596028109918436,
      "grad_norm": 1.5194066762924194,
      "learning_rate": 0.000195267192848454,
      "loss": 0.4266,
      "step": 5519
    },
    {
      "epoch": 0.025964984900796823,
      "grad_norm": 2.846712350845337,
      "learning_rate": 0.00019526624987034052,
      "loss": 0.5871,
      "step": 5520
    },
    {
      "epoch": 0.025969688702409288,
      "grad_norm": 1.856094479560852,
      "learning_rate": 0.00019526530689222704,
      "loss": 0.247,
      "step": 5521
    },
    {
      "epoch": 0.02597439250402175,
      "grad_norm": 1.3747197389602661,
      "learning_rate": 0.00019526436391411356,
      "loss": 0.2785,
      "step": 5522
    },
    {
      "epoch": 0.025979096305634213,
      "grad_norm": 0.7676339149475098,
      "learning_rate": 0.00019526342093600008,
      "loss": 0.1171,
      "step": 5523
    },
    {
      "epoch": 0.025983800107246678,
      "grad_norm": 1.2407879829406738,
      "learning_rate": 0.00019526247795788662,
      "loss": 0.1503,
      "step": 5524
    },
    {
      "epoch": 0.02598850390885914,
      "grad_norm": 0.4781990945339203,
      "learning_rate": 0.00019526153497977314,
      "loss": 0.0413,
      "step": 5525
    },
    {
      "epoch": 0.025993207710471603,
      "grad_norm": 2.118314027786255,
      "learning_rate": 0.00019526059200165966,
      "loss": 0.3104,
      "step": 5526
    },
    {
      "epoch": 0.025997911512084065,
      "grad_norm": 1.2789396047592163,
      "learning_rate": 0.00019525964902354618,
      "loss": 0.1883,
      "step": 5527
    },
    {
      "epoch": 0.02600261531369653,
      "grad_norm": 2.265157461166382,
      "learning_rate": 0.0001952587060454327,
      "loss": 0.3229,
      "step": 5528
    },
    {
      "epoch": 0.026007319115308992,
      "grad_norm": 1.571864128112793,
      "learning_rate": 0.00019525776306731922,
      "loss": 0.2378,
      "step": 5529
    },
    {
      "epoch": 0.026012022916921455,
      "grad_norm": 0.9116162061691284,
      "learning_rate": 0.00019525682008920573,
      "loss": 0.108,
      "step": 5530
    },
    {
      "epoch": 0.02601672671853392,
      "grad_norm": 2.0691487789154053,
      "learning_rate": 0.00019525587711109225,
      "loss": 0.3446,
      "step": 5531
    },
    {
      "epoch": 0.026021430520146382,
      "grad_norm": 3.0114245414733887,
      "learning_rate": 0.00019525493413297877,
      "loss": 0.4461,
      "step": 5532
    },
    {
      "epoch": 0.026026134321758845,
      "grad_norm": 1.3981518745422363,
      "learning_rate": 0.00019525399115486532,
      "loss": 0.2012,
      "step": 5533
    },
    {
      "epoch": 0.02603083812337131,
      "grad_norm": 3.481678009033203,
      "learning_rate": 0.00019525304817675184,
      "loss": 0.6056,
      "step": 5534
    },
    {
      "epoch": 0.026035541924983772,
      "grad_norm": 0.6491610407829285,
      "learning_rate": 0.00019525210519863835,
      "loss": 0.0629,
      "step": 5535
    },
    {
      "epoch": 0.026040245726596235,
      "grad_norm": 1.7195711135864258,
      "learning_rate": 0.00019525116222052487,
      "loss": 0.2318,
      "step": 5536
    },
    {
      "epoch": 0.026044949528208697,
      "grad_norm": 1.1946697235107422,
      "learning_rate": 0.0001952502192424114,
      "loss": 0.038,
      "step": 5537
    },
    {
      "epoch": 0.026049653329821162,
      "grad_norm": 2.143357992172241,
      "learning_rate": 0.0001952492762642979,
      "loss": 0.4291,
      "step": 5538
    },
    {
      "epoch": 0.026054357131433625,
      "grad_norm": 2.7186038494110107,
      "learning_rate": 0.00019524833328618443,
      "loss": 0.4253,
      "step": 5539
    },
    {
      "epoch": 0.026059060933046087,
      "grad_norm": 1.6675987243652344,
      "learning_rate": 0.00019524739030807095,
      "loss": 0.1795,
      "step": 5540
    },
    {
      "epoch": 0.026063764734658552,
      "grad_norm": 0.25202620029449463,
      "learning_rate": 0.00019524644732995747,
      "loss": 0.0198,
      "step": 5541
    },
    {
      "epoch": 0.026068468536271015,
      "grad_norm": 1.581893801689148,
      "learning_rate": 0.000195245504351844,
      "loss": 0.2166,
      "step": 5542
    },
    {
      "epoch": 0.026073172337883477,
      "grad_norm": 1.0953047275543213,
      "learning_rate": 0.00019524456137373053,
      "loss": 0.0743,
      "step": 5543
    },
    {
      "epoch": 0.02607787613949594,
      "grad_norm": 4.332630157470703,
      "learning_rate": 0.00019524361839561705,
      "loss": 1.0946,
      "step": 5544
    },
    {
      "epoch": 0.026082579941108405,
      "grad_norm": 1.3559455871582031,
      "learning_rate": 0.00019524267541750357,
      "loss": 0.1196,
      "step": 5545
    },
    {
      "epoch": 0.026087283742720867,
      "grad_norm": 0.9906763434410095,
      "learning_rate": 0.00019524173243939011,
      "loss": 0.1383,
      "step": 5546
    },
    {
      "epoch": 0.02609198754433333,
      "grad_norm": 3.0013315677642822,
      "learning_rate": 0.0001952407894612766,
      "loss": 0.2553,
      "step": 5547
    },
    {
      "epoch": 0.026096691345945795,
      "grad_norm": 0.27192598581314087,
      "learning_rate": 0.00019523984648316312,
      "loss": 0.0326,
      "step": 5548
    },
    {
      "epoch": 0.026101395147558257,
      "grad_norm": 3.3930041790008545,
      "learning_rate": 0.00019523890350504964,
      "loss": 0.7949,
      "step": 5549
    },
    {
      "epoch": 0.02610609894917072,
      "grad_norm": 2.8890511989593506,
      "learning_rate": 0.00019523796052693616,
      "loss": 0.4966,
      "step": 5550
    },
    {
      "epoch": 0.026110802750783185,
      "grad_norm": 0.7772045135498047,
      "learning_rate": 0.0001952370175488227,
      "loss": 0.0555,
      "step": 5551
    },
    {
      "epoch": 0.026115506552395647,
      "grad_norm": 1.5101406574249268,
      "learning_rate": 0.00019523607457070923,
      "loss": 0.1305,
      "step": 5552
    },
    {
      "epoch": 0.02612021035400811,
      "grad_norm": 1.0703178644180298,
      "learning_rate": 0.00019523513159259574,
      "loss": 0.074,
      "step": 5553
    },
    {
      "epoch": 0.02612491415562057,
      "grad_norm": 3.0892930030822754,
      "learning_rate": 0.00019523418861448226,
      "loss": 0.7232,
      "step": 5554
    },
    {
      "epoch": 0.026129617957233037,
      "grad_norm": 0.1897059977054596,
      "learning_rate": 0.0001952332456363688,
      "loss": 0.0124,
      "step": 5555
    },
    {
      "epoch": 0.0261343217588455,
      "grad_norm": 3.385575532913208,
      "learning_rate": 0.00019523230265825533,
      "loss": 0.4239,
      "step": 5556
    },
    {
      "epoch": 0.02613902556045796,
      "grad_norm": 1.148478388786316,
      "learning_rate": 0.00019523135968014185,
      "loss": 0.1114,
      "step": 5557
    },
    {
      "epoch": 0.026143729362070427,
      "grad_norm": 1.1614880561828613,
      "learning_rate": 0.00019523041670202836,
      "loss": 0.1145,
      "step": 5558
    },
    {
      "epoch": 0.02614843316368289,
      "grad_norm": 2.877596378326416,
      "learning_rate": 0.00019522947372391486,
      "loss": 0.5894,
      "step": 5559
    },
    {
      "epoch": 0.02615313696529535,
      "grad_norm": 0.886395275592804,
      "learning_rate": 0.0001952285307458014,
      "loss": 0.0931,
      "step": 5560
    },
    {
      "epoch": 0.026157840766907813,
      "grad_norm": 1.5375183820724487,
      "learning_rate": 0.00019522758776768792,
      "loss": 0.2279,
      "step": 5561
    },
    {
      "epoch": 0.02616254456852028,
      "grad_norm": 1.456600546836853,
      "learning_rate": 0.00019522664478957444,
      "loss": 0.2186,
      "step": 5562
    },
    {
      "epoch": 0.02616724837013274,
      "grad_norm": 0.20645031332969666,
      "learning_rate": 0.00019522570181146096,
      "loss": 0.0199,
      "step": 5563
    },
    {
      "epoch": 0.026171952171745203,
      "grad_norm": 1.6137433052062988,
      "learning_rate": 0.00019522475883334748,
      "loss": 0.226,
      "step": 5564
    },
    {
      "epoch": 0.02617665597335767,
      "grad_norm": 1.8418831825256348,
      "learning_rate": 0.00019522381585523402,
      "loss": 0.2127,
      "step": 5565
    },
    {
      "epoch": 0.02618135977497013,
      "grad_norm": 1.7342959642410278,
      "learning_rate": 0.00019522287287712054,
      "loss": 0.2962,
      "step": 5566
    },
    {
      "epoch": 0.026186063576582593,
      "grad_norm": 3.3121449947357178,
      "learning_rate": 0.00019522192989900706,
      "loss": 0.66,
      "step": 5567
    },
    {
      "epoch": 0.02619076737819506,
      "grad_norm": 1.7511564493179321,
      "learning_rate": 0.00019522098692089358,
      "loss": 0.3839,
      "step": 5568
    },
    {
      "epoch": 0.02619547117980752,
      "grad_norm": 1.342590093612671,
      "learning_rate": 0.0001952200439427801,
      "loss": 0.1753,
      "step": 5569
    },
    {
      "epoch": 0.026200174981419983,
      "grad_norm": 1.255303978919983,
      "learning_rate": 0.00019521910096466662,
      "loss": 0.13,
      "step": 5570
    },
    {
      "epoch": 0.026204878783032445,
      "grad_norm": 2.6984360218048096,
      "learning_rate": 0.00019521815798655313,
      "loss": 0.74,
      "step": 5571
    },
    {
      "epoch": 0.02620958258464491,
      "grad_norm": 0.8167747259140015,
      "learning_rate": 0.00019521721500843965,
      "loss": 0.0567,
      "step": 5572
    },
    {
      "epoch": 0.026214286386257373,
      "grad_norm": 3.9041643142700195,
      "learning_rate": 0.00019521627203032617,
      "loss": 0.8206,
      "step": 5573
    },
    {
      "epoch": 0.026218990187869835,
      "grad_norm": 2.6412787437438965,
      "learning_rate": 0.00019521532905221272,
      "loss": 0.4571,
      "step": 5574
    },
    {
      "epoch": 0.0262236939894823,
      "grad_norm": 0.749106228351593,
      "learning_rate": 0.00019521438607409924,
      "loss": 0.0735,
      "step": 5575
    },
    {
      "epoch": 0.026228397791094763,
      "grad_norm": 2.395132303237915,
      "learning_rate": 0.00019521344309598575,
      "loss": 0.3993,
      "step": 5576
    },
    {
      "epoch": 0.026233101592707225,
      "grad_norm": 3.017446279525757,
      "learning_rate": 0.00019521250011787227,
      "loss": 0.4746,
      "step": 5577
    },
    {
      "epoch": 0.026237805394319687,
      "grad_norm": 2.384000539779663,
      "learning_rate": 0.0001952115571397588,
      "loss": 0.2946,
      "step": 5578
    },
    {
      "epoch": 0.026242509195932153,
      "grad_norm": 1.9223451614379883,
      "learning_rate": 0.0001952106141616453,
      "loss": 0.2883,
      "step": 5579
    },
    {
      "epoch": 0.026247212997544615,
      "grad_norm": 0.9717599153518677,
      "learning_rate": 0.00019520967118353183,
      "loss": 0.1518,
      "step": 5580
    },
    {
      "epoch": 0.026251916799157077,
      "grad_norm": 1.6324905157089233,
      "learning_rate": 0.00019520872820541835,
      "loss": 0.3167,
      "step": 5581
    },
    {
      "epoch": 0.026256620600769543,
      "grad_norm": 1.9409875869750977,
      "learning_rate": 0.00019520778522730487,
      "loss": 0.344,
      "step": 5582
    },
    {
      "epoch": 0.026261324402382005,
      "grad_norm": 2.1268773078918457,
      "learning_rate": 0.0001952068422491914,
      "loss": 0.4033,
      "step": 5583
    },
    {
      "epoch": 0.026266028203994467,
      "grad_norm": 2.5233278274536133,
      "learning_rate": 0.00019520589927107793,
      "loss": 0.2639,
      "step": 5584
    },
    {
      "epoch": 0.026270732005606933,
      "grad_norm": 1.4433341026306152,
      "learning_rate": 0.00019520495629296445,
      "loss": 0.1888,
      "step": 5585
    },
    {
      "epoch": 0.026275435807219395,
      "grad_norm": 1.7665374279022217,
      "learning_rate": 0.00019520401331485097,
      "loss": 0.5982,
      "step": 5586
    },
    {
      "epoch": 0.026280139608831857,
      "grad_norm": 0.7062855362892151,
      "learning_rate": 0.00019520307033673751,
      "loss": 0.0652,
      "step": 5587
    },
    {
      "epoch": 0.02628484341044432,
      "grad_norm": 0.25681671500205994,
      "learning_rate": 0.00019520212735862403,
      "loss": 0.0242,
      "step": 5588
    },
    {
      "epoch": 0.026289547212056785,
      "grad_norm": 0.7015193700790405,
      "learning_rate": 0.00019520118438051055,
      "loss": 0.0765,
      "step": 5589
    },
    {
      "epoch": 0.026294251013669247,
      "grad_norm": 0.5116950869560242,
      "learning_rate": 0.00019520024140239704,
      "loss": 0.0524,
      "step": 5590
    },
    {
      "epoch": 0.02629895481528171,
      "grad_norm": 1.6283845901489258,
      "learning_rate": 0.00019519929842428356,
      "loss": 0.437,
      "step": 5591
    },
    {
      "epoch": 0.026303658616894175,
      "grad_norm": 0.9369292259216309,
      "learning_rate": 0.0001951983554461701,
      "loss": 0.1262,
      "step": 5592
    },
    {
      "epoch": 0.026308362418506637,
      "grad_norm": 0.606644868850708,
      "learning_rate": 0.00019519741246805663,
      "loss": 0.0714,
      "step": 5593
    },
    {
      "epoch": 0.0263130662201191,
      "grad_norm": 1.194105863571167,
      "learning_rate": 0.00019519646948994314,
      "loss": 0.1744,
      "step": 5594
    },
    {
      "epoch": 0.026317770021731562,
      "grad_norm": 1.3676255941390991,
      "learning_rate": 0.00019519552651182966,
      "loss": 0.3049,
      "step": 5595
    },
    {
      "epoch": 0.026322473823344027,
      "grad_norm": 1.202311396598816,
      "learning_rate": 0.0001951945835337162,
      "loss": 0.2816,
      "step": 5596
    },
    {
      "epoch": 0.02632717762495649,
      "grad_norm": 3.551752805709839,
      "learning_rate": 0.00019519364055560273,
      "loss": 0.9273,
      "step": 5597
    },
    {
      "epoch": 0.02633188142656895,
      "grad_norm": 2.228191375732422,
      "learning_rate": 0.00019519269757748925,
      "loss": 0.1765,
      "step": 5598
    },
    {
      "epoch": 0.026336585228181417,
      "grad_norm": 1.58591890335083,
      "learning_rate": 0.00019519175459937576,
      "loss": 0.1383,
      "step": 5599
    },
    {
      "epoch": 0.02634128902979388,
      "grad_norm": 2.763047218322754,
      "learning_rate": 0.00019519081162126228,
      "loss": 0.6607,
      "step": 5600
    },
    {
      "epoch": 0.02634599283140634,
      "grad_norm": 1.1858781576156616,
      "learning_rate": 0.0001951898686431488,
      "loss": 0.1033,
      "step": 5601
    },
    {
      "epoch": 0.026350696633018807,
      "grad_norm": 0.5556516051292419,
      "learning_rate": 0.00019518892566503532,
      "loss": 0.054,
      "step": 5602
    },
    {
      "epoch": 0.02635540043463127,
      "grad_norm": 0.8551738858222961,
      "learning_rate": 0.00019518798268692184,
      "loss": 0.0566,
      "step": 5603
    },
    {
      "epoch": 0.02636010423624373,
      "grad_norm": 2.5755083560943604,
      "learning_rate": 0.00019518703970880836,
      "loss": 0.4292,
      "step": 5604
    },
    {
      "epoch": 0.026364808037856194,
      "grad_norm": 2.032902240753174,
      "learning_rate": 0.0001951860967306949,
      "loss": 0.3936,
      "step": 5605
    },
    {
      "epoch": 0.02636951183946866,
      "grad_norm": 2.48665452003479,
      "learning_rate": 0.00019518515375258142,
      "loss": 0.3927,
      "step": 5606
    },
    {
      "epoch": 0.02637421564108112,
      "grad_norm": 0.7803639769554138,
      "learning_rate": 0.00019518421077446794,
      "loss": 0.0894,
      "step": 5607
    },
    {
      "epoch": 0.026378919442693584,
      "grad_norm": 3.3793442249298096,
      "learning_rate": 0.00019518326779635446,
      "loss": 0.4865,
      "step": 5608
    },
    {
      "epoch": 0.02638362324430605,
      "grad_norm": 2.4998371601104736,
      "learning_rate": 0.00019518232481824098,
      "loss": 0.2884,
      "step": 5609
    },
    {
      "epoch": 0.02638832704591851,
      "grad_norm": 2.320221185684204,
      "learning_rate": 0.0001951813818401275,
      "loss": 0.4,
      "step": 5610
    },
    {
      "epoch": 0.026393030847530974,
      "grad_norm": 1.0842525959014893,
      "learning_rate": 0.00019518043886201402,
      "loss": 0.1117,
      "step": 5611
    },
    {
      "epoch": 0.026397734649143436,
      "grad_norm": 0.9071452617645264,
      "learning_rate": 0.00019517949588390053,
      "loss": 0.1542,
      "step": 5612
    },
    {
      "epoch": 0.0264024384507559,
      "grad_norm": 2.4705309867858887,
      "learning_rate": 0.00019517855290578705,
      "loss": 0.2809,
      "step": 5613
    },
    {
      "epoch": 0.026407142252368364,
      "grad_norm": 2.3018438816070557,
      "learning_rate": 0.00019517760992767357,
      "loss": 0.4541,
      "step": 5614
    },
    {
      "epoch": 0.026411846053980826,
      "grad_norm": 2.90720796585083,
      "learning_rate": 0.00019517666694956012,
      "loss": 0.9358,
      "step": 5615
    },
    {
      "epoch": 0.02641654985559329,
      "grad_norm": 0.3042326867580414,
      "learning_rate": 0.00019517572397144664,
      "loss": 0.0344,
      "step": 5616
    },
    {
      "epoch": 0.026421253657205754,
      "grad_norm": 0.6717627644538879,
      "learning_rate": 0.00019517478099333315,
      "loss": 0.0969,
      "step": 5617
    },
    {
      "epoch": 0.026425957458818216,
      "grad_norm": 0.8143548965454102,
      "learning_rate": 0.00019517383801521967,
      "loss": 0.2922,
      "step": 5618
    },
    {
      "epoch": 0.02643066126043068,
      "grad_norm": 2.1527202129364014,
      "learning_rate": 0.00019517289503710622,
      "loss": 0.3387,
      "step": 5619
    },
    {
      "epoch": 0.026435365062043144,
      "grad_norm": 1.3385508060455322,
      "learning_rate": 0.00019517195205899274,
      "loss": 0.1708,
      "step": 5620
    },
    {
      "epoch": 0.026440068863655606,
      "grad_norm": 0.6130455732345581,
      "learning_rate": 0.00019517100908087923,
      "loss": 0.0563,
      "step": 5621
    },
    {
      "epoch": 0.026444772665268068,
      "grad_norm": 1.11167573928833,
      "learning_rate": 0.00019517006610276575,
      "loss": 0.2525,
      "step": 5622
    },
    {
      "epoch": 0.026449476466880534,
      "grad_norm": 1.3367490768432617,
      "learning_rate": 0.00019516912312465227,
      "loss": 0.2586,
      "step": 5623
    },
    {
      "epoch": 0.026454180268492996,
      "grad_norm": 1.606568694114685,
      "learning_rate": 0.0001951681801465388,
      "loss": 0.4506,
      "step": 5624
    },
    {
      "epoch": 0.026458884070105458,
      "grad_norm": 2.6646175384521484,
      "learning_rate": 0.00019516723716842533,
      "loss": 0.5985,
      "step": 5625
    },
    {
      "epoch": 0.026463587871717924,
      "grad_norm": 1.1780468225479126,
      "learning_rate": 0.00019516629419031185,
      "loss": 0.1493,
      "step": 5626
    },
    {
      "epoch": 0.026468291673330386,
      "grad_norm": 1.3715221881866455,
      "learning_rate": 0.00019516535121219837,
      "loss": 0.1632,
      "step": 5627
    },
    {
      "epoch": 0.026472995474942848,
      "grad_norm": 1.221707820892334,
      "learning_rate": 0.00019516440823408491,
      "loss": 0.3438,
      "step": 5628
    },
    {
      "epoch": 0.02647769927655531,
      "grad_norm": 3.1624529361724854,
      "learning_rate": 0.00019516346525597143,
      "loss": 0.7049,
      "step": 5629
    },
    {
      "epoch": 0.026482403078167776,
      "grad_norm": 2.1748974323272705,
      "learning_rate": 0.00019516252227785795,
      "loss": 0.4025,
      "step": 5630
    },
    {
      "epoch": 0.026487106879780238,
      "grad_norm": 0.7106088995933533,
      "learning_rate": 0.00019516157929974447,
      "loss": 0.1184,
      "step": 5631
    },
    {
      "epoch": 0.0264918106813927,
      "grad_norm": 2.362910032272339,
      "learning_rate": 0.00019516063632163096,
      "loss": 0.4688,
      "step": 5632
    },
    {
      "epoch": 0.026496514483005166,
      "grad_norm": 0.8690162897109985,
      "learning_rate": 0.0001951596933435175,
      "loss": 0.0873,
      "step": 5633
    },
    {
      "epoch": 0.026501218284617628,
      "grad_norm": 1.2302781343460083,
      "learning_rate": 0.00019515875036540403,
      "loss": 0.2632,
      "step": 5634
    },
    {
      "epoch": 0.02650592208623009,
      "grad_norm": 1.997448205947876,
      "learning_rate": 0.00019515780738729054,
      "loss": 0.2469,
      "step": 5635
    },
    {
      "epoch": 0.026510625887842556,
      "grad_norm": 0.9906391501426697,
      "learning_rate": 0.00019515686440917706,
      "loss": 0.1435,
      "step": 5636
    },
    {
      "epoch": 0.026515329689455018,
      "grad_norm": 3.130600690841675,
      "learning_rate": 0.0001951559214310636,
      "loss": 0.5538,
      "step": 5637
    },
    {
      "epoch": 0.02652003349106748,
      "grad_norm": 1.4297043085098267,
      "learning_rate": 0.00019515497845295013,
      "loss": 0.1841,
      "step": 5638
    },
    {
      "epoch": 0.026524737292679942,
      "grad_norm": 1.1118617057800293,
      "learning_rate": 0.00019515403547483665,
      "loss": 0.1722,
      "step": 5639
    },
    {
      "epoch": 0.026529441094292408,
      "grad_norm": 4.25373649597168,
      "learning_rate": 0.00019515309249672316,
      "loss": 0.7668,
      "step": 5640
    },
    {
      "epoch": 0.02653414489590487,
      "grad_norm": 2.0053372383117676,
      "learning_rate": 0.00019515214951860968,
      "loss": 0.2169,
      "step": 5641
    },
    {
      "epoch": 0.026538848697517332,
      "grad_norm": 1.858629584312439,
      "learning_rate": 0.0001951512065404962,
      "loss": 0.1861,
      "step": 5642
    },
    {
      "epoch": 0.026543552499129798,
      "grad_norm": 3.9862451553344727,
      "learning_rate": 0.00019515026356238272,
      "loss": 0.5371,
      "step": 5643
    },
    {
      "epoch": 0.02654825630074226,
      "grad_norm": 0.7838885188102722,
      "learning_rate": 0.00019514932058426924,
      "loss": 0.0704,
      "step": 5644
    },
    {
      "epoch": 0.026552960102354722,
      "grad_norm": 0.4939206838607788,
      "learning_rate": 0.00019514837760615576,
      "loss": 0.0481,
      "step": 5645
    },
    {
      "epoch": 0.026557663903967188,
      "grad_norm": 3.908339262008667,
      "learning_rate": 0.0001951474346280423,
      "loss": 0.4915,
      "step": 5646
    },
    {
      "epoch": 0.02656236770557965,
      "grad_norm": 2.2329599857330322,
      "learning_rate": 0.00019514649164992882,
      "loss": 0.2514,
      "step": 5647
    },
    {
      "epoch": 0.026567071507192112,
      "grad_norm": 3.931443691253662,
      "learning_rate": 0.00019514554867181534,
      "loss": 0.4724,
      "step": 5648
    },
    {
      "epoch": 0.026571775308804575,
      "grad_norm": 0.41423097252845764,
      "learning_rate": 0.00019514460569370186,
      "loss": 0.0365,
      "step": 5649
    },
    {
      "epoch": 0.02657647911041704,
      "grad_norm": 2.580268383026123,
      "learning_rate": 0.00019514366271558838,
      "loss": 0.2931,
      "step": 5650
    },
    {
      "epoch": 0.026581182912029502,
      "grad_norm": 3.798283100128174,
      "learning_rate": 0.00019514271973747492,
      "loss": 0.6243,
      "step": 5651
    },
    {
      "epoch": 0.026585886713641965,
      "grad_norm": 1.7816195487976074,
      "learning_rate": 0.00019514177675936142,
      "loss": 0.181,
      "step": 5652
    },
    {
      "epoch": 0.02659059051525443,
      "grad_norm": 0.8276561498641968,
      "learning_rate": 0.00019514083378124793,
      "loss": 0.0282,
      "step": 5653
    },
    {
      "epoch": 0.026595294316866892,
      "grad_norm": 4.59876823425293,
      "learning_rate": 0.00019513989080313445,
      "loss": 0.7493,
      "step": 5654
    },
    {
      "epoch": 0.026599998118479354,
      "grad_norm": 1.7507832050323486,
      "learning_rate": 0.000195138947825021,
      "loss": 0.0122,
      "step": 5655
    },
    {
      "epoch": 0.026604701920091817,
      "grad_norm": 14.744696617126465,
      "learning_rate": 0.00019513800484690752,
      "loss": 0.4484,
      "step": 5656
    },
    {
      "epoch": 0.026609405721704282,
      "grad_norm": 3.364131450653076,
      "learning_rate": 0.00019513706186879404,
      "loss": 0.1337,
      "step": 5657
    },
    {
      "epoch": 0.026614109523316744,
      "grad_norm": 2.9734158515930176,
      "learning_rate": 0.00019513611889068055,
      "loss": 0.2583,
      "step": 5658
    },
    {
      "epoch": 0.026618813324929207,
      "grad_norm": 3.492623805999756,
      "learning_rate": 0.00019513517591256707,
      "loss": 0.3897,
      "step": 5659
    },
    {
      "epoch": 0.026623517126541672,
      "grad_norm": 4.432901859283447,
      "learning_rate": 0.00019513423293445362,
      "loss": 0.8772,
      "step": 5660
    },
    {
      "epoch": 0.026628220928154134,
      "grad_norm": 1.9795236587524414,
      "learning_rate": 0.00019513328995634014,
      "loss": 0.1509,
      "step": 5661
    },
    {
      "epoch": 0.026632924729766597,
      "grad_norm": 2.3541171550750732,
      "learning_rate": 0.00019513234697822666,
      "loss": 0.2062,
      "step": 5662
    },
    {
      "epoch": 0.026637628531379062,
      "grad_norm": 2.563781976699829,
      "learning_rate": 0.00019513140400011315,
      "loss": 0.3455,
      "step": 5663
    },
    {
      "epoch": 0.026642332332991524,
      "grad_norm": 3.2644295692443848,
      "learning_rate": 0.0001951304610219997,
      "loss": 0.4328,
      "step": 5664
    },
    {
      "epoch": 0.026647036134603987,
      "grad_norm": 1.471204400062561,
      "learning_rate": 0.0001951295180438862,
      "loss": 0.095,
      "step": 5665
    },
    {
      "epoch": 0.02665173993621645,
      "grad_norm": 1.0329959392547607,
      "learning_rate": 0.00019512857506577273,
      "loss": 0.067,
      "step": 5666
    },
    {
      "epoch": 0.026656443737828914,
      "grad_norm": 1.4694457054138184,
      "learning_rate": 0.00019512763208765925,
      "loss": 0.1252,
      "step": 5667
    },
    {
      "epoch": 0.026661147539441377,
      "grad_norm": 3.622044086456299,
      "learning_rate": 0.00019512668910954577,
      "loss": 0.3872,
      "step": 5668
    },
    {
      "epoch": 0.02666585134105384,
      "grad_norm": 1.4012943506240845,
      "learning_rate": 0.00019512574613143231,
      "loss": 0.3517,
      "step": 5669
    },
    {
      "epoch": 0.026670555142666304,
      "grad_norm": 0.354759156703949,
      "learning_rate": 0.00019512480315331883,
      "loss": 0.0471,
      "step": 5670
    },
    {
      "epoch": 0.026675258944278767,
      "grad_norm": 0.3275616765022278,
      "learning_rate": 0.00019512386017520535,
      "loss": 0.0252,
      "step": 5671
    },
    {
      "epoch": 0.02667996274589123,
      "grad_norm": 2.7747838497161865,
      "learning_rate": 0.00019512291719709187,
      "loss": 0.5486,
      "step": 5672
    },
    {
      "epoch": 0.02668466654750369,
      "grad_norm": 3.536566734313965,
      "learning_rate": 0.0001951219742189784,
      "loss": 0.5471,
      "step": 5673
    },
    {
      "epoch": 0.026689370349116157,
      "grad_norm": 3.8553357124328613,
      "learning_rate": 0.0001951210312408649,
      "loss": 0.1446,
      "step": 5674
    },
    {
      "epoch": 0.02669407415072862,
      "grad_norm": 2.738676071166992,
      "learning_rate": 0.00019512008826275143,
      "loss": 0.4473,
      "step": 5675
    },
    {
      "epoch": 0.02669877795234108,
      "grad_norm": 1.6044868230819702,
      "learning_rate": 0.00019511914528463794,
      "loss": 0.3192,
      "step": 5676
    },
    {
      "epoch": 0.026703481753953547,
      "grad_norm": 1.0641611814498901,
      "learning_rate": 0.00019511820230652446,
      "loss": 0.1071,
      "step": 5677
    },
    {
      "epoch": 0.02670818555556601,
      "grad_norm": 1.5387933254241943,
      "learning_rate": 0.000195117259328411,
      "loss": 0.1115,
      "step": 5678
    },
    {
      "epoch": 0.02671288935717847,
      "grad_norm": 1.1131463050842285,
      "learning_rate": 0.00019511631635029753,
      "loss": 0.1109,
      "step": 5679
    },
    {
      "epoch": 0.026717593158790937,
      "grad_norm": 1.9309676885604858,
      "learning_rate": 0.00019511537337218405,
      "loss": 0.207,
      "step": 5680
    },
    {
      "epoch": 0.0267222969604034,
      "grad_norm": 1.3868404626846313,
      "learning_rate": 0.00019511443039407056,
      "loss": 0.0714,
      "step": 5681
    },
    {
      "epoch": 0.02672700076201586,
      "grad_norm": 0.8839013576507568,
      "learning_rate": 0.00019511348741595708,
      "loss": 0.1403,
      "step": 5682
    },
    {
      "epoch": 0.026731704563628323,
      "grad_norm": 1.987431287765503,
      "learning_rate": 0.0001951125444378436,
      "loss": 0.6417,
      "step": 5683
    },
    {
      "epoch": 0.02673640836524079,
      "grad_norm": 1.18770170211792,
      "learning_rate": 0.00019511160145973012,
      "loss": 0.1524,
      "step": 5684
    },
    {
      "epoch": 0.02674111216685325,
      "grad_norm": 1.9465489387512207,
      "learning_rate": 0.00019511065848161664,
      "loss": 0.3473,
      "step": 5685
    },
    {
      "epoch": 0.026745815968465713,
      "grad_norm": 0.5704033374786377,
      "learning_rate": 0.00019510971550350316,
      "loss": 0.066,
      "step": 5686
    },
    {
      "epoch": 0.02675051977007818,
      "grad_norm": 2.159325361251831,
      "learning_rate": 0.0001951087725253897,
      "loss": 0.3946,
      "step": 5687
    },
    {
      "epoch": 0.02675522357169064,
      "grad_norm": 1.2311021089553833,
      "learning_rate": 0.00019510782954727622,
      "loss": 0.327,
      "step": 5688
    },
    {
      "epoch": 0.026759927373303103,
      "grad_norm": 1.731205940246582,
      "learning_rate": 0.00019510688656916274,
      "loss": 0.3086,
      "step": 5689
    },
    {
      "epoch": 0.026764631174915565,
      "grad_norm": 0.7923035621643066,
      "learning_rate": 0.00019510594359104926,
      "loss": 0.0978,
      "step": 5690
    },
    {
      "epoch": 0.02676933497652803,
      "grad_norm": 0.5764744877815247,
      "learning_rate": 0.00019510500061293578,
      "loss": 0.0762,
      "step": 5691
    },
    {
      "epoch": 0.026774038778140493,
      "grad_norm": 1.857109785079956,
      "learning_rate": 0.00019510405763482232,
      "loss": 0.3025,
      "step": 5692
    },
    {
      "epoch": 0.026778742579752955,
      "grad_norm": 1.9899024963378906,
      "learning_rate": 0.00019510311465670884,
      "loss": 0.2673,
      "step": 5693
    },
    {
      "epoch": 0.02678344638136542,
      "grad_norm": 2.1705451011657715,
      "learning_rate": 0.00019510217167859533,
      "loss": 0.4997,
      "step": 5694
    },
    {
      "epoch": 0.026788150182977883,
      "grad_norm": 1.6284445524215698,
      "learning_rate": 0.00019510122870048185,
      "loss": 0.135,
      "step": 5695
    },
    {
      "epoch": 0.026792853984590345,
      "grad_norm": 0.9522160887718201,
      "learning_rate": 0.0001951002857223684,
      "loss": 0.1788,
      "step": 5696
    },
    {
      "epoch": 0.02679755778620281,
      "grad_norm": 1.072499394416809,
      "learning_rate": 0.00019509934274425492,
      "loss": 0.1467,
      "step": 5697
    },
    {
      "epoch": 0.026802261587815273,
      "grad_norm": 3.125098943710327,
      "learning_rate": 0.00019509839976614144,
      "loss": 0.5219,
      "step": 5698
    },
    {
      "epoch": 0.026806965389427735,
      "grad_norm": 1.0484000444412231,
      "learning_rate": 0.00019509745678802795,
      "loss": 0.309,
      "step": 5699
    },
    {
      "epoch": 0.026811669191040197,
      "grad_norm": 1.3854893445968628,
      "learning_rate": 0.00019509651380991447,
      "loss": 0.1519,
      "step": 5700
    },
    {
      "epoch": 0.026816372992652663,
      "grad_norm": 0.6378698348999023,
      "learning_rate": 0.00019509557083180102,
      "loss": 0.0613,
      "step": 5701
    },
    {
      "epoch": 0.026821076794265125,
      "grad_norm": 4.927017688751221,
      "learning_rate": 0.00019509462785368754,
      "loss": 0.6515,
      "step": 5702
    },
    {
      "epoch": 0.026825780595877587,
      "grad_norm": 2.489928960800171,
      "learning_rate": 0.00019509368487557406,
      "loss": 0.2752,
      "step": 5703
    },
    {
      "epoch": 0.026830484397490053,
      "grad_norm": 2.2140331268310547,
      "learning_rate": 0.00019509274189746057,
      "loss": 0.1915,
      "step": 5704
    },
    {
      "epoch": 0.026835188199102515,
      "grad_norm": 4.82309103012085,
      "learning_rate": 0.0001950917989193471,
      "loss": 0.7729,
      "step": 5705
    },
    {
      "epoch": 0.026839892000714977,
      "grad_norm": 0.11706975102424622,
      "learning_rate": 0.0001950908559412336,
      "loss": 0.0081,
      "step": 5706
    },
    {
      "epoch": 0.02684459580232744,
      "grad_norm": 2.44777512550354,
      "learning_rate": 0.00019508991296312013,
      "loss": 0.4173,
      "step": 5707
    },
    {
      "epoch": 0.026849299603939905,
      "grad_norm": 1.3101747035980225,
      "learning_rate": 0.00019508896998500665,
      "loss": 0.146,
      "step": 5708
    },
    {
      "epoch": 0.026854003405552367,
      "grad_norm": 1.0363900661468506,
      "learning_rate": 0.00019508802700689317,
      "loss": 0.1169,
      "step": 5709
    },
    {
      "epoch": 0.02685870720716483,
      "grad_norm": 2.2398879528045654,
      "learning_rate": 0.00019508708402877971,
      "loss": 0.3101,
      "step": 5710
    },
    {
      "epoch": 0.026863411008777295,
      "grad_norm": 4.408783435821533,
      "learning_rate": 0.00019508614105066623,
      "loss": 0.5593,
      "step": 5711
    },
    {
      "epoch": 0.026868114810389757,
      "grad_norm": 1.0214277505874634,
      "learning_rate": 0.00019508519807255275,
      "loss": 0.0935,
      "step": 5712
    },
    {
      "epoch": 0.02687281861200222,
      "grad_norm": 1.3738149404525757,
      "learning_rate": 0.00019508425509443927,
      "loss": 0.1305,
      "step": 5713
    },
    {
      "epoch": 0.026877522413614685,
      "grad_norm": 5.556337833404541,
      "learning_rate": 0.0001950833121163258,
      "loss": 0.5816,
      "step": 5714
    },
    {
      "epoch": 0.026882226215227147,
      "grad_norm": 0.8155093193054199,
      "learning_rate": 0.0001950823691382123,
      "loss": 0.1593,
      "step": 5715
    },
    {
      "epoch": 0.02688693001683961,
      "grad_norm": 0.8840830326080322,
      "learning_rate": 0.00019508142616009883,
      "loss": 0.0935,
      "step": 5716
    },
    {
      "epoch": 0.02689163381845207,
      "grad_norm": 1.0583386421203613,
      "learning_rate": 0.00019508048318198534,
      "loss": 0.1309,
      "step": 5717
    },
    {
      "epoch": 0.026896337620064537,
      "grad_norm": 1.7579333782196045,
      "learning_rate": 0.00019507954020387186,
      "loss": 0.1939,
      "step": 5718
    },
    {
      "epoch": 0.026901041421677,
      "grad_norm": 2.3031082153320312,
      "learning_rate": 0.0001950785972257584,
      "loss": 0.405,
      "step": 5719
    },
    {
      "epoch": 0.02690574522328946,
      "grad_norm": 2.950942277908325,
      "learning_rate": 0.00019507765424764493,
      "loss": 0.5504,
      "step": 5720
    },
    {
      "epoch": 0.026910449024901927,
      "grad_norm": 0.6653525829315186,
      "learning_rate": 0.00019507671126953145,
      "loss": 0.0598,
      "step": 5721
    },
    {
      "epoch": 0.02691515282651439,
      "grad_norm": 1.8572267293930054,
      "learning_rate": 0.00019507576829141796,
      "loss": 0.479,
      "step": 5722
    },
    {
      "epoch": 0.02691985662812685,
      "grad_norm": 0.9329499006271362,
      "learning_rate": 0.00019507482531330448,
      "loss": 0.1084,
      "step": 5723
    },
    {
      "epoch": 0.026924560429739314,
      "grad_norm": 0.3256268799304962,
      "learning_rate": 0.00019507388233519103,
      "loss": 0.046,
      "step": 5724
    },
    {
      "epoch": 0.02692926423135178,
      "grad_norm": 1.8297405242919922,
      "learning_rate": 0.00019507293935707752,
      "loss": 0.2058,
      "step": 5725
    },
    {
      "epoch": 0.02693396803296424,
      "grad_norm": 2.1581974029541016,
      "learning_rate": 0.00019507199637896404,
      "loss": 0.2118,
      "step": 5726
    },
    {
      "epoch": 0.026938671834576704,
      "grad_norm": 1.6046439409255981,
      "learning_rate": 0.00019507105340085056,
      "loss": 0.313,
      "step": 5727
    },
    {
      "epoch": 0.02694337563618917,
      "grad_norm": 2.2812161445617676,
      "learning_rate": 0.0001950701104227371,
      "loss": 0.5372,
      "step": 5728
    },
    {
      "epoch": 0.02694807943780163,
      "grad_norm": 2.216052770614624,
      "learning_rate": 0.00019506916744462362,
      "loss": 0.317,
      "step": 5729
    },
    {
      "epoch": 0.026952783239414094,
      "grad_norm": 1.923502802848816,
      "learning_rate": 0.00019506822446651014,
      "loss": 0.1808,
      "step": 5730
    },
    {
      "epoch": 0.02695748704102656,
      "grad_norm": 1.6768890619277954,
      "learning_rate": 0.00019506728148839666,
      "loss": 0.3312,
      "step": 5731
    },
    {
      "epoch": 0.02696219084263902,
      "grad_norm": 1.8217270374298096,
      "learning_rate": 0.00019506633851028318,
      "loss": 0.5253,
      "step": 5732
    },
    {
      "epoch": 0.026966894644251484,
      "grad_norm": 0.8583067655563354,
      "learning_rate": 0.00019506539553216972,
      "loss": 0.0895,
      "step": 5733
    },
    {
      "epoch": 0.026971598445863946,
      "grad_norm": 1.988997459411621,
      "learning_rate": 0.00019506445255405624,
      "loss": 0.2461,
      "step": 5734
    },
    {
      "epoch": 0.02697630224747641,
      "grad_norm": 1.9700207710266113,
      "learning_rate": 0.00019506350957594276,
      "loss": 0.2248,
      "step": 5735
    },
    {
      "epoch": 0.026981006049088874,
      "grad_norm": 1.8762359619140625,
      "learning_rate": 0.00019506256659782925,
      "loss": 0.3235,
      "step": 5736
    },
    {
      "epoch": 0.026985709850701336,
      "grad_norm": 2.204537868499756,
      "learning_rate": 0.0001950616236197158,
      "loss": 0.3542,
      "step": 5737
    },
    {
      "epoch": 0.0269904136523138,
      "grad_norm": 1.0154781341552734,
      "learning_rate": 0.00019506068064160232,
      "loss": 0.0792,
      "step": 5738
    },
    {
      "epoch": 0.026995117453926264,
      "grad_norm": 0.47197607159614563,
      "learning_rate": 0.00019505973766348884,
      "loss": 0.0497,
      "step": 5739
    },
    {
      "epoch": 0.026999821255538726,
      "grad_norm": 1.5844721794128418,
      "learning_rate": 0.00019505879468537535,
      "loss": 0.4292,
      "step": 5740
    },
    {
      "epoch": 0.027004525057151188,
      "grad_norm": 3.078460693359375,
      "learning_rate": 0.00019505785170726187,
      "loss": 0.6468,
      "step": 5741
    },
    {
      "epoch": 0.027009228858763654,
      "grad_norm": 1.4140236377716064,
      "learning_rate": 0.00019505690872914842,
      "loss": 0.2788,
      "step": 5742
    },
    {
      "epoch": 0.027013932660376116,
      "grad_norm": 1.572048544883728,
      "learning_rate": 0.00019505596575103494,
      "loss": 0.1961,
      "step": 5743
    },
    {
      "epoch": 0.027018636461988578,
      "grad_norm": 0.47683948278427124,
      "learning_rate": 0.00019505502277292146,
      "loss": 0.052,
      "step": 5744
    },
    {
      "epoch": 0.027023340263601044,
      "grad_norm": 1.2974427938461304,
      "learning_rate": 0.00019505407979480797,
      "loss": 0.3281,
      "step": 5745
    },
    {
      "epoch": 0.027028044065213506,
      "grad_norm": 1.0421113967895508,
      "learning_rate": 0.0001950531368166945,
      "loss": 0.149,
      "step": 5746
    },
    {
      "epoch": 0.027032747866825968,
      "grad_norm": 0.6192794442176819,
      "learning_rate": 0.000195052193838581,
      "loss": 0.0716,
      "step": 5747
    },
    {
      "epoch": 0.027037451668438434,
      "grad_norm": 3.4928293228149414,
      "learning_rate": 0.00019505125086046753,
      "loss": 0.6263,
      "step": 5748
    },
    {
      "epoch": 0.027042155470050896,
      "grad_norm": 1.766548752784729,
      "learning_rate": 0.00019505030788235405,
      "loss": 0.2097,
      "step": 5749
    },
    {
      "epoch": 0.027046859271663358,
      "grad_norm": 0.585250735282898,
      "learning_rate": 0.00019504936490424057,
      "loss": 0.0728,
      "step": 5750
    },
    {
      "epoch": 0.02705156307327582,
      "grad_norm": 0.6264691948890686,
      "learning_rate": 0.00019504842192612711,
      "loss": 0.0343,
      "step": 5751
    },
    {
      "epoch": 0.027056266874888286,
      "grad_norm": 1.2431541681289673,
      "learning_rate": 0.00019504747894801363,
      "loss": 0.1134,
      "step": 5752
    },
    {
      "epoch": 0.027060970676500748,
      "grad_norm": 1.6919338703155518,
      "learning_rate": 0.00019504653596990015,
      "loss": 0.137,
      "step": 5753
    },
    {
      "epoch": 0.02706567447811321,
      "grad_norm": 1.7343889474868774,
      "learning_rate": 0.00019504559299178667,
      "loss": 0.1793,
      "step": 5754
    },
    {
      "epoch": 0.027070378279725676,
      "grad_norm": 0.9234753847122192,
      "learning_rate": 0.00019504465001367322,
      "loss": 0.1522,
      "step": 5755
    },
    {
      "epoch": 0.027075082081338138,
      "grad_norm": 4.331040382385254,
      "learning_rate": 0.0001950437070355597,
      "loss": 0.617,
      "step": 5756
    },
    {
      "epoch": 0.0270797858829506,
      "grad_norm": 8.2249755859375,
      "learning_rate": 0.00019504276405744623,
      "loss": 0.9765,
      "step": 5757
    },
    {
      "epoch": 0.027084489684563062,
      "grad_norm": 1.1354079246520996,
      "learning_rate": 0.00019504182107933274,
      "loss": 0.1113,
      "step": 5758
    },
    {
      "epoch": 0.027089193486175528,
      "grad_norm": 0.3837839365005493,
      "learning_rate": 0.00019504087810121926,
      "loss": 0.0499,
      "step": 5759
    },
    {
      "epoch": 0.02709389728778799,
      "grad_norm": 1.452194094657898,
      "learning_rate": 0.0001950399351231058,
      "loss": 0.151,
      "step": 5760
    },
    {
      "epoch": 0.027098601089400452,
      "grad_norm": 1.3732820749282837,
      "learning_rate": 0.00019503899214499233,
      "loss": 0.0438,
      "step": 5761
    },
    {
      "epoch": 0.027103304891012918,
      "grad_norm": 2.712974786758423,
      "learning_rate": 0.00019503804916687885,
      "loss": 0.1784,
      "step": 5762
    },
    {
      "epoch": 0.02710800869262538,
      "grad_norm": 0.628505289554596,
      "learning_rate": 0.00019503710618876536,
      "loss": 0.0452,
      "step": 5763
    },
    {
      "epoch": 0.027112712494237842,
      "grad_norm": 1.2730616331100464,
      "learning_rate": 0.0001950361632106519,
      "loss": 0.1429,
      "step": 5764
    },
    {
      "epoch": 0.027117416295850308,
      "grad_norm": 7.9303879737854,
      "learning_rate": 0.00019503522023253843,
      "loss": 0.9561,
      "step": 5765
    },
    {
      "epoch": 0.02712212009746277,
      "grad_norm": 3.8244786262512207,
      "learning_rate": 0.00019503427725442495,
      "loss": 0.786,
      "step": 5766
    },
    {
      "epoch": 0.027126823899075232,
      "grad_norm": 1.9009594917297363,
      "learning_rate": 0.00019503333427631144,
      "loss": 0.4626,
      "step": 5767
    },
    {
      "epoch": 0.027131527700687694,
      "grad_norm": 2.3749873638153076,
      "learning_rate": 0.00019503239129819796,
      "loss": 0.1409,
      "step": 5768
    },
    {
      "epoch": 0.02713623150230016,
      "grad_norm": 0.6542741656303406,
      "learning_rate": 0.0001950314483200845,
      "loss": 0.0715,
      "step": 5769
    },
    {
      "epoch": 0.027140935303912622,
      "grad_norm": 1.5333020687103271,
      "learning_rate": 0.00019503050534197102,
      "loss": 0.1501,
      "step": 5770
    },
    {
      "epoch": 0.027145639105525084,
      "grad_norm": 4.52291202545166,
      "learning_rate": 0.00019502956236385754,
      "loss": 0.2523,
      "step": 5771
    },
    {
      "epoch": 0.02715034290713755,
      "grad_norm": 6.104320049285889,
      "learning_rate": 0.00019502861938574406,
      "loss": 0.6628,
      "step": 5772
    },
    {
      "epoch": 0.027155046708750012,
      "grad_norm": 0.8896309733390808,
      "learning_rate": 0.00019502767640763058,
      "loss": 0.0763,
      "step": 5773
    },
    {
      "epoch": 0.027159750510362474,
      "grad_norm": 2.2953989505767822,
      "learning_rate": 0.00019502673342951712,
      "loss": 0.2509,
      "step": 5774
    },
    {
      "epoch": 0.027164454311974937,
      "grad_norm": 0.6733090877532959,
      "learning_rate": 0.00019502579045140364,
      "loss": 0.0586,
      "step": 5775
    },
    {
      "epoch": 0.027169158113587402,
      "grad_norm": 0.8171977400779724,
      "learning_rate": 0.00019502484747329016,
      "loss": 0.0638,
      "step": 5776
    },
    {
      "epoch": 0.027173861915199864,
      "grad_norm": 2.10148286819458,
      "learning_rate": 0.00019502390449517668,
      "loss": 0.5894,
      "step": 5777
    },
    {
      "epoch": 0.027178565716812327,
      "grad_norm": 3.378269910812378,
      "learning_rate": 0.0001950229615170632,
      "loss": 0.3906,
      "step": 5778
    },
    {
      "epoch": 0.027183269518424792,
      "grad_norm": 2.5461349487304688,
      "learning_rate": 0.00019502201853894972,
      "loss": 0.1785,
      "step": 5779
    },
    {
      "epoch": 0.027187973320037254,
      "grad_norm": 1.7865124940872192,
      "learning_rate": 0.00019502107556083624,
      "loss": 0.2179,
      "step": 5780
    },
    {
      "epoch": 0.027192677121649717,
      "grad_norm": 2.07777738571167,
      "learning_rate": 0.00019502013258272275,
      "loss": 0.1224,
      "step": 5781
    },
    {
      "epoch": 0.027197380923262182,
      "grad_norm": 0.11324036121368408,
      "learning_rate": 0.00019501918960460927,
      "loss": 0.0086,
      "step": 5782
    },
    {
      "epoch": 0.027202084724874644,
      "grad_norm": 0.5524550080299377,
      "learning_rate": 0.00019501824662649582,
      "loss": 0.04,
      "step": 5783
    },
    {
      "epoch": 0.027206788526487106,
      "grad_norm": 0.03158131241798401,
      "learning_rate": 0.00019501730364838234,
      "loss": 0.002,
      "step": 5784
    },
    {
      "epoch": 0.02721149232809957,
      "grad_norm": 4.415877819061279,
      "learning_rate": 0.00019501636067026886,
      "loss": 0.3813,
      "step": 5785
    },
    {
      "epoch": 0.027216196129712034,
      "grad_norm": 3.7435898780822754,
      "learning_rate": 0.00019501541769215537,
      "loss": 0.3989,
      "step": 5786
    },
    {
      "epoch": 0.027220899931324496,
      "grad_norm": 2.316387414932251,
      "learning_rate": 0.0001950144747140419,
      "loss": 0.4205,
      "step": 5787
    },
    {
      "epoch": 0.02722560373293696,
      "grad_norm": 10.50442123413086,
      "learning_rate": 0.0001950135317359284,
      "loss": 0.588,
      "step": 5788
    },
    {
      "epoch": 0.027230307534549424,
      "grad_norm": 0.26468339562416077,
      "learning_rate": 0.00019501258875781493,
      "loss": 0.0199,
      "step": 5789
    },
    {
      "epoch": 0.027235011336161886,
      "grad_norm": 0.8950622081756592,
      "learning_rate": 0.00019501164577970145,
      "loss": 0.0774,
      "step": 5790
    },
    {
      "epoch": 0.02723971513777435,
      "grad_norm": 0.5101845264434814,
      "learning_rate": 0.00019501070280158797,
      "loss": 0.0444,
      "step": 5791
    },
    {
      "epoch": 0.02724441893938681,
      "grad_norm": 1.8414651155471802,
      "learning_rate": 0.00019500975982347451,
      "loss": 0.1174,
      "step": 5792
    },
    {
      "epoch": 0.027249122740999276,
      "grad_norm": 2.674842357635498,
      "learning_rate": 0.00019500881684536103,
      "loss": 0.3922,
      "step": 5793
    },
    {
      "epoch": 0.02725382654261174,
      "grad_norm": 2.448458194732666,
      "learning_rate": 0.00019500787386724755,
      "loss": 0.3032,
      "step": 5794
    },
    {
      "epoch": 0.0272585303442242,
      "grad_norm": 2.9380295276641846,
      "learning_rate": 0.00019500693088913407,
      "loss": 0.4402,
      "step": 5795
    },
    {
      "epoch": 0.027263234145836666,
      "grad_norm": 1.5217375755310059,
      "learning_rate": 0.00019500598791102062,
      "loss": 0.1205,
      "step": 5796
    },
    {
      "epoch": 0.02726793794744913,
      "grad_norm": 16.97153091430664,
      "learning_rate": 0.00019500504493290713,
      "loss": 0.4329,
      "step": 5797
    },
    {
      "epoch": 0.02727264174906159,
      "grad_norm": 0.16261793673038483,
      "learning_rate": 0.00019500410195479363,
      "loss": 0.0132,
      "step": 5798
    },
    {
      "epoch": 0.027277345550674056,
      "grad_norm": 3.500272035598755,
      "learning_rate": 0.00019500315897668014,
      "loss": 0.6131,
      "step": 5799
    },
    {
      "epoch": 0.02728204935228652,
      "grad_norm": 0.9492439031600952,
      "learning_rate": 0.00019500221599856666,
      "loss": 0.0883,
      "step": 5800
    },
    {
      "epoch": 0.02728675315389898,
      "grad_norm": 2.311626434326172,
      "learning_rate": 0.0001950012730204532,
      "loss": 0.1394,
      "step": 5801
    },
    {
      "epoch": 0.027291456955511443,
      "grad_norm": 0.6589087843894958,
      "learning_rate": 0.00019500033004233973,
      "loss": 0.0311,
      "step": 5802
    },
    {
      "epoch": 0.02729616075712391,
      "grad_norm": 1.3914856910705566,
      "learning_rate": 0.00019499938706422625,
      "loss": 0.0977,
      "step": 5803
    },
    {
      "epoch": 0.02730086455873637,
      "grad_norm": 7.98688268661499,
      "learning_rate": 0.00019499844408611276,
      "loss": 0.3335,
      "step": 5804
    },
    {
      "epoch": 0.027305568360348833,
      "grad_norm": 2.2521400451660156,
      "learning_rate": 0.0001949975011079993,
      "loss": 0.3377,
      "step": 5805
    },
    {
      "epoch": 0.0273102721619613,
      "grad_norm": 1.5849318504333496,
      "learning_rate": 0.00019499655812988583,
      "loss": 0.1068,
      "step": 5806
    },
    {
      "epoch": 0.02731497596357376,
      "grad_norm": 6.046838760375977,
      "learning_rate": 0.00019499561515177235,
      "loss": 0.5314,
      "step": 5807
    },
    {
      "epoch": 0.027319679765186223,
      "grad_norm": 4.3459014892578125,
      "learning_rate": 0.00019499467217365887,
      "loss": 0.5432,
      "step": 5808
    },
    {
      "epoch": 0.027324383566798685,
      "grad_norm": 0.5305169820785522,
      "learning_rate": 0.00019499372919554538,
      "loss": 0.0473,
      "step": 5809
    },
    {
      "epoch": 0.02732908736841115,
      "grad_norm": 1.3152085542678833,
      "learning_rate": 0.0001949927862174319,
      "loss": 0.2447,
      "step": 5810
    },
    {
      "epoch": 0.027333791170023613,
      "grad_norm": 1.4201529026031494,
      "learning_rate": 0.00019499184323931842,
      "loss": 0.1423,
      "step": 5811
    },
    {
      "epoch": 0.027338494971636075,
      "grad_norm": 0.5365249514579773,
      "learning_rate": 0.00019499090026120494,
      "loss": 0.063,
      "step": 5812
    },
    {
      "epoch": 0.02734319877324854,
      "grad_norm": 2.445924758911133,
      "learning_rate": 0.00019498995728309146,
      "loss": 0.1892,
      "step": 5813
    },
    {
      "epoch": 0.027347902574861003,
      "grad_norm": 2.393817901611328,
      "learning_rate": 0.000194989014304978,
      "loss": 0.5223,
      "step": 5814
    },
    {
      "epoch": 0.027352606376473465,
      "grad_norm": 3.670509099960327,
      "learning_rate": 0.00019498807132686452,
      "loss": 0.8438,
      "step": 5815
    },
    {
      "epoch": 0.02735731017808593,
      "grad_norm": 2.1125593185424805,
      "learning_rate": 0.00019498712834875104,
      "loss": 0.2456,
      "step": 5816
    },
    {
      "epoch": 0.027362013979698393,
      "grad_norm": 1.4764273166656494,
      "learning_rate": 0.00019498618537063756,
      "loss": 0.1965,
      "step": 5817
    },
    {
      "epoch": 0.027366717781310855,
      "grad_norm": 2.0350899696350098,
      "learning_rate": 0.00019498524239252408,
      "loss": 0.2137,
      "step": 5818
    },
    {
      "epoch": 0.027371421582923317,
      "grad_norm": 5.46348237991333,
      "learning_rate": 0.0001949842994144106,
      "loss": 0.611,
      "step": 5819
    },
    {
      "epoch": 0.027376125384535783,
      "grad_norm": 2.2758443355560303,
      "learning_rate": 0.00019498335643629712,
      "loss": 0.4599,
      "step": 5820
    },
    {
      "epoch": 0.027380829186148245,
      "grad_norm": 2.2069902420043945,
      "learning_rate": 0.00019498241345818364,
      "loss": 0.2438,
      "step": 5821
    },
    {
      "epoch": 0.027385532987760707,
      "grad_norm": 2.7805588245391846,
      "learning_rate": 0.00019498147048007015,
      "loss": 0.2938,
      "step": 5822
    },
    {
      "epoch": 0.027390236789373173,
      "grad_norm": 1.925148844718933,
      "learning_rate": 0.00019498052750195667,
      "loss": 0.1317,
      "step": 5823
    },
    {
      "epoch": 0.027394940590985635,
      "grad_norm": 1.340761423110962,
      "learning_rate": 0.00019497958452384322,
      "loss": 0.297,
      "step": 5824
    },
    {
      "epoch": 0.027399644392598097,
      "grad_norm": 1.8409678936004639,
      "learning_rate": 0.00019497864154572974,
      "loss": 0.4298,
      "step": 5825
    },
    {
      "epoch": 0.02740434819421056,
      "grad_norm": 1.5144799947738647,
      "learning_rate": 0.00019497769856761626,
      "loss": 0.3759,
      "step": 5826
    },
    {
      "epoch": 0.027409051995823025,
      "grad_norm": 0.9621650576591492,
      "learning_rate": 0.00019497675558950277,
      "loss": 0.09,
      "step": 5827
    },
    {
      "epoch": 0.027413755797435487,
      "grad_norm": 2.5368447303771973,
      "learning_rate": 0.00019497581261138932,
      "loss": 0.3613,
      "step": 5828
    },
    {
      "epoch": 0.02741845959904795,
      "grad_norm": 6.657562732696533,
      "learning_rate": 0.0001949748696332758,
      "loss": 0.3698,
      "step": 5829
    },
    {
      "epoch": 0.027423163400660415,
      "grad_norm": 2.351172924041748,
      "learning_rate": 0.00019497392665516233,
      "loss": 0.7682,
      "step": 5830
    },
    {
      "epoch": 0.027427867202272877,
      "grad_norm": 2.416555166244507,
      "learning_rate": 0.00019497298367704885,
      "loss": 0.3398,
      "step": 5831
    },
    {
      "epoch": 0.02743257100388534,
      "grad_norm": 1.843084692955017,
      "learning_rate": 0.00019497204069893537,
      "loss": 0.2914,
      "step": 5832
    },
    {
      "epoch": 0.027437274805497805,
      "grad_norm": 1.306475043296814,
      "learning_rate": 0.00019497109772082191,
      "loss": 0.1743,
      "step": 5833
    },
    {
      "epoch": 0.027441978607110267,
      "grad_norm": 1.2744563817977905,
      "learning_rate": 0.00019497015474270843,
      "loss": 0.2067,
      "step": 5834
    },
    {
      "epoch": 0.02744668240872273,
      "grad_norm": 0.8613088726997375,
      "learning_rate": 0.00019496921176459495,
      "loss": 0.0866,
      "step": 5835
    },
    {
      "epoch": 0.02745138621033519,
      "grad_norm": 0.8970971703529358,
      "learning_rate": 0.00019496826878648147,
      "loss": 0.1962,
      "step": 5836
    },
    {
      "epoch": 0.027456090011947657,
      "grad_norm": 2.4520177841186523,
      "learning_rate": 0.00019496732580836802,
      "loss": 0.3122,
      "step": 5837
    },
    {
      "epoch": 0.02746079381356012,
      "grad_norm": 2.0110104084014893,
      "learning_rate": 0.00019496638283025453,
      "loss": 0.494,
      "step": 5838
    },
    {
      "epoch": 0.02746549761517258,
      "grad_norm": 0.9192371964454651,
      "learning_rate": 0.00019496543985214105,
      "loss": 0.164,
      "step": 5839
    },
    {
      "epoch": 0.027470201416785047,
      "grad_norm": 2.5459537506103516,
      "learning_rate": 0.00019496449687402757,
      "loss": 0.3336,
      "step": 5840
    },
    {
      "epoch": 0.02747490521839751,
      "grad_norm": 1.7781211137771606,
      "learning_rate": 0.00019496355389591406,
      "loss": 0.3717,
      "step": 5841
    },
    {
      "epoch": 0.02747960902000997,
      "grad_norm": 1.6638951301574707,
      "learning_rate": 0.0001949626109178006,
      "loss": 0.3021,
      "step": 5842
    },
    {
      "epoch": 0.027484312821622434,
      "grad_norm": 1.166025996208191,
      "learning_rate": 0.00019496166793968713,
      "loss": 0.1332,
      "step": 5843
    },
    {
      "epoch": 0.0274890166232349,
      "grad_norm": 2.117199659347534,
      "learning_rate": 0.00019496072496157365,
      "loss": 0.4383,
      "step": 5844
    },
    {
      "epoch": 0.02749372042484736,
      "grad_norm": 1.374337911605835,
      "learning_rate": 0.00019495978198346016,
      "loss": 0.3057,
      "step": 5845
    },
    {
      "epoch": 0.027498424226459824,
      "grad_norm": 1.364163875579834,
      "learning_rate": 0.0001949588390053467,
      "loss": 0.1955,
      "step": 5846
    },
    {
      "epoch": 0.02750312802807229,
      "grad_norm": 1.469321370124817,
      "learning_rate": 0.00019495789602723323,
      "loss": 0.2171,
      "step": 5847
    },
    {
      "epoch": 0.02750783182968475,
      "grad_norm": 0.3760773241519928,
      "learning_rate": 0.00019495695304911975,
      "loss": 0.032,
      "step": 5848
    },
    {
      "epoch": 0.027512535631297214,
      "grad_norm": 1.8259303569793701,
      "learning_rate": 0.00019495601007100627,
      "loss": 0.3525,
      "step": 5849
    },
    {
      "epoch": 0.02751723943290968,
      "grad_norm": 3.184511423110962,
      "learning_rate": 0.00019495506709289278,
      "loss": 0.3711,
      "step": 5850
    },
    {
      "epoch": 0.02752194323452214,
      "grad_norm": 1.7745826244354248,
      "learning_rate": 0.0001949541241147793,
      "loss": 0.2107,
      "step": 5851
    },
    {
      "epoch": 0.027526647036134604,
      "grad_norm": 2.642307758331299,
      "learning_rate": 0.00019495318113666582,
      "loss": 0.5945,
      "step": 5852
    },
    {
      "epoch": 0.027531350837747066,
      "grad_norm": 1.0164674520492554,
      "learning_rate": 0.00019495223815855234,
      "loss": 0.1447,
      "step": 5853
    },
    {
      "epoch": 0.02753605463935953,
      "grad_norm": 1.6953595876693726,
      "learning_rate": 0.00019495129518043886,
      "loss": 0.2309,
      "step": 5854
    },
    {
      "epoch": 0.027540758440971994,
      "grad_norm": 0.7869205474853516,
      "learning_rate": 0.0001949503522023254,
      "loss": 0.1354,
      "step": 5855
    },
    {
      "epoch": 0.027545462242584456,
      "grad_norm": 4.7925238609313965,
      "learning_rate": 0.00019494940922421192,
      "loss": 0.4278,
      "step": 5856
    },
    {
      "epoch": 0.02755016604419692,
      "grad_norm": 1.3111913204193115,
      "learning_rate": 0.00019494846624609844,
      "loss": 0.235,
      "step": 5857
    },
    {
      "epoch": 0.027554869845809384,
      "grad_norm": 1.0342931747436523,
      "learning_rate": 0.00019494752326798496,
      "loss": 0.1714,
      "step": 5858
    },
    {
      "epoch": 0.027559573647421846,
      "grad_norm": 1.6439939737319946,
      "learning_rate": 0.00019494658028987148,
      "loss": 0.2689,
      "step": 5859
    },
    {
      "epoch": 0.027564277449034308,
      "grad_norm": 1.2898367643356323,
      "learning_rate": 0.000194945637311758,
      "loss": 0.2486,
      "step": 5860
    },
    {
      "epoch": 0.027568981250646774,
      "grad_norm": 1.8132730722427368,
      "learning_rate": 0.00019494469433364452,
      "loss": 0.3203,
      "step": 5861
    },
    {
      "epoch": 0.027573685052259236,
      "grad_norm": 1.9426300525665283,
      "learning_rate": 0.00019494375135553104,
      "loss": 0.2506,
      "step": 5862
    },
    {
      "epoch": 0.027578388853871698,
      "grad_norm": 0.6695948839187622,
      "learning_rate": 0.00019494280837741755,
      "loss": 0.1038,
      "step": 5863
    },
    {
      "epoch": 0.027583092655484164,
      "grad_norm": 2.3577497005462646,
      "learning_rate": 0.0001949418653993041,
      "loss": 0.4984,
      "step": 5864
    },
    {
      "epoch": 0.027587796457096626,
      "grad_norm": 0.8876765966415405,
      "learning_rate": 0.00019494092242119062,
      "loss": 0.1042,
      "step": 5865
    },
    {
      "epoch": 0.027592500258709088,
      "grad_norm": 2.180335760116577,
      "learning_rate": 0.00019493997944307714,
      "loss": 0.365,
      "step": 5866
    },
    {
      "epoch": 0.027597204060321553,
      "grad_norm": 1.4505916833877563,
      "learning_rate": 0.00019493903646496366,
      "loss": 0.1494,
      "step": 5867
    },
    {
      "epoch": 0.027601907861934016,
      "grad_norm": 1.4295759201049805,
      "learning_rate": 0.00019493809348685017,
      "loss": 0.2293,
      "step": 5868
    },
    {
      "epoch": 0.027606611663546478,
      "grad_norm": 3.3818068504333496,
      "learning_rate": 0.00019493715050873672,
      "loss": 0.6658,
      "step": 5869
    },
    {
      "epoch": 0.02761131546515894,
      "grad_norm": 4.892041206359863,
      "learning_rate": 0.00019493620753062324,
      "loss": 1.2848,
      "step": 5870
    },
    {
      "epoch": 0.027616019266771406,
      "grad_norm": 3.401048421859741,
      "learning_rate": 0.00019493526455250976,
      "loss": 0.2616,
      "step": 5871
    },
    {
      "epoch": 0.027620723068383868,
      "grad_norm": 0.459072083234787,
      "learning_rate": 0.00019493432157439625,
      "loss": 0.035,
      "step": 5872
    },
    {
      "epoch": 0.02762542686999633,
      "grad_norm": 4.123901844024658,
      "learning_rate": 0.00019493337859628277,
      "loss": 0.8716,
      "step": 5873
    },
    {
      "epoch": 0.027630130671608796,
      "grad_norm": 2.400010108947754,
      "learning_rate": 0.0001949324356181693,
      "loss": 0.6339,
      "step": 5874
    },
    {
      "epoch": 0.027634834473221258,
      "grad_norm": 0.7930736541748047,
      "learning_rate": 0.00019493149264005583,
      "loss": 0.0973,
      "step": 5875
    },
    {
      "epoch": 0.02763953827483372,
      "grad_norm": 1.261176586151123,
      "learning_rate": 0.00019493054966194235,
      "loss": 0.1095,
      "step": 5876
    },
    {
      "epoch": 0.027644242076446182,
      "grad_norm": 3.1838626861572266,
      "learning_rate": 0.00019492960668382887,
      "loss": 0.5875,
      "step": 5877
    },
    {
      "epoch": 0.027648945878058648,
      "grad_norm": 2.9262170791625977,
      "learning_rate": 0.00019492866370571542,
      "loss": 0.2707,
      "step": 5878
    },
    {
      "epoch": 0.02765364967967111,
      "grad_norm": 0.7775251269340515,
      "learning_rate": 0.00019492772072760193,
      "loss": 0.0915,
      "step": 5879
    },
    {
      "epoch": 0.027658353481283572,
      "grad_norm": 3.5251076221466064,
      "learning_rate": 0.00019492677774948845,
      "loss": 0.3421,
      "step": 5880
    },
    {
      "epoch": 0.027663057282896038,
      "grad_norm": 1.5550775527954102,
      "learning_rate": 0.00019492583477137497,
      "loss": 0.3108,
      "step": 5881
    },
    {
      "epoch": 0.0276677610845085,
      "grad_norm": 1.4704780578613281,
      "learning_rate": 0.0001949248917932615,
      "loss": 0.3944,
      "step": 5882
    },
    {
      "epoch": 0.027672464886120962,
      "grad_norm": 1.1949232816696167,
      "learning_rate": 0.000194923948815148,
      "loss": 0.1226,
      "step": 5883
    },
    {
      "epoch": 0.027677168687733428,
      "grad_norm": 1.2336161136627197,
      "learning_rate": 0.00019492300583703453,
      "loss": 0.2443,
      "step": 5884
    },
    {
      "epoch": 0.02768187248934589,
      "grad_norm": 1.1531362533569336,
      "learning_rate": 0.00019492206285892105,
      "loss": 0.2915,
      "step": 5885
    },
    {
      "epoch": 0.027686576290958352,
      "grad_norm": 0.5043473243713379,
      "learning_rate": 0.00019492111988080756,
      "loss": 0.0529,
      "step": 5886
    },
    {
      "epoch": 0.027691280092570814,
      "grad_norm": 0.7968682646751404,
      "learning_rate": 0.0001949201769026941,
      "loss": 0.1366,
      "step": 5887
    },
    {
      "epoch": 0.02769598389418328,
      "grad_norm": 3.525024652481079,
      "learning_rate": 0.00019491923392458063,
      "loss": 0.2859,
      "step": 5888
    },
    {
      "epoch": 0.027700687695795742,
      "grad_norm": 1.6340919733047485,
      "learning_rate": 0.00019491829094646715,
      "loss": 0.4163,
      "step": 5889
    },
    {
      "epoch": 0.027705391497408204,
      "grad_norm": 0.939902126789093,
      "learning_rate": 0.00019491734796835367,
      "loss": 0.1583,
      "step": 5890
    },
    {
      "epoch": 0.02771009529902067,
      "grad_norm": 1.3546916246414185,
      "learning_rate": 0.00019491640499024018,
      "loss": 0.1713,
      "step": 5891
    },
    {
      "epoch": 0.027714799100633132,
      "grad_norm": 2.8674049377441406,
      "learning_rate": 0.0001949154620121267,
      "loss": 0.5183,
      "step": 5892
    },
    {
      "epoch": 0.027719502902245594,
      "grad_norm": 3.6170542240142822,
      "learning_rate": 0.00019491451903401322,
      "loss": 1.0459,
      "step": 5893
    },
    {
      "epoch": 0.027724206703858056,
      "grad_norm": 0.43940672278404236,
      "learning_rate": 0.00019491357605589974,
      "loss": 0.075,
      "step": 5894
    },
    {
      "epoch": 0.027728910505470522,
      "grad_norm": 1.7164790630340576,
      "learning_rate": 0.00019491263307778626,
      "loss": 0.2697,
      "step": 5895
    },
    {
      "epoch": 0.027733614307082984,
      "grad_norm": 0.6940451860427856,
      "learning_rate": 0.0001949116900996728,
      "loss": 0.1058,
      "step": 5896
    },
    {
      "epoch": 0.027738318108695446,
      "grad_norm": 5.589560508728027,
      "learning_rate": 0.00019491074712155932,
      "loss": 0.8659,
      "step": 5897
    },
    {
      "epoch": 0.027743021910307912,
      "grad_norm": 2.15149188041687,
      "learning_rate": 0.00019490980414344584,
      "loss": 0.2716,
      "step": 5898
    },
    {
      "epoch": 0.027747725711920374,
      "grad_norm": 0.5054000020027161,
      "learning_rate": 0.00019490886116533236,
      "loss": 0.055,
      "step": 5899
    },
    {
      "epoch": 0.027752429513532836,
      "grad_norm": 0.7419652342796326,
      "learning_rate": 0.00019490791818721888,
      "loss": 0.074,
      "step": 5900
    },
    {
      "epoch": 0.027757133315145302,
      "grad_norm": 2.911547899246216,
      "learning_rate": 0.00019490697520910543,
      "loss": 0.389,
      "step": 5901
    },
    {
      "epoch": 0.027761837116757764,
      "grad_norm": 3.319660186767578,
      "learning_rate": 0.00019490603223099194,
      "loss": 0.453,
      "step": 5902
    },
    {
      "epoch": 0.027766540918370226,
      "grad_norm": 4.1217570304870605,
      "learning_rate": 0.00019490508925287844,
      "loss": 0.7159,
      "step": 5903
    },
    {
      "epoch": 0.02777124471998269,
      "grad_norm": 1.813554048538208,
      "learning_rate": 0.00019490414627476495,
      "loss": 0.2395,
      "step": 5904
    },
    {
      "epoch": 0.027775948521595154,
      "grad_norm": 1.3284077644348145,
      "learning_rate": 0.0001949032032966515,
      "loss": 0.2103,
      "step": 5905
    },
    {
      "epoch": 0.027780652323207616,
      "grad_norm": 2.0211822986602783,
      "learning_rate": 0.00019490226031853802,
      "loss": 0.4154,
      "step": 5906
    },
    {
      "epoch": 0.02778535612482008,
      "grad_norm": 1.8111541271209717,
      "learning_rate": 0.00019490131734042454,
      "loss": 0.4111,
      "step": 5907
    },
    {
      "epoch": 0.027790059926432544,
      "grad_norm": 1.5999174118041992,
      "learning_rate": 0.00019490037436231106,
      "loss": 0.2143,
      "step": 5908
    },
    {
      "epoch": 0.027794763728045006,
      "grad_norm": 1.1080740690231323,
      "learning_rate": 0.00019489943138419757,
      "loss": 0.1219,
      "step": 5909
    },
    {
      "epoch": 0.02779946752965747,
      "grad_norm": 1.4189397096633911,
      "learning_rate": 0.00019489848840608412,
      "loss": 0.1301,
      "step": 5910
    },
    {
      "epoch": 0.02780417133126993,
      "grad_norm": 1.6454272270202637,
      "learning_rate": 0.00019489754542797064,
      "loss": 0.1834,
      "step": 5911
    },
    {
      "epoch": 0.027808875132882396,
      "grad_norm": 1.2085286378860474,
      "learning_rate": 0.00019489660244985716,
      "loss": 0.1484,
      "step": 5912
    },
    {
      "epoch": 0.02781357893449486,
      "grad_norm": 2.8448102474212646,
      "learning_rate": 0.00019489565947174368,
      "loss": 0.3334,
      "step": 5913
    },
    {
      "epoch": 0.02781828273610732,
      "grad_norm": 3.053262233734131,
      "learning_rate": 0.0001948947164936302,
      "loss": 0.5234,
      "step": 5914
    },
    {
      "epoch": 0.027822986537719786,
      "grad_norm": 3.363602876663208,
      "learning_rate": 0.0001948937735155167,
      "loss": 0.5348,
      "step": 5915
    },
    {
      "epoch": 0.02782769033933225,
      "grad_norm": 1.936855673789978,
      "learning_rate": 0.00019489283053740323,
      "loss": 0.2467,
      "step": 5916
    },
    {
      "epoch": 0.02783239414094471,
      "grad_norm": 2.566378116607666,
      "learning_rate": 0.00019489188755928975,
      "loss": 0.5307,
      "step": 5917
    },
    {
      "epoch": 0.027837097942557176,
      "grad_norm": 3.420675039291382,
      "learning_rate": 0.00019489094458117627,
      "loss": 0.6032,
      "step": 5918
    },
    {
      "epoch": 0.02784180174416964,
      "grad_norm": 2.3516719341278076,
      "learning_rate": 0.00019489000160306282,
      "loss": 0.1145,
      "step": 5919
    },
    {
      "epoch": 0.0278465055457821,
      "grad_norm": 1.618764042854309,
      "learning_rate": 0.00019488905862494933,
      "loss": 0.2563,
      "step": 5920
    },
    {
      "epoch": 0.027851209347394563,
      "grad_norm": 0.7041650414466858,
      "learning_rate": 0.00019488811564683585,
      "loss": 0.1072,
      "step": 5921
    },
    {
      "epoch": 0.02785591314900703,
      "grad_norm": 1.1727104187011719,
      "learning_rate": 0.00019488717266872237,
      "loss": 0.113,
      "step": 5922
    },
    {
      "epoch": 0.02786061695061949,
      "grad_norm": 0.7289056181907654,
      "learning_rate": 0.0001948862296906089,
      "loss": 0.064,
      "step": 5923
    },
    {
      "epoch": 0.027865320752231953,
      "grad_norm": 2.6839218139648438,
      "learning_rate": 0.0001948852867124954,
      "loss": 0.4825,
      "step": 5924
    },
    {
      "epoch": 0.02787002455384442,
      "grad_norm": 1.8868560791015625,
      "learning_rate": 0.00019488434373438193,
      "loss": 0.3997,
      "step": 5925
    },
    {
      "epoch": 0.02787472835545688,
      "grad_norm": 1.8052477836608887,
      "learning_rate": 0.00019488340075626845,
      "loss": 0.3112,
      "step": 5926
    },
    {
      "epoch": 0.027879432157069343,
      "grad_norm": 1.228520393371582,
      "learning_rate": 0.00019488245777815496,
      "loss": 0.2918,
      "step": 5927
    },
    {
      "epoch": 0.027884135958681805,
      "grad_norm": 2.00644850730896,
      "learning_rate": 0.0001948815148000415,
      "loss": 0.2065,
      "step": 5928
    },
    {
      "epoch": 0.02788883976029427,
      "grad_norm": 1.3414220809936523,
      "learning_rate": 0.00019488057182192803,
      "loss": 0.1617,
      "step": 5929
    },
    {
      "epoch": 0.027893543561906733,
      "grad_norm": 2.5439510345458984,
      "learning_rate": 0.00019487962884381455,
      "loss": 0.7377,
      "step": 5930
    },
    {
      "epoch": 0.027898247363519195,
      "grad_norm": 1.472546935081482,
      "learning_rate": 0.00019487868586570107,
      "loss": 0.3592,
      "step": 5931
    },
    {
      "epoch": 0.02790295116513166,
      "grad_norm": 1.5462836027145386,
      "learning_rate": 0.00019487774288758758,
      "loss": 0.1989,
      "step": 5932
    },
    {
      "epoch": 0.027907654966744123,
      "grad_norm": 1.0364538431167603,
      "learning_rate": 0.00019487679990947413,
      "loss": 0.1665,
      "step": 5933
    },
    {
      "epoch": 0.027912358768356585,
      "grad_norm": 3.3663766384124756,
      "learning_rate": 0.00019487585693136062,
      "loss": 0.6302,
      "step": 5934
    },
    {
      "epoch": 0.02791706256996905,
      "grad_norm": 1.0748608112335205,
      "learning_rate": 0.00019487491395324714,
      "loss": 0.2191,
      "step": 5935
    },
    {
      "epoch": 0.027921766371581513,
      "grad_norm": 3.5373053550720215,
      "learning_rate": 0.00019487397097513366,
      "loss": 0.36,
      "step": 5936
    },
    {
      "epoch": 0.027926470173193975,
      "grad_norm": 0.9493566751480103,
      "learning_rate": 0.0001948730279970202,
      "loss": 0.2498,
      "step": 5937
    },
    {
      "epoch": 0.027931173974806437,
      "grad_norm": 1.3623930215835571,
      "learning_rate": 0.00019487208501890672,
      "loss": 0.2735,
      "step": 5938
    },
    {
      "epoch": 0.027935877776418903,
      "grad_norm": 1.2041016817092896,
      "learning_rate": 0.00019487114204079324,
      "loss": 0.3044,
      "step": 5939
    },
    {
      "epoch": 0.027940581578031365,
      "grad_norm": 2.7077813148498535,
      "learning_rate": 0.00019487019906267976,
      "loss": 0.5796,
      "step": 5940
    },
    {
      "epoch": 0.027945285379643827,
      "grad_norm": 1.2555524110794067,
      "learning_rate": 0.00019486925608456628,
      "loss": 0.1347,
      "step": 5941
    },
    {
      "epoch": 0.027949989181256293,
      "grad_norm": 0.653210461139679,
      "learning_rate": 0.00019486831310645283,
      "loss": 0.0779,
      "step": 5942
    },
    {
      "epoch": 0.027954692982868755,
      "grad_norm": 1.9350594282150269,
      "learning_rate": 0.00019486737012833934,
      "loss": 0.3391,
      "step": 5943
    },
    {
      "epoch": 0.027959396784481217,
      "grad_norm": 2.242975950241089,
      "learning_rate": 0.00019486642715022586,
      "loss": 0.3433,
      "step": 5944
    },
    {
      "epoch": 0.02796410058609368,
      "grad_norm": 2.764050245285034,
      "learning_rate": 0.00019486548417211235,
      "loss": 0.6977,
      "step": 5945
    },
    {
      "epoch": 0.027968804387706145,
      "grad_norm": 1.078410029411316,
      "learning_rate": 0.0001948645411939989,
      "loss": 0.281,
      "step": 5946
    },
    {
      "epoch": 0.027973508189318607,
      "grad_norm": 0.5030441880226135,
      "learning_rate": 0.00019486359821588542,
      "loss": 0.0473,
      "step": 5947
    },
    {
      "epoch": 0.02797821199093107,
      "grad_norm": 0.8212815523147583,
      "learning_rate": 0.00019486265523777194,
      "loss": 0.3105,
      "step": 5948
    },
    {
      "epoch": 0.027982915792543535,
      "grad_norm": 0.5853750705718994,
      "learning_rate": 0.00019486171225965846,
      "loss": 0.0557,
      "step": 5949
    },
    {
      "epoch": 0.027987619594155997,
      "grad_norm": 1.1298556327819824,
      "learning_rate": 0.00019486076928154497,
      "loss": 0.353,
      "step": 5950
    },
    {
      "epoch": 0.02799232339576846,
      "grad_norm": 1.1780710220336914,
      "learning_rate": 0.00019485982630343152,
      "loss": 0.0434,
      "step": 5951
    },
    {
      "epoch": 0.027997027197380925,
      "grad_norm": 1.9738281965255737,
      "learning_rate": 0.00019485888332531804,
      "loss": 0.2743,
      "step": 5952
    },
    {
      "epoch": 0.028001730998993387,
      "grad_norm": 1.1790664196014404,
      "learning_rate": 0.00019485794034720456,
      "loss": 0.1108,
      "step": 5953
    },
    {
      "epoch": 0.02800643480060585,
      "grad_norm": 1.3075462579727173,
      "learning_rate": 0.00019485699736909108,
      "loss": 0.1852,
      "step": 5954
    },
    {
      "epoch": 0.02801113860221831,
      "grad_norm": 3.312615156173706,
      "learning_rate": 0.0001948560543909776,
      "loss": 0.2898,
      "step": 5955
    },
    {
      "epoch": 0.028015842403830777,
      "grad_norm": 0.7458321452140808,
      "learning_rate": 0.0001948551114128641,
      "loss": 0.0833,
      "step": 5956
    },
    {
      "epoch": 0.02802054620544324,
      "grad_norm": 1.5730055570602417,
      "learning_rate": 0.00019485416843475063,
      "loss": 0.1634,
      "step": 5957
    },
    {
      "epoch": 0.0280252500070557,
      "grad_norm": 2.0931828022003174,
      "learning_rate": 0.00019485322545663715,
      "loss": 0.3351,
      "step": 5958
    },
    {
      "epoch": 0.028029953808668167,
      "grad_norm": 1.9086593389511108,
      "learning_rate": 0.00019485228247852367,
      "loss": 0.2832,
      "step": 5959
    },
    {
      "epoch": 0.02803465761028063,
      "grad_norm": 0.7402117848396301,
      "learning_rate": 0.00019485133950041022,
      "loss": 0.1337,
      "step": 5960
    },
    {
      "epoch": 0.02803936141189309,
      "grad_norm": 2.777024745941162,
      "learning_rate": 0.00019485039652229673,
      "loss": 0.6042,
      "step": 5961
    },
    {
      "epoch": 0.028044065213505553,
      "grad_norm": 1.892487645149231,
      "learning_rate": 0.00019484945354418325,
      "loss": 0.2459,
      "step": 5962
    },
    {
      "epoch": 0.02804876901511802,
      "grad_norm": 2.3205671310424805,
      "learning_rate": 0.00019484851056606977,
      "loss": 0.662,
      "step": 5963
    },
    {
      "epoch": 0.02805347281673048,
      "grad_norm": 6.833338737487793,
      "learning_rate": 0.00019484756758795632,
      "loss": 0.3689,
      "step": 5964
    },
    {
      "epoch": 0.028058176618342943,
      "grad_norm": 1.3412593603134155,
      "learning_rate": 0.0001948466246098428,
      "loss": 0.2593,
      "step": 5965
    },
    {
      "epoch": 0.02806288041995541,
      "grad_norm": 0.29727235436439514,
      "learning_rate": 0.00019484568163172933,
      "loss": 0.0296,
      "step": 5966
    },
    {
      "epoch": 0.02806758422156787,
      "grad_norm": 1.2280250787734985,
      "learning_rate": 0.00019484473865361585,
      "loss": 0.3278,
      "step": 5967
    },
    {
      "epoch": 0.028072288023180333,
      "grad_norm": 0.7981410622596741,
      "learning_rate": 0.00019484379567550236,
      "loss": 0.096,
      "step": 5968
    },
    {
      "epoch": 0.0280769918247928,
      "grad_norm": 4.479887962341309,
      "learning_rate": 0.0001948428526973889,
      "loss": 0.5912,
      "step": 5969
    },
    {
      "epoch": 0.02808169562640526,
      "grad_norm": 3.311521053314209,
      "learning_rate": 0.00019484190971927543,
      "loss": 0.5811,
      "step": 5970
    },
    {
      "epoch": 0.028086399428017723,
      "grad_norm": 1.3763911724090576,
      "learning_rate": 0.00019484096674116195,
      "loss": 0.1582,
      "step": 5971
    },
    {
      "epoch": 0.028091103229630186,
      "grad_norm": 1.4876341819763184,
      "learning_rate": 0.00019484002376304847,
      "loss": 0.1713,
      "step": 5972
    },
    {
      "epoch": 0.02809580703124265,
      "grad_norm": 3.712716579437256,
      "learning_rate": 0.000194839080784935,
      "loss": 0.4021,
      "step": 5973
    },
    {
      "epoch": 0.028100510832855113,
      "grad_norm": 3.8672072887420654,
      "learning_rate": 0.00019483813780682153,
      "loss": 0.8671,
      "step": 5974
    },
    {
      "epoch": 0.028105214634467576,
      "grad_norm": 0.8324251770973206,
      "learning_rate": 0.00019483719482870805,
      "loss": 0.0692,
      "step": 5975
    },
    {
      "epoch": 0.02810991843608004,
      "grad_norm": 1.802289605140686,
      "learning_rate": 0.00019483625185059454,
      "loss": 0.2523,
      "step": 5976
    },
    {
      "epoch": 0.028114622237692503,
      "grad_norm": 1.0216031074523926,
      "learning_rate": 0.00019483530887248106,
      "loss": 0.0834,
      "step": 5977
    },
    {
      "epoch": 0.028119326039304966,
      "grad_norm": 0.6666422486305237,
      "learning_rate": 0.0001948343658943676,
      "loss": 0.0693,
      "step": 5978
    },
    {
      "epoch": 0.028124029840917428,
      "grad_norm": 1.9528359174728394,
      "learning_rate": 0.00019483342291625412,
      "loss": 0.2436,
      "step": 5979
    },
    {
      "epoch": 0.028128733642529893,
      "grad_norm": 3.683995008468628,
      "learning_rate": 0.00019483247993814064,
      "loss": 0.5811,
      "step": 5980
    },
    {
      "epoch": 0.028133437444142356,
      "grad_norm": 0.26064610481262207,
      "learning_rate": 0.00019483153696002716,
      "loss": 0.0217,
      "step": 5981
    },
    {
      "epoch": 0.028138141245754818,
      "grad_norm": 2.767205238342285,
      "learning_rate": 0.00019483059398191368,
      "loss": 0.541,
      "step": 5982
    },
    {
      "epoch": 0.028142845047367283,
      "grad_norm": 1.3003658056259155,
      "learning_rate": 0.00019482965100380023,
      "loss": 0.127,
      "step": 5983
    },
    {
      "epoch": 0.028147548848979746,
      "grad_norm": 0.7467012405395508,
      "learning_rate": 0.00019482870802568674,
      "loss": 0.1311,
      "step": 5984
    },
    {
      "epoch": 0.028152252650592208,
      "grad_norm": 1.3436810970306396,
      "learning_rate": 0.00019482776504757326,
      "loss": 0.1329,
      "step": 5985
    },
    {
      "epoch": 0.028156956452204673,
      "grad_norm": 2.900592088699341,
      "learning_rate": 0.00019482682206945978,
      "loss": 0.5349,
      "step": 5986
    },
    {
      "epoch": 0.028161660253817136,
      "grad_norm": 1.2067716121673584,
      "learning_rate": 0.0001948258790913463,
      "loss": 0.1971,
      "step": 5987
    },
    {
      "epoch": 0.028166364055429598,
      "grad_norm": 2.7887895107269287,
      "learning_rate": 0.00019482493611323282,
      "loss": 0.4218,
      "step": 5988
    },
    {
      "epoch": 0.02817106785704206,
      "grad_norm": 2.283890724182129,
      "learning_rate": 0.00019482399313511934,
      "loss": 0.4284,
      "step": 5989
    },
    {
      "epoch": 0.028175771658654526,
      "grad_norm": 4.022670745849609,
      "learning_rate": 0.00019482305015700586,
      "loss": 0.4871,
      "step": 5990
    },
    {
      "epoch": 0.028180475460266988,
      "grad_norm": 3.3103833198547363,
      "learning_rate": 0.00019482210717889237,
      "loss": 0.4745,
      "step": 5991
    },
    {
      "epoch": 0.02818517926187945,
      "grad_norm": 1.0401192903518677,
      "learning_rate": 0.00019482116420077892,
      "loss": 0.1902,
      "step": 5992
    },
    {
      "epoch": 0.028189883063491915,
      "grad_norm": 0.9883379340171814,
      "learning_rate": 0.00019482022122266544,
      "loss": 0.1354,
      "step": 5993
    },
    {
      "epoch": 0.028194586865104378,
      "grad_norm": 0.5236325263977051,
      "learning_rate": 0.00019481927824455196,
      "loss": 0.0556,
      "step": 5994
    },
    {
      "epoch": 0.02819929066671684,
      "grad_norm": 3.23258376121521,
      "learning_rate": 0.00019481833526643848,
      "loss": 0.4991,
      "step": 5995
    },
    {
      "epoch": 0.028203994468329302,
      "grad_norm": 0.6793926954269409,
      "learning_rate": 0.000194817392288325,
      "loss": 0.076,
      "step": 5996
    },
    {
      "epoch": 0.028208698269941768,
      "grad_norm": 1.4737743139266968,
      "learning_rate": 0.0001948164493102115,
      "loss": 0.252,
      "step": 5997
    },
    {
      "epoch": 0.02821340207155423,
      "grad_norm": 3.3798108100891113,
      "learning_rate": 0.00019481550633209803,
      "loss": 0.1893,
      "step": 5998
    },
    {
      "epoch": 0.028218105873166692,
      "grad_norm": 3.3563294410705566,
      "learning_rate": 0.00019481456335398455,
      "loss": 0.4889,
      "step": 5999
    },
    {
      "epoch": 0.028222809674779158,
      "grad_norm": 2.3892152309417725,
      "learning_rate": 0.00019481362037587107,
      "loss": 0.5342,
      "step": 6000
    },
    {
      "epoch": 0.02822751347639162,
      "grad_norm": 2.790825843811035,
      "learning_rate": 0.00019481267739775762,
      "loss": 0.2133,
      "step": 6001
    },
    {
      "epoch": 0.028232217278004082,
      "grad_norm": 3.660457134246826,
      "learning_rate": 0.00019481173441964413,
      "loss": 0.3823,
      "step": 6002
    },
    {
      "epoch": 0.028236921079616548,
      "grad_norm": 1.977088212966919,
      "learning_rate": 0.00019481079144153065,
      "loss": 0.2128,
      "step": 6003
    },
    {
      "epoch": 0.02824162488122901,
      "grad_norm": 4.91064453125,
      "learning_rate": 0.00019480984846341717,
      "loss": 0.1998,
      "step": 6004
    },
    {
      "epoch": 0.028246328682841472,
      "grad_norm": 1.1299246549606323,
      "learning_rate": 0.00019480890548530372,
      "loss": 0.2134,
      "step": 6005
    },
    {
      "epoch": 0.028251032484453934,
      "grad_norm": 0.9338328838348389,
      "learning_rate": 0.00019480796250719024,
      "loss": 0.0819,
      "step": 6006
    },
    {
      "epoch": 0.0282557362860664,
      "grad_norm": 2.3274223804473877,
      "learning_rate": 0.00019480701952907673,
      "loss": 0.2695,
      "step": 6007
    },
    {
      "epoch": 0.028260440087678862,
      "grad_norm": 0.842941403388977,
      "learning_rate": 0.00019480607655096325,
      "loss": 0.0881,
      "step": 6008
    },
    {
      "epoch": 0.028265143889291324,
      "grad_norm": 1.1306647062301636,
      "learning_rate": 0.00019480513357284976,
      "loss": 0.1556,
      "step": 6009
    },
    {
      "epoch": 0.02826984769090379,
      "grad_norm": 0.6780356168746948,
      "learning_rate": 0.0001948041905947363,
      "loss": 0.0852,
      "step": 6010
    },
    {
      "epoch": 0.028274551492516252,
      "grad_norm": 0.6222822070121765,
      "learning_rate": 0.00019480324761662283,
      "loss": 0.039,
      "step": 6011
    },
    {
      "epoch": 0.028279255294128714,
      "grad_norm": 1.1148223876953125,
      "learning_rate": 0.00019480230463850935,
      "loss": 0.1839,
      "step": 6012
    },
    {
      "epoch": 0.02828395909574118,
      "grad_norm": 1.8245625495910645,
      "learning_rate": 0.00019480136166039587,
      "loss": 0.3521,
      "step": 6013
    },
    {
      "epoch": 0.028288662897353642,
      "grad_norm": 1.77046537399292,
      "learning_rate": 0.0001948004186822824,
      "loss": 0.1863,
      "step": 6014
    },
    {
      "epoch": 0.028293366698966104,
      "grad_norm": 4.2113871574401855,
      "learning_rate": 0.00019479947570416893,
      "loss": 0.7418,
      "step": 6015
    },
    {
      "epoch": 0.028298070500578566,
      "grad_norm": 2.454029083251953,
      "learning_rate": 0.00019479853272605545,
      "loss": 0.3409,
      "step": 6016
    },
    {
      "epoch": 0.028302774302191032,
      "grad_norm": 0.37853971123695374,
      "learning_rate": 0.00019479758974794197,
      "loss": 0.041,
      "step": 6017
    },
    {
      "epoch": 0.028307478103803494,
      "grad_norm": 3.9251744747161865,
      "learning_rate": 0.00019479664676982846,
      "loss": 0.6558,
      "step": 6018
    },
    {
      "epoch": 0.028312181905415956,
      "grad_norm": 0.7707700133323669,
      "learning_rate": 0.000194795703791715,
      "loss": 0.1311,
      "step": 6019
    },
    {
      "epoch": 0.028316885707028422,
      "grad_norm": 1.9055007696151733,
      "learning_rate": 0.00019479476081360152,
      "loss": 0.2737,
      "step": 6020
    },
    {
      "epoch": 0.028321589508640884,
      "grad_norm": 4.274170398712158,
      "learning_rate": 0.00019479381783548804,
      "loss": 0.7963,
      "step": 6021
    },
    {
      "epoch": 0.028326293310253346,
      "grad_norm": 0.9053284525871277,
      "learning_rate": 0.00019479287485737456,
      "loss": 0.1066,
      "step": 6022
    },
    {
      "epoch": 0.02833099711186581,
      "grad_norm": 3.070826768875122,
      "learning_rate": 0.0001947919318792611,
      "loss": 0.4019,
      "step": 6023
    },
    {
      "epoch": 0.028335700913478274,
      "grad_norm": 2.8126351833343506,
      "learning_rate": 0.00019479098890114763,
      "loss": 0.445,
      "step": 6024
    },
    {
      "epoch": 0.028340404715090736,
      "grad_norm": 2.9082889556884766,
      "learning_rate": 0.00019479004592303414,
      "loss": 0.3287,
      "step": 6025
    },
    {
      "epoch": 0.0283451085167032,
      "grad_norm": 0.35828229784965515,
      "learning_rate": 0.00019478910294492066,
      "loss": 0.0245,
      "step": 6026
    },
    {
      "epoch": 0.028349812318315664,
      "grad_norm": 2.1204445362091064,
      "learning_rate": 0.00019478815996680718,
      "loss": 0.4095,
      "step": 6027
    },
    {
      "epoch": 0.028354516119928126,
      "grad_norm": 1.0004478693008423,
      "learning_rate": 0.0001947872169886937,
      "loss": 0.0826,
      "step": 6028
    },
    {
      "epoch": 0.02835921992154059,
      "grad_norm": 1.203662633895874,
      "learning_rate": 0.00019478627401058022,
      "loss": 0.2983,
      "step": 6029
    },
    {
      "epoch": 0.028363923723153054,
      "grad_norm": 2.3608694076538086,
      "learning_rate": 0.00019478533103246674,
      "loss": 0.52,
      "step": 6030
    },
    {
      "epoch": 0.028368627524765516,
      "grad_norm": 3.6935269832611084,
      "learning_rate": 0.00019478438805435326,
      "loss": 0.4551,
      "step": 6031
    },
    {
      "epoch": 0.02837333132637798,
      "grad_norm": 2.5116469860076904,
      "learning_rate": 0.00019478344507623977,
      "loss": 0.479,
      "step": 6032
    },
    {
      "epoch": 0.02837803512799044,
      "grad_norm": 1.7275668382644653,
      "learning_rate": 0.00019478250209812632,
      "loss": 0.1259,
      "step": 6033
    },
    {
      "epoch": 0.028382738929602906,
      "grad_norm": 1.2279714345932007,
      "learning_rate": 0.00019478155912001284,
      "loss": 0.2267,
      "step": 6034
    },
    {
      "epoch": 0.02838744273121537,
      "grad_norm": 0.9148026704788208,
      "learning_rate": 0.00019478061614189936,
      "loss": 0.1145,
      "step": 6035
    },
    {
      "epoch": 0.02839214653282783,
      "grad_norm": 4.265451908111572,
      "learning_rate": 0.00019477967316378588,
      "loss": 1.021,
      "step": 6036
    },
    {
      "epoch": 0.028396850334440296,
      "grad_norm": 0.5266849398612976,
      "learning_rate": 0.00019477873018567242,
      "loss": 0.0459,
      "step": 6037
    },
    {
      "epoch": 0.02840155413605276,
      "grad_norm": 0.35785722732543945,
      "learning_rate": 0.0001947777872075589,
      "loss": 0.0315,
      "step": 6038
    },
    {
      "epoch": 0.02840625793766522,
      "grad_norm": 2.5204436779022217,
      "learning_rate": 0.00019477684422944543,
      "loss": 0.2944,
      "step": 6039
    },
    {
      "epoch": 0.028410961739277683,
      "grad_norm": 1.1634985208511353,
      "learning_rate": 0.00019477590125133195,
      "loss": 0.1602,
      "step": 6040
    },
    {
      "epoch": 0.02841566554089015,
      "grad_norm": 3.0381112098693848,
      "learning_rate": 0.00019477495827321847,
      "loss": 0.2668,
      "step": 6041
    },
    {
      "epoch": 0.02842036934250261,
      "grad_norm": 0.6593977808952332,
      "learning_rate": 0.00019477401529510502,
      "loss": 0.1,
      "step": 6042
    },
    {
      "epoch": 0.028425073144115073,
      "grad_norm": 4.3284831047058105,
      "learning_rate": 0.00019477307231699153,
      "loss": 0.434,
      "step": 6043
    },
    {
      "epoch": 0.02842977694572754,
      "grad_norm": 0.8605762720108032,
      "learning_rate": 0.00019477212933887805,
      "loss": 0.1402,
      "step": 6044
    },
    {
      "epoch": 0.02843448074734,
      "grad_norm": 3.0300705432891846,
      "learning_rate": 0.00019477118636076457,
      "loss": 0.7021,
      "step": 6045
    },
    {
      "epoch": 0.028439184548952463,
      "grad_norm": 3.997136354446411,
      "learning_rate": 0.00019477024338265112,
      "loss": 0.5548,
      "step": 6046
    },
    {
      "epoch": 0.02844388835056493,
      "grad_norm": 1.7148329019546509,
      "learning_rate": 0.00019476930040453764,
      "loss": 0.1723,
      "step": 6047
    },
    {
      "epoch": 0.02844859215217739,
      "grad_norm": 2.021045684814453,
      "learning_rate": 0.00019476835742642415,
      "loss": 0.2849,
      "step": 6048
    },
    {
      "epoch": 0.028453295953789853,
      "grad_norm": 1.0494794845581055,
      "learning_rate": 0.00019476741444831065,
      "loss": 0.1541,
      "step": 6049
    },
    {
      "epoch": 0.028457999755402315,
      "grad_norm": 3.372706651687622,
      "learning_rate": 0.00019476647147019716,
      "loss": 0.7103,
      "step": 6050
    },
    {
      "epoch": 0.02846270355701478,
      "grad_norm": 3.9497945308685303,
      "learning_rate": 0.0001947655284920837,
      "loss": 0.3489,
      "step": 6051
    },
    {
      "epoch": 0.028467407358627243,
      "grad_norm": 1.561583399772644,
      "learning_rate": 0.00019476458551397023,
      "loss": 0.1844,
      "step": 6052
    },
    {
      "epoch": 0.028472111160239705,
      "grad_norm": 0.6790079474449158,
      "learning_rate": 0.00019476364253585675,
      "loss": 0.0985,
      "step": 6053
    },
    {
      "epoch": 0.02847681496185217,
      "grad_norm": 2.7106306552886963,
      "learning_rate": 0.00019476269955774327,
      "loss": 0.2315,
      "step": 6054
    },
    {
      "epoch": 0.028481518763464633,
      "grad_norm": 6.064810752868652,
      "learning_rate": 0.0001947617565796298,
      "loss": 0.8601,
      "step": 6055
    },
    {
      "epoch": 0.028486222565077095,
      "grad_norm": 3.379192590713501,
      "learning_rate": 0.00019476081360151633,
      "loss": 0.3712,
      "step": 6056
    },
    {
      "epoch": 0.028490926366689557,
      "grad_norm": 2.425372362136841,
      "learning_rate": 0.00019475987062340285,
      "loss": 0.2517,
      "step": 6057
    },
    {
      "epoch": 0.028495630168302023,
      "grad_norm": 0.8102268576622009,
      "learning_rate": 0.00019475892764528937,
      "loss": 0.0335,
      "step": 6058
    },
    {
      "epoch": 0.028500333969914485,
      "grad_norm": 1.3250030279159546,
      "learning_rate": 0.00019475798466717589,
      "loss": 0.2804,
      "step": 6059
    },
    {
      "epoch": 0.028505037771526947,
      "grad_norm": 2.9571609497070312,
      "learning_rate": 0.0001947570416890624,
      "loss": 0.3447,
      "step": 6060
    },
    {
      "epoch": 0.028509741573139413,
      "grad_norm": 0.5850396752357483,
      "learning_rate": 0.00019475609871094892,
      "loss": 0.0663,
      "step": 6061
    },
    {
      "epoch": 0.028514445374751875,
      "grad_norm": 2.133483409881592,
      "learning_rate": 0.00019475515573283544,
      "loss": 0.0466,
      "step": 6062
    },
    {
      "epoch": 0.028519149176364337,
      "grad_norm": 12.758872032165527,
      "learning_rate": 0.00019475421275472196,
      "loss": 0.2502,
      "step": 6063
    },
    {
      "epoch": 0.028523852977976803,
      "grad_norm": 2.0496950149536133,
      "learning_rate": 0.0001947532697766085,
      "loss": 0.3119,
      "step": 6064
    },
    {
      "epoch": 0.028528556779589265,
      "grad_norm": 0.29091012477874756,
      "learning_rate": 0.00019475232679849503,
      "loss": 0.0378,
      "step": 6065
    },
    {
      "epoch": 0.028533260581201727,
      "grad_norm": 2.606841564178467,
      "learning_rate": 0.00019475138382038154,
      "loss": 0.2915,
      "step": 6066
    },
    {
      "epoch": 0.02853796438281419,
      "grad_norm": 3.8665099143981934,
      "learning_rate": 0.00019475044084226806,
      "loss": 0.4265,
      "step": 6067
    },
    {
      "epoch": 0.028542668184426655,
      "grad_norm": 3.9750277996063232,
      "learning_rate": 0.00019474949786415458,
      "loss": 0.7306,
      "step": 6068
    },
    {
      "epoch": 0.028547371986039117,
      "grad_norm": 0.4435098469257355,
      "learning_rate": 0.0001947485548860411,
      "loss": 0.0608,
      "step": 6069
    },
    {
      "epoch": 0.02855207578765158,
      "grad_norm": 1.8157851696014404,
      "learning_rate": 0.00019474761190792762,
      "loss": 0.1901,
      "step": 6070
    },
    {
      "epoch": 0.028556779589264045,
      "grad_norm": 0.918290913105011,
      "learning_rate": 0.00019474666892981414,
      "loss": 0.0803,
      "step": 6071
    },
    {
      "epoch": 0.028561483390876507,
      "grad_norm": 3.1090707778930664,
      "learning_rate": 0.00019474572595170066,
      "loss": 0.3438,
      "step": 6072
    },
    {
      "epoch": 0.02856618719248897,
      "grad_norm": 1.4979798793792725,
      "learning_rate": 0.0001947447829735872,
      "loss": 0.1364,
      "step": 6073
    },
    {
      "epoch": 0.02857089099410143,
      "grad_norm": 1.9817193746566772,
      "learning_rate": 0.00019474383999547372,
      "loss": 0.2148,
      "step": 6074
    },
    {
      "epoch": 0.028575594795713897,
      "grad_norm": 1.956834077835083,
      "learning_rate": 0.00019474289701736024,
      "loss": 0.1586,
      "step": 6075
    },
    {
      "epoch": 0.02858029859732636,
      "grad_norm": 0.6837072968482971,
      "learning_rate": 0.00019474195403924676,
      "loss": 0.0692,
      "step": 6076
    },
    {
      "epoch": 0.02858500239893882,
      "grad_norm": 0.5700764656066895,
      "learning_rate": 0.00019474101106113328,
      "loss": 0.0854,
      "step": 6077
    },
    {
      "epoch": 0.028589706200551287,
      "grad_norm": 3.1877286434173584,
      "learning_rate": 0.00019474006808301982,
      "loss": 0.4008,
      "step": 6078
    },
    {
      "epoch": 0.02859441000216375,
      "grad_norm": 1.5944679975509644,
      "learning_rate": 0.00019473912510490634,
      "loss": 0.1564,
      "step": 6079
    },
    {
      "epoch": 0.02859911380377621,
      "grad_norm": 3.5390725135803223,
      "learning_rate": 0.00019473818212679283,
      "loss": 0.5978,
      "step": 6080
    },
    {
      "epoch": 0.028603817605388677,
      "grad_norm": 0.7211732268333435,
      "learning_rate": 0.00019473723914867935,
      "loss": 0.0564,
      "step": 6081
    },
    {
      "epoch": 0.02860852140700114,
      "grad_norm": 0.24976590275764465,
      "learning_rate": 0.00019473629617056587,
      "loss": 0.0186,
      "step": 6082
    },
    {
      "epoch": 0.0286132252086136,
      "grad_norm": 1.1704272031784058,
      "learning_rate": 0.00019473535319245242,
      "loss": 0.1132,
      "step": 6083
    },
    {
      "epoch": 0.028617929010226063,
      "grad_norm": 0.47564899921417236,
      "learning_rate": 0.00019473441021433893,
      "loss": 0.0535,
      "step": 6084
    },
    {
      "epoch": 0.02862263281183853,
      "grad_norm": 0.23717574775218964,
      "learning_rate": 0.00019473346723622545,
      "loss": 0.0156,
      "step": 6085
    },
    {
      "epoch": 0.02862733661345099,
      "grad_norm": 2.592223644256592,
      "learning_rate": 0.00019473252425811197,
      "loss": 0.2162,
      "step": 6086
    },
    {
      "epoch": 0.028632040415063453,
      "grad_norm": 5.154963493347168,
      "learning_rate": 0.00019473158127999852,
      "loss": 1.079,
      "step": 6087
    },
    {
      "epoch": 0.02863674421667592,
      "grad_norm": 0.8648050427436829,
      "learning_rate": 0.00019473063830188504,
      "loss": 0.0602,
      "step": 6088
    },
    {
      "epoch": 0.02864144801828838,
      "grad_norm": 5.414679527282715,
      "learning_rate": 0.00019472969532377155,
      "loss": 1.8048,
      "step": 6089
    },
    {
      "epoch": 0.028646151819900843,
      "grad_norm": 0.7107358574867249,
      "learning_rate": 0.00019472875234565807,
      "loss": 0.0501,
      "step": 6090
    },
    {
      "epoch": 0.028650855621513305,
      "grad_norm": 1.5483644008636475,
      "learning_rate": 0.0001947278093675446,
      "loss": 0.1737,
      "step": 6091
    },
    {
      "epoch": 0.02865555942312577,
      "grad_norm": 4.451284885406494,
      "learning_rate": 0.0001947268663894311,
      "loss": 1.0531,
      "step": 6092
    },
    {
      "epoch": 0.028660263224738233,
      "grad_norm": 0.479925274848938,
      "learning_rate": 0.00019472592341131763,
      "loss": 0.0468,
      "step": 6093
    },
    {
      "epoch": 0.028664967026350695,
      "grad_norm": 1.5801645517349243,
      "learning_rate": 0.00019472498043320415,
      "loss": 0.2587,
      "step": 6094
    },
    {
      "epoch": 0.02866967082796316,
      "grad_norm": 0.7410310506820679,
      "learning_rate": 0.00019472403745509067,
      "loss": 0.0804,
      "step": 6095
    },
    {
      "epoch": 0.028674374629575623,
      "grad_norm": 0.6954376101493835,
      "learning_rate": 0.0001947230944769772,
      "loss": 0.1478,
      "step": 6096
    },
    {
      "epoch": 0.028679078431188085,
      "grad_norm": 1.6868855953216553,
      "learning_rate": 0.00019472215149886373,
      "loss": 0.3183,
      "step": 6097
    },
    {
      "epoch": 0.02868378223280055,
      "grad_norm": 1.009814739227295,
      "learning_rate": 0.00019472120852075025,
      "loss": 0.2073,
      "step": 6098
    },
    {
      "epoch": 0.028688486034413013,
      "grad_norm": 0.5926522612571716,
      "learning_rate": 0.00019472026554263677,
      "loss": 0.0757,
      "step": 6099
    },
    {
      "epoch": 0.028693189836025475,
      "grad_norm": 1.0778828859329224,
      "learning_rate": 0.00019471932256452329,
      "loss": 0.1517,
      "step": 6100
    },
    {
      "epoch": 0.028697893637637938,
      "grad_norm": 0.32731905579566956,
      "learning_rate": 0.0001947183795864098,
      "loss": 0.022,
      "step": 6101
    },
    {
      "epoch": 0.028702597439250403,
      "grad_norm": 3.601633310317993,
      "learning_rate": 0.00019471743660829632,
      "loss": 0.5373,
      "step": 6102
    },
    {
      "epoch": 0.028707301240862865,
      "grad_norm": 0.7223300933837891,
      "learning_rate": 0.00019471649363018284,
      "loss": 0.0748,
      "step": 6103
    },
    {
      "epoch": 0.028712005042475328,
      "grad_norm": 1.917460322380066,
      "learning_rate": 0.00019471555065206936,
      "loss": 0.2373,
      "step": 6104
    },
    {
      "epoch": 0.028716708844087793,
      "grad_norm": 2.559037923812866,
      "learning_rate": 0.0001947146076739559,
      "loss": 0.4446,
      "step": 6105
    },
    {
      "epoch": 0.028721412645700255,
      "grad_norm": 2.0130693912506104,
      "learning_rate": 0.00019471366469584243,
      "loss": 0.2211,
      "step": 6106
    },
    {
      "epoch": 0.028726116447312718,
      "grad_norm": 2.8427510261535645,
      "learning_rate": 0.00019471272171772894,
      "loss": 0.5789,
      "step": 6107
    },
    {
      "epoch": 0.02873082024892518,
      "grad_norm": 2.878124713897705,
      "learning_rate": 0.00019471177873961546,
      "loss": 0.3471,
      "step": 6108
    },
    {
      "epoch": 0.028735524050537645,
      "grad_norm": 1.7062042951583862,
      "learning_rate": 0.00019471083576150198,
      "loss": 0.1883,
      "step": 6109
    },
    {
      "epoch": 0.028740227852150108,
      "grad_norm": 4.68415641784668,
      "learning_rate": 0.00019470989278338853,
      "loss": 0.3853,
      "step": 6110
    },
    {
      "epoch": 0.02874493165376257,
      "grad_norm": 3.6172776222229004,
      "learning_rate": 0.00019470894980527502,
      "loss": 0.5594,
      "step": 6111
    },
    {
      "epoch": 0.028749635455375035,
      "grad_norm": 3.761784315109253,
      "learning_rate": 0.00019470800682716154,
      "loss": 0.7256,
      "step": 6112
    },
    {
      "epoch": 0.028754339256987498,
      "grad_norm": 1.0174986124038696,
      "learning_rate": 0.00019470706384904806,
      "loss": 0.0965,
      "step": 6113
    },
    {
      "epoch": 0.02875904305859996,
      "grad_norm": 2.644023895263672,
      "learning_rate": 0.0001947061208709346,
      "loss": 0.3185,
      "step": 6114
    },
    {
      "epoch": 0.028763746860212425,
      "grad_norm": 3.7894418239593506,
      "learning_rate": 0.00019470517789282112,
      "loss": 0.5596,
      "step": 6115
    },
    {
      "epoch": 0.028768450661824888,
      "grad_norm": 2.674546480178833,
      "learning_rate": 0.00019470423491470764,
      "loss": 0.2678,
      "step": 6116
    },
    {
      "epoch": 0.02877315446343735,
      "grad_norm": 3.2515504360198975,
      "learning_rate": 0.00019470329193659416,
      "loss": 0.4744,
      "step": 6117
    },
    {
      "epoch": 0.028777858265049812,
      "grad_norm": 1.6108558177947998,
      "learning_rate": 0.00019470234895848068,
      "loss": 0.2046,
      "step": 6118
    },
    {
      "epoch": 0.028782562066662278,
      "grad_norm": 0.6835373640060425,
      "learning_rate": 0.00019470140598036722,
      "loss": 0.0742,
      "step": 6119
    },
    {
      "epoch": 0.02878726586827474,
      "grad_norm": 0.8089701533317566,
      "learning_rate": 0.00019470046300225374,
      "loss": 0.0953,
      "step": 6120
    },
    {
      "epoch": 0.028791969669887202,
      "grad_norm": 2.028984785079956,
      "learning_rate": 0.00019469952002414026,
      "loss": 0.1101,
      "step": 6121
    },
    {
      "epoch": 0.028796673471499667,
      "grad_norm": 0.4797181487083435,
      "learning_rate": 0.00019469857704602678,
      "loss": 0.0565,
      "step": 6122
    },
    {
      "epoch": 0.02880137727311213,
      "grad_norm": 1.9313770532608032,
      "learning_rate": 0.0001946976340679133,
      "loss": 0.1928,
      "step": 6123
    },
    {
      "epoch": 0.028806081074724592,
      "grad_norm": 0.8445772528648376,
      "learning_rate": 0.00019469669108979982,
      "loss": 0.1089,
      "step": 6124
    },
    {
      "epoch": 0.028810784876337054,
      "grad_norm": 2.4970579147338867,
      "learning_rate": 0.00019469574811168633,
      "loss": 0.3101,
      "step": 6125
    },
    {
      "epoch": 0.02881548867794952,
      "grad_norm": 2.4224560260772705,
      "learning_rate": 0.00019469480513357285,
      "loss": 0.4949,
      "step": 6126
    },
    {
      "epoch": 0.028820192479561982,
      "grad_norm": 3.3609790802001953,
      "learning_rate": 0.00019469386215545937,
      "loss": 0.734,
      "step": 6127
    },
    {
      "epoch": 0.028824896281174444,
      "grad_norm": 3.3853814601898193,
      "learning_rate": 0.00019469291917734592,
      "loss": 0.7012,
      "step": 6128
    },
    {
      "epoch": 0.02882960008278691,
      "grad_norm": 1.70370614528656,
      "learning_rate": 0.00019469197619923244,
      "loss": 0.4016,
      "step": 6129
    },
    {
      "epoch": 0.028834303884399372,
      "grad_norm": 1.3762023448944092,
      "learning_rate": 0.00019469103322111895,
      "loss": 0.0957,
      "step": 6130
    },
    {
      "epoch": 0.028839007686011834,
      "grad_norm": 1.9485681056976318,
      "learning_rate": 0.00019469009024300547,
      "loss": 0.1943,
      "step": 6131
    },
    {
      "epoch": 0.0288437114876243,
      "grad_norm": 1.4387418031692505,
      "learning_rate": 0.000194689147264892,
      "loss": 0.1549,
      "step": 6132
    },
    {
      "epoch": 0.028848415289236762,
      "grad_norm": 1.0008937120437622,
      "learning_rate": 0.0001946882042867785,
      "loss": 0.1618,
      "step": 6133
    },
    {
      "epoch": 0.028853119090849224,
      "grad_norm": 1.58609938621521,
      "learning_rate": 0.00019468726130866503,
      "loss": 0.1921,
      "step": 6134
    },
    {
      "epoch": 0.028857822892461686,
      "grad_norm": 4.643029689788818,
      "learning_rate": 0.00019468631833055155,
      "loss": 0.5665,
      "step": 6135
    },
    {
      "epoch": 0.028862526694074152,
      "grad_norm": 2.0687921047210693,
      "learning_rate": 0.00019468537535243807,
      "loss": 0.2255,
      "step": 6136
    },
    {
      "epoch": 0.028867230495686614,
      "grad_norm": 2.1363818645477295,
      "learning_rate": 0.0001946844323743246,
      "loss": 0.1553,
      "step": 6137
    },
    {
      "epoch": 0.028871934297299076,
      "grad_norm": 2.3567750453948975,
      "learning_rate": 0.00019468348939621113,
      "loss": 0.2069,
      "step": 6138
    },
    {
      "epoch": 0.028876638098911542,
      "grad_norm": 3.2816174030303955,
      "learning_rate": 0.00019468254641809765,
      "loss": 0.7952,
      "step": 6139
    },
    {
      "epoch": 0.028881341900524004,
      "grad_norm": 1.4110894203186035,
      "learning_rate": 0.00019468160343998417,
      "loss": 0.3538,
      "step": 6140
    },
    {
      "epoch": 0.028886045702136466,
      "grad_norm": 2.5326993465423584,
      "learning_rate": 0.00019468066046187069,
      "loss": 0.8339,
      "step": 6141
    },
    {
      "epoch": 0.02889074950374893,
      "grad_norm": 0.6965090036392212,
      "learning_rate": 0.0001946797174837572,
      "loss": 0.1214,
      "step": 6142
    },
    {
      "epoch": 0.028895453305361394,
      "grad_norm": 2.3076772689819336,
      "learning_rate": 0.00019467877450564372,
      "loss": 0.3805,
      "step": 6143
    },
    {
      "epoch": 0.028900157106973856,
      "grad_norm": 2.246230363845825,
      "learning_rate": 0.00019467783152753024,
      "loss": 0.5309,
      "step": 6144
    },
    {
      "epoch": 0.028904860908586318,
      "grad_norm": 4.053242206573486,
      "learning_rate": 0.00019467688854941676,
      "loss": 0.3474,
      "step": 6145
    },
    {
      "epoch": 0.028909564710198784,
      "grad_norm": 0.8773916363716125,
      "learning_rate": 0.0001946759455713033,
      "loss": 0.116,
      "step": 6146
    },
    {
      "epoch": 0.028914268511811246,
      "grad_norm": 0.584191620349884,
      "learning_rate": 0.00019467500259318983,
      "loss": 0.0592,
      "step": 6147
    },
    {
      "epoch": 0.028918972313423708,
      "grad_norm": 1.9763176441192627,
      "learning_rate": 0.00019467405961507634,
      "loss": 0.2251,
      "step": 6148
    },
    {
      "epoch": 0.028923676115036174,
      "grad_norm": 1.5401691198349,
      "learning_rate": 0.00019467311663696286,
      "loss": 0.1335,
      "step": 6149
    },
    {
      "epoch": 0.028928379916648636,
      "grad_norm": 2.545813798904419,
      "learning_rate": 0.00019467217365884938,
      "loss": 0.6761,
      "step": 6150
    },
    {
      "epoch": 0.028933083718261098,
      "grad_norm": 1.2807385921478271,
      "learning_rate": 0.00019467123068073593,
      "loss": 0.1427,
      "step": 6151
    },
    {
      "epoch": 0.02893778751987356,
      "grad_norm": 1.7839332818984985,
      "learning_rate": 0.00019467028770262245,
      "loss": 0.2096,
      "step": 6152
    },
    {
      "epoch": 0.028942491321486026,
      "grad_norm": 1.0071040391921997,
      "learning_rate": 0.00019466934472450896,
      "loss": 0.0744,
      "step": 6153
    },
    {
      "epoch": 0.028947195123098488,
      "grad_norm": 2.884894847869873,
      "learning_rate": 0.00019466840174639546,
      "loss": 0.4708,
      "step": 6154
    },
    {
      "epoch": 0.02895189892471095,
      "grad_norm": 2.009289503097534,
      "learning_rate": 0.000194667458768282,
      "loss": 0.3885,
      "step": 6155
    },
    {
      "epoch": 0.028956602726323416,
      "grad_norm": 2.1573054790496826,
      "learning_rate": 0.00019466651579016852,
      "loss": 0.257,
      "step": 6156
    },
    {
      "epoch": 0.028961306527935878,
      "grad_norm": 1.4162687063217163,
      "learning_rate": 0.00019466557281205504,
      "loss": 0.2076,
      "step": 6157
    },
    {
      "epoch": 0.02896601032954834,
      "grad_norm": 1.1645792722702026,
      "learning_rate": 0.00019466462983394156,
      "loss": 0.0589,
      "step": 6158
    },
    {
      "epoch": 0.028970714131160803,
      "grad_norm": 1.8196345567703247,
      "learning_rate": 0.00019466368685582808,
      "loss": 0.2944,
      "step": 6159
    },
    {
      "epoch": 0.028975417932773268,
      "grad_norm": 4.576624393463135,
      "learning_rate": 0.00019466274387771462,
      "loss": 0.3875,
      "step": 6160
    },
    {
      "epoch": 0.02898012173438573,
      "grad_norm": 1.7755624055862427,
      "learning_rate": 0.00019466180089960114,
      "loss": 0.337,
      "step": 6161
    },
    {
      "epoch": 0.028984825535998193,
      "grad_norm": 1.4117695093154907,
      "learning_rate": 0.00019466085792148766,
      "loss": 0.0985,
      "step": 6162
    },
    {
      "epoch": 0.028989529337610658,
      "grad_norm": 3.507018804550171,
      "learning_rate": 0.00019465991494337418,
      "loss": 0.6245,
      "step": 6163
    },
    {
      "epoch": 0.02899423313922312,
      "grad_norm": 0.44902652502059937,
      "learning_rate": 0.0001946589719652607,
      "loss": 0.0757,
      "step": 6164
    },
    {
      "epoch": 0.028998936940835583,
      "grad_norm": 2.0377416610717773,
      "learning_rate": 0.00019465802898714721,
      "loss": 0.2858,
      "step": 6165
    },
    {
      "epoch": 0.029003640742448048,
      "grad_norm": 2.242278575897217,
      "learning_rate": 0.00019465708600903373,
      "loss": 0.5137,
      "step": 6166
    },
    {
      "epoch": 0.02900834454406051,
      "grad_norm": 2.0100297927856445,
      "learning_rate": 0.00019465614303092025,
      "loss": 0.4027,
      "step": 6167
    },
    {
      "epoch": 0.029013048345672973,
      "grad_norm": 1.8180676698684692,
      "learning_rate": 0.00019465520005280677,
      "loss": 0.2731,
      "step": 6168
    },
    {
      "epoch": 0.029017752147285435,
      "grad_norm": 3.0988237857818604,
      "learning_rate": 0.00019465425707469332,
      "loss": 0.6694,
      "step": 6169
    },
    {
      "epoch": 0.0290224559488979,
      "grad_norm": 2.127643585205078,
      "learning_rate": 0.00019465331409657984,
      "loss": 0.4328,
      "step": 6170
    },
    {
      "epoch": 0.029027159750510362,
      "grad_norm": 0.7272937893867493,
      "learning_rate": 0.00019465237111846635,
      "loss": 0.0782,
      "step": 6171
    },
    {
      "epoch": 0.029031863552122825,
      "grad_norm": 1.4279330968856812,
      "learning_rate": 0.00019465142814035287,
      "loss": 0.222,
      "step": 6172
    },
    {
      "epoch": 0.02903656735373529,
      "grad_norm": 0.8104825019836426,
      "learning_rate": 0.0001946504851622394,
      "loss": 0.1592,
      "step": 6173
    },
    {
      "epoch": 0.029041271155347752,
      "grad_norm": 1.9974421262741089,
      "learning_rate": 0.0001946495421841259,
      "loss": 0.4208,
      "step": 6174
    },
    {
      "epoch": 0.029045974956960215,
      "grad_norm": 2.479335308074951,
      "learning_rate": 0.00019464859920601243,
      "loss": 0.3469,
      "step": 6175
    },
    {
      "epoch": 0.029050678758572677,
      "grad_norm": 8.507369041442871,
      "learning_rate": 0.00019464765622789895,
      "loss": 0.3894,
      "step": 6176
    },
    {
      "epoch": 0.029055382560185142,
      "grad_norm": 0.8795732259750366,
      "learning_rate": 0.00019464671324978547,
      "loss": 0.1069,
      "step": 6177
    },
    {
      "epoch": 0.029060086361797605,
      "grad_norm": 0.35184720158576965,
      "learning_rate": 0.000194645770271672,
      "loss": 0.0489,
      "step": 6178
    },
    {
      "epoch": 0.029064790163410067,
      "grad_norm": 1.5334699153900146,
      "learning_rate": 0.00019464482729355853,
      "loss": 0.3108,
      "step": 6179
    },
    {
      "epoch": 0.029069493965022532,
      "grad_norm": 0.9410684704780579,
      "learning_rate": 0.00019464388431544505,
      "loss": 0.1202,
      "step": 6180
    },
    {
      "epoch": 0.029074197766634995,
      "grad_norm": 0.7396723031997681,
      "learning_rate": 0.00019464294133733157,
      "loss": 0.0692,
      "step": 6181
    },
    {
      "epoch": 0.029078901568247457,
      "grad_norm": 0.8908210396766663,
      "learning_rate": 0.0001946419983592181,
      "loss": 0.1398,
      "step": 6182
    },
    {
      "epoch": 0.029083605369859922,
      "grad_norm": 2.7864603996276855,
      "learning_rate": 0.00019464105538110463,
      "loss": 0.4798,
      "step": 6183
    },
    {
      "epoch": 0.029088309171472385,
      "grad_norm": 1.012270212173462,
      "learning_rate": 0.00019464011240299115,
      "loss": 0.1084,
      "step": 6184
    },
    {
      "epoch": 0.029093012973084847,
      "grad_norm": 1.2749390602111816,
      "learning_rate": 0.00019463916942487764,
      "loss": 0.1031,
      "step": 6185
    },
    {
      "epoch": 0.02909771677469731,
      "grad_norm": 2.430088996887207,
      "learning_rate": 0.00019463822644676416,
      "loss": 0.6488,
      "step": 6186
    },
    {
      "epoch": 0.029102420576309775,
      "grad_norm": 1.213985562324524,
      "learning_rate": 0.0001946372834686507,
      "loss": 0.209,
      "step": 6187
    },
    {
      "epoch": 0.029107124377922237,
      "grad_norm": 0.3866753578186035,
      "learning_rate": 0.00019463634049053723,
      "loss": 0.0359,
      "step": 6188
    },
    {
      "epoch": 0.0291118281795347,
      "grad_norm": 1.0478806495666504,
      "learning_rate": 0.00019463539751242374,
      "loss": 0.1213,
      "step": 6189
    },
    {
      "epoch": 0.029116531981147165,
      "grad_norm": 2.24385142326355,
      "learning_rate": 0.00019463445453431026,
      "loss": 0.3353,
      "step": 6190
    },
    {
      "epoch": 0.029121235782759627,
      "grad_norm": 2.384335994720459,
      "learning_rate": 0.00019463351155619678,
      "loss": 0.3996,
      "step": 6191
    },
    {
      "epoch": 0.02912593958437209,
      "grad_norm": 4.531621932983398,
      "learning_rate": 0.00019463256857808333,
      "loss": 0.658,
      "step": 6192
    },
    {
      "epoch": 0.02913064338598455,
      "grad_norm": 3.5240535736083984,
      "learning_rate": 0.00019463162559996985,
      "loss": 0.671,
      "step": 6193
    },
    {
      "epoch": 0.029135347187597017,
      "grad_norm": 3.31947660446167,
      "learning_rate": 0.00019463068262185636,
      "loss": 0.3173,
      "step": 6194
    },
    {
      "epoch": 0.02914005098920948,
      "grad_norm": 0.43945232033729553,
      "learning_rate": 0.00019462973964374288,
      "loss": 0.0288,
      "step": 6195
    },
    {
      "epoch": 0.02914475479082194,
      "grad_norm": 1.529227614402771,
      "learning_rate": 0.0001946287966656294,
      "loss": 0.0986,
      "step": 6196
    },
    {
      "epoch": 0.029149458592434407,
      "grad_norm": 3.2535200119018555,
      "learning_rate": 0.00019462785368751592,
      "loss": 0.2942,
      "step": 6197
    },
    {
      "epoch": 0.02915416239404687,
      "grad_norm": 2.7762370109558105,
      "learning_rate": 0.00019462691070940244,
      "loss": 0.4425,
      "step": 6198
    },
    {
      "epoch": 0.02915886619565933,
      "grad_norm": 3.274014711380005,
      "learning_rate": 0.00019462596773128896,
      "loss": 0.4642,
      "step": 6199
    },
    {
      "epoch": 0.029163569997271797,
      "grad_norm": 3.0035414695739746,
      "learning_rate": 0.00019462502475317548,
      "loss": 0.4572,
      "step": 6200
    },
    {
      "epoch": 0.02916827379888426,
      "grad_norm": 7.598464488983154,
      "learning_rate": 0.00019462408177506202,
      "loss": 0.8222,
      "step": 6201
    },
    {
      "epoch": 0.02917297760049672,
      "grad_norm": 0.5246642231941223,
      "learning_rate": 0.00019462313879694854,
      "loss": 0.0287,
      "step": 6202
    },
    {
      "epoch": 0.029177681402109183,
      "grad_norm": 6.7918009757995605,
      "learning_rate": 0.00019462219581883506,
      "loss": 0.1749,
      "step": 6203
    },
    {
      "epoch": 0.02918238520372165,
      "grad_norm": 7.067188739776611,
      "learning_rate": 0.00019462125284072158,
      "loss": 0.8134,
      "step": 6204
    },
    {
      "epoch": 0.02918708900533411,
      "grad_norm": 2.8415887355804443,
      "learning_rate": 0.0001946203098626081,
      "loss": 0.1677,
      "step": 6205
    },
    {
      "epoch": 0.029191792806946573,
      "grad_norm": 3.660695791244507,
      "learning_rate": 0.00019461936688449461,
      "loss": 0.7033,
      "step": 6206
    },
    {
      "epoch": 0.02919649660855904,
      "grad_norm": 2.893955707550049,
      "learning_rate": 0.00019461842390638113,
      "loss": 0.3376,
      "step": 6207
    },
    {
      "epoch": 0.0292012004101715,
      "grad_norm": 3.8631157875061035,
      "learning_rate": 0.00019461748092826765,
      "loss": 0.7809,
      "step": 6208
    },
    {
      "epoch": 0.029205904211783963,
      "grad_norm": 0.8393070697784424,
      "learning_rate": 0.00019461653795015417,
      "loss": 0.0748,
      "step": 6209
    },
    {
      "epoch": 0.029210608013396425,
      "grad_norm": 1.3192733526229858,
      "learning_rate": 0.00019461559497204072,
      "loss": 0.0828,
      "step": 6210
    },
    {
      "epoch": 0.02921531181500889,
      "grad_norm": 1.6757041215896606,
      "learning_rate": 0.00019461465199392724,
      "loss": 0.1838,
      "step": 6211
    },
    {
      "epoch": 0.029220015616621353,
      "grad_norm": 2.124098539352417,
      "learning_rate": 0.00019461370901581375,
      "loss": 0.1349,
      "step": 6212
    },
    {
      "epoch": 0.029224719418233815,
      "grad_norm": 0.6368910074234009,
      "learning_rate": 0.00019461276603770027,
      "loss": 0.0404,
      "step": 6213
    },
    {
      "epoch": 0.02922942321984628,
      "grad_norm": 1.5732697248458862,
      "learning_rate": 0.00019461182305958682,
      "loss": 0.122,
      "step": 6214
    },
    {
      "epoch": 0.029234127021458743,
      "grad_norm": 5.974575996398926,
      "learning_rate": 0.00019461088008147334,
      "loss": 1.1396,
      "step": 6215
    },
    {
      "epoch": 0.029238830823071205,
      "grad_norm": 6.062781810760498,
      "learning_rate": 0.00019460993710335983,
      "loss": 0.5317,
      "step": 6216
    },
    {
      "epoch": 0.02924353462468367,
      "grad_norm": 0.44683775305747986,
      "learning_rate": 0.00019460899412524635,
      "loss": 0.0408,
      "step": 6217
    },
    {
      "epoch": 0.029248238426296133,
      "grad_norm": 2.5819029808044434,
      "learning_rate": 0.00019460805114713287,
      "loss": 0.1129,
      "step": 6218
    },
    {
      "epoch": 0.029252942227908595,
      "grad_norm": 4.359823226928711,
      "learning_rate": 0.0001946071081690194,
      "loss": 0.6317,
      "step": 6219
    },
    {
      "epoch": 0.029257646029521057,
      "grad_norm": 2.389239549636841,
      "learning_rate": 0.00019460616519090593,
      "loss": 0.2273,
      "step": 6220
    },
    {
      "epoch": 0.029262349831133523,
      "grad_norm": 2.7170345783233643,
      "learning_rate": 0.00019460522221279245,
      "loss": 0.1962,
      "step": 6221
    },
    {
      "epoch": 0.029267053632745985,
      "grad_norm": 3.718745708465576,
      "learning_rate": 0.00019460427923467897,
      "loss": 0.5797,
      "step": 6222
    },
    {
      "epoch": 0.029271757434358447,
      "grad_norm": 0.40476348996162415,
      "learning_rate": 0.0001946033362565655,
      "loss": 0.026,
      "step": 6223
    },
    {
      "epoch": 0.029276461235970913,
      "grad_norm": 3.713287353515625,
      "learning_rate": 0.00019460239327845203,
      "loss": 0.4138,
      "step": 6224
    },
    {
      "epoch": 0.029281165037583375,
      "grad_norm": 1.4730751514434814,
      "learning_rate": 0.00019460145030033855,
      "loss": 0.1519,
      "step": 6225
    },
    {
      "epoch": 0.029285868839195837,
      "grad_norm": 4.947871208190918,
      "learning_rate": 0.00019460050732222507,
      "loss": 0.5934,
      "step": 6226
    },
    {
      "epoch": 0.0292905726408083,
      "grad_norm": 4.427356719970703,
      "learning_rate": 0.00019459956434411156,
      "loss": 0.4141,
      "step": 6227
    },
    {
      "epoch": 0.029295276442420765,
      "grad_norm": 0.9120168089866638,
      "learning_rate": 0.0001945986213659981,
      "loss": 0.0739,
      "step": 6228
    },
    {
      "epoch": 0.029299980244033227,
      "grad_norm": 2.1886627674102783,
      "learning_rate": 0.00019459767838788463,
      "loss": 0.2672,
      "step": 6229
    },
    {
      "epoch": 0.02930468404564569,
      "grad_norm": 3.4106059074401855,
      "learning_rate": 0.00019459673540977114,
      "loss": 0.3958,
      "step": 6230
    },
    {
      "epoch": 0.029309387847258155,
      "grad_norm": 0.43045544624328613,
      "learning_rate": 0.00019459579243165766,
      "loss": 0.0428,
      "step": 6231
    },
    {
      "epoch": 0.029314091648870617,
      "grad_norm": 4.740723609924316,
      "learning_rate": 0.0001945948494535442,
      "loss": 0.6639,
      "step": 6232
    },
    {
      "epoch": 0.02931879545048308,
      "grad_norm": 3.8561596870422363,
      "learning_rate": 0.00019459390647543073,
      "loss": 0.7741,
      "step": 6233
    },
    {
      "epoch": 0.029323499252095545,
      "grad_norm": 0.10762188583612442,
      "learning_rate": 0.00019459296349731725,
      "loss": 0.0064,
      "step": 6234
    },
    {
      "epoch": 0.029328203053708007,
      "grad_norm": 0.8570061922073364,
      "learning_rate": 0.00019459202051920376,
      "loss": 0.117,
      "step": 6235
    },
    {
      "epoch": 0.02933290685532047,
      "grad_norm": 2.0130178928375244,
      "learning_rate": 0.00019459107754109028,
      "loss": 0.2895,
      "step": 6236
    },
    {
      "epoch": 0.029337610656932932,
      "grad_norm": 1.9185019731521606,
      "learning_rate": 0.0001945901345629768,
      "loss": 0.1476,
      "step": 6237
    },
    {
      "epoch": 0.029342314458545397,
      "grad_norm": 1.309326171875,
      "learning_rate": 0.00019458919158486332,
      "loss": 0.0798,
      "step": 6238
    },
    {
      "epoch": 0.02934701826015786,
      "grad_norm": 2.1189322471618652,
      "learning_rate": 0.00019458824860674984,
      "loss": 0.1786,
      "step": 6239
    },
    {
      "epoch": 0.02935172206177032,
      "grad_norm": 1.2951111793518066,
      "learning_rate": 0.00019458730562863636,
      "loss": 0.1392,
      "step": 6240
    },
    {
      "epoch": 0.029356425863382787,
      "grad_norm": 2.6653058528900146,
      "learning_rate": 0.00019458636265052288,
      "loss": 0.4971,
      "step": 6241
    },
    {
      "epoch": 0.02936112966499525,
      "grad_norm": 0.3482815623283386,
      "learning_rate": 0.00019458541967240942,
      "loss": 0.0215,
      "step": 6242
    },
    {
      "epoch": 0.02936583346660771,
      "grad_norm": 0.4953249394893646,
      "learning_rate": 0.00019458447669429594,
      "loss": 0.0283,
      "step": 6243
    },
    {
      "epoch": 0.029370537268220174,
      "grad_norm": 1.6068894863128662,
      "learning_rate": 0.00019458353371618246,
      "loss": 0.12,
      "step": 6244
    },
    {
      "epoch": 0.02937524106983264,
      "grad_norm": 0.40949735045433044,
      "learning_rate": 0.00019458259073806898,
      "loss": 0.0211,
      "step": 6245
    },
    {
      "epoch": 0.0293799448714451,
      "grad_norm": 2.242755174636841,
      "learning_rate": 0.00019458164775995552,
      "loss": 0.2095,
      "step": 6246
    },
    {
      "epoch": 0.029384648673057564,
      "grad_norm": 4.045004367828369,
      "learning_rate": 0.00019458070478184201,
      "loss": 0.464,
      "step": 6247
    },
    {
      "epoch": 0.02938935247467003,
      "grad_norm": 2.3727684020996094,
      "learning_rate": 0.00019457976180372853,
      "loss": 0.3661,
      "step": 6248
    },
    {
      "epoch": 0.02939405627628249,
      "grad_norm": 0.2846749424934387,
      "learning_rate": 0.00019457881882561505,
      "loss": 0.0241,
      "step": 6249
    },
    {
      "epoch": 0.029398760077894954,
      "grad_norm": 0.4740734100341797,
      "learning_rate": 0.00019457787584750157,
      "loss": 0.0441,
      "step": 6250
    },
    {
      "epoch": 0.02940346387950742,
      "grad_norm": 3.2770438194274902,
      "learning_rate": 0.00019457693286938812,
      "loss": 0.2443,
      "step": 6251
    },
    {
      "epoch": 0.02940816768111988,
      "grad_norm": 2.9454309940338135,
      "learning_rate": 0.00019457598989127464,
      "loss": 0.1403,
      "step": 6252
    },
    {
      "epoch": 0.029412871482732344,
      "grad_norm": 2.3453636169433594,
      "learning_rate": 0.00019457504691316115,
      "loss": 0.4614,
      "step": 6253
    },
    {
      "epoch": 0.029417575284344806,
      "grad_norm": 0.4748900830745697,
      "learning_rate": 0.00019457410393504767,
      "loss": 0.0425,
      "step": 6254
    },
    {
      "epoch": 0.02942227908595727,
      "grad_norm": 6.139457702636719,
      "learning_rate": 0.00019457316095693422,
      "loss": 0.5635,
      "step": 6255
    },
    {
      "epoch": 0.029426982887569734,
      "grad_norm": 3.524850606918335,
      "learning_rate": 0.00019457221797882074,
      "loss": 0.359,
      "step": 6256
    },
    {
      "epoch": 0.029431686689182196,
      "grad_norm": 6.867663383483887,
      "learning_rate": 0.00019457127500070726,
      "loss": 0.6885,
      "step": 6257
    },
    {
      "epoch": 0.02943639049079466,
      "grad_norm": 0.621263861656189,
      "learning_rate": 0.00019457033202259375,
      "loss": 0.0328,
      "step": 6258
    },
    {
      "epoch": 0.029441094292407124,
      "grad_norm": 9.35570240020752,
      "learning_rate": 0.00019456938904448027,
      "loss": 0.848,
      "step": 6259
    },
    {
      "epoch": 0.029445798094019586,
      "grad_norm": 5.817246437072754,
      "learning_rate": 0.0001945684460663668,
      "loss": 0.5332,
      "step": 6260
    },
    {
      "epoch": 0.029450501895632048,
      "grad_norm": 5.942312717437744,
      "learning_rate": 0.00019456750308825333,
      "loss": 0.7327,
      "step": 6261
    },
    {
      "epoch": 0.029455205697244514,
      "grad_norm": 3.328172206878662,
      "learning_rate": 0.00019456656011013985,
      "loss": 0.3079,
      "step": 6262
    },
    {
      "epoch": 0.029459909498856976,
      "grad_norm": 0.5923718214035034,
      "learning_rate": 0.00019456561713202637,
      "loss": 0.0411,
      "step": 6263
    },
    {
      "epoch": 0.029464613300469438,
      "grad_norm": 6.602901935577393,
      "learning_rate": 0.0001945646741539129,
      "loss": 0.7603,
      "step": 6264
    },
    {
      "epoch": 0.029469317102081904,
      "grad_norm": 1.3041659593582153,
      "learning_rate": 0.00019456373117579943,
      "loss": 0.1274,
      "step": 6265
    },
    {
      "epoch": 0.029474020903694366,
      "grad_norm": 1.7595679759979248,
      "learning_rate": 0.00019456278819768595,
      "loss": 0.1231,
      "step": 6266
    },
    {
      "epoch": 0.029478724705306828,
      "grad_norm": 2.189617872238159,
      "learning_rate": 0.00019456184521957247,
      "loss": 0.2763,
      "step": 6267
    },
    {
      "epoch": 0.029483428506919294,
      "grad_norm": 2.069166421890259,
      "learning_rate": 0.000194560902241459,
      "loss": 0.3168,
      "step": 6268
    },
    {
      "epoch": 0.029488132308531756,
      "grad_norm": 1.2439888715744019,
      "learning_rate": 0.0001945599592633455,
      "loss": 0.1139,
      "step": 6269
    },
    {
      "epoch": 0.029492836110144218,
      "grad_norm": 3.876006603240967,
      "learning_rate": 0.00019455901628523203,
      "loss": 0.6871,
      "step": 6270
    },
    {
      "epoch": 0.02949753991175668,
      "grad_norm": 1.0660289525985718,
      "learning_rate": 0.00019455807330711854,
      "loss": 0.0754,
      "step": 6271
    },
    {
      "epoch": 0.029502243713369146,
      "grad_norm": 2.8561267852783203,
      "learning_rate": 0.00019455713032900506,
      "loss": 0.2942,
      "step": 6272
    },
    {
      "epoch": 0.029506947514981608,
      "grad_norm": 2.554783821105957,
      "learning_rate": 0.0001945561873508916,
      "loss": 0.3113,
      "step": 6273
    },
    {
      "epoch": 0.02951165131659407,
      "grad_norm": 2.924480676651001,
      "learning_rate": 0.00019455524437277813,
      "loss": 0.3403,
      "step": 6274
    },
    {
      "epoch": 0.029516355118206536,
      "grad_norm": 2.6177186965942383,
      "learning_rate": 0.00019455430139466465,
      "loss": 0.2395,
      "step": 6275
    },
    {
      "epoch": 0.029521058919818998,
      "grad_norm": 3.6400249004364014,
      "learning_rate": 0.00019455335841655116,
      "loss": 0.813,
      "step": 6276
    },
    {
      "epoch": 0.02952576272143146,
      "grad_norm": 3.8475191593170166,
      "learning_rate": 0.00019455241543843768,
      "loss": 0.3581,
      "step": 6277
    },
    {
      "epoch": 0.029530466523043922,
      "grad_norm": 3.0628745555877686,
      "learning_rate": 0.0001945514724603242,
      "loss": 0.7437,
      "step": 6278
    },
    {
      "epoch": 0.029535170324656388,
      "grad_norm": 2.275675058364868,
      "learning_rate": 0.00019455052948221072,
      "loss": 0.245,
      "step": 6279
    },
    {
      "epoch": 0.02953987412626885,
      "grad_norm": 1.635467290878296,
      "learning_rate": 0.00019454958650409724,
      "loss": 0.1876,
      "step": 6280
    },
    {
      "epoch": 0.029544577927881312,
      "grad_norm": 2.370687961578369,
      "learning_rate": 0.00019454864352598376,
      "loss": 0.3904,
      "step": 6281
    },
    {
      "epoch": 0.029549281729493778,
      "grad_norm": 1.1666972637176514,
      "learning_rate": 0.0001945477005478703,
      "loss": 0.1399,
      "step": 6282
    },
    {
      "epoch": 0.02955398553110624,
      "grad_norm": 0.5011782050132751,
      "learning_rate": 0.00019454675756975682,
      "loss": 0.0565,
      "step": 6283
    },
    {
      "epoch": 0.029558689332718702,
      "grad_norm": 3.2290642261505127,
      "learning_rate": 0.00019454581459164334,
      "loss": 0.4474,
      "step": 6284
    },
    {
      "epoch": 0.029563393134331168,
      "grad_norm": 1.7985029220581055,
      "learning_rate": 0.00019454487161352986,
      "loss": 0.3023,
      "step": 6285
    },
    {
      "epoch": 0.02956809693594363,
      "grad_norm": 0.16714270412921906,
      "learning_rate": 0.00019454392863541638,
      "loss": 0.0151,
      "step": 6286
    },
    {
      "epoch": 0.029572800737556092,
      "grad_norm": 0.47956374287605286,
      "learning_rate": 0.00019454298565730292,
      "loss": 0.0452,
      "step": 6287
    },
    {
      "epoch": 0.029577504539168555,
      "grad_norm": 2.012664794921875,
      "learning_rate": 0.00019454204267918944,
      "loss": 0.5349,
      "step": 6288
    },
    {
      "epoch": 0.02958220834078102,
      "grad_norm": 1.4279823303222656,
      "learning_rate": 0.00019454109970107593,
      "loss": 0.2086,
      "step": 6289
    },
    {
      "epoch": 0.029586912142393482,
      "grad_norm": 2.3040261268615723,
      "learning_rate": 0.00019454015672296245,
      "loss": 0.6317,
      "step": 6290
    },
    {
      "epoch": 0.029591615944005945,
      "grad_norm": 1.327612280845642,
      "learning_rate": 0.00019453921374484897,
      "loss": 0.1167,
      "step": 6291
    },
    {
      "epoch": 0.02959631974561841,
      "grad_norm": 1.896337866783142,
      "learning_rate": 0.00019453827076673552,
      "loss": 0.3672,
      "step": 6292
    },
    {
      "epoch": 0.029601023547230872,
      "grad_norm": 1.641302227973938,
      "learning_rate": 0.00019453732778862204,
      "loss": 0.3555,
      "step": 6293
    },
    {
      "epoch": 0.029605727348843335,
      "grad_norm": 2.1311843395233154,
      "learning_rate": 0.00019453638481050855,
      "loss": 0.4811,
      "step": 6294
    },
    {
      "epoch": 0.029610431150455797,
      "grad_norm": 2.248609781265259,
      "learning_rate": 0.00019453544183239507,
      "loss": 0.4139,
      "step": 6295
    },
    {
      "epoch": 0.029615134952068262,
      "grad_norm": 1.920338749885559,
      "learning_rate": 0.00019453449885428162,
      "loss": 0.514,
      "step": 6296
    },
    {
      "epoch": 0.029619838753680724,
      "grad_norm": 3.222970724105835,
      "learning_rate": 0.00019453355587616814,
      "loss": 0.8285,
      "step": 6297
    },
    {
      "epoch": 0.029624542555293187,
      "grad_norm": 1.609912395477295,
      "learning_rate": 0.00019453261289805466,
      "loss": 0.2417,
      "step": 6298
    },
    {
      "epoch": 0.029629246356905652,
      "grad_norm": 0.6883845925331116,
      "learning_rate": 0.00019453166991994117,
      "loss": 0.0806,
      "step": 6299
    },
    {
      "epoch": 0.029633950158518114,
      "grad_norm": 2.591392755508423,
      "learning_rate": 0.00019453072694182767,
      "loss": 0.6365,
      "step": 6300
    },
    {
      "epoch": 0.029638653960130577,
      "grad_norm": 0.787835955619812,
      "learning_rate": 0.0001945297839637142,
      "loss": 0.1052,
      "step": 6301
    },
    {
      "epoch": 0.029643357761743042,
      "grad_norm": 0.2510950565338135,
      "learning_rate": 0.00019452884098560073,
      "loss": 0.0179,
      "step": 6302
    },
    {
      "epoch": 0.029648061563355504,
      "grad_norm": 0.49234744906425476,
      "learning_rate": 0.00019452789800748725,
      "loss": 0.0546,
      "step": 6303
    },
    {
      "epoch": 0.029652765364967967,
      "grad_norm": 1.7764214277267456,
      "learning_rate": 0.00019452695502937377,
      "loss": 0.339,
      "step": 6304
    },
    {
      "epoch": 0.02965746916658043,
      "grad_norm": 1.3582781553268433,
      "learning_rate": 0.0001945260120512603,
      "loss": 0.1771,
      "step": 6305
    },
    {
      "epoch": 0.029662172968192894,
      "grad_norm": 1.3031476736068726,
      "learning_rate": 0.00019452506907314683,
      "loss": 0.181,
      "step": 6306
    },
    {
      "epoch": 0.029666876769805357,
      "grad_norm": 1.7828909158706665,
      "learning_rate": 0.00019452412609503335,
      "loss": 0.3693,
      "step": 6307
    },
    {
      "epoch": 0.02967158057141782,
      "grad_norm": 2.5983078479766846,
      "learning_rate": 0.00019452318311691987,
      "loss": 0.3256,
      "step": 6308
    },
    {
      "epoch": 0.029676284373030284,
      "grad_norm": 0.8716217875480652,
      "learning_rate": 0.0001945222401388064,
      "loss": 0.1133,
      "step": 6309
    },
    {
      "epoch": 0.029680988174642747,
      "grad_norm": 5.389306545257568,
      "learning_rate": 0.0001945212971606929,
      "loss": 0.6141,
      "step": 6310
    },
    {
      "epoch": 0.02968569197625521,
      "grad_norm": 1.0874738693237305,
      "learning_rate": 0.00019452035418257943,
      "loss": 0.205,
      "step": 6311
    },
    {
      "epoch": 0.02969039577786767,
      "grad_norm": 1.133641004562378,
      "learning_rate": 0.00019451941120446594,
      "loss": 0.2035,
      "step": 6312
    },
    {
      "epoch": 0.029695099579480137,
      "grad_norm": 2.1003658771514893,
      "learning_rate": 0.00019451846822635246,
      "loss": 0.3254,
      "step": 6313
    },
    {
      "epoch": 0.0296998033810926,
      "grad_norm": 5.056933879852295,
      "learning_rate": 0.000194517525248239,
      "loss": 0.682,
      "step": 6314
    },
    {
      "epoch": 0.02970450718270506,
      "grad_norm": 1.2399976253509521,
      "learning_rate": 0.00019451658227012553,
      "loss": 0.199,
      "step": 6315
    },
    {
      "epoch": 0.029709210984317527,
      "grad_norm": 1.8192713260650635,
      "learning_rate": 0.00019451563929201205,
      "loss": 0.3269,
      "step": 6316
    },
    {
      "epoch": 0.02971391478592999,
      "grad_norm": 1.4925320148468018,
      "learning_rate": 0.00019451469631389856,
      "loss": 0.2108,
      "step": 6317
    },
    {
      "epoch": 0.02971861858754245,
      "grad_norm": 1.8759421110153198,
      "learning_rate": 0.00019451375333578508,
      "loss": 0.4297,
      "step": 6318
    },
    {
      "epoch": 0.029723322389154917,
      "grad_norm": 1.1855063438415527,
      "learning_rate": 0.00019451281035767163,
      "loss": 0.1295,
      "step": 6319
    },
    {
      "epoch": 0.02972802619076738,
      "grad_norm": 0.7599632740020752,
      "learning_rate": 0.00019451186737955812,
      "loss": 0.0778,
      "step": 6320
    },
    {
      "epoch": 0.02973272999237984,
      "grad_norm": 2.136186361312866,
      "learning_rate": 0.00019451092440144464,
      "loss": 0.2716,
      "step": 6321
    },
    {
      "epoch": 0.029737433793992303,
      "grad_norm": 2.52988600730896,
      "learning_rate": 0.00019450998142333116,
      "loss": 0.6504,
      "step": 6322
    },
    {
      "epoch": 0.02974213759560477,
      "grad_norm": 0.8453467488288879,
      "learning_rate": 0.0001945090384452177,
      "loss": 0.119,
      "step": 6323
    },
    {
      "epoch": 0.02974684139721723,
      "grad_norm": 1.6053310632705688,
      "learning_rate": 0.00019450809546710422,
      "loss": 0.1168,
      "step": 6324
    },
    {
      "epoch": 0.029751545198829693,
      "grad_norm": 1.4798482656478882,
      "learning_rate": 0.00019450715248899074,
      "loss": 0.2663,
      "step": 6325
    },
    {
      "epoch": 0.02975624900044216,
      "grad_norm": 2.6666624546051025,
      "learning_rate": 0.00019450620951087726,
      "loss": 0.4912,
      "step": 6326
    },
    {
      "epoch": 0.02976095280205462,
      "grad_norm": 0.8159404397010803,
      "learning_rate": 0.00019450526653276378,
      "loss": 0.094,
      "step": 6327
    },
    {
      "epoch": 0.029765656603667083,
      "grad_norm": 1.0649691820144653,
      "learning_rate": 0.00019450432355465032,
      "loss": 0.2741,
      "step": 6328
    },
    {
      "epoch": 0.029770360405279545,
      "grad_norm": 1.2450246810913086,
      "learning_rate": 0.00019450338057653684,
      "loss": 0.1621,
      "step": 6329
    },
    {
      "epoch": 0.02977506420689201,
      "grad_norm": 3.1867806911468506,
      "learning_rate": 0.00019450243759842336,
      "loss": 0.4038,
      "step": 6330
    },
    {
      "epoch": 0.029779768008504473,
      "grad_norm": 1.0259485244750977,
      "learning_rate": 0.00019450149462030985,
      "loss": 0.1209,
      "step": 6331
    },
    {
      "epoch": 0.029784471810116935,
      "grad_norm": 4.100006103515625,
      "learning_rate": 0.0001945005516421964,
      "loss": 0.6335,
      "step": 6332
    },
    {
      "epoch": 0.0297891756117294,
      "grad_norm": 2.3597171306610107,
      "learning_rate": 0.00019449960866408292,
      "loss": 0.3424,
      "step": 6333
    },
    {
      "epoch": 0.029793879413341863,
      "grad_norm": 3.152336835861206,
      "learning_rate": 0.00019449866568596944,
      "loss": 0.291,
      "step": 6334
    },
    {
      "epoch": 0.029798583214954325,
      "grad_norm": 1.4718695878982544,
      "learning_rate": 0.00019449772270785595,
      "loss": 0.1158,
      "step": 6335
    },
    {
      "epoch": 0.02980328701656679,
      "grad_norm": 1.1875574588775635,
      "learning_rate": 0.00019449677972974247,
      "loss": 0.1162,
      "step": 6336
    },
    {
      "epoch": 0.029807990818179253,
      "grad_norm": 1.5892456769943237,
      "learning_rate": 0.00019449583675162902,
      "loss": 0.3662,
      "step": 6337
    },
    {
      "epoch": 0.029812694619791715,
      "grad_norm": 2.551828622817993,
      "learning_rate": 0.00019449489377351554,
      "loss": 0.2284,
      "step": 6338
    },
    {
      "epoch": 0.029817398421404177,
      "grad_norm": 0.46244513988494873,
      "learning_rate": 0.00019449395079540206,
      "loss": 0.0374,
      "step": 6339
    },
    {
      "epoch": 0.029822102223016643,
      "grad_norm": 0.8255166411399841,
      "learning_rate": 0.00019449300781728857,
      "loss": 0.1061,
      "step": 6340
    },
    {
      "epoch": 0.029826806024629105,
      "grad_norm": 1.3453677892684937,
      "learning_rate": 0.0001944920648391751,
      "loss": 0.1408,
      "step": 6341
    },
    {
      "epoch": 0.029831509826241567,
      "grad_norm": 5.997614860534668,
      "learning_rate": 0.0001944911218610616,
      "loss": 0.5018,
      "step": 6342
    },
    {
      "epoch": 0.029836213627854033,
      "grad_norm": 4.886133670806885,
      "learning_rate": 0.00019449017888294813,
      "loss": 1.2105,
      "step": 6343
    },
    {
      "epoch": 0.029840917429466495,
      "grad_norm": 1.6905068159103394,
      "learning_rate": 0.00019448923590483465,
      "loss": 0.2196,
      "step": 6344
    },
    {
      "epoch": 0.029845621231078957,
      "grad_norm": 2.4862678050994873,
      "learning_rate": 0.00019448829292672117,
      "loss": 0.3195,
      "step": 6345
    },
    {
      "epoch": 0.02985032503269142,
      "grad_norm": 3.0108726024627686,
      "learning_rate": 0.0001944873499486077,
      "loss": 0.6386,
      "step": 6346
    },
    {
      "epoch": 0.029855028834303885,
      "grad_norm": 1.2411350011825562,
      "learning_rate": 0.00019448640697049423,
      "loss": 0.1191,
      "step": 6347
    },
    {
      "epoch": 0.029859732635916347,
      "grad_norm": 1.149895429611206,
      "learning_rate": 0.00019448546399238075,
      "loss": 0.1022,
      "step": 6348
    },
    {
      "epoch": 0.02986443643752881,
      "grad_norm": 2.0543811321258545,
      "learning_rate": 0.00019448452101426727,
      "loss": 0.2231,
      "step": 6349
    },
    {
      "epoch": 0.029869140239141275,
      "grad_norm": 1.3991758823394775,
      "learning_rate": 0.0001944835780361538,
      "loss": 0.2744,
      "step": 6350
    },
    {
      "epoch": 0.029873844040753737,
      "grad_norm": 2.115321159362793,
      "learning_rate": 0.0001944826350580403,
      "loss": 0.1407,
      "step": 6351
    },
    {
      "epoch": 0.0298785478423662,
      "grad_norm": 1.4736604690551758,
      "learning_rate": 0.00019448169207992682,
      "loss": 0.3048,
      "step": 6352
    },
    {
      "epoch": 0.029883251643978665,
      "grad_norm": 4.5264201164245605,
      "learning_rate": 0.00019448074910181334,
      "loss": 0.8739,
      "step": 6353
    },
    {
      "epoch": 0.029887955445591127,
      "grad_norm": 1.2639843225479126,
      "learning_rate": 0.00019447980612369986,
      "loss": 0.2057,
      "step": 6354
    },
    {
      "epoch": 0.02989265924720359,
      "grad_norm": 2.841351270675659,
      "learning_rate": 0.0001944788631455864,
      "loss": 0.5651,
      "step": 6355
    },
    {
      "epoch": 0.02989736304881605,
      "grad_norm": 1.525394082069397,
      "learning_rate": 0.00019447792016747293,
      "loss": 0.1532,
      "step": 6356
    },
    {
      "epoch": 0.029902066850428517,
      "grad_norm": 1.7402851581573486,
      "learning_rate": 0.00019447697718935945,
      "loss": 0.1505,
      "step": 6357
    },
    {
      "epoch": 0.02990677065204098,
      "grad_norm": 2.7618300914764404,
      "learning_rate": 0.00019447603421124596,
      "loss": 0.8475,
      "step": 6358
    },
    {
      "epoch": 0.02991147445365344,
      "grad_norm": 4.465121269226074,
      "learning_rate": 0.00019447509123313248,
      "loss": 0.4061,
      "step": 6359
    },
    {
      "epoch": 0.029916178255265907,
      "grad_norm": 5.383145809173584,
      "learning_rate": 0.00019447414825501903,
      "loss": 0.3829,
      "step": 6360
    },
    {
      "epoch": 0.02992088205687837,
      "grad_norm": 0.37960004806518555,
      "learning_rate": 0.00019447320527690555,
      "loss": 0.0325,
      "step": 6361
    },
    {
      "epoch": 0.02992558585849083,
      "grad_norm": 3.2195587158203125,
      "learning_rate": 0.00019447226229879204,
      "loss": 0.3484,
      "step": 6362
    },
    {
      "epoch": 0.029930289660103294,
      "grad_norm": 1.077454686164856,
      "learning_rate": 0.00019447131932067856,
      "loss": 0.1109,
      "step": 6363
    },
    {
      "epoch": 0.02993499346171576,
      "grad_norm": 0.2829119563102722,
      "learning_rate": 0.0001944703763425651,
      "loss": 0.038,
      "step": 6364
    },
    {
      "epoch": 0.02993969726332822,
      "grad_norm": 8.069663047790527,
      "learning_rate": 0.00019446943336445162,
      "loss": 0.3424,
      "step": 6365
    },
    {
      "epoch": 0.029944401064940684,
      "grad_norm": 1.1800986528396606,
      "learning_rate": 0.00019446849038633814,
      "loss": 0.1364,
      "step": 6366
    },
    {
      "epoch": 0.02994910486655315,
      "grad_norm": 2.1498754024505615,
      "learning_rate": 0.00019446754740822466,
      "loss": 0.2641,
      "step": 6367
    },
    {
      "epoch": 0.02995380866816561,
      "grad_norm": 0.9528509378433228,
      "learning_rate": 0.00019446660443011118,
      "loss": 0.087,
      "step": 6368
    },
    {
      "epoch": 0.029958512469778074,
      "grad_norm": 6.127645969390869,
      "learning_rate": 0.00019446566145199772,
      "loss": 0.548,
      "step": 6369
    },
    {
      "epoch": 0.02996321627139054,
      "grad_norm": 1.7557692527770996,
      "learning_rate": 0.00019446471847388424,
      "loss": 0.1976,
      "step": 6370
    },
    {
      "epoch": 0.029967920073003,
      "grad_norm": 1.1418139934539795,
      "learning_rate": 0.00019446377549577076,
      "loss": 0.1861,
      "step": 6371
    },
    {
      "epoch": 0.029972623874615464,
      "grad_norm": 1.749019980430603,
      "learning_rate": 0.00019446283251765728,
      "loss": 0.2552,
      "step": 6372
    },
    {
      "epoch": 0.029977327676227926,
      "grad_norm": 2.385024070739746,
      "learning_rate": 0.0001944618895395438,
      "loss": 0.2175,
      "step": 6373
    },
    {
      "epoch": 0.02998203147784039,
      "grad_norm": 0.20232298970222473,
      "learning_rate": 0.00019446094656143032,
      "loss": 0.0185,
      "step": 6374
    },
    {
      "epoch": 0.029986735279452854,
      "grad_norm": 2.148247718811035,
      "learning_rate": 0.00019446000358331684,
      "loss": 0.5046,
      "step": 6375
    },
    {
      "epoch": 0.029991439081065316,
      "grad_norm": 1.5847032070159912,
      "learning_rate": 0.00019445906060520335,
      "loss": 0.185,
      "step": 6376
    },
    {
      "epoch": 0.02999614288267778,
      "grad_norm": 1.677072286605835,
      "learning_rate": 0.00019445811762708987,
      "loss": 0.2101,
      "step": 6377
    },
    {
      "epoch": 0.030000846684290244,
      "grad_norm": 2.9698994159698486,
      "learning_rate": 0.00019445717464897642,
      "loss": 0.1907,
      "step": 6378
    },
    {
      "epoch": 0.030005550485902706,
      "grad_norm": 4.59476900100708,
      "learning_rate": 0.00019445623167086294,
      "loss": 0.8343,
      "step": 6379
    },
    {
      "epoch": 0.03001025428751517,
      "grad_norm": 1.4962068796157837,
      "learning_rate": 0.00019445528869274946,
      "loss": 0.1544,
      "step": 6380
    },
    {
      "epoch": 0.030014958089127634,
      "grad_norm": 3.5923309326171875,
      "learning_rate": 0.00019445434571463597,
      "loss": 0.3086,
      "step": 6381
    },
    {
      "epoch": 0.030019661890740096,
      "grad_norm": 2.57366681098938,
      "learning_rate": 0.0001944534027365225,
      "loss": 0.3158,
      "step": 6382
    },
    {
      "epoch": 0.030024365692352558,
      "grad_norm": 1.0558937788009644,
      "learning_rate": 0.000194452459758409,
      "loss": 0.1587,
      "step": 6383
    },
    {
      "epoch": 0.030029069493965024,
      "grad_norm": 3.2090227603912354,
      "learning_rate": 0.00019445151678029553,
      "loss": 0.7076,
      "step": 6384
    },
    {
      "epoch": 0.030033773295577486,
      "grad_norm": 1.0361182689666748,
      "learning_rate": 0.00019445057380218205,
      "loss": 0.1562,
      "step": 6385
    },
    {
      "epoch": 0.030038477097189948,
      "grad_norm": 1.1860072612762451,
      "learning_rate": 0.00019444963082406857,
      "loss": 0.1419,
      "step": 6386
    },
    {
      "epoch": 0.030043180898802414,
      "grad_norm": 0.9401779174804688,
      "learning_rate": 0.0001944486878459551,
      "loss": 0.1241,
      "step": 6387
    },
    {
      "epoch": 0.030047884700414876,
      "grad_norm": 1.1169019937515259,
      "learning_rate": 0.00019444774486784163,
      "loss": 0.1145,
      "step": 6388
    },
    {
      "epoch": 0.030052588502027338,
      "grad_norm": 0.7461461424827576,
      "learning_rate": 0.00019444680188972815,
      "loss": 0.0764,
      "step": 6389
    },
    {
      "epoch": 0.0300572923036398,
      "grad_norm": 2.400869369506836,
      "learning_rate": 0.00019444585891161467,
      "loss": 0.4971,
      "step": 6390
    },
    {
      "epoch": 0.030061996105252266,
      "grad_norm": 1.8960494995117188,
      "learning_rate": 0.00019444491593350121,
      "loss": 0.2939,
      "step": 6391
    },
    {
      "epoch": 0.030066699906864728,
      "grad_norm": 0.6944420337677002,
      "learning_rate": 0.00019444397295538773,
      "loss": 0.0545,
      "step": 6392
    },
    {
      "epoch": 0.03007140370847719,
      "grad_norm": 3.090912342071533,
      "learning_rate": 0.00019444302997727422,
      "loss": 0.6457,
      "step": 6393
    },
    {
      "epoch": 0.030076107510089656,
      "grad_norm": 3.431558609008789,
      "learning_rate": 0.00019444208699916074,
      "loss": 0.6559,
      "step": 6394
    },
    {
      "epoch": 0.030080811311702118,
      "grad_norm": 1.8397235870361328,
      "learning_rate": 0.00019444114402104726,
      "loss": 0.4862,
      "step": 6395
    },
    {
      "epoch": 0.03008551511331458,
      "grad_norm": 0.6222590208053589,
      "learning_rate": 0.0001944402010429338,
      "loss": 0.0394,
      "step": 6396
    },
    {
      "epoch": 0.030090218914927046,
      "grad_norm": 0.39559900760650635,
      "learning_rate": 0.00019443925806482033,
      "loss": 0.04,
      "step": 6397
    },
    {
      "epoch": 0.030094922716539508,
      "grad_norm": 1.1173540353775024,
      "learning_rate": 0.00019443831508670685,
      "loss": 0.101,
      "step": 6398
    },
    {
      "epoch": 0.03009962651815197,
      "grad_norm": 6.262368202209473,
      "learning_rate": 0.00019443737210859336,
      "loss": 0.4288,
      "step": 6399
    },
    {
      "epoch": 0.030104330319764432,
      "grad_norm": 2.3813884258270264,
      "learning_rate": 0.00019443642913047988,
      "loss": 0.2834,
      "step": 6400
    },
    {
      "epoch": 0.030109034121376898,
      "grad_norm": 4.918899059295654,
      "learning_rate": 0.00019443548615236643,
      "loss": 0.2323,
      "step": 6401
    },
    {
      "epoch": 0.03011373792298936,
      "grad_norm": 1.6479551792144775,
      "learning_rate": 0.00019443454317425295,
      "loss": 0.0753,
      "step": 6402
    },
    {
      "epoch": 0.030118441724601822,
      "grad_norm": 8.938908576965332,
      "learning_rate": 0.00019443360019613947,
      "loss": 0.3068,
      "step": 6403
    },
    {
      "epoch": 0.030123145526214288,
      "grad_norm": 3.606379985809326,
      "learning_rate": 0.00019443265721802598,
      "loss": 0.3881,
      "step": 6404
    },
    {
      "epoch": 0.03012784932782675,
      "grad_norm": 0.8206337094306946,
      "learning_rate": 0.0001944317142399125,
      "loss": 0.0849,
      "step": 6405
    },
    {
      "epoch": 0.030132553129439212,
      "grad_norm": 1.8809282779693604,
      "learning_rate": 0.00019443077126179902,
      "loss": 0.4052,
      "step": 6406
    },
    {
      "epoch": 0.030137256931051674,
      "grad_norm": 2.9012675285339355,
      "learning_rate": 0.00019442982828368554,
      "loss": 0.4451,
      "step": 6407
    },
    {
      "epoch": 0.03014196073266414,
      "grad_norm": 0.41158536076545715,
      "learning_rate": 0.00019442888530557206,
      "loss": 0.0261,
      "step": 6408
    },
    {
      "epoch": 0.030146664534276602,
      "grad_norm": 2.3984079360961914,
      "learning_rate": 0.00019442794232745858,
      "loss": 0.1576,
      "step": 6409
    },
    {
      "epoch": 0.030151368335889064,
      "grad_norm": 0.0725620910525322,
      "learning_rate": 0.00019442699934934512,
      "loss": 0.0046,
      "step": 6410
    },
    {
      "epoch": 0.03015607213750153,
      "grad_norm": 2.448936700820923,
      "learning_rate": 0.00019442605637123164,
      "loss": 0.3241,
      "step": 6411
    },
    {
      "epoch": 0.030160775939113992,
      "grad_norm": 0.5900617241859436,
      "learning_rate": 0.00019442511339311816,
      "loss": 0.064,
      "step": 6412
    },
    {
      "epoch": 0.030165479740726454,
      "grad_norm": 10.27359390258789,
      "learning_rate": 0.00019442417041500468,
      "loss": 0.6484,
      "step": 6413
    },
    {
      "epoch": 0.03017018354233892,
      "grad_norm": 3.8091061115264893,
      "learning_rate": 0.0001944232274368912,
      "loss": 1.0473,
      "step": 6414
    },
    {
      "epoch": 0.030174887343951382,
      "grad_norm": 2.0583696365356445,
      "learning_rate": 0.00019442228445877772,
      "loss": 0.2078,
      "step": 6415
    },
    {
      "epoch": 0.030179591145563844,
      "grad_norm": 2.5122454166412354,
      "learning_rate": 0.00019442134148066424,
      "loss": 0.4392,
      "step": 6416
    },
    {
      "epoch": 0.030184294947176307,
      "grad_norm": 1.0911341905593872,
      "learning_rate": 0.00019442039850255075,
      "loss": 0.0941,
      "step": 6417
    },
    {
      "epoch": 0.030188998748788772,
      "grad_norm": 2.8989343643188477,
      "learning_rate": 0.00019441945552443727,
      "loss": 0.4058,
      "step": 6418
    },
    {
      "epoch": 0.030193702550401234,
      "grad_norm": 2.0780868530273438,
      "learning_rate": 0.00019441851254632382,
      "loss": 0.2779,
      "step": 6419
    },
    {
      "epoch": 0.030198406352013697,
      "grad_norm": 1.5093743801116943,
      "learning_rate": 0.00019441756956821034,
      "loss": 0.2065,
      "step": 6420
    },
    {
      "epoch": 0.030203110153626162,
      "grad_norm": 2.5682880878448486,
      "learning_rate": 0.00019441662659009686,
      "loss": 0.286,
      "step": 6421
    },
    {
      "epoch": 0.030207813955238624,
      "grad_norm": 2.100623846054077,
      "learning_rate": 0.00019441568361198337,
      "loss": 0.4777,
      "step": 6422
    },
    {
      "epoch": 0.030212517756851087,
      "grad_norm": 1.184941291809082,
      "learning_rate": 0.00019441474063386992,
      "loss": 0.1168,
      "step": 6423
    },
    {
      "epoch": 0.03021722155846355,
      "grad_norm": 0.4304966926574707,
      "learning_rate": 0.0001944137976557564,
      "loss": 0.0585,
      "step": 6424
    },
    {
      "epoch": 0.030221925360076014,
      "grad_norm": 0.4354367256164551,
      "learning_rate": 0.00019441285467764293,
      "loss": 0.0583,
      "step": 6425
    },
    {
      "epoch": 0.030226629161688476,
      "grad_norm": 2.7901291847229004,
      "learning_rate": 0.00019441191169952945,
      "loss": 0.4968,
      "step": 6426
    },
    {
      "epoch": 0.03023133296330094,
      "grad_norm": 2.6191859245300293,
      "learning_rate": 0.00019441096872141597,
      "loss": 0.2595,
      "step": 6427
    },
    {
      "epoch": 0.030236036764913404,
      "grad_norm": 0.8632087707519531,
      "learning_rate": 0.0001944100257433025,
      "loss": 0.0773,
      "step": 6428
    },
    {
      "epoch": 0.030240740566525866,
      "grad_norm": 1.3391830921173096,
      "learning_rate": 0.00019440908276518903,
      "loss": 0.1323,
      "step": 6429
    },
    {
      "epoch": 0.03024544436813833,
      "grad_norm": 1.7095770835876465,
      "learning_rate": 0.00019440813978707555,
      "loss": 0.2518,
      "step": 6430
    },
    {
      "epoch": 0.030250148169750794,
      "grad_norm": 5.136524677276611,
      "learning_rate": 0.00019440719680896207,
      "loss": 0.8062,
      "step": 6431
    },
    {
      "epoch": 0.030254851971363256,
      "grad_norm": 2.726245164871216,
      "learning_rate": 0.00019440625383084861,
      "loss": 0.4268,
      "step": 6432
    },
    {
      "epoch": 0.03025955577297572,
      "grad_norm": 3.1002352237701416,
      "learning_rate": 0.00019440531085273513,
      "loss": 0.4947,
      "step": 6433
    },
    {
      "epoch": 0.03026425957458818,
      "grad_norm": 0.9764885902404785,
      "learning_rate": 0.00019440436787462165,
      "loss": 0.0903,
      "step": 6434
    },
    {
      "epoch": 0.030268963376200646,
      "grad_norm": 4.62320613861084,
      "learning_rate": 0.00019440342489650817,
      "loss": 1.2709,
      "step": 6435
    },
    {
      "epoch": 0.03027366717781311,
      "grad_norm": 1.024721384048462,
      "learning_rate": 0.00019440248191839466,
      "loss": 0.2824,
      "step": 6436
    },
    {
      "epoch": 0.03027837097942557,
      "grad_norm": 1.8655412197113037,
      "learning_rate": 0.0001944015389402812,
      "loss": 0.1475,
      "step": 6437
    },
    {
      "epoch": 0.030283074781038036,
      "grad_norm": 2.0810227394104004,
      "learning_rate": 0.00019440059596216773,
      "loss": 0.1968,
      "step": 6438
    },
    {
      "epoch": 0.0302877785826505,
      "grad_norm": 2.815762996673584,
      "learning_rate": 0.00019439965298405425,
      "loss": 0.2724,
      "step": 6439
    },
    {
      "epoch": 0.03029248238426296,
      "grad_norm": 1.6628390550613403,
      "learning_rate": 0.00019439871000594076,
      "loss": 0.1964,
      "step": 6440
    },
    {
      "epoch": 0.030297186185875423,
      "grad_norm": 1.2743525505065918,
      "learning_rate": 0.0001943977670278273,
      "loss": 0.0989,
      "step": 6441
    },
    {
      "epoch": 0.03030188998748789,
      "grad_norm": 1.6808216571807861,
      "learning_rate": 0.00019439682404971383,
      "loss": 0.2052,
      "step": 6442
    },
    {
      "epoch": 0.03030659378910035,
      "grad_norm": 1.5556111335754395,
      "learning_rate": 0.00019439588107160035,
      "loss": 0.3281,
      "step": 6443
    },
    {
      "epoch": 0.030311297590712813,
      "grad_norm": 2.8759546279907227,
      "learning_rate": 0.00019439493809348687,
      "loss": 0.6149,
      "step": 6444
    },
    {
      "epoch": 0.03031600139232528,
      "grad_norm": 1.4541500806808472,
      "learning_rate": 0.00019439399511537338,
      "loss": 0.2723,
      "step": 6445
    },
    {
      "epoch": 0.03032070519393774,
      "grad_norm": 0.07495106756687164,
      "learning_rate": 0.0001943930521372599,
      "loss": 0.0049,
      "step": 6446
    },
    {
      "epoch": 0.030325408995550203,
      "grad_norm": 1.8947021961212158,
      "learning_rate": 0.00019439210915914642,
      "loss": 0.2905,
      "step": 6447
    },
    {
      "epoch": 0.03033011279716267,
      "grad_norm": 1.4180561304092407,
      "learning_rate": 0.00019439116618103294,
      "loss": 0.3188,
      "step": 6448
    },
    {
      "epoch": 0.03033481659877513,
      "grad_norm": 0.18470363318920135,
      "learning_rate": 0.00019439022320291946,
      "loss": 0.0146,
      "step": 6449
    },
    {
      "epoch": 0.030339520400387593,
      "grad_norm": 1.1334502696990967,
      "learning_rate": 0.00019438928022480598,
      "loss": 0.2184,
      "step": 6450
    },
    {
      "epoch": 0.030344224202000055,
      "grad_norm": 0.73228520154953,
      "learning_rate": 0.00019438833724669252,
      "loss": 0.0562,
      "step": 6451
    },
    {
      "epoch": 0.03034892800361252,
      "grad_norm": 0.5351917147636414,
      "learning_rate": 0.00019438739426857904,
      "loss": 0.0458,
      "step": 6452
    },
    {
      "epoch": 0.030353631805224983,
      "grad_norm": 1.607349157333374,
      "learning_rate": 0.00019438645129046556,
      "loss": 0.1078,
      "step": 6453
    },
    {
      "epoch": 0.030358335606837445,
      "grad_norm": 1.4587148427963257,
      "learning_rate": 0.00019438550831235208,
      "loss": 0.136,
      "step": 6454
    },
    {
      "epoch": 0.03036303940844991,
      "grad_norm": 6.14486026763916,
      "learning_rate": 0.0001943845653342386,
      "loss": 0.6862,
      "step": 6455
    },
    {
      "epoch": 0.030367743210062373,
      "grad_norm": 5.79245138168335,
      "learning_rate": 0.00019438362235612512,
      "loss": 0.2745,
      "step": 6456
    },
    {
      "epoch": 0.030372447011674835,
      "grad_norm": 2.0893194675445557,
      "learning_rate": 0.00019438267937801164,
      "loss": 0.3204,
      "step": 6457
    },
    {
      "epoch": 0.030377150813287297,
      "grad_norm": 2.0107736587524414,
      "learning_rate": 0.00019438173639989815,
      "loss": 0.3623,
      "step": 6458
    },
    {
      "epoch": 0.030381854614899763,
      "grad_norm": 4.104315280914307,
      "learning_rate": 0.00019438079342178467,
      "loss": 0.7413,
      "step": 6459
    },
    {
      "epoch": 0.030386558416512225,
      "grad_norm": 1.6275148391723633,
      "learning_rate": 0.00019437985044367122,
      "loss": 0.3175,
      "step": 6460
    },
    {
      "epoch": 0.030391262218124687,
      "grad_norm": 0.5466936826705933,
      "learning_rate": 0.00019437890746555774,
      "loss": 0.0616,
      "step": 6461
    },
    {
      "epoch": 0.030395966019737153,
      "grad_norm": 1.6297606229782104,
      "learning_rate": 0.00019437796448744426,
      "loss": 0.1674,
      "step": 6462
    },
    {
      "epoch": 0.030400669821349615,
      "grad_norm": 1.5252541303634644,
      "learning_rate": 0.00019437702150933077,
      "loss": 0.2705,
      "step": 6463
    },
    {
      "epoch": 0.030405373622962077,
      "grad_norm": 1.2825032472610474,
      "learning_rate": 0.00019437607853121732,
      "loss": 0.2704,
      "step": 6464
    },
    {
      "epoch": 0.030410077424574543,
      "grad_norm": 1.0832810401916504,
      "learning_rate": 0.00019437513555310384,
      "loss": 0.166,
      "step": 6465
    },
    {
      "epoch": 0.030414781226187005,
      "grad_norm": 1.2305822372436523,
      "learning_rate": 0.00019437419257499036,
      "loss": 0.1149,
      "step": 6466
    },
    {
      "epoch": 0.030419485027799467,
      "grad_norm": 0.9070440530776978,
      "learning_rate": 0.00019437324959687685,
      "loss": 0.2378,
      "step": 6467
    },
    {
      "epoch": 0.03042418882941193,
      "grad_norm": 1.6247786283493042,
      "learning_rate": 0.00019437230661876337,
      "loss": 0.2869,
      "step": 6468
    },
    {
      "epoch": 0.030428892631024395,
      "grad_norm": 1.1523096561431885,
      "learning_rate": 0.0001943713636406499,
      "loss": 0.1518,
      "step": 6469
    },
    {
      "epoch": 0.030433596432636857,
      "grad_norm": 2.732665538787842,
      "learning_rate": 0.00019437042066253643,
      "loss": 0.346,
      "step": 6470
    },
    {
      "epoch": 0.03043830023424932,
      "grad_norm": 0.3808542788028717,
      "learning_rate": 0.00019436947768442295,
      "loss": 0.0339,
      "step": 6471
    },
    {
      "epoch": 0.030443004035861785,
      "grad_norm": 1.2158712148666382,
      "learning_rate": 0.00019436853470630947,
      "loss": 0.1516,
      "step": 6472
    },
    {
      "epoch": 0.030447707837474247,
      "grad_norm": 1.637434720993042,
      "learning_rate": 0.00019436759172819601,
      "loss": 0.4041,
      "step": 6473
    },
    {
      "epoch": 0.03045241163908671,
      "grad_norm": 1.6745898723602295,
      "learning_rate": 0.00019436664875008253,
      "loss": 0.3723,
      "step": 6474
    },
    {
      "epoch": 0.03045711544069917,
      "grad_norm": 0.8733351230621338,
      "learning_rate": 0.00019436570577196905,
      "loss": 0.1212,
      "step": 6475
    },
    {
      "epoch": 0.030461819242311637,
      "grad_norm": 2.961371898651123,
      "learning_rate": 0.00019436476279385557,
      "loss": 0.6505,
      "step": 6476
    },
    {
      "epoch": 0.0304665230439241,
      "grad_norm": 1.520495057106018,
      "learning_rate": 0.0001943638198157421,
      "loss": 0.171,
      "step": 6477
    },
    {
      "epoch": 0.03047122684553656,
      "grad_norm": 0.2952817976474762,
      "learning_rate": 0.0001943628768376286,
      "loss": 0.028,
      "step": 6478
    },
    {
      "epoch": 0.030475930647149027,
      "grad_norm": 0.27833008766174316,
      "learning_rate": 0.00019436193385951513,
      "loss": 0.0239,
      "step": 6479
    },
    {
      "epoch": 0.03048063444876149,
      "grad_norm": 1.102993130683899,
      "learning_rate": 0.00019436099088140165,
      "loss": 0.1625,
      "step": 6480
    },
    {
      "epoch": 0.03048533825037395,
      "grad_norm": 1.3697903156280518,
      "learning_rate": 0.00019436004790328816,
      "loss": 0.134,
      "step": 6481
    },
    {
      "epoch": 0.030490042051986417,
      "grad_norm": 1.2428351640701294,
      "learning_rate": 0.0001943591049251747,
      "loss": 0.113,
      "step": 6482
    },
    {
      "epoch": 0.03049474585359888,
      "grad_norm": 0.4702762961387634,
      "learning_rate": 0.00019435816194706123,
      "loss": 0.0355,
      "step": 6483
    },
    {
      "epoch": 0.03049944965521134,
      "grad_norm": 0.5709445476531982,
      "learning_rate": 0.00019435721896894775,
      "loss": 0.0475,
      "step": 6484
    },
    {
      "epoch": 0.030504153456823804,
      "grad_norm": 2.0182926654815674,
      "learning_rate": 0.00019435627599083427,
      "loss": 0.1912,
      "step": 6485
    },
    {
      "epoch": 0.03050885725843627,
      "grad_norm": 1.290469765663147,
      "learning_rate": 0.00019435533301272078,
      "loss": 0.0985,
      "step": 6486
    },
    {
      "epoch": 0.03051356106004873,
      "grad_norm": 1.284533977508545,
      "learning_rate": 0.0001943543900346073,
      "loss": 0.1003,
      "step": 6487
    },
    {
      "epoch": 0.030518264861661194,
      "grad_norm": 1.2810841798782349,
      "learning_rate": 0.00019435344705649382,
      "loss": 0.1008,
      "step": 6488
    },
    {
      "epoch": 0.03052296866327366,
      "grad_norm": 1.9810082912445068,
      "learning_rate": 0.00019435250407838034,
      "loss": 0.4816,
      "step": 6489
    },
    {
      "epoch": 0.03052767246488612,
      "grad_norm": 1.6197713613510132,
      "learning_rate": 0.00019435156110026686,
      "loss": 0.3212,
      "step": 6490
    },
    {
      "epoch": 0.030532376266498584,
      "grad_norm": 0.38189372420310974,
      "learning_rate": 0.0001943506181221534,
      "loss": 0.0348,
      "step": 6491
    },
    {
      "epoch": 0.030537080068111046,
      "grad_norm": 1.4963899850845337,
      "learning_rate": 0.00019434967514403992,
      "loss": 0.1154,
      "step": 6492
    },
    {
      "epoch": 0.03054178386972351,
      "grad_norm": 1.5933834314346313,
      "learning_rate": 0.00019434873216592644,
      "loss": 0.3824,
      "step": 6493
    },
    {
      "epoch": 0.030546487671335974,
      "grad_norm": 3.2020010948181152,
      "learning_rate": 0.00019434778918781296,
      "loss": 0.6779,
      "step": 6494
    },
    {
      "epoch": 0.030551191472948436,
      "grad_norm": 2.815549850463867,
      "learning_rate": 0.00019434684620969948,
      "loss": 0.2714,
      "step": 6495
    },
    {
      "epoch": 0.0305558952745609,
      "grad_norm": 0.4893445372581482,
      "learning_rate": 0.00019434590323158602,
      "loss": 0.0542,
      "step": 6496
    },
    {
      "epoch": 0.030560599076173364,
      "grad_norm": 1.1884245872497559,
      "learning_rate": 0.00019434496025347254,
      "loss": 0.1391,
      "step": 6497
    },
    {
      "epoch": 0.030565302877785826,
      "grad_norm": 1.8102924823760986,
      "learning_rate": 0.00019434401727535904,
      "loss": 0.4883,
      "step": 6498
    },
    {
      "epoch": 0.03057000667939829,
      "grad_norm": 0.7155305743217468,
      "learning_rate": 0.00019434307429724555,
      "loss": 0.079,
      "step": 6499
    },
    {
      "epoch": 0.030574710481010754,
      "grad_norm": 1.2878248691558838,
      "learning_rate": 0.00019434213131913207,
      "loss": 0.2325,
      "step": 6500
    },
    {
      "epoch": 0.030579414282623216,
      "grad_norm": 2.8340671062469482,
      "learning_rate": 0.00019434118834101862,
      "loss": 0.2436,
      "step": 6501
    },
    {
      "epoch": 0.030584118084235678,
      "grad_norm": 3.066051959991455,
      "learning_rate": 0.00019434024536290514,
      "loss": 0.5867,
      "step": 6502
    },
    {
      "epoch": 0.030588821885848144,
      "grad_norm": 0.07652368396520615,
      "learning_rate": 0.00019433930238479166,
      "loss": 0.0026,
      "step": 6503
    },
    {
      "epoch": 0.030593525687460606,
      "grad_norm": 3.754725456237793,
      "learning_rate": 0.00019433835940667817,
      "loss": 0.3865,
      "step": 6504
    },
    {
      "epoch": 0.030598229489073068,
      "grad_norm": 3.8136937618255615,
      "learning_rate": 0.00019433741642856472,
      "loss": 0.3149,
      "step": 6505
    },
    {
      "epoch": 0.030602933290685534,
      "grad_norm": 1.172081708908081,
      "learning_rate": 0.00019433647345045124,
      "loss": 0.1282,
      "step": 6506
    },
    {
      "epoch": 0.030607637092297996,
      "grad_norm": 0.9541491866111755,
      "learning_rate": 0.00019433553047233776,
      "loss": 0.0985,
      "step": 6507
    },
    {
      "epoch": 0.030612340893910458,
      "grad_norm": 1.8047999143600464,
      "learning_rate": 0.00019433458749422428,
      "loss": 0.1299,
      "step": 6508
    },
    {
      "epoch": 0.03061704469552292,
      "grad_norm": 1.415100336074829,
      "learning_rate": 0.00019433364451611077,
      "loss": 0.1628,
      "step": 6509
    },
    {
      "epoch": 0.030621748497135386,
      "grad_norm": 0.8035449385643005,
      "learning_rate": 0.0001943327015379973,
      "loss": 0.0669,
      "step": 6510
    },
    {
      "epoch": 0.030626452298747848,
      "grad_norm": 4.456322193145752,
      "learning_rate": 0.00019433175855988383,
      "loss": 0.7177,
      "step": 6511
    },
    {
      "epoch": 0.03063115610036031,
      "grad_norm": 2.0534820556640625,
      "learning_rate": 0.00019433081558177035,
      "loss": 0.5614,
      "step": 6512
    },
    {
      "epoch": 0.030635859901972776,
      "grad_norm": 0.3998655378818512,
      "learning_rate": 0.00019432987260365687,
      "loss": 0.0355,
      "step": 6513
    },
    {
      "epoch": 0.030640563703585238,
      "grad_norm": 0.48593801259994507,
      "learning_rate": 0.00019432892962554341,
      "loss": 0.042,
      "step": 6514
    },
    {
      "epoch": 0.0306452675051977,
      "grad_norm": 1.2043172121047974,
      "learning_rate": 0.00019432798664742993,
      "loss": 0.0864,
      "step": 6515
    },
    {
      "epoch": 0.030649971306810166,
      "grad_norm": 2.145203113555908,
      "learning_rate": 0.00019432704366931645,
      "loss": 0.1639,
      "step": 6516
    },
    {
      "epoch": 0.030654675108422628,
      "grad_norm": 3.6995174884796143,
      "learning_rate": 0.00019432610069120297,
      "loss": 0.4537,
      "step": 6517
    },
    {
      "epoch": 0.03065937891003509,
      "grad_norm": 1.2653405666351318,
      "learning_rate": 0.0001943251577130895,
      "loss": 0.2706,
      "step": 6518
    },
    {
      "epoch": 0.030664082711647552,
      "grad_norm": 2.973998546600342,
      "learning_rate": 0.000194324214734976,
      "loss": 0.3419,
      "step": 6519
    },
    {
      "epoch": 0.030668786513260018,
      "grad_norm": 0.8546651601791382,
      "learning_rate": 0.00019432327175686253,
      "loss": 0.0808,
      "step": 6520
    },
    {
      "epoch": 0.03067349031487248,
      "grad_norm": 0.22938930988311768,
      "learning_rate": 0.00019432232877874905,
      "loss": 0.0218,
      "step": 6521
    },
    {
      "epoch": 0.030678194116484942,
      "grad_norm": 2.247527837753296,
      "learning_rate": 0.00019432138580063556,
      "loss": 0.2102,
      "step": 6522
    },
    {
      "epoch": 0.030682897918097408,
      "grad_norm": 5.10119104385376,
      "learning_rate": 0.0001943204428225221,
      "loss": 0.3591,
      "step": 6523
    },
    {
      "epoch": 0.03068760171970987,
      "grad_norm": 1.641451358795166,
      "learning_rate": 0.00019431949984440863,
      "loss": 0.4374,
      "step": 6524
    },
    {
      "epoch": 0.030692305521322332,
      "grad_norm": 0.5923657417297363,
      "learning_rate": 0.00019431855686629515,
      "loss": 0.0474,
      "step": 6525
    },
    {
      "epoch": 0.030697009322934794,
      "grad_norm": 3.848423719406128,
      "learning_rate": 0.00019431761388818167,
      "loss": 0.4198,
      "step": 6526
    },
    {
      "epoch": 0.03070171312454726,
      "grad_norm": 0.5744450092315674,
      "learning_rate": 0.00019431667091006818,
      "loss": 0.0289,
      "step": 6527
    },
    {
      "epoch": 0.030706416926159722,
      "grad_norm": 1.2393770217895508,
      "learning_rate": 0.00019431572793195473,
      "loss": 0.1577,
      "step": 6528
    },
    {
      "epoch": 0.030711120727772184,
      "grad_norm": 2.971503496170044,
      "learning_rate": 0.00019431478495384122,
      "loss": 0.5125,
      "step": 6529
    },
    {
      "epoch": 0.03071582452938465,
      "grad_norm": 2.8139679431915283,
      "learning_rate": 0.00019431384197572774,
      "loss": 0.3387,
      "step": 6530
    },
    {
      "epoch": 0.030720528330997112,
      "grad_norm": 1.2388744354248047,
      "learning_rate": 0.00019431289899761426,
      "loss": 0.0677,
      "step": 6531
    },
    {
      "epoch": 0.030725232132609574,
      "grad_norm": 2.0627713203430176,
      "learning_rate": 0.0001943119560195008,
      "loss": 0.3691,
      "step": 6532
    },
    {
      "epoch": 0.03072993593422204,
      "grad_norm": 2.5035080909729004,
      "learning_rate": 0.00019431101304138732,
      "loss": 0.1986,
      "step": 6533
    },
    {
      "epoch": 0.030734639735834502,
      "grad_norm": 0.7502789497375488,
      "learning_rate": 0.00019431007006327384,
      "loss": 0.1399,
      "step": 6534
    },
    {
      "epoch": 0.030739343537446964,
      "grad_norm": 3.7273976802825928,
      "learning_rate": 0.00019430912708516036,
      "loss": 0.363,
      "step": 6535
    },
    {
      "epoch": 0.030744047339059426,
      "grad_norm": 1.842814326286316,
      "learning_rate": 0.00019430818410704688,
      "loss": 0.122,
      "step": 6536
    },
    {
      "epoch": 0.030748751140671892,
      "grad_norm": 1.0779694318771362,
      "learning_rate": 0.00019430724112893342,
      "loss": 0.11,
      "step": 6537
    },
    {
      "epoch": 0.030753454942284354,
      "grad_norm": 3.2906394004821777,
      "learning_rate": 0.00019430629815081994,
      "loss": 0.3594,
      "step": 6538
    },
    {
      "epoch": 0.030758158743896816,
      "grad_norm": 1.637792944908142,
      "learning_rate": 0.00019430535517270646,
      "loss": 0.3021,
      "step": 6539
    },
    {
      "epoch": 0.030762862545509282,
      "grad_norm": 1.3170140981674194,
      "learning_rate": 0.00019430441219459295,
      "loss": 0.1904,
      "step": 6540
    },
    {
      "epoch": 0.030767566347121744,
      "grad_norm": 0.40721920132637024,
      "learning_rate": 0.0001943034692164795,
      "loss": 0.05,
      "step": 6541
    },
    {
      "epoch": 0.030772270148734206,
      "grad_norm": 0.7504590749740601,
      "learning_rate": 0.00019430252623836602,
      "loss": 0.1077,
      "step": 6542
    },
    {
      "epoch": 0.03077697395034667,
      "grad_norm": 3.6620709896087646,
      "learning_rate": 0.00019430158326025254,
      "loss": 0.302,
      "step": 6543
    },
    {
      "epoch": 0.030781677751959134,
      "grad_norm": 2.3165652751922607,
      "learning_rate": 0.00019430064028213906,
      "loss": 0.6008,
      "step": 6544
    },
    {
      "epoch": 0.030786381553571596,
      "grad_norm": 1.6465941667556763,
      "learning_rate": 0.00019429969730402557,
      "loss": 0.2274,
      "step": 6545
    },
    {
      "epoch": 0.03079108535518406,
      "grad_norm": 3.5866901874542236,
      "learning_rate": 0.00019429875432591212,
      "loss": 0.1943,
      "step": 6546
    },
    {
      "epoch": 0.030795789156796524,
      "grad_norm": 3.3015408515930176,
      "learning_rate": 0.00019429781134779864,
      "loss": 0.9123,
      "step": 6547
    },
    {
      "epoch": 0.030800492958408986,
      "grad_norm": 2.6780850887298584,
      "learning_rate": 0.00019429686836968516,
      "loss": 0.328,
      "step": 6548
    },
    {
      "epoch": 0.03080519676002145,
      "grad_norm": 1.855486273765564,
      "learning_rate": 0.00019429592539157168,
      "loss": 0.2078,
      "step": 6549
    },
    {
      "epoch": 0.030809900561633914,
      "grad_norm": 4.069692134857178,
      "learning_rate": 0.0001942949824134582,
      "loss": 0.4842,
      "step": 6550
    },
    {
      "epoch": 0.030814604363246376,
      "grad_norm": 2.5629613399505615,
      "learning_rate": 0.0001942940394353447,
      "loss": 0.1328,
      "step": 6551
    },
    {
      "epoch": 0.03081930816485884,
      "grad_norm": 1.7857770919799805,
      "learning_rate": 0.00019429309645723123,
      "loss": 0.1174,
      "step": 6552
    },
    {
      "epoch": 0.0308240119664713,
      "grad_norm": 3.83858060836792,
      "learning_rate": 0.00019429215347911775,
      "loss": 0.8311,
      "step": 6553
    },
    {
      "epoch": 0.030828715768083766,
      "grad_norm": 3.1531753540039062,
      "learning_rate": 0.00019429121050100427,
      "loss": 0.4121,
      "step": 6554
    },
    {
      "epoch": 0.03083341956969623,
      "grad_norm": 2.541322946548462,
      "learning_rate": 0.00019429026752289081,
      "loss": 0.0745,
      "step": 6555
    },
    {
      "epoch": 0.03083812337130869,
      "grad_norm": 1.1135985851287842,
      "learning_rate": 0.00019428932454477733,
      "loss": 0.1418,
      "step": 6556
    },
    {
      "epoch": 0.030842827172921156,
      "grad_norm": 1.2179279327392578,
      "learning_rate": 0.00019428838156666385,
      "loss": 0.2243,
      "step": 6557
    },
    {
      "epoch": 0.03084753097453362,
      "grad_norm": 0.583823561668396,
      "learning_rate": 0.00019428743858855037,
      "loss": 0.0323,
      "step": 6558
    },
    {
      "epoch": 0.03085223477614608,
      "grad_norm": 2.1775848865509033,
      "learning_rate": 0.0001942864956104369,
      "loss": 0.1631,
      "step": 6559
    },
    {
      "epoch": 0.030856938577758543,
      "grad_norm": 0.4462360143661499,
      "learning_rate": 0.0001942855526323234,
      "loss": 0.027,
      "step": 6560
    },
    {
      "epoch": 0.03086164237937101,
      "grad_norm": 1.6900262832641602,
      "learning_rate": 0.00019428460965420993,
      "loss": 0.2254,
      "step": 6561
    },
    {
      "epoch": 0.03086634618098347,
      "grad_norm": 2.1854891777038574,
      "learning_rate": 0.00019428366667609645,
      "loss": 0.2977,
      "step": 6562
    },
    {
      "epoch": 0.030871049982595933,
      "grad_norm": 1.4217666387557983,
      "learning_rate": 0.00019428272369798296,
      "loss": 0.211,
      "step": 6563
    },
    {
      "epoch": 0.0308757537842084,
      "grad_norm": 3.701533317565918,
      "learning_rate": 0.0001942817807198695,
      "loss": 0.4052,
      "step": 6564
    },
    {
      "epoch": 0.03088045758582086,
      "grad_norm": 0.7149052619934082,
      "learning_rate": 0.00019428083774175603,
      "loss": 0.108,
      "step": 6565
    },
    {
      "epoch": 0.030885161387433323,
      "grad_norm": 1.7338674068450928,
      "learning_rate": 0.00019427989476364255,
      "loss": 0.2397,
      "step": 6566
    },
    {
      "epoch": 0.03088986518904579,
      "grad_norm": 1.6680059432983398,
      "learning_rate": 0.00019427895178552907,
      "loss": 0.2299,
      "step": 6567
    },
    {
      "epoch": 0.03089456899065825,
      "grad_norm": 2.4230291843414307,
      "learning_rate": 0.00019427800880741558,
      "loss": 0.4305,
      "step": 6568
    },
    {
      "epoch": 0.030899272792270713,
      "grad_norm": 1.5810812711715698,
      "learning_rate": 0.00019427706582930213,
      "loss": 0.0867,
      "step": 6569
    },
    {
      "epoch": 0.030903976593883175,
      "grad_norm": 2.531686305999756,
      "learning_rate": 0.00019427612285118865,
      "loss": 0.2951,
      "step": 6570
    },
    {
      "epoch": 0.03090868039549564,
      "grad_norm": 0.3922656178474426,
      "learning_rate": 0.00019427517987307514,
      "loss": 0.0306,
      "step": 6571
    },
    {
      "epoch": 0.030913384197108103,
      "grad_norm": 0.7081758975982666,
      "learning_rate": 0.00019427423689496166,
      "loss": 0.0366,
      "step": 6572
    },
    {
      "epoch": 0.030918087998720565,
      "grad_norm": 3.336671829223633,
      "learning_rate": 0.0001942732939168482,
      "loss": 0.5443,
      "step": 6573
    },
    {
      "epoch": 0.03092279180033303,
      "grad_norm": 1.0831305980682373,
      "learning_rate": 0.00019427235093873472,
      "loss": 0.0558,
      "step": 6574
    },
    {
      "epoch": 0.030927495601945493,
      "grad_norm": 3.5300347805023193,
      "learning_rate": 0.00019427140796062124,
      "loss": 0.4995,
      "step": 6575
    },
    {
      "epoch": 0.030932199403557955,
      "grad_norm": 3.675628423690796,
      "learning_rate": 0.00019427046498250776,
      "loss": 0.274,
      "step": 6576
    },
    {
      "epoch": 0.030936903205170417,
      "grad_norm": 5.584899425506592,
      "learning_rate": 0.00019426952200439428,
      "loss": 1.0023,
      "step": 6577
    },
    {
      "epoch": 0.030941607006782883,
      "grad_norm": 3.042238235473633,
      "learning_rate": 0.00019426857902628082,
      "loss": 0.4206,
      "step": 6578
    },
    {
      "epoch": 0.030946310808395345,
      "grad_norm": 1.7206311225891113,
      "learning_rate": 0.00019426763604816734,
      "loss": 0.1114,
      "step": 6579
    },
    {
      "epoch": 0.030951014610007807,
      "grad_norm": 0.7862346768379211,
      "learning_rate": 0.00019426669307005386,
      "loss": 0.0502,
      "step": 6580
    },
    {
      "epoch": 0.030955718411620273,
      "grad_norm": 3.227891206741333,
      "learning_rate": 0.00019426575009194038,
      "loss": 0.653,
      "step": 6581
    },
    {
      "epoch": 0.030960422213232735,
      "grad_norm": 2.216078042984009,
      "learning_rate": 0.0001942648071138269,
      "loss": 0.1719,
      "step": 6582
    },
    {
      "epoch": 0.030965126014845197,
      "grad_norm": 2.3906140327453613,
      "learning_rate": 0.00019426386413571342,
      "loss": 0.2874,
      "step": 6583
    },
    {
      "epoch": 0.030969829816457663,
      "grad_norm": 2.32515287399292,
      "learning_rate": 0.00019426292115759994,
      "loss": 0.3946,
      "step": 6584
    },
    {
      "epoch": 0.030974533618070125,
      "grad_norm": 2.527709484100342,
      "learning_rate": 0.00019426197817948646,
      "loss": 0.1317,
      "step": 6585
    },
    {
      "epoch": 0.030979237419682587,
      "grad_norm": 0.9018152356147766,
      "learning_rate": 0.00019426103520137297,
      "loss": 0.1351,
      "step": 6586
    },
    {
      "epoch": 0.03098394122129505,
      "grad_norm": 1.6771363019943237,
      "learning_rate": 0.00019426009222325952,
      "loss": 0.376,
      "step": 6587
    },
    {
      "epoch": 0.030988645022907515,
      "grad_norm": 0.6152621507644653,
      "learning_rate": 0.00019425914924514604,
      "loss": 0.0543,
      "step": 6588
    },
    {
      "epoch": 0.030993348824519977,
      "grad_norm": 8.493843078613281,
      "learning_rate": 0.00019425820626703256,
      "loss": 1.1279,
      "step": 6589
    },
    {
      "epoch": 0.03099805262613244,
      "grad_norm": 0.1523132175207138,
      "learning_rate": 0.00019425726328891908,
      "loss": 0.0117,
      "step": 6590
    },
    {
      "epoch": 0.031002756427744905,
      "grad_norm": 1.5320560932159424,
      "learning_rate": 0.0001942563203108056,
      "loss": 0.161,
      "step": 6591
    },
    {
      "epoch": 0.031007460229357367,
      "grad_norm": 0.9339213371276855,
      "learning_rate": 0.0001942553773326921,
      "loss": 0.1718,
      "step": 6592
    },
    {
      "epoch": 0.03101216403096983,
      "grad_norm": 0.2574848234653473,
      "learning_rate": 0.00019425443435457863,
      "loss": 0.0176,
      "step": 6593
    },
    {
      "epoch": 0.03101686783258229,
      "grad_norm": 0.46722617745399475,
      "learning_rate": 0.00019425349137646515,
      "loss": 0.0295,
      "step": 6594
    },
    {
      "epoch": 0.031021571634194757,
      "grad_norm": 0.6619762182235718,
      "learning_rate": 0.00019425254839835167,
      "loss": 0.0611,
      "step": 6595
    },
    {
      "epoch": 0.03102627543580722,
      "grad_norm": 2.62650203704834,
      "learning_rate": 0.00019425160542023821,
      "loss": 0.3317,
      "step": 6596
    },
    {
      "epoch": 0.03103097923741968,
      "grad_norm": 4.650107383728027,
      "learning_rate": 0.00019425066244212473,
      "loss": 0.3779,
      "step": 6597
    },
    {
      "epoch": 0.031035683039032147,
      "grad_norm": 0.7446341514587402,
      "learning_rate": 0.00019424971946401125,
      "loss": 0.0511,
      "step": 6598
    },
    {
      "epoch": 0.03104038684064461,
      "grad_norm": 0.6691493391990662,
      "learning_rate": 0.00019424877648589777,
      "loss": 0.036,
      "step": 6599
    },
    {
      "epoch": 0.03104509064225707,
      "grad_norm": 1.1466214656829834,
      "learning_rate": 0.0001942478335077843,
      "loss": 0.105,
      "step": 6600
    },
    {
      "epoch": 0.031049794443869537,
      "grad_norm": 2.096674919128418,
      "learning_rate": 0.00019424689052967083,
      "loss": 0.095,
      "step": 6601
    },
    {
      "epoch": 0.031054498245482,
      "grad_norm": 1.9316456317901611,
      "learning_rate": 0.00019424594755155733,
      "loss": 0.0845,
      "step": 6602
    },
    {
      "epoch": 0.03105920204709446,
      "grad_norm": 4.758993148803711,
      "learning_rate": 0.00019424500457344385,
      "loss": 0.462,
      "step": 6603
    },
    {
      "epoch": 0.031063905848706923,
      "grad_norm": 5.590376853942871,
      "learning_rate": 0.00019424406159533036,
      "loss": 0.5507,
      "step": 6604
    },
    {
      "epoch": 0.03106860965031939,
      "grad_norm": 3.778576135635376,
      "learning_rate": 0.0001942431186172169,
      "loss": 0.3225,
      "step": 6605
    },
    {
      "epoch": 0.03107331345193185,
      "grad_norm": 4.153085231781006,
      "learning_rate": 0.00019424217563910343,
      "loss": 0.2719,
      "step": 6606
    },
    {
      "epoch": 0.031078017253544313,
      "grad_norm": 3.9762401580810547,
      "learning_rate": 0.00019424123266098995,
      "loss": 0.2818,
      "step": 6607
    },
    {
      "epoch": 0.03108272105515678,
      "grad_norm": 0.4202956557273865,
      "learning_rate": 0.00019424028968287647,
      "loss": 0.016,
      "step": 6608
    },
    {
      "epoch": 0.03108742485676924,
      "grad_norm": 8.615387916564941,
      "learning_rate": 0.00019423934670476298,
      "loss": 0.7814,
      "step": 6609
    },
    {
      "epoch": 0.031092128658381703,
      "grad_norm": 4.339492321014404,
      "learning_rate": 0.00019423840372664953,
      "loss": 0.4892,
      "step": 6610
    },
    {
      "epoch": 0.031096832459994166,
      "grad_norm": 4.834526538848877,
      "learning_rate": 0.00019423746074853605,
      "loss": 0.7223,
      "step": 6611
    },
    {
      "epoch": 0.03110153626160663,
      "grad_norm": 4.493717670440674,
      "learning_rate": 0.00019423651777042257,
      "loss": 0.4698,
      "step": 6612
    },
    {
      "epoch": 0.031106240063219093,
      "grad_norm": 7.173920154571533,
      "learning_rate": 0.00019423557479230906,
      "loss": 0.9116,
      "step": 6613
    },
    {
      "epoch": 0.031110943864831556,
      "grad_norm": 2.12741756439209,
      "learning_rate": 0.0001942346318141956,
      "loss": 0.2007,
      "step": 6614
    },
    {
      "epoch": 0.03111564766644402,
      "grad_norm": 1.2516952753067017,
      "learning_rate": 0.00019423368883608212,
      "loss": 0.0933,
      "step": 6615
    },
    {
      "epoch": 0.031120351468056483,
      "grad_norm": 1.4360240697860718,
      "learning_rate": 0.00019423274585796864,
      "loss": 0.1558,
      "step": 6616
    },
    {
      "epoch": 0.031125055269668946,
      "grad_norm": 2.7349050045013428,
      "learning_rate": 0.00019423180287985516,
      "loss": 0.3235,
      "step": 6617
    },
    {
      "epoch": 0.03112975907128141,
      "grad_norm": 0.11193085461854935,
      "learning_rate": 0.00019423085990174168,
      "loss": 0.0062,
      "step": 6618
    },
    {
      "epoch": 0.031134462872893873,
      "grad_norm": 2.8200783729553223,
      "learning_rate": 0.00019422991692362822,
      "loss": 0.2695,
      "step": 6619
    },
    {
      "epoch": 0.031139166674506336,
      "grad_norm": 1.545461654663086,
      "learning_rate": 0.00019422897394551474,
      "loss": 0.0967,
      "step": 6620
    },
    {
      "epoch": 0.031143870476118798,
      "grad_norm": 1.429455280303955,
      "learning_rate": 0.00019422803096740126,
      "loss": 0.1443,
      "step": 6621
    },
    {
      "epoch": 0.031148574277731263,
      "grad_norm": 6.881253719329834,
      "learning_rate": 0.00019422708798928778,
      "loss": 1.3457,
      "step": 6622
    },
    {
      "epoch": 0.031153278079343726,
      "grad_norm": 0.2963370084762573,
      "learning_rate": 0.0001942261450111743,
      "loss": 0.0162,
      "step": 6623
    },
    {
      "epoch": 0.031157981880956188,
      "grad_norm": 1.8700584173202515,
      "learning_rate": 0.00019422520203306082,
      "loss": 0.2434,
      "step": 6624
    },
    {
      "epoch": 0.031162685682568653,
      "grad_norm": 1.0685570240020752,
      "learning_rate": 0.00019422425905494734,
      "loss": 0.0837,
      "step": 6625
    },
    {
      "epoch": 0.031167389484181116,
      "grad_norm": 3.79141902923584,
      "learning_rate": 0.00019422331607683386,
      "loss": 0.4879,
      "step": 6626
    },
    {
      "epoch": 0.031172093285793578,
      "grad_norm": 1.5587608814239502,
      "learning_rate": 0.00019422237309872037,
      "loss": 0.0666,
      "step": 6627
    },
    {
      "epoch": 0.03117679708740604,
      "grad_norm": 3.211846113204956,
      "learning_rate": 0.00019422143012060692,
      "loss": 0.5951,
      "step": 6628
    },
    {
      "epoch": 0.031181500889018506,
      "grad_norm": 3.3209381103515625,
      "learning_rate": 0.00019422048714249344,
      "loss": 0.3172,
      "step": 6629
    },
    {
      "epoch": 0.031186204690630968,
      "grad_norm": 1.5374805927276611,
      "learning_rate": 0.00019421954416437996,
      "loss": 0.154,
      "step": 6630
    },
    {
      "epoch": 0.03119090849224343,
      "grad_norm": 1.460212230682373,
      "learning_rate": 0.00019421860118626648,
      "loss": 0.1106,
      "step": 6631
    },
    {
      "epoch": 0.031195612293855896,
      "grad_norm": 1.8616998195648193,
      "learning_rate": 0.00019421765820815302,
      "loss": 0.2293,
      "step": 6632
    },
    {
      "epoch": 0.031200316095468358,
      "grad_norm": 1.79591965675354,
      "learning_rate": 0.0001942167152300395,
      "loss": 0.2307,
      "step": 6633
    },
    {
      "epoch": 0.03120501989708082,
      "grad_norm": 3.8191394805908203,
      "learning_rate": 0.00019421577225192603,
      "loss": 0.636,
      "step": 6634
    },
    {
      "epoch": 0.031209723698693285,
      "grad_norm": 1.9432281255722046,
      "learning_rate": 0.00019421482927381255,
      "loss": 0.2373,
      "step": 6635
    },
    {
      "epoch": 0.031214427500305748,
      "grad_norm": 1.9655271768569946,
      "learning_rate": 0.00019421388629569907,
      "loss": 0.2681,
      "step": 6636
    },
    {
      "epoch": 0.03121913130191821,
      "grad_norm": 1.1623469591140747,
      "learning_rate": 0.00019421294331758561,
      "loss": 0.1113,
      "step": 6637
    },
    {
      "epoch": 0.031223835103530672,
      "grad_norm": 2.814448833465576,
      "learning_rate": 0.00019421200033947213,
      "loss": 0.255,
      "step": 6638
    },
    {
      "epoch": 0.031228538905143138,
      "grad_norm": 1.141015648841858,
      "learning_rate": 0.00019421105736135865,
      "loss": 0.0879,
      "step": 6639
    },
    {
      "epoch": 0.0312332427067556,
      "grad_norm": 0.5641154646873474,
      "learning_rate": 0.00019421011438324517,
      "loss": 0.0313,
      "step": 6640
    },
    {
      "epoch": 0.031237946508368062,
      "grad_norm": 0.608486533164978,
      "learning_rate": 0.00019420917140513172,
      "loss": 0.0437,
      "step": 6641
    },
    {
      "epoch": 0.031242650309980528,
      "grad_norm": 5.399486064910889,
      "learning_rate": 0.00019420822842701823,
      "loss": 0.945,
      "step": 6642
    },
    {
      "epoch": 0.03124735411159299,
      "grad_norm": 3.530827522277832,
      "learning_rate": 0.00019420728544890475,
      "loss": 0.4562,
      "step": 6643
    },
    {
      "epoch": 0.031252057913205455,
      "grad_norm": 1.581001877784729,
      "learning_rate": 0.00019420634247079125,
      "loss": 0.1431,
      "step": 6644
    },
    {
      "epoch": 0.031256761714817914,
      "grad_norm": 2.5212056636810303,
      "learning_rate": 0.00019420539949267776,
      "loss": 0.2676,
      "step": 6645
    },
    {
      "epoch": 0.03126146551643038,
      "grad_norm": 1.9400033950805664,
      "learning_rate": 0.0001942044565145643,
      "loss": 0.2808,
      "step": 6646
    },
    {
      "epoch": 0.031266169318042845,
      "grad_norm": 3.166008949279785,
      "learning_rate": 0.00019420351353645083,
      "loss": 0.6288,
      "step": 6647
    },
    {
      "epoch": 0.031270873119655304,
      "grad_norm": 1.207938551902771,
      "learning_rate": 0.00019420257055833735,
      "loss": 0.2135,
      "step": 6648
    },
    {
      "epoch": 0.03127557692126777,
      "grad_norm": 0.9384145140647888,
      "learning_rate": 0.00019420162758022387,
      "loss": 0.0602,
      "step": 6649
    },
    {
      "epoch": 0.03128028072288023,
      "grad_norm": 0.7565354704856873,
      "learning_rate": 0.0001942006846021104,
      "loss": 0.0602,
      "step": 6650
    },
    {
      "epoch": 0.031284984524492694,
      "grad_norm": 3.8423869609832764,
      "learning_rate": 0.00019419974162399693,
      "loss": 0.6535,
      "step": 6651
    },
    {
      "epoch": 0.03128968832610516,
      "grad_norm": 4.908348083496094,
      "learning_rate": 0.00019419879864588345,
      "loss": 0.9906,
      "step": 6652
    },
    {
      "epoch": 0.03129439212771762,
      "grad_norm": 3.2702524662017822,
      "learning_rate": 0.00019419785566776997,
      "loss": 0.514,
      "step": 6653
    },
    {
      "epoch": 0.031299095929330084,
      "grad_norm": 1.5089452266693115,
      "learning_rate": 0.00019419691268965649,
      "loss": 0.1492,
      "step": 6654
    },
    {
      "epoch": 0.03130379973094255,
      "grad_norm": 1.058898687362671,
      "learning_rate": 0.000194195969711543,
      "loss": 0.1809,
      "step": 6655
    },
    {
      "epoch": 0.03130850353255501,
      "grad_norm": 3.309206008911133,
      "learning_rate": 0.00019419502673342952,
      "loss": 0.3907,
      "step": 6656
    },
    {
      "epoch": 0.031313207334167474,
      "grad_norm": 3.7562618255615234,
      "learning_rate": 0.00019419408375531604,
      "loss": 0.4826,
      "step": 6657
    },
    {
      "epoch": 0.03131791113577994,
      "grad_norm": 2.55957293510437,
      "learning_rate": 0.00019419314077720256,
      "loss": 0.4844,
      "step": 6658
    },
    {
      "epoch": 0.0313226149373924,
      "grad_norm": 1.2876683473587036,
      "learning_rate": 0.00019419219779908908,
      "loss": 0.1647,
      "step": 6659
    },
    {
      "epoch": 0.031327318739004864,
      "grad_norm": 1.0029703378677368,
      "learning_rate": 0.00019419125482097562,
      "loss": 0.1852,
      "step": 6660
    },
    {
      "epoch": 0.03133202254061733,
      "grad_norm": 1.7859522104263306,
      "learning_rate": 0.00019419031184286214,
      "loss": 0.2517,
      "step": 6661
    },
    {
      "epoch": 0.03133672634222979,
      "grad_norm": 2.0412755012512207,
      "learning_rate": 0.00019418936886474866,
      "loss": 0.3607,
      "step": 6662
    },
    {
      "epoch": 0.031341430143842254,
      "grad_norm": 2.0505518913269043,
      "learning_rate": 0.00019418842588663518,
      "loss": 0.4005,
      "step": 6663
    },
    {
      "epoch": 0.03134613394545472,
      "grad_norm": 2.0685932636260986,
      "learning_rate": 0.0001941874829085217,
      "loss": 0.3805,
      "step": 6664
    },
    {
      "epoch": 0.03135083774706718,
      "grad_norm": 3.4678664207458496,
      "learning_rate": 0.00019418653993040822,
      "loss": 0.257,
      "step": 6665
    },
    {
      "epoch": 0.031355541548679644,
      "grad_norm": 3.2374188899993896,
      "learning_rate": 0.00019418559695229474,
      "loss": 0.5154,
      "step": 6666
    },
    {
      "epoch": 0.0313602453502921,
      "grad_norm": 1.5368473529815674,
      "learning_rate": 0.00019418465397418126,
      "loss": 0.2408,
      "step": 6667
    },
    {
      "epoch": 0.03136494915190457,
      "grad_norm": 1.1576504707336426,
      "learning_rate": 0.00019418371099606777,
      "loss": 0.163,
      "step": 6668
    },
    {
      "epoch": 0.031369652953517034,
      "grad_norm": 3.1283137798309326,
      "learning_rate": 0.00019418276801795432,
      "loss": 0.5648,
      "step": 6669
    },
    {
      "epoch": 0.03137435675512949,
      "grad_norm": 2.6192972660064697,
      "learning_rate": 0.00019418182503984084,
      "loss": 0.5088,
      "step": 6670
    },
    {
      "epoch": 0.03137906055674196,
      "grad_norm": 0.7328169941902161,
      "learning_rate": 0.00019418088206172736,
      "loss": 0.067,
      "step": 6671
    },
    {
      "epoch": 0.031383764358354424,
      "grad_norm": 1.617835283279419,
      "learning_rate": 0.00019417993908361388,
      "loss": 0.3064,
      "step": 6672
    },
    {
      "epoch": 0.03138846815996688,
      "grad_norm": 2.611680746078491,
      "learning_rate": 0.00019417899610550042,
      "loss": 0.4125,
      "step": 6673
    },
    {
      "epoch": 0.03139317196157935,
      "grad_norm": 4.029139041900635,
      "learning_rate": 0.00019417805312738694,
      "loss": 0.8124,
      "step": 6674
    },
    {
      "epoch": 0.031397875763191814,
      "grad_norm": 2.0367045402526855,
      "learning_rate": 0.00019417711014927343,
      "loss": 0.422,
      "step": 6675
    },
    {
      "epoch": 0.03140257956480427,
      "grad_norm": 1.825591802597046,
      "learning_rate": 0.00019417616717115995,
      "loss": 0.2198,
      "step": 6676
    },
    {
      "epoch": 0.03140728336641674,
      "grad_norm": 1.8974355459213257,
      "learning_rate": 0.00019417522419304647,
      "loss": 0.2948,
      "step": 6677
    },
    {
      "epoch": 0.031411987168029204,
      "grad_norm": 1.6206514835357666,
      "learning_rate": 0.00019417428121493301,
      "loss": 0.1784,
      "step": 6678
    },
    {
      "epoch": 0.03141669096964166,
      "grad_norm": 1.6853080987930298,
      "learning_rate": 0.00019417333823681953,
      "loss": 0.1832,
      "step": 6679
    },
    {
      "epoch": 0.03142139477125413,
      "grad_norm": 1.2099182605743408,
      "learning_rate": 0.00019417239525870605,
      "loss": 0.3638,
      "step": 6680
    },
    {
      "epoch": 0.031426098572866594,
      "grad_norm": 2.103053569793701,
      "learning_rate": 0.00019417145228059257,
      "loss": 0.4572,
      "step": 6681
    },
    {
      "epoch": 0.03143080237447905,
      "grad_norm": 2.185175895690918,
      "learning_rate": 0.00019417050930247912,
      "loss": 0.4638,
      "step": 6682
    },
    {
      "epoch": 0.03143550617609152,
      "grad_norm": 1.7028262615203857,
      "learning_rate": 0.00019416956632436563,
      "loss": 0.2087,
      "step": 6683
    },
    {
      "epoch": 0.03144020997770398,
      "grad_norm": 1.3190425634384155,
      "learning_rate": 0.00019416862334625215,
      "loss": 0.0592,
      "step": 6684
    },
    {
      "epoch": 0.03144491377931644,
      "grad_norm": 1.493847131729126,
      "learning_rate": 0.00019416768036813867,
      "loss": 0.2667,
      "step": 6685
    },
    {
      "epoch": 0.03144961758092891,
      "grad_norm": 1.6522181034088135,
      "learning_rate": 0.0001941667373900252,
      "loss": 0.2445,
      "step": 6686
    },
    {
      "epoch": 0.03145432138254137,
      "grad_norm": 2.4990134239196777,
      "learning_rate": 0.0001941657944119117,
      "loss": 0.5121,
      "step": 6687
    },
    {
      "epoch": 0.03145902518415383,
      "grad_norm": 0.5197243690490723,
      "learning_rate": 0.00019416485143379823,
      "loss": 0.0534,
      "step": 6688
    },
    {
      "epoch": 0.0314637289857663,
      "grad_norm": 4.1208577156066895,
      "learning_rate": 0.00019416390845568475,
      "loss": 0.3434,
      "step": 6689
    },
    {
      "epoch": 0.03146843278737876,
      "grad_norm": 2.1183598041534424,
      "learning_rate": 0.00019416296547757127,
      "loss": 0.1994,
      "step": 6690
    },
    {
      "epoch": 0.03147313658899122,
      "grad_norm": 1.4639700651168823,
      "learning_rate": 0.0001941620224994578,
      "loss": 0.2838,
      "step": 6691
    },
    {
      "epoch": 0.03147784039060369,
      "grad_norm": 1.1118009090423584,
      "learning_rate": 0.00019416107952134433,
      "loss": 0.1371,
      "step": 6692
    },
    {
      "epoch": 0.03148254419221615,
      "grad_norm": 2.472261428833008,
      "learning_rate": 0.00019416013654323085,
      "loss": 0.3573,
      "step": 6693
    },
    {
      "epoch": 0.03148724799382861,
      "grad_norm": 1.1034059524536133,
      "learning_rate": 0.00019415919356511737,
      "loss": 0.1447,
      "step": 6694
    },
    {
      "epoch": 0.03149195179544108,
      "grad_norm": 0.5702904462814331,
      "learning_rate": 0.00019415825058700389,
      "loss": 0.0518,
      "step": 6695
    },
    {
      "epoch": 0.03149665559705354,
      "grad_norm": 1.7408447265625,
      "learning_rate": 0.0001941573076088904,
      "loss": 0.2122,
      "step": 6696
    },
    {
      "epoch": 0.031501359398666,
      "grad_norm": 0.4517189562320709,
      "learning_rate": 0.00019415636463077692,
      "loss": 0.0525,
      "step": 6697
    },
    {
      "epoch": 0.03150606320027847,
      "grad_norm": 3.4413583278656006,
      "learning_rate": 0.00019415542165266344,
      "loss": 0.6554,
      "step": 6698
    },
    {
      "epoch": 0.03151076700189093,
      "grad_norm": 2.249443292617798,
      "learning_rate": 0.00019415447867454996,
      "loss": 0.3326,
      "step": 6699
    },
    {
      "epoch": 0.03151547080350339,
      "grad_norm": 1.3445374965667725,
      "learning_rate": 0.0001941535356964365,
      "loss": 0.2202,
      "step": 6700
    },
    {
      "epoch": 0.03152017460511585,
      "grad_norm": 4.612929821014404,
      "learning_rate": 0.00019415259271832302,
      "loss": 0.6141,
      "step": 6701
    },
    {
      "epoch": 0.03152487840672832,
      "grad_norm": 1.73894202709198,
      "learning_rate": 0.00019415164974020954,
      "loss": 0.1728,
      "step": 6702
    },
    {
      "epoch": 0.03152958220834078,
      "grad_norm": 4.371235370635986,
      "learning_rate": 0.00019415070676209606,
      "loss": 0.5373,
      "step": 6703
    },
    {
      "epoch": 0.03153428600995324,
      "grad_norm": 1.6379179954528809,
      "learning_rate": 0.00019414976378398258,
      "loss": 0.148,
      "step": 6704
    },
    {
      "epoch": 0.03153898981156571,
      "grad_norm": 5.7830657958984375,
      "learning_rate": 0.00019414882080586913,
      "loss": 1.1072,
      "step": 6705
    },
    {
      "epoch": 0.03154369361317817,
      "grad_norm": 2.2323355674743652,
      "learning_rate": 0.00019414787782775562,
      "loss": 0.0975,
      "step": 6706
    },
    {
      "epoch": 0.03154839741479063,
      "grad_norm": 1.1683284044265747,
      "learning_rate": 0.00019414693484964214,
      "loss": 0.1156,
      "step": 6707
    },
    {
      "epoch": 0.0315531012164031,
      "grad_norm": 2.2706151008605957,
      "learning_rate": 0.00019414599187152866,
      "loss": 0.4091,
      "step": 6708
    },
    {
      "epoch": 0.03155780501801556,
      "grad_norm": 0.84249347448349,
      "learning_rate": 0.00019414504889341517,
      "loss": 0.0727,
      "step": 6709
    },
    {
      "epoch": 0.03156250881962802,
      "grad_norm": 1.152107834815979,
      "learning_rate": 0.00019414410591530172,
      "loss": 0.1279,
      "step": 6710
    },
    {
      "epoch": 0.03156721262124049,
      "grad_norm": 1.8799717426300049,
      "learning_rate": 0.00019414316293718824,
      "loss": 0.2026,
      "step": 6711
    },
    {
      "epoch": 0.03157191642285295,
      "grad_norm": 1.8285523653030396,
      "learning_rate": 0.00019414221995907476,
      "loss": 0.224,
      "step": 6712
    },
    {
      "epoch": 0.03157662022446541,
      "grad_norm": 9.789427757263184,
      "learning_rate": 0.00019414127698096128,
      "loss": 0.3549,
      "step": 6713
    },
    {
      "epoch": 0.03158132402607788,
      "grad_norm": 3.4372658729553223,
      "learning_rate": 0.00019414033400284782,
      "loss": 0.6406,
      "step": 6714
    },
    {
      "epoch": 0.03158602782769034,
      "grad_norm": 0.8253321051597595,
      "learning_rate": 0.00019413939102473434,
      "loss": 0.0768,
      "step": 6715
    },
    {
      "epoch": 0.0315907316293028,
      "grad_norm": 0.814440906047821,
      "learning_rate": 0.00019413844804662086,
      "loss": 0.0797,
      "step": 6716
    },
    {
      "epoch": 0.03159543543091527,
      "grad_norm": 1.595234751701355,
      "learning_rate": 0.00019413750506850738,
      "loss": 0.19,
      "step": 6717
    },
    {
      "epoch": 0.031600139232527726,
      "grad_norm": 2.8996267318725586,
      "learning_rate": 0.00019413656209039387,
      "loss": 0.2531,
      "step": 6718
    },
    {
      "epoch": 0.03160484303414019,
      "grad_norm": 2.9894676208496094,
      "learning_rate": 0.00019413561911228041,
      "loss": 0.5944,
      "step": 6719
    },
    {
      "epoch": 0.03160954683575266,
      "grad_norm": 0.672939121723175,
      "learning_rate": 0.00019413467613416693,
      "loss": 0.0478,
      "step": 6720
    },
    {
      "epoch": 0.031614250637365116,
      "grad_norm": 4.9442315101623535,
      "learning_rate": 0.00019413373315605345,
      "loss": 0.8437,
      "step": 6721
    },
    {
      "epoch": 0.03161895443897758,
      "grad_norm": 4.553886413574219,
      "learning_rate": 0.00019413279017793997,
      "loss": 0.8367,
      "step": 6722
    },
    {
      "epoch": 0.03162365824059005,
      "grad_norm": 2.2621169090270996,
      "learning_rate": 0.00019413184719982652,
      "loss": 0.1931,
      "step": 6723
    },
    {
      "epoch": 0.031628362042202506,
      "grad_norm": 3.553635835647583,
      "learning_rate": 0.00019413090422171303,
      "loss": 0.3586,
      "step": 6724
    },
    {
      "epoch": 0.03163306584381497,
      "grad_norm": 3.6549794673919678,
      "learning_rate": 0.00019412996124359955,
      "loss": 0.699,
      "step": 6725
    },
    {
      "epoch": 0.03163776964542744,
      "grad_norm": 3.1197874546051025,
      "learning_rate": 0.00019412901826548607,
      "loss": 0.5193,
      "step": 6726
    },
    {
      "epoch": 0.031642473447039896,
      "grad_norm": 2.4692511558532715,
      "learning_rate": 0.0001941280752873726,
      "loss": 0.4391,
      "step": 6727
    },
    {
      "epoch": 0.03164717724865236,
      "grad_norm": 1.893418550491333,
      "learning_rate": 0.0001941271323092591,
      "loss": 0.3601,
      "step": 6728
    },
    {
      "epoch": 0.03165188105026483,
      "grad_norm": 1.919053077697754,
      "learning_rate": 0.00019412618933114563,
      "loss": 0.3126,
      "step": 6729
    },
    {
      "epoch": 0.031656584851877285,
      "grad_norm": 1.0446540117263794,
      "learning_rate": 0.00019412524635303215,
      "loss": 0.1043,
      "step": 6730
    },
    {
      "epoch": 0.03166128865348975,
      "grad_norm": 1.5572820901870728,
      "learning_rate": 0.00019412430337491867,
      "loss": 0.2485,
      "step": 6731
    },
    {
      "epoch": 0.03166599245510222,
      "grad_norm": 2.504192352294922,
      "learning_rate": 0.0001941233603968052,
      "loss": 0.2593,
      "step": 6732
    },
    {
      "epoch": 0.031670696256714675,
      "grad_norm": 2.7241148948669434,
      "learning_rate": 0.00019412241741869173,
      "loss": 0.5336,
      "step": 6733
    },
    {
      "epoch": 0.03167540005832714,
      "grad_norm": 1.267582654953003,
      "learning_rate": 0.00019412147444057825,
      "loss": 0.2452,
      "step": 6734
    },
    {
      "epoch": 0.0316801038599396,
      "grad_norm": 0.9156602621078491,
      "learning_rate": 0.00019412053146246477,
      "loss": 0.1983,
      "step": 6735
    },
    {
      "epoch": 0.031684807661552065,
      "grad_norm": 1.5564508438110352,
      "learning_rate": 0.00019411958848435129,
      "loss": 0.3086,
      "step": 6736
    },
    {
      "epoch": 0.03168951146316453,
      "grad_norm": 1.5938241481781006,
      "learning_rate": 0.0001941186455062378,
      "loss": 0.2948,
      "step": 6737
    },
    {
      "epoch": 0.03169421526477699,
      "grad_norm": 1.1308213472366333,
      "learning_rate": 0.00019411770252812432,
      "loss": 0.1523,
      "step": 6738
    },
    {
      "epoch": 0.031698919066389455,
      "grad_norm": 0.7039918899536133,
      "learning_rate": 0.00019411675955001084,
      "loss": 0.1433,
      "step": 6739
    },
    {
      "epoch": 0.03170362286800192,
      "grad_norm": 1.4290815591812134,
      "learning_rate": 0.00019411581657189736,
      "loss": 0.343,
      "step": 6740
    },
    {
      "epoch": 0.03170832666961438,
      "grad_norm": 1.9827752113342285,
      "learning_rate": 0.0001941148735937839,
      "loss": 0.2235,
      "step": 6741
    },
    {
      "epoch": 0.031713030471226845,
      "grad_norm": 1.569754719734192,
      "learning_rate": 0.00019411393061567042,
      "loss": 0.2827,
      "step": 6742
    },
    {
      "epoch": 0.03171773427283931,
      "grad_norm": 0.4904095530509949,
      "learning_rate": 0.00019411298763755694,
      "loss": 0.0544,
      "step": 6743
    },
    {
      "epoch": 0.03172243807445177,
      "grad_norm": 1.9944623708724976,
      "learning_rate": 0.00019411204465944346,
      "loss": 0.426,
      "step": 6744
    },
    {
      "epoch": 0.031727141876064235,
      "grad_norm": 1.65156888961792,
      "learning_rate": 0.00019411110168132998,
      "loss": 0.4326,
      "step": 6745
    },
    {
      "epoch": 0.0317318456776767,
      "grad_norm": 1.0627471208572388,
      "learning_rate": 0.00019411015870321653,
      "loss": 0.1303,
      "step": 6746
    },
    {
      "epoch": 0.03173654947928916,
      "grad_norm": 2.3572065830230713,
      "learning_rate": 0.00019410921572510304,
      "loss": 0.3224,
      "step": 6747
    },
    {
      "epoch": 0.031741253280901625,
      "grad_norm": 2.892616033554077,
      "learning_rate": 0.00019410827274698956,
      "loss": 0.5065,
      "step": 6748
    },
    {
      "epoch": 0.03174595708251409,
      "grad_norm": 2.0429205894470215,
      "learning_rate": 0.00019410732976887606,
      "loss": 0.2459,
      "step": 6749
    },
    {
      "epoch": 0.03175066088412655,
      "grad_norm": 5.214649677276611,
      "learning_rate": 0.0001941063867907626,
      "loss": 1.1634,
      "step": 6750
    },
    {
      "epoch": 0.031755364685739015,
      "grad_norm": 3.7942731380462646,
      "learning_rate": 0.00019410544381264912,
      "loss": 0.2462,
      "step": 6751
    },
    {
      "epoch": 0.031760068487351474,
      "grad_norm": 2.8542401790618896,
      "learning_rate": 0.00019410450083453564,
      "loss": 0.447,
      "step": 6752
    },
    {
      "epoch": 0.03176477228896394,
      "grad_norm": 3.683454751968384,
      "learning_rate": 0.00019410355785642216,
      "loss": 0.8455,
      "step": 6753
    },
    {
      "epoch": 0.031769476090576405,
      "grad_norm": 0.7902878522872925,
      "learning_rate": 0.00019410261487830868,
      "loss": 0.0808,
      "step": 6754
    },
    {
      "epoch": 0.031774179892188864,
      "grad_norm": 1.311742901802063,
      "learning_rate": 0.00019410167190019522,
      "loss": 0.1559,
      "step": 6755
    },
    {
      "epoch": 0.03177888369380133,
      "grad_norm": 2.445404052734375,
      "learning_rate": 0.00019410072892208174,
      "loss": 0.4757,
      "step": 6756
    },
    {
      "epoch": 0.031783587495413795,
      "grad_norm": 1.9728420972824097,
      "learning_rate": 0.00019409978594396826,
      "loss": 0.2853,
      "step": 6757
    },
    {
      "epoch": 0.031788291297026254,
      "grad_norm": 2.1090259552001953,
      "learning_rate": 0.00019409884296585478,
      "loss": 0.3602,
      "step": 6758
    },
    {
      "epoch": 0.03179299509863872,
      "grad_norm": 1.9752088785171509,
      "learning_rate": 0.0001940978999877413,
      "loss": 0.4905,
      "step": 6759
    },
    {
      "epoch": 0.031797698900251185,
      "grad_norm": 1.3393781185150146,
      "learning_rate": 0.00019409695700962781,
      "loss": 0.319,
      "step": 6760
    },
    {
      "epoch": 0.031802402701863644,
      "grad_norm": 1.3284653425216675,
      "learning_rate": 0.00019409601403151433,
      "loss": 0.181,
      "step": 6761
    },
    {
      "epoch": 0.03180710650347611,
      "grad_norm": 1.7388923168182373,
      "learning_rate": 0.00019409507105340085,
      "loss": 0.3525,
      "step": 6762
    },
    {
      "epoch": 0.031811810305088575,
      "grad_norm": 2.2057907581329346,
      "learning_rate": 0.00019409412807528737,
      "loss": 0.4296,
      "step": 6763
    },
    {
      "epoch": 0.031816514106701034,
      "grad_norm": 8.48857593536377,
      "learning_rate": 0.00019409318509717392,
      "loss": 0.6252,
      "step": 6764
    },
    {
      "epoch": 0.0318212179083135,
      "grad_norm": 1.7816147804260254,
      "learning_rate": 0.00019409224211906043,
      "loss": 0.3568,
      "step": 6765
    },
    {
      "epoch": 0.031825921709925965,
      "grad_norm": 1.9633406400680542,
      "learning_rate": 0.00019409129914094695,
      "loss": 0.4423,
      "step": 6766
    },
    {
      "epoch": 0.031830625511538424,
      "grad_norm": 1.483982801437378,
      "learning_rate": 0.00019409035616283347,
      "loss": 0.3008,
      "step": 6767
    },
    {
      "epoch": 0.03183532931315089,
      "grad_norm": 1.00736665725708,
      "learning_rate": 0.00019408941318472,
      "loss": 0.1731,
      "step": 6768
    },
    {
      "epoch": 0.03184003311476335,
      "grad_norm": 4.134696006774902,
      "learning_rate": 0.0001940884702066065,
      "loss": 0.7887,
      "step": 6769
    },
    {
      "epoch": 0.031844736916375814,
      "grad_norm": 1.2440385818481445,
      "learning_rate": 0.00019408752722849303,
      "loss": 0.2875,
      "step": 6770
    },
    {
      "epoch": 0.03184944071798828,
      "grad_norm": 1.2339823246002197,
      "learning_rate": 0.00019408658425037955,
      "loss": 0.2093,
      "step": 6771
    },
    {
      "epoch": 0.03185414451960074,
      "grad_norm": 4.207327842712402,
      "learning_rate": 0.00019408564127226607,
      "loss": 0.4632,
      "step": 6772
    },
    {
      "epoch": 0.031858848321213204,
      "grad_norm": 3.2544264793395996,
      "learning_rate": 0.0001940846982941526,
      "loss": 0.4331,
      "step": 6773
    },
    {
      "epoch": 0.03186355212282567,
      "grad_norm": 0.6989462375640869,
      "learning_rate": 0.00019408375531603913,
      "loss": 0.1047,
      "step": 6774
    },
    {
      "epoch": 0.03186825592443813,
      "grad_norm": 1.3049492835998535,
      "learning_rate": 0.00019408281233792565,
      "loss": 0.2707,
      "step": 6775
    },
    {
      "epoch": 0.031872959726050594,
      "grad_norm": 3.736135721206665,
      "learning_rate": 0.00019408186935981217,
      "loss": 0.2309,
      "step": 6776
    },
    {
      "epoch": 0.03187766352766306,
      "grad_norm": 0.373843252658844,
      "learning_rate": 0.00019408092638169869,
      "loss": 0.0265,
      "step": 6777
    },
    {
      "epoch": 0.03188236732927552,
      "grad_norm": 1.582045078277588,
      "learning_rate": 0.00019407998340358523,
      "loss": 0.3462,
      "step": 6778
    },
    {
      "epoch": 0.031887071130887984,
      "grad_norm": 0.9484081864356995,
      "learning_rate": 0.00019407904042547172,
      "loss": 0.1303,
      "step": 6779
    },
    {
      "epoch": 0.03189177493250045,
      "grad_norm": 2.2345802783966064,
      "learning_rate": 0.00019407809744735824,
      "loss": 0.3619,
      "step": 6780
    },
    {
      "epoch": 0.03189647873411291,
      "grad_norm": 1.9447691440582275,
      "learning_rate": 0.00019407715446924476,
      "loss": 0.3161,
      "step": 6781
    },
    {
      "epoch": 0.031901182535725374,
      "grad_norm": 3.0333144664764404,
      "learning_rate": 0.0001940762114911313,
      "loss": 0.4686,
      "step": 6782
    },
    {
      "epoch": 0.03190588633733784,
      "grad_norm": 2.748750925064087,
      "learning_rate": 0.00019407526851301782,
      "loss": 0.5642,
      "step": 6783
    },
    {
      "epoch": 0.0319105901389503,
      "grad_norm": 1.3820210695266724,
      "learning_rate": 0.00019407432553490434,
      "loss": 0.1739,
      "step": 6784
    },
    {
      "epoch": 0.031915293940562764,
      "grad_norm": 0.21575327217578888,
      "learning_rate": 0.00019407338255679086,
      "loss": 0.0264,
      "step": 6785
    },
    {
      "epoch": 0.03191999774217522,
      "grad_norm": 1.8863450288772583,
      "learning_rate": 0.00019407243957867738,
      "loss": 0.221,
      "step": 6786
    },
    {
      "epoch": 0.03192470154378769,
      "grad_norm": 2.381084442138672,
      "learning_rate": 0.00019407149660056393,
      "loss": 0.2697,
      "step": 6787
    },
    {
      "epoch": 0.031929405345400154,
      "grad_norm": 1.714889407157898,
      "learning_rate": 0.00019407055362245044,
      "loss": 0.1747,
      "step": 6788
    },
    {
      "epoch": 0.03193410914701261,
      "grad_norm": 1.4740482568740845,
      "learning_rate": 0.00019406961064433696,
      "loss": 0.1301,
      "step": 6789
    },
    {
      "epoch": 0.03193881294862508,
      "grad_norm": 2.1696434020996094,
      "learning_rate": 0.00019406866766622348,
      "loss": 0.1963,
      "step": 6790
    },
    {
      "epoch": 0.031943516750237544,
      "grad_norm": 1.2530663013458252,
      "learning_rate": 0.00019406772468811,
      "loss": 0.1942,
      "step": 6791
    },
    {
      "epoch": 0.03194822055185,
      "grad_norm": 0.5564070343971252,
      "learning_rate": 0.00019406678170999652,
      "loss": 0.0636,
      "step": 6792
    },
    {
      "epoch": 0.03195292435346247,
      "grad_norm": 2.153454303741455,
      "learning_rate": 0.00019406583873188304,
      "loss": 0.4392,
      "step": 6793
    },
    {
      "epoch": 0.031957628155074934,
      "grad_norm": 2.667938232421875,
      "learning_rate": 0.00019406489575376956,
      "loss": 0.4439,
      "step": 6794
    },
    {
      "epoch": 0.03196233195668739,
      "grad_norm": 3.1413187980651855,
      "learning_rate": 0.00019406395277565608,
      "loss": 0.5218,
      "step": 6795
    },
    {
      "epoch": 0.03196703575829986,
      "grad_norm": 4.721220016479492,
      "learning_rate": 0.00019406300979754262,
      "loss": 1.3586,
      "step": 6796
    },
    {
      "epoch": 0.031971739559912324,
      "grad_norm": 2.1902668476104736,
      "learning_rate": 0.00019406206681942914,
      "loss": 0.4933,
      "step": 6797
    },
    {
      "epoch": 0.03197644336152478,
      "grad_norm": 1.1167577505111694,
      "learning_rate": 0.00019406112384131566,
      "loss": 0.2919,
      "step": 6798
    },
    {
      "epoch": 0.03198114716313725,
      "grad_norm": 1.4938937425613403,
      "learning_rate": 0.00019406018086320218,
      "loss": 0.3252,
      "step": 6799
    },
    {
      "epoch": 0.031985850964749714,
      "grad_norm": 0.5305777788162231,
      "learning_rate": 0.0001940592378850887,
      "loss": 0.0677,
      "step": 6800
    },
    {
      "epoch": 0.03199055476636217,
      "grad_norm": 6.0941267013549805,
      "learning_rate": 0.00019405829490697521,
      "loss": 0.3209,
      "step": 6801
    },
    {
      "epoch": 0.03199525856797464,
      "grad_norm": 1.542710542678833,
      "learning_rate": 0.00019405735192886173,
      "loss": 0.1962,
      "step": 6802
    },
    {
      "epoch": 0.0319999623695871,
      "grad_norm": 1.3082399368286133,
      "learning_rate": 0.00019405640895074825,
      "loss": 0.1227,
      "step": 6803
    },
    {
      "epoch": 0.03200466617119956,
      "grad_norm": 1.115575909614563,
      "learning_rate": 0.00019405546597263477,
      "loss": 0.1362,
      "step": 6804
    },
    {
      "epoch": 0.03200936997281203,
      "grad_norm": 1.5846307277679443,
      "learning_rate": 0.00019405452299452132,
      "loss": 0.2039,
      "step": 6805
    },
    {
      "epoch": 0.03201407377442449,
      "grad_norm": 1.2886805534362793,
      "learning_rate": 0.00019405358001640783,
      "loss": 0.1033,
      "step": 6806
    },
    {
      "epoch": 0.03201877757603695,
      "grad_norm": 21.81639862060547,
      "learning_rate": 0.00019405263703829435,
      "loss": 0.3307,
      "step": 6807
    },
    {
      "epoch": 0.03202348137764942,
      "grad_norm": 1.5899584293365479,
      "learning_rate": 0.00019405169406018087,
      "loss": 0.1444,
      "step": 6808
    },
    {
      "epoch": 0.03202818517926188,
      "grad_norm": 0.9598864316940308,
      "learning_rate": 0.0001940507510820674,
      "loss": 0.0955,
      "step": 6809
    },
    {
      "epoch": 0.03203288898087434,
      "grad_norm": 3.828972339630127,
      "learning_rate": 0.0001940498081039539,
      "loss": 0.6236,
      "step": 6810
    },
    {
      "epoch": 0.03203759278248681,
      "grad_norm": 5.437652111053467,
      "learning_rate": 0.00019404886512584043,
      "loss": 0.4853,
      "step": 6811
    },
    {
      "epoch": 0.03204229658409927,
      "grad_norm": 1.2888622283935547,
      "learning_rate": 0.00019404792214772695,
      "loss": 0.0903,
      "step": 6812
    },
    {
      "epoch": 0.03204700038571173,
      "grad_norm": 2.3821768760681152,
      "learning_rate": 0.00019404697916961347,
      "loss": 0.2279,
      "step": 6813
    },
    {
      "epoch": 0.0320517041873242,
      "grad_norm": 6.639342784881592,
      "learning_rate": 0.0001940460361915,
      "loss": 0.4534,
      "step": 6814
    },
    {
      "epoch": 0.03205640798893666,
      "grad_norm": 1.7996854782104492,
      "learning_rate": 0.00019404509321338653,
      "loss": 0.2098,
      "step": 6815
    },
    {
      "epoch": 0.03206111179054912,
      "grad_norm": 3.9594547748565674,
      "learning_rate": 0.00019404415023527305,
      "loss": 0.5985,
      "step": 6816
    },
    {
      "epoch": 0.03206581559216159,
      "grad_norm": 3.9125277996063232,
      "learning_rate": 0.00019404320725715957,
      "loss": 0.4609,
      "step": 6817
    },
    {
      "epoch": 0.03207051939377405,
      "grad_norm": 4.192464351654053,
      "learning_rate": 0.00019404226427904609,
      "loss": 0.4552,
      "step": 6818
    },
    {
      "epoch": 0.03207522319538651,
      "grad_norm": 0.6021427512168884,
      "learning_rate": 0.00019404132130093263,
      "loss": 0.0503,
      "step": 6819
    },
    {
      "epoch": 0.03207992699699897,
      "grad_norm": 2.4034762382507324,
      "learning_rate": 0.00019404037832281915,
      "loss": 0.2452,
      "step": 6820
    },
    {
      "epoch": 0.03208463079861144,
      "grad_norm": 3.0569300651550293,
      "learning_rate": 0.00019403943534470567,
      "loss": 0.3747,
      "step": 6821
    },
    {
      "epoch": 0.0320893346002239,
      "grad_norm": 2.4064948558807373,
      "learning_rate": 0.00019403849236659216,
      "loss": 0.5325,
      "step": 6822
    },
    {
      "epoch": 0.03209403840183636,
      "grad_norm": 2.015333652496338,
      "learning_rate": 0.0001940375493884787,
      "loss": 0.1869,
      "step": 6823
    },
    {
      "epoch": 0.03209874220344883,
      "grad_norm": 5.274205684661865,
      "learning_rate": 0.00019403660641036522,
      "loss": 0.1729,
      "step": 6824
    },
    {
      "epoch": 0.03210344600506129,
      "grad_norm": 1.8549121618270874,
      "learning_rate": 0.00019403566343225174,
      "loss": 0.2581,
      "step": 6825
    },
    {
      "epoch": 0.03210814980667375,
      "grad_norm": 1.0687695741653442,
      "learning_rate": 0.00019403472045413826,
      "loss": 0.1313,
      "step": 6826
    },
    {
      "epoch": 0.03211285360828622,
      "grad_norm": 0.8450769186019897,
      "learning_rate": 0.00019403377747602478,
      "loss": 0.0763,
      "step": 6827
    },
    {
      "epoch": 0.03211755740989868,
      "grad_norm": 1.3128958940505981,
      "learning_rate": 0.00019403283449791133,
      "loss": 0.1395,
      "step": 6828
    },
    {
      "epoch": 0.03212226121151114,
      "grad_norm": 1.7877931594848633,
      "learning_rate": 0.00019403189151979784,
      "loss": 0.2115,
      "step": 6829
    },
    {
      "epoch": 0.03212696501312361,
      "grad_norm": 2.610964298248291,
      "learning_rate": 0.00019403094854168436,
      "loss": 0.3824,
      "step": 6830
    },
    {
      "epoch": 0.03213166881473607,
      "grad_norm": 1.0106146335601807,
      "learning_rate": 0.00019403000556357088,
      "loss": 0.0659,
      "step": 6831
    },
    {
      "epoch": 0.03213637261634853,
      "grad_norm": 1.3080955743789673,
      "learning_rate": 0.0001940290625854574,
      "loss": 0.1073,
      "step": 6832
    },
    {
      "epoch": 0.032141076417961,
      "grad_norm": 2.059814214706421,
      "learning_rate": 0.00019402811960734392,
      "loss": 0.184,
      "step": 6833
    },
    {
      "epoch": 0.03214578021957346,
      "grad_norm": 3.203855037689209,
      "learning_rate": 0.00019402717662923044,
      "loss": 0.7708,
      "step": 6834
    },
    {
      "epoch": 0.03215048402118592,
      "grad_norm": 2.5340521335601807,
      "learning_rate": 0.00019402623365111696,
      "loss": 0.4209,
      "step": 6835
    },
    {
      "epoch": 0.03215518782279839,
      "grad_norm": 4.560084342956543,
      "learning_rate": 0.00019402529067300348,
      "loss": 0.7281,
      "step": 6836
    },
    {
      "epoch": 0.032159891624410845,
      "grad_norm": 0.6462848782539368,
      "learning_rate": 0.00019402434769489002,
      "loss": 0.0547,
      "step": 6837
    },
    {
      "epoch": 0.03216459542602331,
      "grad_norm": 3.316951036453247,
      "learning_rate": 0.00019402340471677654,
      "loss": 0.4364,
      "step": 6838
    },
    {
      "epoch": 0.03216929922763578,
      "grad_norm": 0.8092406392097473,
      "learning_rate": 0.00019402246173866306,
      "loss": 0.0913,
      "step": 6839
    },
    {
      "epoch": 0.032174003029248235,
      "grad_norm": 0.2890070676803589,
      "learning_rate": 0.00019402151876054958,
      "loss": 0.0172,
      "step": 6840
    },
    {
      "epoch": 0.0321787068308607,
      "grad_norm": 2.0131192207336426,
      "learning_rate": 0.0001940205757824361,
      "loss": 0.2722,
      "step": 6841
    },
    {
      "epoch": 0.03218341063247317,
      "grad_norm": 1.7247573137283325,
      "learning_rate": 0.00019401963280432261,
      "loss": 0.2661,
      "step": 6842
    },
    {
      "epoch": 0.032188114434085625,
      "grad_norm": 1.4124807119369507,
      "learning_rate": 0.00019401868982620913,
      "loss": 0.1381,
      "step": 6843
    },
    {
      "epoch": 0.03219281823569809,
      "grad_norm": 2.370617389678955,
      "learning_rate": 0.00019401774684809565,
      "loss": 0.3993,
      "step": 6844
    },
    {
      "epoch": 0.03219752203731056,
      "grad_norm": 1.9522380828857422,
      "learning_rate": 0.00019401680386998217,
      "loss": 0.2032,
      "step": 6845
    },
    {
      "epoch": 0.032202225838923015,
      "grad_norm": 1.7340537309646606,
      "learning_rate": 0.00019401586089186872,
      "loss": 0.1708,
      "step": 6846
    },
    {
      "epoch": 0.03220692964053548,
      "grad_norm": 2.414490222930908,
      "learning_rate": 0.00019401491791375523,
      "loss": 0.4921,
      "step": 6847
    },
    {
      "epoch": 0.03221163344214795,
      "grad_norm": 1.088320255279541,
      "learning_rate": 0.00019401397493564175,
      "loss": 0.163,
      "step": 6848
    },
    {
      "epoch": 0.032216337243760405,
      "grad_norm": 4.702730178833008,
      "learning_rate": 0.00019401303195752827,
      "loss": 1.1762,
      "step": 6849
    },
    {
      "epoch": 0.03222104104537287,
      "grad_norm": 2.3964297771453857,
      "learning_rate": 0.00019401208897941482,
      "loss": 0.5752,
      "step": 6850
    },
    {
      "epoch": 0.03222574484698534,
      "grad_norm": 1.0317360162734985,
      "learning_rate": 0.00019401114600130134,
      "loss": 0.089,
      "step": 6851
    },
    {
      "epoch": 0.032230448648597795,
      "grad_norm": 1.4606032371520996,
      "learning_rate": 0.00019401020302318786,
      "loss": 0.1222,
      "step": 6852
    },
    {
      "epoch": 0.03223515245021026,
      "grad_norm": 1.0889320373535156,
      "learning_rate": 0.00019400926004507435,
      "loss": 0.0628,
      "step": 6853
    },
    {
      "epoch": 0.03223985625182272,
      "grad_norm": 1.0007051229476929,
      "learning_rate": 0.00019400831706696087,
      "loss": 0.1131,
      "step": 6854
    },
    {
      "epoch": 0.032244560053435185,
      "grad_norm": 1.6132984161376953,
      "learning_rate": 0.0001940073740888474,
      "loss": 0.1403,
      "step": 6855
    },
    {
      "epoch": 0.03224926385504765,
      "grad_norm": 0.737013041973114,
      "learning_rate": 0.00019400643111073393,
      "loss": 0.0405,
      "step": 6856
    },
    {
      "epoch": 0.03225396765666011,
      "grad_norm": 13.239990234375,
      "learning_rate": 0.00019400548813262045,
      "loss": 0.4229,
      "step": 6857
    },
    {
      "epoch": 0.032258671458272575,
      "grad_norm": 2.8037326335906982,
      "learning_rate": 0.00019400454515450697,
      "loss": 0.5686,
      "step": 6858
    },
    {
      "epoch": 0.03226337525988504,
      "grad_norm": 0.5013546943664551,
      "learning_rate": 0.00019400360217639349,
      "loss": 0.0444,
      "step": 6859
    },
    {
      "epoch": 0.0322680790614975,
      "grad_norm": 0.2092120349407196,
      "learning_rate": 0.00019400265919828003,
      "loss": 0.0192,
      "step": 6860
    },
    {
      "epoch": 0.032272782863109965,
      "grad_norm": 2.317298412322998,
      "learning_rate": 0.00019400171622016655,
      "loss": 0.326,
      "step": 6861
    },
    {
      "epoch": 0.03227748666472243,
      "grad_norm": 0.28774294257164,
      "learning_rate": 0.00019400077324205307,
      "loss": 0.0172,
      "step": 6862
    },
    {
      "epoch": 0.03228219046633489,
      "grad_norm": 6.203358173370361,
      "learning_rate": 0.0001939998302639396,
      "loss": 0.9593,
      "step": 6863
    },
    {
      "epoch": 0.032286894267947355,
      "grad_norm": 1.4503016471862793,
      "learning_rate": 0.0001939988872858261,
      "loss": 0.1381,
      "step": 6864
    },
    {
      "epoch": 0.03229159806955982,
      "grad_norm": 0.4258226752281189,
      "learning_rate": 0.00019399794430771262,
      "loss": 0.0313,
      "step": 6865
    },
    {
      "epoch": 0.03229630187117228,
      "grad_norm": 0.4952619671821594,
      "learning_rate": 0.00019399700132959914,
      "loss": 0.0314,
      "step": 6866
    },
    {
      "epoch": 0.032301005672784745,
      "grad_norm": 2.4896953105926514,
      "learning_rate": 0.00019399605835148566,
      "loss": 0.3575,
      "step": 6867
    },
    {
      "epoch": 0.03230570947439721,
      "grad_norm": 5.8234686851501465,
      "learning_rate": 0.00019399511537337218,
      "loss": 0.492,
      "step": 6868
    },
    {
      "epoch": 0.03231041327600967,
      "grad_norm": 0.32515642046928406,
      "learning_rate": 0.00019399417239525873,
      "loss": 0.0309,
      "step": 6869
    },
    {
      "epoch": 0.032315117077622135,
      "grad_norm": 0.48978278040885925,
      "learning_rate": 0.00019399322941714524,
      "loss": 0.0517,
      "step": 6870
    },
    {
      "epoch": 0.032319820879234594,
      "grad_norm": 5.1776442527771,
      "learning_rate": 0.00019399228643903176,
      "loss": 0.598,
      "step": 6871
    },
    {
      "epoch": 0.03232452468084706,
      "grad_norm": 3.5386855602264404,
      "learning_rate": 0.00019399134346091828,
      "loss": 0.6623,
      "step": 6872
    },
    {
      "epoch": 0.032329228482459525,
      "grad_norm": 0.5432993769645691,
      "learning_rate": 0.0001939904004828048,
      "loss": 0.0325,
      "step": 6873
    },
    {
      "epoch": 0.032333932284071984,
      "grad_norm": 3.51238751411438,
      "learning_rate": 0.00019398945750469132,
      "loss": 0.6253,
      "step": 6874
    },
    {
      "epoch": 0.03233863608568445,
      "grad_norm": 4.076833248138428,
      "learning_rate": 0.00019398851452657784,
      "loss": 0.67,
      "step": 6875
    },
    {
      "epoch": 0.032343339887296915,
      "grad_norm": 2.967548370361328,
      "learning_rate": 0.00019398757154846436,
      "loss": 0.5509,
      "step": 6876
    },
    {
      "epoch": 0.032348043688909374,
      "grad_norm": 3.4127817153930664,
      "learning_rate": 0.00019398662857035088,
      "loss": 0.688,
      "step": 6877
    },
    {
      "epoch": 0.03235274749052184,
      "grad_norm": 2.1968424320220947,
      "learning_rate": 0.00019398568559223742,
      "loss": 0.4359,
      "step": 6878
    },
    {
      "epoch": 0.032357451292134305,
      "grad_norm": 1.1791167259216309,
      "learning_rate": 0.00019398474261412394,
      "loss": 0.1343,
      "step": 6879
    },
    {
      "epoch": 0.032362155093746764,
      "grad_norm": 3.348052740097046,
      "learning_rate": 0.00019398379963601046,
      "loss": 0.3835,
      "step": 6880
    },
    {
      "epoch": 0.03236685889535923,
      "grad_norm": 0.5926575660705566,
      "learning_rate": 0.00019398285665789698,
      "loss": 0.0833,
      "step": 6881
    },
    {
      "epoch": 0.032371562696971695,
      "grad_norm": 0.8857259154319763,
      "learning_rate": 0.00019398191367978352,
      "loss": 0.1695,
      "step": 6882
    },
    {
      "epoch": 0.032376266498584154,
      "grad_norm": 1.519885540008545,
      "learning_rate": 0.00019398097070167004,
      "loss": 0.3418,
      "step": 6883
    },
    {
      "epoch": 0.03238097030019662,
      "grad_norm": 2.848754405975342,
      "learning_rate": 0.00019398002772355653,
      "loss": 0.5213,
      "step": 6884
    },
    {
      "epoch": 0.032385674101809085,
      "grad_norm": 1.4124079942703247,
      "learning_rate": 0.00019397908474544305,
      "loss": 0.2457,
      "step": 6885
    },
    {
      "epoch": 0.032390377903421544,
      "grad_norm": 1.3509316444396973,
      "learning_rate": 0.00019397814176732957,
      "loss": 0.2288,
      "step": 6886
    },
    {
      "epoch": 0.03239508170503401,
      "grad_norm": 4.038928985595703,
      "learning_rate": 0.00019397719878921612,
      "loss": 0.6732,
      "step": 6887
    },
    {
      "epoch": 0.03239978550664647,
      "grad_norm": 1.8371140956878662,
      "learning_rate": 0.00019397625581110263,
      "loss": 0.2403,
      "step": 6888
    },
    {
      "epoch": 0.032404489308258934,
      "grad_norm": 2.3006489276885986,
      "learning_rate": 0.00019397531283298915,
      "loss": 0.6106,
      "step": 6889
    },
    {
      "epoch": 0.0324091931098714,
      "grad_norm": 0.8836134076118469,
      "learning_rate": 0.00019397436985487567,
      "loss": 0.1038,
      "step": 6890
    },
    {
      "epoch": 0.03241389691148386,
      "grad_norm": 1.4097234010696411,
      "learning_rate": 0.00019397342687676222,
      "loss": 0.2161,
      "step": 6891
    },
    {
      "epoch": 0.032418600713096324,
      "grad_norm": 1.4079856872558594,
      "learning_rate": 0.00019397248389864874,
      "loss": 0.2182,
      "step": 6892
    },
    {
      "epoch": 0.03242330451470879,
      "grad_norm": 0.6376672983169556,
      "learning_rate": 0.00019397154092053526,
      "loss": 0.0685,
      "step": 6893
    },
    {
      "epoch": 0.03242800831632125,
      "grad_norm": 3.682593584060669,
      "learning_rate": 0.00019397059794242177,
      "loss": 0.772,
      "step": 6894
    },
    {
      "epoch": 0.032432712117933714,
      "grad_norm": 1.0691992044448853,
      "learning_rate": 0.00019396965496430827,
      "loss": 0.2866,
      "step": 6895
    },
    {
      "epoch": 0.03243741591954618,
      "grad_norm": 1.2029907703399658,
      "learning_rate": 0.0001939687119861948,
      "loss": 0.264,
      "step": 6896
    },
    {
      "epoch": 0.03244211972115864,
      "grad_norm": 0.4926554560661316,
      "learning_rate": 0.00019396776900808133,
      "loss": 0.0437,
      "step": 6897
    },
    {
      "epoch": 0.032446823522771104,
      "grad_norm": 2.1628313064575195,
      "learning_rate": 0.00019396682602996785,
      "loss": 0.3706,
      "step": 6898
    },
    {
      "epoch": 0.03245152732438357,
      "grad_norm": 4.089775562286377,
      "learning_rate": 0.00019396588305185437,
      "loss": 0.8457,
      "step": 6899
    },
    {
      "epoch": 0.03245623112599603,
      "grad_norm": 1.4333877563476562,
      "learning_rate": 0.0001939649400737409,
      "loss": 0.3645,
      "step": 6900
    },
    {
      "epoch": 0.032460934927608494,
      "grad_norm": 0.4885963797569275,
      "learning_rate": 0.00019396399709562743,
      "loss": 0.0292,
      "step": 6901
    },
    {
      "epoch": 0.03246563872922096,
      "grad_norm": 1.1934853792190552,
      "learning_rate": 0.00019396305411751395,
      "loss": 0.0574,
      "step": 6902
    },
    {
      "epoch": 0.03247034253083342,
      "grad_norm": 1.318669080734253,
      "learning_rate": 0.00019396211113940047,
      "loss": 0.153,
      "step": 6903
    },
    {
      "epoch": 0.032475046332445884,
      "grad_norm": 1.061678171157837,
      "learning_rate": 0.000193961168161287,
      "loss": 0.1462,
      "step": 6904
    },
    {
      "epoch": 0.03247975013405834,
      "grad_norm": 0.7383221387863159,
      "learning_rate": 0.0001939602251831735,
      "loss": 0.0679,
      "step": 6905
    },
    {
      "epoch": 0.03248445393567081,
      "grad_norm": 2.6643266677856445,
      "learning_rate": 0.00019395928220506002,
      "loss": 0.2585,
      "step": 6906
    },
    {
      "epoch": 0.032489157737283274,
      "grad_norm": 1.1432740688323975,
      "learning_rate": 0.00019395833922694654,
      "loss": 0.1008,
      "step": 6907
    },
    {
      "epoch": 0.03249386153889573,
      "grad_norm": 6.870702743530273,
      "learning_rate": 0.00019395739624883306,
      "loss": 1.1664,
      "step": 6908
    },
    {
      "epoch": 0.0324985653405082,
      "grad_norm": 0.47445306181907654,
      "learning_rate": 0.0001939564532707196,
      "loss": 0.0546,
      "step": 6909
    },
    {
      "epoch": 0.032503269142120664,
      "grad_norm": 0.6905246376991272,
      "learning_rate": 0.00019395551029260613,
      "loss": 0.0524,
      "step": 6910
    },
    {
      "epoch": 0.03250797294373312,
      "grad_norm": 1.220988154411316,
      "learning_rate": 0.00019395456731449264,
      "loss": 0.0818,
      "step": 6911
    },
    {
      "epoch": 0.03251267674534559,
      "grad_norm": 4.525380611419678,
      "learning_rate": 0.00019395362433637916,
      "loss": 0.3899,
      "step": 6912
    },
    {
      "epoch": 0.032517380546958054,
      "grad_norm": 1.8761229515075684,
      "learning_rate": 0.00019395268135826568,
      "loss": 0.1064,
      "step": 6913
    },
    {
      "epoch": 0.03252208434857051,
      "grad_norm": 0.7723118662834167,
      "learning_rate": 0.00019395173838015223,
      "loss": 0.0606,
      "step": 6914
    },
    {
      "epoch": 0.03252678815018298,
      "grad_norm": 17.508575439453125,
      "learning_rate": 0.00019395079540203872,
      "loss": 0.722,
      "step": 6915
    },
    {
      "epoch": 0.032531491951795444,
      "grad_norm": 1.5874344110488892,
      "learning_rate": 0.00019394985242392524,
      "loss": 0.1067,
      "step": 6916
    },
    {
      "epoch": 0.0325361957534079,
      "grad_norm": 1.8368120193481445,
      "learning_rate": 0.00019394890944581176,
      "loss": 0.1375,
      "step": 6917
    },
    {
      "epoch": 0.03254089955502037,
      "grad_norm": 0.5263005495071411,
      "learning_rate": 0.00019394796646769828,
      "loss": 0.0236,
      "step": 6918
    },
    {
      "epoch": 0.032545603356632834,
      "grad_norm": 5.070244312286377,
      "learning_rate": 0.00019394702348958482,
      "loss": 0.6267,
      "step": 6919
    },
    {
      "epoch": 0.03255030715824529,
      "grad_norm": 6.1775946617126465,
      "learning_rate": 0.00019394608051147134,
      "loss": 0.438,
      "step": 6920
    },
    {
      "epoch": 0.03255501095985776,
      "grad_norm": 4.424349784851074,
      "learning_rate": 0.00019394513753335786,
      "loss": 0.0598,
      "step": 6921
    },
    {
      "epoch": 0.032559714761470224,
      "grad_norm": 3.3763234615325928,
      "learning_rate": 0.00019394419455524438,
      "loss": 0.332,
      "step": 6922
    },
    {
      "epoch": 0.03256441856308268,
      "grad_norm": 4.351993083953857,
      "learning_rate": 0.00019394325157713092,
      "loss": 0.6587,
      "step": 6923
    },
    {
      "epoch": 0.03256912236469515,
      "grad_norm": 2.820077896118164,
      "learning_rate": 0.00019394230859901744,
      "loss": 0.0881,
      "step": 6924
    },
    {
      "epoch": 0.03257382616630761,
      "grad_norm": 0.3557768166065216,
      "learning_rate": 0.00019394136562090396,
      "loss": 0.0107,
      "step": 6925
    },
    {
      "epoch": 0.03257852996792007,
      "grad_norm": 1.8886439800262451,
      "learning_rate": 0.00019394042264279045,
      "loss": 0.137,
      "step": 6926
    },
    {
      "epoch": 0.03258323376953254,
      "grad_norm": 1.9632925987243652,
      "learning_rate": 0.00019393947966467697,
      "loss": 0.0894,
      "step": 6927
    },
    {
      "epoch": 0.032587937571145,
      "grad_norm": 4.111635208129883,
      "learning_rate": 0.00019393853668656352,
      "loss": 0.324,
      "step": 6928
    },
    {
      "epoch": 0.03259264137275746,
      "grad_norm": 0.38982266187667847,
      "learning_rate": 0.00019393759370845003,
      "loss": 0.0295,
      "step": 6929
    },
    {
      "epoch": 0.03259734517436993,
      "grad_norm": 4.968851566314697,
      "learning_rate": 0.00019393665073033655,
      "loss": 0.5928,
      "step": 6930
    },
    {
      "epoch": 0.03260204897598239,
      "grad_norm": 1.7670295238494873,
      "learning_rate": 0.00019393570775222307,
      "loss": 0.0612,
      "step": 6931
    },
    {
      "epoch": 0.03260675277759485,
      "grad_norm": 2.9705026149749756,
      "learning_rate": 0.00019393476477410962,
      "loss": 0.2877,
      "step": 6932
    },
    {
      "epoch": 0.03261145657920732,
      "grad_norm": 5.468349456787109,
      "learning_rate": 0.00019393382179599614,
      "loss": 0.488,
      "step": 6933
    },
    {
      "epoch": 0.03261616038081978,
      "grad_norm": 6.3084940910339355,
      "learning_rate": 0.00019393287881788265,
      "loss": 0.6869,
      "step": 6934
    },
    {
      "epoch": 0.03262086418243224,
      "grad_norm": 0.7028156518936157,
      "learning_rate": 0.00019393193583976917,
      "loss": 0.0449,
      "step": 6935
    },
    {
      "epoch": 0.03262556798404471,
      "grad_norm": 3.5142273902893066,
      "learning_rate": 0.0001939309928616557,
      "loss": 0.3844,
      "step": 6936
    },
    {
      "epoch": 0.03263027178565717,
      "grad_norm": 1.7346198558807373,
      "learning_rate": 0.0001939300498835422,
      "loss": 0.0878,
      "step": 6937
    },
    {
      "epoch": 0.03263497558726963,
      "grad_norm": 5.1944708824157715,
      "learning_rate": 0.00019392910690542873,
      "loss": 1.0549,
      "step": 6938
    },
    {
      "epoch": 0.0326396793888821,
      "grad_norm": 0.8494113087654114,
      "learning_rate": 0.00019392816392731525,
      "loss": 0.0538,
      "step": 6939
    },
    {
      "epoch": 0.03264438319049456,
      "grad_norm": 4.006723880767822,
      "learning_rate": 0.00019392722094920177,
      "loss": 0.3928,
      "step": 6940
    },
    {
      "epoch": 0.03264908699210702,
      "grad_norm": 1.4893205165863037,
      "learning_rate": 0.0001939262779710883,
      "loss": 0.1047,
      "step": 6941
    },
    {
      "epoch": 0.03265379079371948,
      "grad_norm": 3.3056752681732178,
      "learning_rate": 0.00019392533499297483,
      "loss": 0.365,
      "step": 6942
    },
    {
      "epoch": 0.03265849459533195,
      "grad_norm": 0.4911888837814331,
      "learning_rate": 0.00019392439201486135,
      "loss": 0.032,
      "step": 6943
    },
    {
      "epoch": 0.03266319839694441,
      "grad_norm": 3.2687418460845947,
      "learning_rate": 0.00019392344903674787,
      "loss": 0.4113,
      "step": 6944
    },
    {
      "epoch": 0.03266790219855687,
      "grad_norm": 1.0040409564971924,
      "learning_rate": 0.0001939225060586344,
      "loss": 0.098,
      "step": 6945
    },
    {
      "epoch": 0.03267260600016934,
      "grad_norm": 1.0944277048110962,
      "learning_rate": 0.0001939215630805209,
      "loss": 0.0762,
      "step": 6946
    },
    {
      "epoch": 0.0326773098017818,
      "grad_norm": 1.3579005002975464,
      "learning_rate": 0.00019392062010240742,
      "loss": 0.1652,
      "step": 6947
    },
    {
      "epoch": 0.03268201360339426,
      "grad_norm": 5.204540252685547,
      "learning_rate": 0.00019391967712429394,
      "loss": 1.4971,
      "step": 6948
    },
    {
      "epoch": 0.03268671740500673,
      "grad_norm": 2.5984060764312744,
      "learning_rate": 0.00019391873414618046,
      "loss": 0.5709,
      "step": 6949
    },
    {
      "epoch": 0.03269142120661919,
      "grad_norm": 3.3171026706695557,
      "learning_rate": 0.000193917791168067,
      "loss": 0.3195,
      "step": 6950
    },
    {
      "epoch": 0.03269612500823165,
      "grad_norm": 4.590709686279297,
      "learning_rate": 0.00019391684818995353,
      "loss": 0.3155,
      "step": 6951
    },
    {
      "epoch": 0.03270082880984412,
      "grad_norm": 2.881833076477051,
      "learning_rate": 0.00019391590521184004,
      "loss": 0.0866,
      "step": 6952
    },
    {
      "epoch": 0.03270553261145658,
      "grad_norm": 0.4474090337753296,
      "learning_rate": 0.00019391496223372656,
      "loss": 0.0184,
      "step": 6953
    },
    {
      "epoch": 0.03271023641306904,
      "grad_norm": 0.034321095794439316,
      "learning_rate": 0.00019391401925561308,
      "loss": 0.0014,
      "step": 6954
    },
    {
      "epoch": 0.03271494021468151,
      "grad_norm": 2.643573522567749,
      "learning_rate": 0.00019391307627749963,
      "loss": 0.2155,
      "step": 6955
    },
    {
      "epoch": 0.03271964401629397,
      "grad_norm": 3.5061099529266357,
      "learning_rate": 0.00019391213329938615,
      "loss": 0.1677,
      "step": 6956
    },
    {
      "epoch": 0.03272434781790643,
      "grad_norm": 1.843511939048767,
      "learning_rate": 0.00019391119032127264,
      "loss": 0.1355,
      "step": 6957
    },
    {
      "epoch": 0.0327290516195189,
      "grad_norm": 1.0122764110565186,
      "learning_rate": 0.00019391024734315916,
      "loss": 0.0579,
      "step": 6958
    },
    {
      "epoch": 0.032733755421131355,
      "grad_norm": 4.431096076965332,
      "learning_rate": 0.0001939093043650457,
      "loss": 0.6305,
      "step": 6959
    },
    {
      "epoch": 0.03273845922274382,
      "grad_norm": 0.582858681678772,
      "learning_rate": 0.00019390836138693222,
      "loss": 0.0441,
      "step": 6960
    },
    {
      "epoch": 0.03274316302435629,
      "grad_norm": 2.8879315853118896,
      "learning_rate": 0.00019390741840881874,
      "loss": 0.1865,
      "step": 6961
    },
    {
      "epoch": 0.032747866825968745,
      "grad_norm": 4.747519493103027,
      "learning_rate": 0.00019390647543070526,
      "loss": 0.5759,
      "step": 6962
    },
    {
      "epoch": 0.03275257062758121,
      "grad_norm": 1.070277452468872,
      "learning_rate": 0.00019390553245259178,
      "loss": 0.0756,
      "step": 6963
    },
    {
      "epoch": 0.03275727442919368,
      "grad_norm": 5.019468784332275,
      "learning_rate": 0.00019390458947447832,
      "loss": 0.8472,
      "step": 6964
    },
    {
      "epoch": 0.032761978230806135,
      "grad_norm": 2.006037950515747,
      "learning_rate": 0.00019390364649636484,
      "loss": 0.1415,
      "step": 6965
    },
    {
      "epoch": 0.0327666820324186,
      "grad_norm": 1.3910678625106812,
      "learning_rate": 0.00019390270351825136,
      "loss": 0.1149,
      "step": 6966
    },
    {
      "epoch": 0.03277138583403107,
      "grad_norm": 6.7169904708862305,
      "learning_rate": 0.00019390176054013788,
      "loss": 1.544,
      "step": 6967
    },
    {
      "epoch": 0.032776089635643525,
      "grad_norm": 1.935198426246643,
      "learning_rate": 0.0001939008175620244,
      "loss": 0.1647,
      "step": 6968
    },
    {
      "epoch": 0.03278079343725599,
      "grad_norm": 0.44542962312698364,
      "learning_rate": 0.00019389987458391092,
      "loss": 0.0228,
      "step": 6969
    },
    {
      "epoch": 0.032785497238868457,
      "grad_norm": 2.4465205669403076,
      "learning_rate": 0.00019389893160579743,
      "loss": 0.2303,
      "step": 6970
    },
    {
      "epoch": 0.032790201040480915,
      "grad_norm": 1.8043749332427979,
      "learning_rate": 0.00019389798862768395,
      "loss": 0.1343,
      "step": 6971
    },
    {
      "epoch": 0.03279490484209338,
      "grad_norm": 1.9177544116973877,
      "learning_rate": 0.00019389704564957047,
      "loss": 0.2747,
      "step": 6972
    },
    {
      "epoch": 0.032799608643705847,
      "grad_norm": 2.4107420444488525,
      "learning_rate": 0.00019389610267145702,
      "loss": 0.3301,
      "step": 6973
    },
    {
      "epoch": 0.032804312445318305,
      "grad_norm": 3.015162944793701,
      "learning_rate": 0.00019389515969334354,
      "loss": 0.4078,
      "step": 6974
    },
    {
      "epoch": 0.03280901624693077,
      "grad_norm": 1.2232608795166016,
      "learning_rate": 0.00019389421671523005,
      "loss": 0.093,
      "step": 6975
    },
    {
      "epoch": 0.03281372004854323,
      "grad_norm": 2.2914605140686035,
      "learning_rate": 0.00019389327373711657,
      "loss": 0.3631,
      "step": 6976
    },
    {
      "epoch": 0.032818423850155695,
      "grad_norm": 3.0788753032684326,
      "learning_rate": 0.0001938923307590031,
      "loss": 0.3732,
      "step": 6977
    },
    {
      "epoch": 0.03282312765176816,
      "grad_norm": 3.1867856979370117,
      "learning_rate": 0.0001938913877808896,
      "loss": 0.4796,
      "step": 6978
    },
    {
      "epoch": 0.03282783145338062,
      "grad_norm": 3.1455681324005127,
      "learning_rate": 0.00019389044480277613,
      "loss": 0.3927,
      "step": 6979
    },
    {
      "epoch": 0.032832535254993085,
      "grad_norm": 3.2536709308624268,
      "learning_rate": 0.00019388950182466265,
      "loss": 0.3621,
      "step": 6980
    },
    {
      "epoch": 0.03283723905660555,
      "grad_norm": 1.003291130065918,
      "learning_rate": 0.00019388855884654917,
      "loss": 0.1412,
      "step": 6981
    },
    {
      "epoch": 0.03284194285821801,
      "grad_norm": 1.6132378578186035,
      "learning_rate": 0.0001938876158684357,
      "loss": 0.131,
      "step": 6982
    },
    {
      "epoch": 0.032846646659830475,
      "grad_norm": 2.277634620666504,
      "learning_rate": 0.00019388667289032223,
      "loss": 0.3923,
      "step": 6983
    },
    {
      "epoch": 0.03285135046144294,
      "grad_norm": 2.377154588699341,
      "learning_rate": 0.00019388572991220875,
      "loss": 0.2568,
      "step": 6984
    },
    {
      "epoch": 0.0328560542630554,
      "grad_norm": 0.9939454197883606,
      "learning_rate": 0.00019388478693409527,
      "loss": 0.118,
      "step": 6985
    },
    {
      "epoch": 0.032860758064667865,
      "grad_norm": 1.2609198093414307,
      "learning_rate": 0.0001938838439559818,
      "loss": 0.1167,
      "step": 6986
    },
    {
      "epoch": 0.03286546186628033,
      "grad_norm": 0.6994308829307556,
      "learning_rate": 0.00019388290097786833,
      "loss": 0.0442,
      "step": 6987
    },
    {
      "epoch": 0.03287016566789279,
      "grad_norm": 2.6981709003448486,
      "learning_rate": 0.00019388195799975482,
      "loss": 0.4142,
      "step": 6988
    },
    {
      "epoch": 0.032874869469505255,
      "grad_norm": 4.813000679016113,
      "learning_rate": 0.00019388101502164134,
      "loss": 0.8445,
      "step": 6989
    },
    {
      "epoch": 0.03287957327111772,
      "grad_norm": 4.171833515167236,
      "learning_rate": 0.00019388007204352786,
      "loss": 0.4601,
      "step": 6990
    },
    {
      "epoch": 0.03288427707273018,
      "grad_norm": 0.9066559672355652,
      "learning_rate": 0.0001938791290654144,
      "loss": 0.0832,
      "step": 6991
    },
    {
      "epoch": 0.032888980874342645,
      "grad_norm": 0.8895557522773743,
      "learning_rate": 0.00019387818608730093,
      "loss": 0.0867,
      "step": 6992
    },
    {
      "epoch": 0.032893684675955104,
      "grad_norm": 0.43125221133232117,
      "learning_rate": 0.00019387724310918744,
      "loss": 0.0461,
      "step": 6993
    },
    {
      "epoch": 0.03289838847756757,
      "grad_norm": 1.65623939037323,
      "learning_rate": 0.00019387630013107396,
      "loss": 0.1234,
      "step": 6994
    },
    {
      "epoch": 0.032903092279180035,
      "grad_norm": 1.9668760299682617,
      "learning_rate": 0.00019387535715296048,
      "loss": 0.2637,
      "step": 6995
    },
    {
      "epoch": 0.032907796080792494,
      "grad_norm": 2.6951277256011963,
      "learning_rate": 0.00019387441417484703,
      "loss": 0.3197,
      "step": 6996
    },
    {
      "epoch": 0.03291249988240496,
      "grad_norm": 0.760022759437561,
      "learning_rate": 0.00019387347119673355,
      "loss": 0.0464,
      "step": 6997
    },
    {
      "epoch": 0.032917203684017425,
      "grad_norm": 3.769838809967041,
      "learning_rate": 0.00019387252821862007,
      "loss": 0.8711,
      "step": 6998
    },
    {
      "epoch": 0.032921907485629884,
      "grad_norm": 2.7267045974731445,
      "learning_rate": 0.00019387158524050658,
      "loss": 0.5672,
      "step": 6999
    },
    {
      "epoch": 0.03292661128724235,
      "grad_norm": 4.725768089294434,
      "learning_rate": 0.0001938706422623931,
      "loss": 0.8417,
      "step": 7000
    },
    {
      "epoch": 0.032931315088854815,
      "grad_norm": 9.046344757080078,
      "learning_rate": 0.00019386969928427962,
      "loss": 1.0909,
      "step": 7001
    },
    {
      "epoch": 0.032936018890467274,
      "grad_norm": 4.85754919052124,
      "learning_rate": 0.00019386875630616614,
      "loss": 0.8592,
      "step": 7002
    },
    {
      "epoch": 0.03294072269207974,
      "grad_norm": 15.53588581085205,
      "learning_rate": 0.00019386781332805266,
      "loss": 0.9711,
      "step": 7003
    },
    {
      "epoch": 0.032945426493692205,
      "grad_norm": 1.0550793409347534,
      "learning_rate": 0.00019386687034993918,
      "loss": 0.0859,
      "step": 7004
    },
    {
      "epoch": 0.032950130295304664,
      "grad_norm": 3.3750126361846924,
      "learning_rate": 0.00019386592737182572,
      "loss": 0.7323,
      "step": 7005
    },
    {
      "epoch": 0.03295483409691713,
      "grad_norm": 2.7226340770721436,
      "learning_rate": 0.00019386498439371224,
      "loss": 0.288,
      "step": 7006
    },
    {
      "epoch": 0.032959537898529595,
      "grad_norm": 3.5115549564361572,
      "learning_rate": 0.00019386404141559876,
      "loss": 0.5239,
      "step": 7007
    },
    {
      "epoch": 0.032964241700142054,
      "grad_norm": 3.5816707611083984,
      "learning_rate": 0.00019386309843748528,
      "loss": 0.4898,
      "step": 7008
    },
    {
      "epoch": 0.03296894550175452,
      "grad_norm": 1.7633931636810303,
      "learning_rate": 0.0001938621554593718,
      "loss": 0.3004,
      "step": 7009
    },
    {
      "epoch": 0.03297364930336698,
      "grad_norm": 2.24007248878479,
      "learning_rate": 0.00019386121248125832,
      "loss": 0.302,
      "step": 7010
    },
    {
      "epoch": 0.032978353104979444,
      "grad_norm": 0.8528265953063965,
      "learning_rate": 0.00019386026950314483,
      "loss": 0.0959,
      "step": 7011
    },
    {
      "epoch": 0.03298305690659191,
      "grad_norm": 3.025757312774658,
      "learning_rate": 0.00019385932652503135,
      "loss": 0.6201,
      "step": 7012
    },
    {
      "epoch": 0.03298776070820437,
      "grad_norm": 1.536695122718811,
      "learning_rate": 0.00019385838354691787,
      "loss": 0.1792,
      "step": 7013
    },
    {
      "epoch": 0.032992464509816834,
      "grad_norm": 2.5753026008605957,
      "learning_rate": 0.00019385744056880442,
      "loss": 0.4952,
      "step": 7014
    },
    {
      "epoch": 0.0329971683114293,
      "grad_norm": 1.9339540004730225,
      "learning_rate": 0.00019385649759069094,
      "loss": 0.4522,
      "step": 7015
    },
    {
      "epoch": 0.03300187211304176,
      "grad_norm": 1.2004015445709229,
      "learning_rate": 0.00019385555461257745,
      "loss": 0.1581,
      "step": 7016
    },
    {
      "epoch": 0.033006575914654224,
      "grad_norm": 0.9633607864379883,
      "learning_rate": 0.00019385461163446397,
      "loss": 0.1823,
      "step": 7017
    },
    {
      "epoch": 0.03301127971626669,
      "grad_norm": 1.8757390975952148,
      "learning_rate": 0.0001938536686563505,
      "loss": 0.4457,
      "step": 7018
    },
    {
      "epoch": 0.03301598351787915,
      "grad_norm": 2.60441517829895,
      "learning_rate": 0.000193852725678237,
      "loss": 0.5738,
      "step": 7019
    },
    {
      "epoch": 0.033020687319491614,
      "grad_norm": 1.1444344520568848,
      "learning_rate": 0.00019385178270012353,
      "loss": 0.1548,
      "step": 7020
    },
    {
      "epoch": 0.03302539112110408,
      "grad_norm": 0.5843891501426697,
      "learning_rate": 0.00019385083972201005,
      "loss": 0.0693,
      "step": 7021
    },
    {
      "epoch": 0.03303009492271654,
      "grad_norm": 1.3955658674240112,
      "learning_rate": 0.00019384989674389657,
      "loss": 0.1841,
      "step": 7022
    },
    {
      "epoch": 0.033034798724329004,
      "grad_norm": 1.4508180618286133,
      "learning_rate": 0.0001938489537657831,
      "loss": 0.2506,
      "step": 7023
    },
    {
      "epoch": 0.03303950252594147,
      "grad_norm": 2.373845338821411,
      "learning_rate": 0.00019384801078766963,
      "loss": 0.3072,
      "step": 7024
    },
    {
      "epoch": 0.03304420632755393,
      "grad_norm": 0.922997772693634,
      "learning_rate": 0.00019384706780955615,
      "loss": 0.1202,
      "step": 7025
    },
    {
      "epoch": 0.033048910129166394,
      "grad_norm": 0.9539775848388672,
      "learning_rate": 0.00019384612483144267,
      "loss": 0.2602,
      "step": 7026
    },
    {
      "epoch": 0.03305361393077885,
      "grad_norm": 0.8597283363342285,
      "learning_rate": 0.0001938451818533292,
      "loss": 0.1073,
      "step": 7027
    },
    {
      "epoch": 0.03305831773239132,
      "grad_norm": 1.5264784097671509,
      "learning_rate": 0.00019384423887521573,
      "loss": 0.3481,
      "step": 7028
    },
    {
      "epoch": 0.033063021534003784,
      "grad_norm": 2.053499221801758,
      "learning_rate": 0.00019384329589710225,
      "loss": 0.3408,
      "step": 7029
    },
    {
      "epoch": 0.03306772533561624,
      "grad_norm": 1.1307752132415771,
      "learning_rate": 0.00019384235291898874,
      "loss": 0.3057,
      "step": 7030
    },
    {
      "epoch": 0.03307242913722871,
      "grad_norm": 1.0368465185165405,
      "learning_rate": 0.00019384140994087526,
      "loss": 0.225,
      "step": 7031
    },
    {
      "epoch": 0.033077132938841174,
      "grad_norm": 0.8284668922424316,
      "learning_rate": 0.0001938404669627618,
      "loss": 0.1073,
      "step": 7032
    },
    {
      "epoch": 0.03308183674045363,
      "grad_norm": 0.40381360054016113,
      "learning_rate": 0.00019383952398464833,
      "loss": 0.0351,
      "step": 7033
    },
    {
      "epoch": 0.0330865405420661,
      "grad_norm": 1.425614595413208,
      "learning_rate": 0.00019383858100653484,
      "loss": 0.2751,
      "step": 7034
    },
    {
      "epoch": 0.033091244343678564,
      "grad_norm": 4.5542073249816895,
      "learning_rate": 0.00019383763802842136,
      "loss": 0.8257,
      "step": 7035
    },
    {
      "epoch": 0.03309594814529102,
      "grad_norm": 0.8397344350814819,
      "learning_rate": 0.00019383669505030788,
      "loss": 0.168,
      "step": 7036
    },
    {
      "epoch": 0.03310065194690349,
      "grad_norm": 2.680637836456299,
      "learning_rate": 0.00019383575207219443,
      "loss": 0.6068,
      "step": 7037
    },
    {
      "epoch": 0.033105355748515954,
      "grad_norm": 1.494497537612915,
      "learning_rate": 0.00019383480909408095,
      "loss": 0.2937,
      "step": 7038
    },
    {
      "epoch": 0.03311005955012841,
      "grad_norm": 0.47272464632987976,
      "learning_rate": 0.00019383386611596747,
      "loss": 0.064,
      "step": 7039
    },
    {
      "epoch": 0.03311476335174088,
      "grad_norm": 1.1116302013397217,
      "learning_rate": 0.00019383292313785398,
      "loss": 0.2125,
      "step": 7040
    },
    {
      "epoch": 0.033119467153353344,
      "grad_norm": 2.102646827697754,
      "learning_rate": 0.0001938319801597405,
      "loss": 0.3937,
      "step": 7041
    },
    {
      "epoch": 0.0331241709549658,
      "grad_norm": 2.916569948196411,
      "learning_rate": 0.00019383103718162702,
      "loss": 0.6449,
      "step": 7042
    },
    {
      "epoch": 0.03312887475657827,
      "grad_norm": 0.7222225666046143,
      "learning_rate": 0.00019383009420351354,
      "loss": 0.0774,
      "step": 7043
    },
    {
      "epoch": 0.03313357855819073,
      "grad_norm": 1.5848230123519897,
      "learning_rate": 0.00019382915122540006,
      "loss": 0.3115,
      "step": 7044
    },
    {
      "epoch": 0.03313828235980319,
      "grad_norm": 2.667470693588257,
      "learning_rate": 0.00019382820824728658,
      "loss": 0.4182,
      "step": 7045
    },
    {
      "epoch": 0.03314298616141566,
      "grad_norm": 1.7587003707885742,
      "learning_rate": 0.00019382726526917312,
      "loss": 0.3751,
      "step": 7046
    },
    {
      "epoch": 0.03314768996302812,
      "grad_norm": 0.9536260962486267,
      "learning_rate": 0.00019382632229105964,
      "loss": 0.2304,
      "step": 7047
    },
    {
      "epoch": 0.03315239376464058,
      "grad_norm": 1.6783555746078491,
      "learning_rate": 0.00019382537931294616,
      "loss": 0.1941,
      "step": 7048
    },
    {
      "epoch": 0.03315709756625305,
      "grad_norm": 2.1192872524261475,
      "learning_rate": 0.00019382443633483268,
      "loss": 0.3042,
      "step": 7049
    },
    {
      "epoch": 0.03316180136786551,
      "grad_norm": 2.2231409549713135,
      "learning_rate": 0.0001938234933567192,
      "loss": 0.4754,
      "step": 7050
    },
    {
      "epoch": 0.03316650516947797,
      "grad_norm": 5.815262794494629,
      "learning_rate": 0.00019382255037860572,
      "loss": 0.2206,
      "step": 7051
    },
    {
      "epoch": 0.03317120897109044,
      "grad_norm": 2.3821747303009033,
      "learning_rate": 0.00019382160740049223,
      "loss": 0.2408,
      "step": 7052
    },
    {
      "epoch": 0.0331759127727029,
      "grad_norm": 5.203239917755127,
      "learning_rate": 0.00019382066442237875,
      "loss": 0.3354,
      "step": 7053
    },
    {
      "epoch": 0.03318061657431536,
      "grad_norm": 2.6138877868652344,
      "learning_rate": 0.00019381972144426527,
      "loss": 0.2944,
      "step": 7054
    },
    {
      "epoch": 0.03318532037592783,
      "grad_norm": 1.1865497827529907,
      "learning_rate": 0.00019381877846615182,
      "loss": 0.1512,
      "step": 7055
    },
    {
      "epoch": 0.03319002417754029,
      "grad_norm": 2.166372537612915,
      "learning_rate": 0.00019381783548803834,
      "loss": 0.2579,
      "step": 7056
    },
    {
      "epoch": 0.03319472797915275,
      "grad_norm": 2.5564939975738525,
      "learning_rate": 0.00019381689250992485,
      "loss": 0.3907,
      "step": 7057
    },
    {
      "epoch": 0.03319943178076522,
      "grad_norm": 2.956251382827759,
      "learning_rate": 0.00019381594953181137,
      "loss": 0.5401,
      "step": 7058
    },
    {
      "epoch": 0.03320413558237768,
      "grad_norm": 1.000915288925171,
      "learning_rate": 0.00019381500655369792,
      "loss": 0.1039,
      "step": 7059
    },
    {
      "epoch": 0.03320883938399014,
      "grad_norm": 1.74310302734375,
      "learning_rate": 0.00019381406357558444,
      "loss": 0.2893,
      "step": 7060
    },
    {
      "epoch": 0.0332135431856026,
      "grad_norm": 2.4646875858306885,
      "learning_rate": 0.00019381312059747093,
      "loss": 0.2599,
      "step": 7061
    },
    {
      "epoch": 0.033218246987215067,
      "grad_norm": 1.7889797687530518,
      "learning_rate": 0.00019381217761935745,
      "loss": 0.1494,
      "step": 7062
    },
    {
      "epoch": 0.03322295078882753,
      "grad_norm": 2.5258917808532715,
      "learning_rate": 0.00019381123464124397,
      "loss": 0.5927,
      "step": 7063
    },
    {
      "epoch": 0.03322765459043999,
      "grad_norm": 0.644095242023468,
      "learning_rate": 0.0001938102916631305,
      "loss": 0.1083,
      "step": 7064
    },
    {
      "epoch": 0.033232358392052457,
      "grad_norm": 1.5993602275848389,
      "learning_rate": 0.00019380934868501703,
      "loss": 0.2048,
      "step": 7065
    },
    {
      "epoch": 0.03323706219366492,
      "grad_norm": 0.38982754945755005,
      "learning_rate": 0.00019380840570690355,
      "loss": 0.0352,
      "step": 7066
    },
    {
      "epoch": 0.03324176599527738,
      "grad_norm": 0.6465166211128235,
      "learning_rate": 0.00019380746272879007,
      "loss": 0.0928,
      "step": 7067
    },
    {
      "epoch": 0.033246469796889846,
      "grad_norm": 3.5985918045043945,
      "learning_rate": 0.0001938065197506766,
      "loss": 0.6239,
      "step": 7068
    },
    {
      "epoch": 0.03325117359850231,
      "grad_norm": 3.8224315643310547,
      "learning_rate": 0.00019380557677256313,
      "loss": 0.43,
      "step": 7069
    },
    {
      "epoch": 0.03325587740011477,
      "grad_norm": 1.9118800163269043,
      "learning_rate": 0.00019380463379444965,
      "loss": 0.3424,
      "step": 7070
    },
    {
      "epoch": 0.033260581201727236,
      "grad_norm": 2.030230760574341,
      "learning_rate": 0.00019380369081633617,
      "loss": 0.3361,
      "step": 7071
    },
    {
      "epoch": 0.0332652850033397,
      "grad_norm": 0.8586405515670776,
      "learning_rate": 0.0001938027478382227,
      "loss": 0.0701,
      "step": 7072
    },
    {
      "epoch": 0.03326998880495216,
      "grad_norm": 2.1322433948516846,
      "learning_rate": 0.0001938018048601092,
      "loss": 0.5748,
      "step": 7073
    },
    {
      "epoch": 0.033274692606564626,
      "grad_norm": 1.6811832189559937,
      "learning_rate": 0.00019380086188199573,
      "loss": 0.2197,
      "step": 7074
    },
    {
      "epoch": 0.03327939640817709,
      "grad_norm": 2.7922511100769043,
      "learning_rate": 0.00019379991890388224,
      "loss": 0.27,
      "step": 7075
    },
    {
      "epoch": 0.03328410020978955,
      "grad_norm": 1.0728610754013062,
      "learning_rate": 0.00019379897592576876,
      "loss": 0.2018,
      "step": 7076
    },
    {
      "epoch": 0.033288804011402016,
      "grad_norm": 0.45567217469215393,
      "learning_rate": 0.00019379803294765528,
      "loss": 0.0396,
      "step": 7077
    },
    {
      "epoch": 0.033293507813014475,
      "grad_norm": 0.8841723203659058,
      "learning_rate": 0.00019379708996954183,
      "loss": 0.0975,
      "step": 7078
    },
    {
      "epoch": 0.03329821161462694,
      "grad_norm": 2.6907739639282227,
      "learning_rate": 0.00019379614699142835,
      "loss": 0.4324,
      "step": 7079
    },
    {
      "epoch": 0.033302915416239406,
      "grad_norm": 1.8904976844787598,
      "learning_rate": 0.00019379520401331487,
      "loss": 0.4437,
      "step": 7080
    },
    {
      "epoch": 0.033307619217851865,
      "grad_norm": 1.8122663497924805,
      "learning_rate": 0.00019379426103520138,
      "loss": 0.1904,
      "step": 7081
    },
    {
      "epoch": 0.03331232301946433,
      "grad_norm": 2.8804702758789062,
      "learning_rate": 0.0001937933180570879,
      "loss": 0.6095,
      "step": 7082
    },
    {
      "epoch": 0.033317026821076796,
      "grad_norm": 1.7890933752059937,
      "learning_rate": 0.00019379237507897442,
      "loss": 0.3636,
      "step": 7083
    },
    {
      "epoch": 0.033321730622689255,
      "grad_norm": 0.6381569504737854,
      "learning_rate": 0.00019379143210086094,
      "loss": 0.101,
      "step": 7084
    },
    {
      "epoch": 0.03332643442430172,
      "grad_norm": 1.4235516786575317,
      "learning_rate": 0.00019379048912274746,
      "loss": 0.2951,
      "step": 7085
    },
    {
      "epoch": 0.033331138225914186,
      "grad_norm": 0.9080163836479187,
      "learning_rate": 0.00019378954614463398,
      "loss": 0.1086,
      "step": 7086
    },
    {
      "epoch": 0.033335842027526645,
      "grad_norm": 3.5653321743011475,
      "learning_rate": 0.00019378860316652052,
      "loss": 0.6073,
      "step": 7087
    },
    {
      "epoch": 0.03334054582913911,
      "grad_norm": 0.6645449995994568,
      "learning_rate": 0.00019378766018840704,
      "loss": 0.1214,
      "step": 7088
    },
    {
      "epoch": 0.033345249630751576,
      "grad_norm": 1.2826248407363892,
      "learning_rate": 0.00019378671721029356,
      "loss": 0.2004,
      "step": 7089
    },
    {
      "epoch": 0.033349953432364035,
      "grad_norm": 0.8119668960571289,
      "learning_rate": 0.00019378577423218008,
      "loss": 0.1333,
      "step": 7090
    },
    {
      "epoch": 0.0333546572339765,
      "grad_norm": 1.1780873537063599,
      "learning_rate": 0.00019378483125406662,
      "loss": 0.2067,
      "step": 7091
    },
    {
      "epoch": 0.033359361035588966,
      "grad_norm": 1.4247277975082397,
      "learning_rate": 0.00019378388827595312,
      "loss": 0.3001,
      "step": 7092
    },
    {
      "epoch": 0.033364064837201425,
      "grad_norm": 0.8905694484710693,
      "learning_rate": 0.00019378294529783963,
      "loss": 0.111,
      "step": 7093
    },
    {
      "epoch": 0.03336876863881389,
      "grad_norm": 2.1895501613616943,
      "learning_rate": 0.00019378200231972615,
      "loss": 0.1713,
      "step": 7094
    },
    {
      "epoch": 0.03337347244042635,
      "grad_norm": 2.8460941314697266,
      "learning_rate": 0.00019378105934161267,
      "loss": 0.3593,
      "step": 7095
    },
    {
      "epoch": 0.033378176242038815,
      "grad_norm": 4.992747783660889,
      "learning_rate": 0.00019378011636349922,
      "loss": 0.5755,
      "step": 7096
    },
    {
      "epoch": 0.03338288004365128,
      "grad_norm": 2.1858468055725098,
      "learning_rate": 0.00019377917338538574,
      "loss": 0.3374,
      "step": 7097
    },
    {
      "epoch": 0.03338758384526374,
      "grad_norm": 3.7120001316070557,
      "learning_rate": 0.00019377823040727225,
      "loss": 0.5318,
      "step": 7098
    },
    {
      "epoch": 0.033392287646876205,
      "grad_norm": 2.972527027130127,
      "learning_rate": 0.00019377728742915877,
      "loss": 0.5523,
      "step": 7099
    },
    {
      "epoch": 0.03339699144848867,
      "grad_norm": 1.9514261484146118,
      "learning_rate": 0.00019377634445104532,
      "loss": 0.2879,
      "step": 7100
    },
    {
      "epoch": 0.03340169525010113,
      "grad_norm": 1.8015763759613037,
      "learning_rate": 0.00019377540147293184,
      "loss": 0.1787,
      "step": 7101
    },
    {
      "epoch": 0.033406399051713595,
      "grad_norm": 3.066863775253296,
      "learning_rate": 0.00019377445849481836,
      "loss": 0.2088,
      "step": 7102
    },
    {
      "epoch": 0.03341110285332606,
      "grad_norm": 4.256289005279541,
      "learning_rate": 0.00019377351551670488,
      "loss": 0.103,
      "step": 7103
    },
    {
      "epoch": 0.03341580665493852,
      "grad_norm": 6.206126689910889,
      "learning_rate": 0.00019377257253859137,
      "loss": 0.93,
      "step": 7104
    },
    {
      "epoch": 0.033420510456550985,
      "grad_norm": 0.23314546048641205,
      "learning_rate": 0.0001937716295604779,
      "loss": 0.0195,
      "step": 7105
    },
    {
      "epoch": 0.03342521425816345,
      "grad_norm": 4.213676452636719,
      "learning_rate": 0.00019377068658236443,
      "loss": 0.7473,
      "step": 7106
    },
    {
      "epoch": 0.03342991805977591,
      "grad_norm": 3.745028257369995,
      "learning_rate": 0.00019376974360425095,
      "loss": 0.3407,
      "step": 7107
    },
    {
      "epoch": 0.033434621861388375,
      "grad_norm": 1.0352104902267456,
      "learning_rate": 0.00019376880062613747,
      "loss": 0.071,
      "step": 7108
    },
    {
      "epoch": 0.03343932566300084,
      "grad_norm": 2.4488539695739746,
      "learning_rate": 0.00019376785764802401,
      "loss": 0.1257,
      "step": 7109
    },
    {
      "epoch": 0.0334440294646133,
      "grad_norm": 5.146512508392334,
      "learning_rate": 0.00019376691466991053,
      "loss": 1.0013,
      "step": 7110
    },
    {
      "epoch": 0.033448733266225765,
      "grad_norm": 1.1758599281311035,
      "learning_rate": 0.00019376597169179705,
      "loss": 0.0817,
      "step": 7111
    },
    {
      "epoch": 0.033453437067838224,
      "grad_norm": 1.7813711166381836,
      "learning_rate": 0.00019376502871368357,
      "loss": 0.1675,
      "step": 7112
    },
    {
      "epoch": 0.03345814086945069,
      "grad_norm": 3.083369255065918,
      "learning_rate": 0.0001937640857355701,
      "loss": 0.332,
      "step": 7113
    },
    {
      "epoch": 0.033462844671063155,
      "grad_norm": 3.316448450088501,
      "learning_rate": 0.0001937631427574566,
      "loss": 0.3787,
      "step": 7114
    },
    {
      "epoch": 0.033467548472675614,
      "grad_norm": 2.219399929046631,
      "learning_rate": 0.00019376219977934313,
      "loss": 0.3091,
      "step": 7115
    },
    {
      "epoch": 0.03347225227428808,
      "grad_norm": 2.4226255416870117,
      "learning_rate": 0.00019376125680122964,
      "loss": 0.121,
      "step": 7116
    },
    {
      "epoch": 0.033476956075900545,
      "grad_norm": 2.411874532699585,
      "learning_rate": 0.00019376031382311616,
      "loss": 0.3414,
      "step": 7117
    },
    {
      "epoch": 0.033481659877513004,
      "grad_norm": 1.8067489862442017,
      "learning_rate": 0.0001937593708450027,
      "loss": 0.1979,
      "step": 7118
    },
    {
      "epoch": 0.03348636367912547,
      "grad_norm": 2.3972322940826416,
      "learning_rate": 0.00019375842786688923,
      "loss": 0.4467,
      "step": 7119
    },
    {
      "epoch": 0.033491067480737935,
      "grad_norm": 3.269998550415039,
      "learning_rate": 0.00019375748488877575,
      "loss": 0.4648,
      "step": 7120
    },
    {
      "epoch": 0.033495771282350394,
      "grad_norm": 3.5407347679138184,
      "learning_rate": 0.00019375654191066227,
      "loss": 0.8386,
      "step": 7121
    },
    {
      "epoch": 0.03350047508396286,
      "grad_norm": 1.590772032737732,
      "learning_rate": 0.00019375559893254878,
      "loss": 0.1962,
      "step": 7122
    },
    {
      "epoch": 0.033505178885575325,
      "grad_norm": 4.863162994384766,
      "learning_rate": 0.0001937546559544353,
      "loss": 0.605,
      "step": 7123
    },
    {
      "epoch": 0.033509882687187784,
      "grad_norm": 0.7959831357002258,
      "learning_rate": 0.00019375371297632182,
      "loss": 0.1074,
      "step": 7124
    },
    {
      "epoch": 0.03351458648880025,
      "grad_norm": 1.2095718383789062,
      "learning_rate": 0.00019375276999820834,
      "loss": 0.1754,
      "step": 7125
    },
    {
      "epoch": 0.033519290290412715,
      "grad_norm": 2.0084798336029053,
      "learning_rate": 0.00019375182702009486,
      "loss": 0.4709,
      "step": 7126
    },
    {
      "epoch": 0.033523994092025174,
      "grad_norm": 1.6294461488723755,
      "learning_rate": 0.00019375088404198138,
      "loss": 0.2737,
      "step": 7127
    },
    {
      "epoch": 0.03352869789363764,
      "grad_norm": 2.2236742973327637,
      "learning_rate": 0.00019374994106386792,
      "loss": 0.2817,
      "step": 7128
    },
    {
      "epoch": 0.0335334016952501,
      "grad_norm": 1.2869778871536255,
      "learning_rate": 0.00019374899808575444,
      "loss": 0.1509,
      "step": 7129
    },
    {
      "epoch": 0.033538105496862564,
      "grad_norm": 2.1254327297210693,
      "learning_rate": 0.00019374805510764096,
      "loss": 0.4484,
      "step": 7130
    },
    {
      "epoch": 0.03354280929847503,
      "grad_norm": 0.9289021492004395,
      "learning_rate": 0.00019374711212952748,
      "loss": 0.0946,
      "step": 7131
    },
    {
      "epoch": 0.03354751310008749,
      "grad_norm": 1.7224926948547363,
      "learning_rate": 0.00019374616915141402,
      "loss": 0.5501,
      "step": 7132
    },
    {
      "epoch": 0.033552216901699954,
      "grad_norm": 0.6566666960716248,
      "learning_rate": 0.00019374522617330054,
      "loss": 0.1177,
      "step": 7133
    },
    {
      "epoch": 0.03355692070331242,
      "grad_norm": 1.5783435106277466,
      "learning_rate": 0.00019374428319518706,
      "loss": 0.2456,
      "step": 7134
    },
    {
      "epoch": 0.03356162450492488,
      "grad_norm": 1.3954871892929077,
      "learning_rate": 0.00019374334021707355,
      "loss": 0.1829,
      "step": 7135
    },
    {
      "epoch": 0.033566328306537344,
      "grad_norm": 1.1684998273849487,
      "learning_rate": 0.00019374239723896007,
      "loss": 0.2359,
      "step": 7136
    },
    {
      "epoch": 0.03357103210814981,
      "grad_norm": 1.3496922254562378,
      "learning_rate": 0.00019374145426084662,
      "loss": 0.1997,
      "step": 7137
    },
    {
      "epoch": 0.03357573590976227,
      "grad_norm": 1.8394232988357544,
      "learning_rate": 0.00019374051128273314,
      "loss": 0.5423,
      "step": 7138
    },
    {
      "epoch": 0.033580439711374734,
      "grad_norm": 1.352892279624939,
      "learning_rate": 0.00019373956830461965,
      "loss": 0.2102,
      "step": 7139
    },
    {
      "epoch": 0.0335851435129872,
      "grad_norm": 1.097839593887329,
      "learning_rate": 0.00019373862532650617,
      "loss": 0.1233,
      "step": 7140
    },
    {
      "epoch": 0.03358984731459966,
      "grad_norm": 0.5774573683738708,
      "learning_rate": 0.00019373768234839272,
      "loss": 0.0523,
      "step": 7141
    },
    {
      "epoch": 0.033594551116212124,
      "grad_norm": 2.1361913681030273,
      "learning_rate": 0.00019373673937027924,
      "loss": 0.3501,
      "step": 7142
    },
    {
      "epoch": 0.03359925491782459,
      "grad_norm": 1.1757054328918457,
      "learning_rate": 0.00019373579639216576,
      "loss": 0.1328,
      "step": 7143
    },
    {
      "epoch": 0.03360395871943705,
      "grad_norm": 1.9894719123840332,
      "learning_rate": 0.00019373485341405228,
      "loss": 0.225,
      "step": 7144
    },
    {
      "epoch": 0.033608662521049514,
      "grad_norm": 2.5464911460876465,
      "learning_rate": 0.0001937339104359388,
      "loss": 0.5612,
      "step": 7145
    },
    {
      "epoch": 0.03361336632266197,
      "grad_norm": 2.106503486633301,
      "learning_rate": 0.0001937329674578253,
      "loss": 0.4899,
      "step": 7146
    },
    {
      "epoch": 0.03361807012427444,
      "grad_norm": 0.5366644263267517,
      "learning_rate": 0.00019373202447971183,
      "loss": 0.0926,
      "step": 7147
    },
    {
      "epoch": 0.033622773925886904,
      "grad_norm": 1.4007688760757446,
      "learning_rate": 0.00019373108150159835,
      "loss": 0.1935,
      "step": 7148
    },
    {
      "epoch": 0.03362747772749936,
      "grad_norm": 3.4692742824554443,
      "learning_rate": 0.00019373013852348487,
      "loss": 0.4706,
      "step": 7149
    },
    {
      "epoch": 0.03363218152911183,
      "grad_norm": 0.9438860416412354,
      "learning_rate": 0.00019372919554537141,
      "loss": 0.1037,
      "step": 7150
    },
    {
      "epoch": 0.033636885330724293,
      "grad_norm": 5.124773979187012,
      "learning_rate": 0.00019372825256725793,
      "loss": 0.7648,
      "step": 7151
    },
    {
      "epoch": 0.03364158913233675,
      "grad_norm": 0.09086205065250397,
      "learning_rate": 0.00019372730958914445,
      "loss": 0.0047,
      "step": 7152
    },
    {
      "epoch": 0.03364629293394922,
      "grad_norm": 2.198338270187378,
      "learning_rate": 0.00019372636661103097,
      "loss": 0.2106,
      "step": 7153
    },
    {
      "epoch": 0.033650996735561683,
      "grad_norm": 1.140523076057434,
      "learning_rate": 0.0001937254236329175,
      "loss": 0.1902,
      "step": 7154
    },
    {
      "epoch": 0.03365570053717414,
      "grad_norm": 1.7140758037567139,
      "learning_rate": 0.000193724480654804,
      "loss": 0.3502,
      "step": 7155
    },
    {
      "epoch": 0.03366040433878661,
      "grad_norm": 0.2547084391117096,
      "learning_rate": 0.00019372353767669053,
      "loss": 0.0317,
      "step": 7156
    },
    {
      "epoch": 0.03366510814039907,
      "grad_norm": 1.5469229221343994,
      "learning_rate": 0.00019372259469857704,
      "loss": 0.2884,
      "step": 7157
    },
    {
      "epoch": 0.03366981194201153,
      "grad_norm": 0.907678484916687,
      "learning_rate": 0.00019372165172046356,
      "loss": 0.1613,
      "step": 7158
    },
    {
      "epoch": 0.033674515743624,
      "grad_norm": 2.8462913036346436,
      "learning_rate": 0.0001937207087423501,
      "loss": 0.2986,
      "step": 7159
    },
    {
      "epoch": 0.03367921954523646,
      "grad_norm": 1.2466715574264526,
      "learning_rate": 0.00019371976576423663,
      "loss": 0.2066,
      "step": 7160
    },
    {
      "epoch": 0.03368392334684892,
      "grad_norm": 0.8889022469520569,
      "learning_rate": 0.00019371882278612315,
      "loss": 0.0696,
      "step": 7161
    },
    {
      "epoch": 0.03368862714846139,
      "grad_norm": 0.9585548639297485,
      "learning_rate": 0.00019371787980800966,
      "loss": 0.0864,
      "step": 7162
    },
    {
      "epoch": 0.033693330950073846,
      "grad_norm": 3.3430867195129395,
      "learning_rate": 0.00019371693682989618,
      "loss": 0.4651,
      "step": 7163
    },
    {
      "epoch": 0.03369803475168631,
      "grad_norm": 0.686732292175293,
      "learning_rate": 0.00019371599385178273,
      "loss": 0.0672,
      "step": 7164
    },
    {
      "epoch": 0.03370273855329878,
      "grad_norm": 1.0860642194747925,
      "learning_rate": 0.00019371505087366925,
      "loss": 0.1045,
      "step": 7165
    },
    {
      "epoch": 0.033707442354911236,
      "grad_norm": 0.6035283207893372,
      "learning_rate": 0.00019371410789555574,
      "loss": 0.0415,
      "step": 7166
    },
    {
      "epoch": 0.0337121461565237,
      "grad_norm": 2.229931354522705,
      "learning_rate": 0.00019371316491744226,
      "loss": 0.4527,
      "step": 7167
    },
    {
      "epoch": 0.03371684995813617,
      "grad_norm": 3.608062982559204,
      "learning_rate": 0.0001937122219393288,
      "loss": 0.3061,
      "step": 7168
    },
    {
      "epoch": 0.033721553759748626,
      "grad_norm": 1.2892751693725586,
      "learning_rate": 0.00019371127896121532,
      "loss": 0.1839,
      "step": 7169
    },
    {
      "epoch": 0.03372625756136109,
      "grad_norm": 1.9808214902877808,
      "learning_rate": 0.00019371033598310184,
      "loss": 0.1902,
      "step": 7170
    },
    {
      "epoch": 0.03373096136297356,
      "grad_norm": 2.6921448707580566,
      "learning_rate": 0.00019370939300498836,
      "loss": 0.3483,
      "step": 7171
    },
    {
      "epoch": 0.033735665164586016,
      "grad_norm": 1.6164177656173706,
      "learning_rate": 0.00019370845002687488,
      "loss": 0.1698,
      "step": 7172
    },
    {
      "epoch": 0.03374036896619848,
      "grad_norm": 6.389256000518799,
      "learning_rate": 0.00019370750704876142,
      "loss": 0.6368,
      "step": 7173
    },
    {
      "epoch": 0.03374507276781095,
      "grad_norm": 1.103724479675293,
      "learning_rate": 0.00019370656407064794,
      "loss": 0.1092,
      "step": 7174
    },
    {
      "epoch": 0.033749776569423406,
      "grad_norm": 2.641620397567749,
      "learning_rate": 0.00019370562109253446,
      "loss": 0.6884,
      "step": 7175
    },
    {
      "epoch": 0.03375448037103587,
      "grad_norm": 4.328917980194092,
      "learning_rate": 0.00019370467811442098,
      "loss": 1.0781,
      "step": 7176
    },
    {
      "epoch": 0.03375918417264834,
      "grad_norm": 1.648179054260254,
      "learning_rate": 0.00019370373513630747,
      "loss": 0.2113,
      "step": 7177
    },
    {
      "epoch": 0.033763887974260796,
      "grad_norm": 0.8624279499053955,
      "learning_rate": 0.00019370279215819402,
      "loss": 0.0984,
      "step": 7178
    },
    {
      "epoch": 0.03376859177587326,
      "grad_norm": 3.0031979084014893,
      "learning_rate": 0.00019370184918008054,
      "loss": 0.7517,
      "step": 7179
    },
    {
      "epoch": 0.03377329557748572,
      "grad_norm": 2.4095547199249268,
      "learning_rate": 0.00019370090620196705,
      "loss": 0.3841,
      "step": 7180
    },
    {
      "epoch": 0.033777999379098186,
      "grad_norm": 5.716548919677734,
      "learning_rate": 0.00019369996322385357,
      "loss": 0.7011,
      "step": 7181
    },
    {
      "epoch": 0.03378270318071065,
      "grad_norm": 0.9816299676895142,
      "learning_rate": 0.00019369902024574012,
      "loss": 0.0817,
      "step": 7182
    },
    {
      "epoch": 0.03378740698232311,
      "grad_norm": 2.1237401962280273,
      "learning_rate": 0.00019369807726762664,
      "loss": 0.5082,
      "step": 7183
    },
    {
      "epoch": 0.033792110783935576,
      "grad_norm": 1.5029850006103516,
      "learning_rate": 0.00019369713428951316,
      "loss": 0.3118,
      "step": 7184
    },
    {
      "epoch": 0.03379681458554804,
      "grad_norm": 1.105517029762268,
      "learning_rate": 0.00019369619131139968,
      "loss": 0.1129,
      "step": 7185
    },
    {
      "epoch": 0.0338015183871605,
      "grad_norm": 1.4518779516220093,
      "learning_rate": 0.0001936952483332862,
      "loss": 0.4067,
      "step": 7186
    },
    {
      "epoch": 0.033806222188772966,
      "grad_norm": 1.8039013147354126,
      "learning_rate": 0.0001936943053551727,
      "loss": 0.3252,
      "step": 7187
    },
    {
      "epoch": 0.03381092599038543,
      "grad_norm": 2.1100223064422607,
      "learning_rate": 0.00019369336237705923,
      "loss": 0.2305,
      "step": 7188
    },
    {
      "epoch": 0.03381562979199789,
      "grad_norm": 0.68897545337677,
      "learning_rate": 0.00019369241939894575,
      "loss": 0.1107,
      "step": 7189
    },
    {
      "epoch": 0.033820333593610356,
      "grad_norm": 1.4445767402648926,
      "learning_rate": 0.00019369147642083227,
      "loss": 0.3952,
      "step": 7190
    },
    {
      "epoch": 0.03382503739522282,
      "grad_norm": 2.1501858234405518,
      "learning_rate": 0.00019369053344271881,
      "loss": 0.5692,
      "step": 7191
    },
    {
      "epoch": 0.03382974119683528,
      "grad_norm": 0.5376213192939758,
      "learning_rate": 0.00019368959046460533,
      "loss": 0.0593,
      "step": 7192
    },
    {
      "epoch": 0.033834444998447746,
      "grad_norm": 1.0416111946105957,
      "learning_rate": 0.00019368864748649185,
      "loss": 0.1692,
      "step": 7193
    },
    {
      "epoch": 0.03383914880006021,
      "grad_norm": 2.079378843307495,
      "learning_rate": 0.00019368770450837837,
      "loss": 0.4538,
      "step": 7194
    },
    {
      "epoch": 0.03384385260167267,
      "grad_norm": 0.7953429818153381,
      "learning_rate": 0.0001936867615302649,
      "loss": 0.2708,
      "step": 7195
    },
    {
      "epoch": 0.033848556403285136,
      "grad_norm": 0.778893768787384,
      "learning_rate": 0.00019368581855215143,
      "loss": 0.1339,
      "step": 7196
    },
    {
      "epoch": 0.033853260204897595,
      "grad_norm": 0.9352437257766724,
      "learning_rate": 0.00019368487557403793,
      "loss": 0.3149,
      "step": 7197
    },
    {
      "epoch": 0.03385796400651006,
      "grad_norm": 1.5481938123703003,
      "learning_rate": 0.00019368393259592444,
      "loss": 0.1644,
      "step": 7198
    },
    {
      "epoch": 0.033862667808122526,
      "grad_norm": 0.565386176109314,
      "learning_rate": 0.00019368298961781096,
      "loss": 0.0719,
      "step": 7199
    },
    {
      "epoch": 0.033867371609734985,
      "grad_norm": 2.176149606704712,
      "learning_rate": 0.0001936820466396975,
      "loss": 0.3758,
      "step": 7200
    },
    {
      "epoch": 0.03387207541134745,
      "grad_norm": 2.7418441772460938,
      "learning_rate": 0.00019368110366158403,
      "loss": 0.0617,
      "step": 7201
    },
    {
      "epoch": 0.033876779212959916,
      "grad_norm": 6.473177433013916,
      "learning_rate": 0.00019368016068347055,
      "loss": 0.2381,
      "step": 7202
    },
    {
      "epoch": 0.033881483014572375,
      "grad_norm": 5.553435802459717,
      "learning_rate": 0.00019367921770535706,
      "loss": 0.064,
      "step": 7203
    },
    {
      "epoch": 0.03388618681618484,
      "grad_norm": 6.297757148742676,
      "learning_rate": 0.00019367827472724358,
      "loss": 0.7301,
      "step": 7204
    },
    {
      "epoch": 0.033890890617797306,
      "grad_norm": 1.7770969867706299,
      "learning_rate": 0.00019367733174913013,
      "loss": 0.3171,
      "step": 7205
    },
    {
      "epoch": 0.033895594419409765,
      "grad_norm": 0.8283272385597229,
      "learning_rate": 0.00019367638877101665,
      "loss": 0.0757,
      "step": 7206
    },
    {
      "epoch": 0.03390029822102223,
      "grad_norm": 2.246899366378784,
      "learning_rate": 0.00019367544579290317,
      "loss": 0.2804,
      "step": 7207
    },
    {
      "epoch": 0.033905002022634696,
      "grad_norm": 1.8394834995269775,
      "learning_rate": 0.00019367450281478966,
      "loss": 0.2304,
      "step": 7208
    },
    {
      "epoch": 0.033909705824247155,
      "grad_norm": 2.1683552265167236,
      "learning_rate": 0.0001936735598366762,
      "loss": 0.3568,
      "step": 7209
    },
    {
      "epoch": 0.03391440962585962,
      "grad_norm": 2.428302049636841,
      "learning_rate": 0.00019367261685856272,
      "loss": 0.1918,
      "step": 7210
    },
    {
      "epoch": 0.033919113427472086,
      "grad_norm": 0.6314009428024292,
      "learning_rate": 0.00019367167388044924,
      "loss": 0.0706,
      "step": 7211
    },
    {
      "epoch": 0.033923817229084545,
      "grad_norm": 1.6011431217193604,
      "learning_rate": 0.00019367073090233576,
      "loss": 0.1699,
      "step": 7212
    },
    {
      "epoch": 0.03392852103069701,
      "grad_norm": 2.030592679977417,
      "learning_rate": 0.00019366978792422228,
      "loss": 0.2251,
      "step": 7213
    },
    {
      "epoch": 0.03393322483230947,
      "grad_norm": 5.8578081130981445,
      "learning_rate": 0.00019366884494610882,
      "loss": 0.4947,
      "step": 7214
    },
    {
      "epoch": 0.033937928633921935,
      "grad_norm": 1.2632036209106445,
      "learning_rate": 0.00019366790196799534,
      "loss": 0.1089,
      "step": 7215
    },
    {
      "epoch": 0.0339426324355344,
      "grad_norm": 4.5216546058654785,
      "learning_rate": 0.00019366695898988186,
      "loss": 0.6435,
      "step": 7216
    },
    {
      "epoch": 0.03394733623714686,
      "grad_norm": 5.176784992218018,
      "learning_rate": 0.00019366601601176838,
      "loss": 1.0104,
      "step": 7217
    },
    {
      "epoch": 0.033952040038759325,
      "grad_norm": 1.8132407665252686,
      "learning_rate": 0.0001936650730336549,
      "loss": 0.3366,
      "step": 7218
    },
    {
      "epoch": 0.03395674384037179,
      "grad_norm": 1.1588795185089111,
      "learning_rate": 0.00019366413005554142,
      "loss": 0.1363,
      "step": 7219
    },
    {
      "epoch": 0.03396144764198425,
      "grad_norm": 0.7489223480224609,
      "learning_rate": 0.00019366318707742794,
      "loss": 0.0705,
      "step": 7220
    },
    {
      "epoch": 0.033966151443596715,
      "grad_norm": 1.6465399265289307,
      "learning_rate": 0.00019366224409931445,
      "loss": 0.3179,
      "step": 7221
    },
    {
      "epoch": 0.03397085524520918,
      "grad_norm": 2.5408694744110107,
      "learning_rate": 0.00019366130112120097,
      "loss": 0.2463,
      "step": 7222
    },
    {
      "epoch": 0.03397555904682164,
      "grad_norm": 0.538841962814331,
      "learning_rate": 0.00019366035814308752,
      "loss": 0.0432,
      "step": 7223
    },
    {
      "epoch": 0.033980262848434105,
      "grad_norm": 3.726210355758667,
      "learning_rate": 0.00019365941516497404,
      "loss": 0.5083,
      "step": 7224
    },
    {
      "epoch": 0.03398496665004657,
      "grad_norm": 2.482672691345215,
      "learning_rate": 0.00019365847218686056,
      "loss": 0.481,
      "step": 7225
    },
    {
      "epoch": 0.03398967045165903,
      "grad_norm": 3.2870888710021973,
      "learning_rate": 0.00019365752920874708,
      "loss": 0.6665,
      "step": 7226
    },
    {
      "epoch": 0.033994374253271495,
      "grad_norm": 2.5769617557525635,
      "learning_rate": 0.0001936565862306336,
      "loss": 0.3584,
      "step": 7227
    },
    {
      "epoch": 0.03399907805488396,
      "grad_norm": 0.9138727784156799,
      "learning_rate": 0.0001936556432525201,
      "loss": 0.2212,
      "step": 7228
    },
    {
      "epoch": 0.03400378185649642,
      "grad_norm": 2.647233724594116,
      "learning_rate": 0.00019365470027440663,
      "loss": 0.298,
      "step": 7229
    },
    {
      "epoch": 0.034008485658108885,
      "grad_norm": 1.4005255699157715,
      "learning_rate": 0.00019365375729629315,
      "loss": 0.2286,
      "step": 7230
    },
    {
      "epoch": 0.034013189459721344,
      "grad_norm": 6.140973091125488,
      "learning_rate": 0.00019365281431817967,
      "loss": 0.7106,
      "step": 7231
    },
    {
      "epoch": 0.03401789326133381,
      "grad_norm": 0.6622228026390076,
      "learning_rate": 0.00019365187134006621,
      "loss": 0.1034,
      "step": 7232
    },
    {
      "epoch": 0.034022597062946275,
      "grad_norm": 3.070244312286377,
      "learning_rate": 0.00019365092836195273,
      "loss": 0.7665,
      "step": 7233
    },
    {
      "epoch": 0.034027300864558734,
      "grad_norm": 2.20263671875,
      "learning_rate": 0.00019364998538383925,
      "loss": 0.2187,
      "step": 7234
    },
    {
      "epoch": 0.0340320046661712,
      "grad_norm": 1.966347098350525,
      "learning_rate": 0.00019364904240572577,
      "loss": 0.2739,
      "step": 7235
    },
    {
      "epoch": 0.034036708467783665,
      "grad_norm": 8.26419448852539,
      "learning_rate": 0.0001936480994276123,
      "loss": 0.7853,
      "step": 7236
    },
    {
      "epoch": 0.034041412269396124,
      "grad_norm": 1.702906608581543,
      "learning_rate": 0.00019364715644949883,
      "loss": 0.346,
      "step": 7237
    },
    {
      "epoch": 0.03404611607100859,
      "grad_norm": 2.5126733779907227,
      "learning_rate": 0.00019364621347138535,
      "loss": 0.5566,
      "step": 7238
    },
    {
      "epoch": 0.034050819872621055,
      "grad_norm": 1.1966437101364136,
      "learning_rate": 0.00019364527049327184,
      "loss": 0.1636,
      "step": 7239
    },
    {
      "epoch": 0.034055523674233514,
      "grad_norm": 1.8893669843673706,
      "learning_rate": 0.00019364432751515836,
      "loss": 0.2896,
      "step": 7240
    },
    {
      "epoch": 0.03406022747584598,
      "grad_norm": 0.9056746363639832,
      "learning_rate": 0.0001936433845370449,
      "loss": 0.218,
      "step": 7241
    },
    {
      "epoch": 0.034064931277458445,
      "grad_norm": 0.7801287174224854,
      "learning_rate": 0.00019364244155893143,
      "loss": 0.1895,
      "step": 7242
    },
    {
      "epoch": 0.034069635079070903,
      "grad_norm": 0.7153329253196716,
      "learning_rate": 0.00019364149858081795,
      "loss": 0.0885,
      "step": 7243
    },
    {
      "epoch": 0.03407433888068337,
      "grad_norm": 1.691687822341919,
      "learning_rate": 0.00019364055560270446,
      "loss": 0.2801,
      "step": 7244
    },
    {
      "epoch": 0.034079042682295835,
      "grad_norm": 1.6489636898040771,
      "learning_rate": 0.00019363961262459098,
      "loss": 0.1412,
      "step": 7245
    },
    {
      "epoch": 0.034083746483908293,
      "grad_norm": 1.4580950736999512,
      "learning_rate": 0.00019363866964647753,
      "loss": 0.3751,
      "step": 7246
    },
    {
      "epoch": 0.03408845028552076,
      "grad_norm": 0.7673486471176147,
      "learning_rate": 0.00019363772666836405,
      "loss": 0.0709,
      "step": 7247
    },
    {
      "epoch": 0.03409315408713322,
      "grad_norm": 1.0301250219345093,
      "learning_rate": 0.00019363678369025057,
      "loss": 0.1037,
      "step": 7248
    },
    {
      "epoch": 0.03409785788874568,
      "grad_norm": 1.94977867603302,
      "learning_rate": 0.00019363584071213709,
      "loss": 0.3597,
      "step": 7249
    },
    {
      "epoch": 0.03410256169035815,
      "grad_norm": 0.8972741365432739,
      "learning_rate": 0.0001936348977340236,
      "loss": 0.1278,
      "step": 7250
    },
    {
      "epoch": 0.03410726549197061,
      "grad_norm": 6.844522953033447,
      "learning_rate": 0.00019363395475591012,
      "loss": 1.0621,
      "step": 7251
    },
    {
      "epoch": 0.03411196929358307,
      "grad_norm": 0.2423684000968933,
      "learning_rate": 0.00019363301177779664,
      "loss": 0.0042,
      "step": 7252
    },
    {
      "epoch": 0.03411667309519554,
      "grad_norm": 2.334702968597412,
      "learning_rate": 0.00019363206879968316,
      "loss": 0.3758,
      "step": 7253
    },
    {
      "epoch": 0.034121376896808,
      "grad_norm": 0.7527632117271423,
      "learning_rate": 0.00019363112582156968,
      "loss": 0.1003,
      "step": 7254
    },
    {
      "epoch": 0.03412608069842046,
      "grad_norm": 2.484199285507202,
      "learning_rate": 0.00019363018284345622,
      "loss": 0.1351,
      "step": 7255
    },
    {
      "epoch": 0.03413078450003293,
      "grad_norm": 3.6860716342926025,
      "learning_rate": 0.00019362923986534274,
      "loss": 0.4819,
      "step": 7256
    },
    {
      "epoch": 0.03413548830164539,
      "grad_norm": 4.069406032562256,
      "learning_rate": 0.00019362829688722926,
      "loss": 0.4852,
      "step": 7257
    },
    {
      "epoch": 0.03414019210325785,
      "grad_norm": 2.401106834411621,
      "learning_rate": 0.00019362735390911578,
      "loss": 0.2551,
      "step": 7258
    },
    {
      "epoch": 0.03414489590487032,
      "grad_norm": 2.451016902923584,
      "learning_rate": 0.0001936264109310023,
      "loss": 0.5618,
      "step": 7259
    },
    {
      "epoch": 0.03414959970648278,
      "grad_norm": 1.6824826002120972,
      "learning_rate": 0.00019362546795288882,
      "loss": 0.2011,
      "step": 7260
    },
    {
      "epoch": 0.03415430350809524,
      "grad_norm": 0.9968608021736145,
      "learning_rate": 0.00019362452497477534,
      "loss": 0.0824,
      "step": 7261
    },
    {
      "epoch": 0.03415900730970771,
      "grad_norm": 1.3141803741455078,
      "learning_rate": 0.00019362358199666185,
      "loss": 0.1197,
      "step": 7262
    },
    {
      "epoch": 0.03416371111132017,
      "grad_norm": 3.0318288803100586,
      "learning_rate": 0.00019362263901854837,
      "loss": 0.5443,
      "step": 7263
    },
    {
      "epoch": 0.03416841491293263,
      "grad_norm": 0.8827099800109863,
      "learning_rate": 0.00019362169604043492,
      "loss": 0.1184,
      "step": 7264
    },
    {
      "epoch": 0.03417311871454509,
      "grad_norm": 1.7355711460113525,
      "learning_rate": 0.00019362075306232144,
      "loss": 0.2641,
      "step": 7265
    },
    {
      "epoch": 0.03417782251615756,
      "grad_norm": 0.6311693787574768,
      "learning_rate": 0.00019361981008420796,
      "loss": 0.0557,
      "step": 7266
    },
    {
      "epoch": 0.03418252631777002,
      "grad_norm": 0.8182794451713562,
      "learning_rate": 0.00019361886710609448,
      "loss": 0.0703,
      "step": 7267
    },
    {
      "epoch": 0.03418723011938248,
      "grad_norm": 3.0047473907470703,
      "learning_rate": 0.00019361792412798102,
      "loss": 0.3687,
      "step": 7268
    },
    {
      "epoch": 0.03419193392099495,
      "grad_norm": 5.262264728546143,
      "learning_rate": 0.00019361698114986754,
      "loss": 1.2772,
      "step": 7269
    },
    {
      "epoch": 0.03419663772260741,
      "grad_norm": 3.3304550647735596,
      "learning_rate": 0.00019361603817175403,
      "loss": 0.6227,
      "step": 7270
    },
    {
      "epoch": 0.03420134152421987,
      "grad_norm": 2.808131694793701,
      "learning_rate": 0.00019361509519364055,
      "loss": 0.3628,
      "step": 7271
    },
    {
      "epoch": 0.03420604532583234,
      "grad_norm": 1.2870025634765625,
      "learning_rate": 0.00019361415221552707,
      "loss": 0.2247,
      "step": 7272
    },
    {
      "epoch": 0.0342107491274448,
      "grad_norm": 2.670816659927368,
      "learning_rate": 0.00019361320923741361,
      "loss": 0.5587,
      "step": 7273
    },
    {
      "epoch": 0.03421545292905726,
      "grad_norm": 3.207073211669922,
      "learning_rate": 0.00019361226625930013,
      "loss": 0.7201,
      "step": 7274
    },
    {
      "epoch": 0.03422015673066973,
      "grad_norm": 3.5531365871429443,
      "learning_rate": 0.00019361132328118665,
      "loss": 0.5103,
      "step": 7275
    },
    {
      "epoch": 0.03422486053228219,
      "grad_norm": 1.7485567331314087,
      "learning_rate": 0.00019361038030307317,
      "loss": 0.4164,
      "step": 7276
    },
    {
      "epoch": 0.03422956433389465,
      "grad_norm": 0.7634260654449463,
      "learning_rate": 0.0001936094373249597,
      "loss": 0.0588,
      "step": 7277
    },
    {
      "epoch": 0.03423426813550712,
      "grad_norm": 2.4531009197235107,
      "learning_rate": 0.00019360849434684623,
      "loss": 0.5093,
      "step": 7278
    },
    {
      "epoch": 0.03423897193711958,
      "grad_norm": 1.2886542081832886,
      "learning_rate": 0.00019360755136873275,
      "loss": 0.3107,
      "step": 7279
    },
    {
      "epoch": 0.03424367573873204,
      "grad_norm": 3.2141494750976562,
      "learning_rate": 0.00019360660839061927,
      "loss": 0.7675,
      "step": 7280
    },
    {
      "epoch": 0.03424837954034451,
      "grad_norm": 1.7001385688781738,
      "learning_rate": 0.00019360566541250576,
      "loss": 0.4098,
      "step": 7281
    },
    {
      "epoch": 0.034253083341956966,
      "grad_norm": 1.4376916885375977,
      "learning_rate": 0.0001936047224343923,
      "loss": 0.3956,
      "step": 7282
    },
    {
      "epoch": 0.03425778714356943,
      "grad_norm": 1.3779009580612183,
      "learning_rate": 0.00019360377945627883,
      "loss": 0.3002,
      "step": 7283
    },
    {
      "epoch": 0.0342624909451819,
      "grad_norm": 0.7257205843925476,
      "learning_rate": 0.00019360283647816535,
      "loss": 0.1608,
      "step": 7284
    },
    {
      "epoch": 0.034267194746794356,
      "grad_norm": 0.7862487435340881,
      "learning_rate": 0.00019360189350005186,
      "loss": 0.1442,
      "step": 7285
    },
    {
      "epoch": 0.03427189854840682,
      "grad_norm": 2.45139217376709,
      "learning_rate": 0.00019360095052193838,
      "loss": 0.6762,
      "step": 7286
    },
    {
      "epoch": 0.03427660235001929,
      "grad_norm": 1.2190366983413696,
      "learning_rate": 0.00019360000754382493,
      "loss": 0.2242,
      "step": 7287
    },
    {
      "epoch": 0.034281306151631746,
      "grad_norm": 0.5508155226707458,
      "learning_rate": 0.00019359906456571145,
      "loss": 0.0693,
      "step": 7288
    },
    {
      "epoch": 0.03428600995324421,
      "grad_norm": 1.9420522451400757,
      "learning_rate": 0.00019359812158759797,
      "loss": 0.2904,
      "step": 7289
    },
    {
      "epoch": 0.03429071375485668,
      "grad_norm": 0.7912713289260864,
      "learning_rate": 0.00019359717860948449,
      "loss": 0.1122,
      "step": 7290
    },
    {
      "epoch": 0.034295417556469136,
      "grad_norm": 2.324049711227417,
      "learning_rate": 0.000193596235631371,
      "loss": 0.4025,
      "step": 7291
    },
    {
      "epoch": 0.0343001213580816,
      "grad_norm": 1.041015863418579,
      "learning_rate": 0.00019359529265325752,
      "loss": 0.1936,
      "step": 7292
    },
    {
      "epoch": 0.03430482515969407,
      "grad_norm": 0.5516696572303772,
      "learning_rate": 0.00019359434967514404,
      "loss": 0.0968,
      "step": 7293
    },
    {
      "epoch": 0.034309528961306526,
      "grad_norm": 1.5355817079544067,
      "learning_rate": 0.00019359340669703056,
      "loss": 0.4705,
      "step": 7294
    },
    {
      "epoch": 0.03431423276291899,
      "grad_norm": 1.9904080629348755,
      "learning_rate": 0.00019359246371891708,
      "loss": 0.4099,
      "step": 7295
    },
    {
      "epoch": 0.03431893656453146,
      "grad_norm": 1.077247142791748,
      "learning_rate": 0.00019359152074080362,
      "loss": 0.1934,
      "step": 7296
    },
    {
      "epoch": 0.034323640366143916,
      "grad_norm": 0.7447641491889954,
      "learning_rate": 0.00019359057776269014,
      "loss": 0.091,
      "step": 7297
    },
    {
      "epoch": 0.03432834416775638,
      "grad_norm": 1.4049454927444458,
      "learning_rate": 0.00019358963478457666,
      "loss": 0.3241,
      "step": 7298
    },
    {
      "epoch": 0.03433304796936884,
      "grad_norm": 0.6911177635192871,
      "learning_rate": 0.00019358869180646318,
      "loss": 0.0861,
      "step": 7299
    },
    {
      "epoch": 0.034337751770981306,
      "grad_norm": 3.464484453201294,
      "learning_rate": 0.00019358774882834973,
      "loss": 0.3156,
      "step": 7300
    },
    {
      "epoch": 0.03434245557259377,
      "grad_norm": 5.1921257972717285,
      "learning_rate": 0.00019358680585023622,
      "loss": 0.8583,
      "step": 7301
    },
    {
      "epoch": 0.03434715937420623,
      "grad_norm": 1.5463374853134155,
      "learning_rate": 0.00019358586287212274,
      "loss": 0.1238,
      "step": 7302
    },
    {
      "epoch": 0.034351863175818696,
      "grad_norm": 2.46580171585083,
      "learning_rate": 0.00019358491989400925,
      "loss": 0.1761,
      "step": 7303
    },
    {
      "epoch": 0.03435656697743116,
      "grad_norm": 0.6320915222167969,
      "learning_rate": 0.00019358397691589577,
      "loss": 0.0906,
      "step": 7304
    },
    {
      "epoch": 0.03436127077904362,
      "grad_norm": 0.9240373373031616,
      "learning_rate": 0.00019358303393778232,
      "loss": 0.1324,
      "step": 7305
    },
    {
      "epoch": 0.034365974580656086,
      "grad_norm": 1.6652493476867676,
      "learning_rate": 0.00019358209095966884,
      "loss": 0.1085,
      "step": 7306
    },
    {
      "epoch": 0.03437067838226855,
      "grad_norm": 4.841516017913818,
      "learning_rate": 0.00019358114798155536,
      "loss": 1.0113,
      "step": 7307
    },
    {
      "epoch": 0.03437538218388101,
      "grad_norm": 0.6406686305999756,
      "learning_rate": 0.00019358020500344188,
      "loss": 0.0629,
      "step": 7308
    },
    {
      "epoch": 0.034380085985493476,
      "grad_norm": 4.260677337646484,
      "learning_rate": 0.00019357926202532842,
      "loss": 0.4454,
      "step": 7309
    },
    {
      "epoch": 0.03438478978710594,
      "grad_norm": 0.782738983631134,
      "learning_rate": 0.00019357831904721494,
      "loss": 0.0704,
      "step": 7310
    },
    {
      "epoch": 0.0343894935887184,
      "grad_norm": 1.7755154371261597,
      "learning_rate": 0.00019357737606910146,
      "loss": 0.1209,
      "step": 7311
    },
    {
      "epoch": 0.034394197390330866,
      "grad_norm": 0.1836550235748291,
      "learning_rate": 0.00019357643309098795,
      "loss": 0.0171,
      "step": 7312
    },
    {
      "epoch": 0.03439890119194333,
      "grad_norm": 2.3164985179901123,
      "learning_rate": 0.00019357549011287447,
      "loss": 0.3021,
      "step": 7313
    },
    {
      "epoch": 0.03440360499355579,
      "grad_norm": 3.477818250656128,
      "learning_rate": 0.00019357454713476101,
      "loss": 0.4173,
      "step": 7314
    },
    {
      "epoch": 0.034408308795168256,
      "grad_norm": 3.164074182510376,
      "learning_rate": 0.00019357360415664753,
      "loss": 0.3327,
      "step": 7315
    },
    {
      "epoch": 0.034413012596780715,
      "grad_norm": 4.280966281890869,
      "learning_rate": 0.00019357266117853405,
      "loss": 0.8951,
      "step": 7316
    },
    {
      "epoch": 0.03441771639839318,
      "grad_norm": 3.7052133083343506,
      "learning_rate": 0.00019357171820042057,
      "loss": 0.5727,
      "step": 7317
    },
    {
      "epoch": 0.034422420200005646,
      "grad_norm": 0.5100008845329285,
      "learning_rate": 0.00019357077522230712,
      "loss": 0.0432,
      "step": 7318
    },
    {
      "epoch": 0.034427124001618105,
      "grad_norm": 3.870593786239624,
      "learning_rate": 0.00019356983224419363,
      "loss": 0.7538,
      "step": 7319
    },
    {
      "epoch": 0.03443182780323057,
      "grad_norm": 2.9257619380950928,
      "learning_rate": 0.00019356888926608015,
      "loss": 0.3616,
      "step": 7320
    },
    {
      "epoch": 0.034436531604843036,
      "grad_norm": 0.25595909357070923,
      "learning_rate": 0.00019356794628796667,
      "loss": 0.0164,
      "step": 7321
    },
    {
      "epoch": 0.034441235406455495,
      "grad_norm": 0.3585268557071686,
      "learning_rate": 0.0001935670033098532,
      "loss": 0.0304,
      "step": 7322
    },
    {
      "epoch": 0.03444593920806796,
      "grad_norm": 1.4610496759414673,
      "learning_rate": 0.0001935660603317397,
      "loss": 0.1633,
      "step": 7323
    },
    {
      "epoch": 0.034450643009680426,
      "grad_norm": 2.2561869621276855,
      "learning_rate": 0.00019356511735362623,
      "loss": 0.3247,
      "step": 7324
    },
    {
      "epoch": 0.034455346811292885,
      "grad_norm": 0.6065999269485474,
      "learning_rate": 0.00019356417437551275,
      "loss": 0.0633,
      "step": 7325
    },
    {
      "epoch": 0.03446005061290535,
      "grad_norm": 1.4416924715042114,
      "learning_rate": 0.00019356323139739926,
      "loss": 0.1841,
      "step": 7326
    },
    {
      "epoch": 0.034464754414517816,
      "grad_norm": 1.5931283235549927,
      "learning_rate": 0.00019356228841928578,
      "loss": 0.1961,
      "step": 7327
    },
    {
      "epoch": 0.034469458216130275,
      "grad_norm": 1.661655068397522,
      "learning_rate": 0.00019356134544117233,
      "loss": 0.1454,
      "step": 7328
    },
    {
      "epoch": 0.03447416201774274,
      "grad_norm": 2.96816086769104,
      "learning_rate": 0.00019356040246305885,
      "loss": 0.4816,
      "step": 7329
    },
    {
      "epoch": 0.034478865819355206,
      "grad_norm": 0.7915515899658203,
      "learning_rate": 0.00019355945948494537,
      "loss": 0.0913,
      "step": 7330
    },
    {
      "epoch": 0.034483569620967665,
      "grad_norm": 2.0861806869506836,
      "learning_rate": 0.00019355851650683189,
      "loss": 0.2336,
      "step": 7331
    },
    {
      "epoch": 0.03448827342258013,
      "grad_norm": 1.6348899602890015,
      "learning_rate": 0.0001935575735287184,
      "loss": 0.4226,
      "step": 7332
    },
    {
      "epoch": 0.03449297722419259,
      "grad_norm": 0.9710705876350403,
      "learning_rate": 0.00019355663055060492,
      "loss": 0.0714,
      "step": 7333
    },
    {
      "epoch": 0.034497681025805055,
      "grad_norm": 0.6278936862945557,
      "learning_rate": 0.00019355568757249144,
      "loss": 0.0609,
      "step": 7334
    },
    {
      "epoch": 0.03450238482741752,
      "grad_norm": 4.182271957397461,
      "learning_rate": 0.00019355474459437796,
      "loss": 0.6643,
      "step": 7335
    },
    {
      "epoch": 0.03450708862902998,
      "grad_norm": 2.1804561614990234,
      "learning_rate": 0.00019355380161626448,
      "loss": 0.2332,
      "step": 7336
    },
    {
      "epoch": 0.034511792430642445,
      "grad_norm": 0.808509349822998,
      "learning_rate": 0.00019355285863815102,
      "loss": 0.0628,
      "step": 7337
    },
    {
      "epoch": 0.03451649623225491,
      "grad_norm": 3.1722664833068848,
      "learning_rate": 0.00019355191566003754,
      "loss": 0.3922,
      "step": 7338
    },
    {
      "epoch": 0.03452120003386737,
      "grad_norm": 0.8446516990661621,
      "learning_rate": 0.00019355097268192406,
      "loss": 0.0757,
      "step": 7339
    },
    {
      "epoch": 0.034525903835479835,
      "grad_norm": 2.8231022357940674,
      "learning_rate": 0.00019355002970381058,
      "loss": 0.3803,
      "step": 7340
    },
    {
      "epoch": 0.0345306076370923,
      "grad_norm": 3.502246856689453,
      "learning_rate": 0.00019354908672569713,
      "loss": 0.7678,
      "step": 7341
    },
    {
      "epoch": 0.03453531143870476,
      "grad_norm": 1.4108996391296387,
      "learning_rate": 0.00019354814374758364,
      "loss": 0.3876,
      "step": 7342
    },
    {
      "epoch": 0.034540015240317225,
      "grad_norm": 0.15016940236091614,
      "learning_rate": 0.00019354720076947014,
      "loss": 0.0118,
      "step": 7343
    },
    {
      "epoch": 0.03454471904192969,
      "grad_norm": 0.7007669806480408,
      "learning_rate": 0.00019354625779135665,
      "loss": 0.0681,
      "step": 7344
    },
    {
      "epoch": 0.03454942284354215,
      "grad_norm": 0.6264048218727112,
      "learning_rate": 0.00019354531481324317,
      "loss": 0.0477,
      "step": 7345
    },
    {
      "epoch": 0.034554126645154615,
      "grad_norm": 0.3039744794368744,
      "learning_rate": 0.00019354437183512972,
      "loss": 0.0189,
      "step": 7346
    },
    {
      "epoch": 0.03455883044676708,
      "grad_norm": 0.21880686283111572,
      "learning_rate": 0.00019354342885701624,
      "loss": 0.0171,
      "step": 7347
    },
    {
      "epoch": 0.03456353424837954,
      "grad_norm": 2.492938995361328,
      "learning_rate": 0.00019354248587890276,
      "loss": 0.5257,
      "step": 7348
    },
    {
      "epoch": 0.034568238049992005,
      "grad_norm": 2.293170690536499,
      "learning_rate": 0.00019354154290078927,
      "loss": 0.585,
      "step": 7349
    },
    {
      "epoch": 0.03457294185160446,
      "grad_norm": 1.3089826107025146,
      "learning_rate": 0.00019354059992267582,
      "loss": 0.2242,
      "step": 7350
    },
    {
      "epoch": 0.03457764565321693,
      "grad_norm": 1.875707745552063,
      "learning_rate": 0.00019353965694456234,
      "loss": 0.1462,
      "step": 7351
    },
    {
      "epoch": 0.034582349454829395,
      "grad_norm": 0.23328262567520142,
      "learning_rate": 0.00019353871396644886,
      "loss": 0.018,
      "step": 7352
    },
    {
      "epoch": 0.03458705325644185,
      "grad_norm": 4.863093852996826,
      "learning_rate": 0.00019353777098833538,
      "loss": 0.3398,
      "step": 7353
    },
    {
      "epoch": 0.03459175705805432,
      "grad_norm": 2.577458620071411,
      "learning_rate": 0.0001935368280102219,
      "loss": 0.2178,
      "step": 7354
    },
    {
      "epoch": 0.034596460859666785,
      "grad_norm": 2.004866123199463,
      "learning_rate": 0.00019353588503210841,
      "loss": 0.1832,
      "step": 7355
    },
    {
      "epoch": 0.03460116466127924,
      "grad_norm": 2.9075002670288086,
      "learning_rate": 0.00019353494205399493,
      "loss": 0.1242,
      "step": 7356
    },
    {
      "epoch": 0.03460586846289171,
      "grad_norm": 0.7521189451217651,
      "learning_rate": 0.00019353399907588145,
      "loss": 0.0796,
      "step": 7357
    },
    {
      "epoch": 0.034610572264504175,
      "grad_norm": 0.7524497509002686,
      "learning_rate": 0.00019353305609776797,
      "loss": 0.06,
      "step": 7358
    },
    {
      "epoch": 0.03461527606611663,
      "grad_norm": 1.6528892517089844,
      "learning_rate": 0.00019353211311965452,
      "loss": 0.2578,
      "step": 7359
    },
    {
      "epoch": 0.0346199798677291,
      "grad_norm": 0.5961946845054626,
      "learning_rate": 0.00019353117014154103,
      "loss": 0.0615,
      "step": 7360
    },
    {
      "epoch": 0.034624683669341565,
      "grad_norm": 4.365881443023682,
      "learning_rate": 0.00019353022716342755,
      "loss": 0.3867,
      "step": 7361
    },
    {
      "epoch": 0.03462938747095402,
      "grad_norm": 1.5606212615966797,
      "learning_rate": 0.00019352928418531407,
      "loss": 0.1824,
      "step": 7362
    },
    {
      "epoch": 0.03463409127256649,
      "grad_norm": 0.5642591714859009,
      "learning_rate": 0.0001935283412072006,
      "loss": 0.0341,
      "step": 7363
    },
    {
      "epoch": 0.034638795074178955,
      "grad_norm": 2.071638584136963,
      "learning_rate": 0.0001935273982290871,
      "loss": 0.2302,
      "step": 7364
    },
    {
      "epoch": 0.03464349887579141,
      "grad_norm": 2.3261218070983887,
      "learning_rate": 0.00019352645525097363,
      "loss": 0.2864,
      "step": 7365
    },
    {
      "epoch": 0.03464820267740388,
      "grad_norm": 1.026539921760559,
      "learning_rate": 0.00019352551227286015,
      "loss": 0.066,
      "step": 7366
    },
    {
      "epoch": 0.03465290647901634,
      "grad_norm": 0.6821259260177612,
      "learning_rate": 0.00019352456929474666,
      "loss": 0.0704,
      "step": 7367
    },
    {
      "epoch": 0.0346576102806288,
      "grad_norm": 0.20408028364181519,
      "learning_rate": 0.0001935236263166332,
      "loss": 0.0156,
      "step": 7368
    },
    {
      "epoch": 0.03466231408224127,
      "grad_norm": 0.5903578996658325,
      "learning_rate": 0.00019352268333851973,
      "loss": 0.0541,
      "step": 7369
    },
    {
      "epoch": 0.03466701788385373,
      "grad_norm": 2.6048614978790283,
      "learning_rate": 0.00019352174036040625,
      "loss": 0.2274,
      "step": 7370
    },
    {
      "epoch": 0.03467172168546619,
      "grad_norm": 4.840681552886963,
      "learning_rate": 0.00019352079738229277,
      "loss": 0.681,
      "step": 7371
    },
    {
      "epoch": 0.03467642548707866,
      "grad_norm": 3.679708242416382,
      "learning_rate": 0.00019351985440417929,
      "loss": 0.5721,
      "step": 7372
    },
    {
      "epoch": 0.03468112928869112,
      "grad_norm": 2.1892879009246826,
      "learning_rate": 0.00019351891142606583,
      "loss": 0.2399,
      "step": 7373
    },
    {
      "epoch": 0.03468583309030358,
      "grad_norm": 0.31450948119163513,
      "learning_rate": 0.00019351796844795232,
      "loss": 0.0238,
      "step": 7374
    },
    {
      "epoch": 0.03469053689191605,
      "grad_norm": 2.5022385120391846,
      "learning_rate": 0.00019351702546983884,
      "loss": 0.2808,
      "step": 7375
    },
    {
      "epoch": 0.03469524069352851,
      "grad_norm": 3.8397376537323,
      "learning_rate": 0.00019351608249172536,
      "loss": 0.4186,
      "step": 7376
    },
    {
      "epoch": 0.03469994449514097,
      "grad_norm": 1.5914695262908936,
      "learning_rate": 0.0001935151395136119,
      "loss": 0.1578,
      "step": 7377
    },
    {
      "epoch": 0.03470464829675344,
      "grad_norm": 0.4726722836494446,
      "learning_rate": 0.00019351419653549842,
      "loss": 0.0268,
      "step": 7378
    },
    {
      "epoch": 0.0347093520983659,
      "grad_norm": 3.248481273651123,
      "learning_rate": 0.00019351325355738494,
      "loss": 0.2985,
      "step": 7379
    },
    {
      "epoch": 0.03471405589997836,
      "grad_norm": 3.1248443126678467,
      "learning_rate": 0.00019351231057927146,
      "loss": 0.3393,
      "step": 7380
    },
    {
      "epoch": 0.03471875970159083,
      "grad_norm": 9.309720039367676,
      "learning_rate": 0.00019351136760115798,
      "loss": 0.2369,
      "step": 7381
    },
    {
      "epoch": 0.03472346350320329,
      "grad_norm": 3.7873661518096924,
      "learning_rate": 0.00019351042462304453,
      "loss": 0.3362,
      "step": 7382
    },
    {
      "epoch": 0.03472816730481575,
      "grad_norm": 1.6005895137786865,
      "learning_rate": 0.00019350948164493104,
      "loss": 0.1546,
      "step": 7383
    },
    {
      "epoch": 0.03473287110642821,
      "grad_norm": 1.464680790901184,
      "learning_rate": 0.00019350853866681756,
      "loss": 0.0931,
      "step": 7384
    },
    {
      "epoch": 0.03473757490804068,
      "grad_norm": 4.517632961273193,
      "learning_rate": 0.00019350759568870408,
      "loss": 0.5988,
      "step": 7385
    },
    {
      "epoch": 0.03474227870965314,
      "grad_norm": 0.20478883385658264,
      "learning_rate": 0.00019350665271059057,
      "loss": 0.0177,
      "step": 7386
    },
    {
      "epoch": 0.0347469825112656,
      "grad_norm": 2.4384167194366455,
      "learning_rate": 0.00019350570973247712,
      "loss": 0.2168,
      "step": 7387
    },
    {
      "epoch": 0.03475168631287807,
      "grad_norm": 0.6520280838012695,
      "learning_rate": 0.00019350476675436364,
      "loss": 0.0137,
      "step": 7388
    },
    {
      "epoch": 0.03475639011449053,
      "grad_norm": 4.037373065948486,
      "learning_rate": 0.00019350382377625016,
      "loss": 1.0456,
      "step": 7389
    },
    {
      "epoch": 0.03476109391610299,
      "grad_norm": 2.9819016456604004,
      "learning_rate": 0.00019350288079813667,
      "loss": 0.4751,
      "step": 7390
    },
    {
      "epoch": 0.03476579771771546,
      "grad_norm": 0.7749155163764954,
      "learning_rate": 0.00019350193782002322,
      "loss": 0.066,
      "step": 7391
    },
    {
      "epoch": 0.03477050151932792,
      "grad_norm": 1.2350534200668335,
      "learning_rate": 0.00019350099484190974,
      "loss": 0.2196,
      "step": 7392
    },
    {
      "epoch": 0.03477520532094038,
      "grad_norm": 2.146824598312378,
      "learning_rate": 0.00019350005186379626,
      "loss": 0.3047,
      "step": 7393
    },
    {
      "epoch": 0.03477990912255285,
      "grad_norm": 1.6828731298446655,
      "learning_rate": 0.00019349910888568278,
      "loss": 0.4094,
      "step": 7394
    },
    {
      "epoch": 0.03478461292416531,
      "grad_norm": 2.65620756149292,
      "learning_rate": 0.0001934981659075693,
      "loss": 0.8788,
      "step": 7395
    },
    {
      "epoch": 0.03478931672577777,
      "grad_norm": 1.7264814376831055,
      "learning_rate": 0.00019349722292945581,
      "loss": 0.4674,
      "step": 7396
    },
    {
      "epoch": 0.03479402052739024,
      "grad_norm": 3.1833086013793945,
      "learning_rate": 0.00019349627995134233,
      "loss": 0.389,
      "step": 7397
    },
    {
      "epoch": 0.0347987243290027,
      "grad_norm": 4.059956073760986,
      "learning_rate": 0.00019349533697322885,
      "loss": 0.6993,
      "step": 7398
    },
    {
      "epoch": 0.03480342813061516,
      "grad_norm": 2.16750431060791,
      "learning_rate": 0.00019349439399511537,
      "loss": 0.5571,
      "step": 7399
    },
    {
      "epoch": 0.03480813193222763,
      "grad_norm": 1.6033358573913574,
      "learning_rate": 0.00019349345101700192,
      "loss": 0.1622,
      "step": 7400
    },
    {
      "epoch": 0.034812835733840086,
      "grad_norm": 2.8542068004608154,
      "learning_rate": 0.00019349250803888843,
      "loss": 0.3497,
      "step": 7401
    },
    {
      "epoch": 0.03481753953545255,
      "grad_norm": 4.669417858123779,
      "learning_rate": 0.00019349156506077495,
      "loss": 0.4541,
      "step": 7402
    },
    {
      "epoch": 0.03482224333706502,
      "grad_norm": 2.200197458267212,
      "learning_rate": 0.00019349062208266147,
      "loss": 0.2864,
      "step": 7403
    },
    {
      "epoch": 0.034826947138677476,
      "grad_norm": 0.7642728090286255,
      "learning_rate": 0.000193489679104548,
      "loss": 0.0917,
      "step": 7404
    },
    {
      "epoch": 0.03483165094028994,
      "grad_norm": 0.2879190444946289,
      "learning_rate": 0.0001934887361264345,
      "loss": 0.0257,
      "step": 7405
    },
    {
      "epoch": 0.03483635474190241,
      "grad_norm": 3.776933431625366,
      "learning_rate": 0.00019348779314832103,
      "loss": 0.6149,
      "step": 7406
    },
    {
      "epoch": 0.034841058543514866,
      "grad_norm": 1.7166759967803955,
      "learning_rate": 0.00019348685017020755,
      "loss": 0.2625,
      "step": 7407
    },
    {
      "epoch": 0.03484576234512733,
      "grad_norm": 0.8481324911117554,
      "learning_rate": 0.00019348590719209406,
      "loss": 0.0984,
      "step": 7408
    },
    {
      "epoch": 0.0348504661467398,
      "grad_norm": 2.219123601913452,
      "learning_rate": 0.0001934849642139806,
      "loss": 0.3312,
      "step": 7409
    },
    {
      "epoch": 0.034855169948352256,
      "grad_norm": 3.826564073562622,
      "learning_rate": 0.00019348402123586713,
      "loss": 0.6003,
      "step": 7410
    },
    {
      "epoch": 0.03485987374996472,
      "grad_norm": 2.958150863647461,
      "learning_rate": 0.00019348307825775365,
      "loss": 0.5229,
      "step": 7411
    },
    {
      "epoch": 0.03486457755157719,
      "grad_norm": 2.009328603744507,
      "learning_rate": 0.00019348213527964017,
      "loss": 0.2375,
      "step": 7412
    },
    {
      "epoch": 0.034869281353189646,
      "grad_norm": 0.9956020712852478,
      "learning_rate": 0.00019348119230152669,
      "loss": 0.0988,
      "step": 7413
    },
    {
      "epoch": 0.03487398515480211,
      "grad_norm": 1.7468024492263794,
      "learning_rate": 0.00019348024932341323,
      "loss": 0.2223,
      "step": 7414
    },
    {
      "epoch": 0.03487868895641458,
      "grad_norm": 1.0333688259124756,
      "learning_rate": 0.00019347930634529975,
      "loss": 0.1306,
      "step": 7415
    },
    {
      "epoch": 0.034883392758027036,
      "grad_norm": 1.7681082487106323,
      "learning_rate": 0.00019347836336718627,
      "loss": 0.302,
      "step": 7416
    },
    {
      "epoch": 0.0348880965596395,
      "grad_norm": 1.4939073324203491,
      "learning_rate": 0.00019347742038907276,
      "loss": 0.1658,
      "step": 7417
    },
    {
      "epoch": 0.03489280036125196,
      "grad_norm": 1.118952751159668,
      "learning_rate": 0.0001934764774109593,
      "loss": 0.1402,
      "step": 7418
    },
    {
      "epoch": 0.034897504162864426,
      "grad_norm": 3.2167391777038574,
      "learning_rate": 0.00019347553443284582,
      "loss": 0.3496,
      "step": 7419
    },
    {
      "epoch": 0.03490220796447689,
      "grad_norm": 2.107257604598999,
      "learning_rate": 0.00019347459145473234,
      "loss": 0.4139,
      "step": 7420
    },
    {
      "epoch": 0.03490691176608935,
      "grad_norm": 0.7743653655052185,
      "learning_rate": 0.00019347364847661886,
      "loss": 0.1172,
      "step": 7421
    },
    {
      "epoch": 0.034911615567701816,
      "grad_norm": 1.6775490045547485,
      "learning_rate": 0.00019347270549850538,
      "loss": 0.3662,
      "step": 7422
    },
    {
      "epoch": 0.03491631936931428,
      "grad_norm": 1.3659873008728027,
      "learning_rate": 0.00019347176252039193,
      "loss": 0.1681,
      "step": 7423
    },
    {
      "epoch": 0.03492102317092674,
      "grad_norm": 0.5088157653808594,
      "learning_rate": 0.00019347081954227844,
      "loss": 0.0556,
      "step": 7424
    },
    {
      "epoch": 0.034925726972539206,
      "grad_norm": 3.351942539215088,
      "learning_rate": 0.00019346987656416496,
      "loss": 0.591,
      "step": 7425
    },
    {
      "epoch": 0.03493043077415167,
      "grad_norm": 2.3290727138519287,
      "learning_rate": 0.00019346893358605148,
      "loss": 0.4777,
      "step": 7426
    },
    {
      "epoch": 0.03493513457576413,
      "grad_norm": 2.1644346714019775,
      "learning_rate": 0.000193467990607938,
      "loss": 0.3982,
      "step": 7427
    },
    {
      "epoch": 0.034939838377376596,
      "grad_norm": 0.8045289516448975,
      "learning_rate": 0.00019346704762982452,
      "loss": 0.1658,
      "step": 7428
    },
    {
      "epoch": 0.03494454217898906,
      "grad_norm": 1.492570161819458,
      "learning_rate": 0.00019346610465171104,
      "loss": 0.1922,
      "step": 7429
    },
    {
      "epoch": 0.03494924598060152,
      "grad_norm": 0.6505817770957947,
      "learning_rate": 0.00019346516167359756,
      "loss": 0.1022,
      "step": 7430
    },
    {
      "epoch": 0.034953949782213986,
      "grad_norm": 1.221740961074829,
      "learning_rate": 0.00019346421869548407,
      "loss": 0.2135,
      "step": 7431
    },
    {
      "epoch": 0.03495865358382645,
      "grad_norm": 0.9096590280532837,
      "learning_rate": 0.00019346327571737062,
      "loss": 0.1239,
      "step": 7432
    },
    {
      "epoch": 0.03496335738543891,
      "grad_norm": 3.2495148181915283,
      "learning_rate": 0.00019346233273925714,
      "loss": 0.9179,
      "step": 7433
    },
    {
      "epoch": 0.034968061187051376,
      "grad_norm": 1.3395941257476807,
      "learning_rate": 0.00019346138976114366,
      "loss": 0.2092,
      "step": 7434
    },
    {
      "epoch": 0.034972764988663835,
      "grad_norm": 0.7945483922958374,
      "learning_rate": 0.00019346044678303018,
      "loss": 0.0957,
      "step": 7435
    },
    {
      "epoch": 0.0349774687902763,
      "grad_norm": 0.5646767020225525,
      "learning_rate": 0.0001934595038049167,
      "loss": 0.0689,
      "step": 7436
    },
    {
      "epoch": 0.034982172591888766,
      "grad_norm": 1.0498402118682861,
      "learning_rate": 0.00019345856082680321,
      "loss": 0.1004,
      "step": 7437
    },
    {
      "epoch": 0.034986876393501225,
      "grad_norm": 1.9752761125564575,
      "learning_rate": 0.00019345761784868973,
      "loss": 0.2601,
      "step": 7438
    },
    {
      "epoch": 0.03499158019511369,
      "grad_norm": 0.7845500707626343,
      "learning_rate": 0.00019345667487057625,
      "loss": 0.085,
      "step": 7439
    },
    {
      "epoch": 0.034996283996726156,
      "grad_norm": 1.1436771154403687,
      "learning_rate": 0.00019345573189246277,
      "loss": 0.151,
      "step": 7440
    },
    {
      "epoch": 0.035000987798338615,
      "grad_norm": 4.07843542098999,
      "learning_rate": 0.00019345478891434932,
      "loss": 0.7603,
      "step": 7441
    },
    {
      "epoch": 0.03500569159995108,
      "grad_norm": 0.3640785217285156,
      "learning_rate": 0.00019345384593623583,
      "loss": 0.0526,
      "step": 7442
    },
    {
      "epoch": 0.035010395401563546,
      "grad_norm": 4.244807720184326,
      "learning_rate": 0.00019345290295812235,
      "loss": 0.6461,
      "step": 7443
    },
    {
      "epoch": 0.035015099203176005,
      "grad_norm": 2.841259479522705,
      "learning_rate": 0.00019345195998000887,
      "loss": 0.5938,
      "step": 7444
    },
    {
      "epoch": 0.03501980300478847,
      "grad_norm": 0.3729807138442993,
      "learning_rate": 0.0001934510170018954,
      "loss": 0.0307,
      "step": 7445
    },
    {
      "epoch": 0.035024506806400936,
      "grad_norm": 0.5457319021224976,
      "learning_rate": 0.00019345007402378194,
      "loss": 0.0467,
      "step": 7446
    },
    {
      "epoch": 0.035029210608013395,
      "grad_norm": 1.873602271080017,
      "learning_rate": 0.00019344913104566845,
      "loss": 0.1288,
      "step": 7447
    },
    {
      "epoch": 0.03503391440962586,
      "grad_norm": 0.7388213276863098,
      "learning_rate": 0.00019344818806755495,
      "loss": 0.0651,
      "step": 7448
    },
    {
      "epoch": 0.035038618211238326,
      "grad_norm": 0.24523857235908508,
      "learning_rate": 0.00019344724508944146,
      "loss": 0.0186,
      "step": 7449
    },
    {
      "epoch": 0.035043322012850785,
      "grad_norm": 0.17224928736686707,
      "learning_rate": 0.000193446302111328,
      "loss": 0.0115,
      "step": 7450
    },
    {
      "epoch": 0.03504802581446325,
      "grad_norm": 3.531606674194336,
      "learning_rate": 0.00019344535913321453,
      "loss": 0.7684,
      "step": 7451
    },
    {
      "epoch": 0.03505272961607571,
      "grad_norm": 5.348654747009277,
      "learning_rate": 0.00019344441615510105,
      "loss": 0.3754,
      "step": 7452
    },
    {
      "epoch": 0.035057433417688175,
      "grad_norm": 7.034491539001465,
      "learning_rate": 0.00019344347317698757,
      "loss": 1.1525,
      "step": 7453
    },
    {
      "epoch": 0.03506213721930064,
      "grad_norm": 0.7281990051269531,
      "learning_rate": 0.00019344253019887409,
      "loss": 0.0737,
      "step": 7454
    },
    {
      "epoch": 0.0350668410209131,
      "grad_norm": 5.273698806762695,
      "learning_rate": 0.00019344158722076063,
      "loss": 0.3729,
      "step": 7455
    },
    {
      "epoch": 0.035071544822525565,
      "grad_norm": 0.7280347943305969,
      "learning_rate": 0.00019344064424264715,
      "loss": 0.0308,
      "step": 7456
    },
    {
      "epoch": 0.03507624862413803,
      "grad_norm": 1.544693112373352,
      "learning_rate": 0.00019343970126453367,
      "loss": 0.0689,
      "step": 7457
    },
    {
      "epoch": 0.03508095242575049,
      "grad_norm": 2.3605258464813232,
      "learning_rate": 0.0001934387582864202,
      "loss": 0.2879,
      "step": 7458
    },
    {
      "epoch": 0.035085656227362955,
      "grad_norm": 3.1283812522888184,
      "learning_rate": 0.0001934378153083067,
      "loss": 0.7268,
      "step": 7459
    },
    {
      "epoch": 0.03509036002897542,
      "grad_norm": 3.361727476119995,
      "learning_rate": 0.00019343687233019322,
      "loss": 0.262,
      "step": 7460
    },
    {
      "epoch": 0.03509506383058788,
      "grad_norm": 1.6237802505493164,
      "learning_rate": 0.00019343592935207974,
      "loss": 0.0812,
      "step": 7461
    },
    {
      "epoch": 0.035099767632200345,
      "grad_norm": 5.897375583648682,
      "learning_rate": 0.00019343498637396626,
      "loss": 0.8234,
      "step": 7462
    },
    {
      "epoch": 0.03510447143381281,
      "grad_norm": 0.19928671419620514,
      "learning_rate": 0.00019343404339585278,
      "loss": 0.0131,
      "step": 7463
    },
    {
      "epoch": 0.03510917523542527,
      "grad_norm": 0.2661259174346924,
      "learning_rate": 0.00019343310041773933,
      "loss": 0.022,
      "step": 7464
    },
    {
      "epoch": 0.035113879037037735,
      "grad_norm": 5.74849271774292,
      "learning_rate": 0.00019343215743962584,
      "loss": 0.5592,
      "step": 7465
    },
    {
      "epoch": 0.0351185828386502,
      "grad_norm": 1.9448771476745605,
      "learning_rate": 0.00019343121446151236,
      "loss": 0.1497,
      "step": 7466
    },
    {
      "epoch": 0.03512328664026266,
      "grad_norm": 2.461993932723999,
      "learning_rate": 0.00019343027148339888,
      "loss": 0.5314,
      "step": 7467
    },
    {
      "epoch": 0.035127990441875125,
      "grad_norm": 2.1021316051483154,
      "learning_rate": 0.0001934293285052854,
      "loss": 0.238,
      "step": 7468
    },
    {
      "epoch": 0.03513269424348758,
      "grad_norm": 1.6536040306091309,
      "learning_rate": 0.00019342838552717192,
      "loss": 0.1043,
      "step": 7469
    },
    {
      "epoch": 0.03513739804510005,
      "grad_norm": 1.84912109375,
      "learning_rate": 0.00019342744254905844,
      "loss": 0.1029,
      "step": 7470
    },
    {
      "epoch": 0.035142101846712515,
      "grad_norm": 2.940094470977783,
      "learning_rate": 0.00019342649957094496,
      "loss": 0.5458,
      "step": 7471
    },
    {
      "epoch": 0.03514680564832497,
      "grad_norm": 4.057462692260742,
      "learning_rate": 0.00019342555659283147,
      "loss": 0.7132,
      "step": 7472
    },
    {
      "epoch": 0.03515150944993744,
      "grad_norm": 0.3141371011734009,
      "learning_rate": 0.00019342461361471802,
      "loss": 0.0267,
      "step": 7473
    },
    {
      "epoch": 0.035156213251549905,
      "grad_norm": 1.2131376266479492,
      "learning_rate": 0.00019342367063660454,
      "loss": 0.1003,
      "step": 7474
    },
    {
      "epoch": 0.03516091705316236,
      "grad_norm": 0.7816063761711121,
      "learning_rate": 0.00019342272765849106,
      "loss": 0.1123,
      "step": 7475
    },
    {
      "epoch": 0.03516562085477483,
      "grad_norm": 1.6476994752883911,
      "learning_rate": 0.00019342178468037758,
      "loss": 0.1285,
      "step": 7476
    },
    {
      "epoch": 0.035170324656387295,
      "grad_norm": 4.116107940673828,
      "learning_rate": 0.00019342084170226412,
      "loss": 0.6258,
      "step": 7477
    },
    {
      "epoch": 0.03517502845799975,
      "grad_norm": 1.1437288522720337,
      "learning_rate": 0.00019341989872415064,
      "loss": 0.1418,
      "step": 7478
    },
    {
      "epoch": 0.03517973225961222,
      "grad_norm": 4.701920509338379,
      "learning_rate": 0.00019341895574603713,
      "loss": 0.4102,
      "step": 7479
    },
    {
      "epoch": 0.035184436061224685,
      "grad_norm": 2.970153331756592,
      "learning_rate": 0.00019341801276792365,
      "loss": 0.2086,
      "step": 7480
    },
    {
      "epoch": 0.03518913986283714,
      "grad_norm": 2.287147045135498,
      "learning_rate": 0.00019341706978981017,
      "loss": 0.2346,
      "step": 7481
    },
    {
      "epoch": 0.03519384366444961,
      "grad_norm": 4.185837745666504,
      "learning_rate": 0.00019341612681169672,
      "loss": 1.0336,
      "step": 7482
    },
    {
      "epoch": 0.035198547466062075,
      "grad_norm": 1.577722191810608,
      "learning_rate": 0.00019341518383358323,
      "loss": 0.131,
      "step": 7483
    },
    {
      "epoch": 0.03520325126767453,
      "grad_norm": 1.8016347885131836,
      "learning_rate": 0.00019341424085546975,
      "loss": 0.2054,
      "step": 7484
    },
    {
      "epoch": 0.035207955069287,
      "grad_norm": 0.5699160695075989,
      "learning_rate": 0.00019341329787735627,
      "loss": 0.0466,
      "step": 7485
    },
    {
      "epoch": 0.03521265887089946,
      "grad_norm": 1.0321481227874756,
      "learning_rate": 0.0001934123548992428,
      "loss": 0.0912,
      "step": 7486
    },
    {
      "epoch": 0.03521736267251192,
      "grad_norm": 0.1962706297636032,
      "learning_rate": 0.00019341141192112934,
      "loss": 0.0142,
      "step": 7487
    },
    {
      "epoch": 0.03522206647412439,
      "grad_norm": 2.555185079574585,
      "learning_rate": 0.00019341046894301585,
      "loss": 0.3705,
      "step": 7488
    },
    {
      "epoch": 0.03522677027573685,
      "grad_norm": 0.8460936546325684,
      "learning_rate": 0.00019340952596490237,
      "loss": 0.1193,
      "step": 7489
    },
    {
      "epoch": 0.03523147407734931,
      "grad_norm": 1.7549699544906616,
      "learning_rate": 0.00019340858298678886,
      "loss": 0.4068,
      "step": 7490
    },
    {
      "epoch": 0.03523617787896178,
      "grad_norm": 1.3422443866729736,
      "learning_rate": 0.0001934076400086754,
      "loss": 0.1398,
      "step": 7491
    },
    {
      "epoch": 0.03524088168057424,
      "grad_norm": 1.6899218559265137,
      "learning_rate": 0.00019340669703056193,
      "loss": 0.161,
      "step": 7492
    },
    {
      "epoch": 0.0352455854821867,
      "grad_norm": 0.9388659000396729,
      "learning_rate": 0.00019340575405244845,
      "loss": 0.0349,
      "step": 7493
    },
    {
      "epoch": 0.03525028928379917,
      "grad_norm": 1.3961812257766724,
      "learning_rate": 0.00019340481107433497,
      "loss": 0.0823,
      "step": 7494
    },
    {
      "epoch": 0.03525499308541163,
      "grad_norm": 1.708897352218628,
      "learning_rate": 0.00019340386809622149,
      "loss": 0.2019,
      "step": 7495
    },
    {
      "epoch": 0.03525969688702409,
      "grad_norm": 3.1373748779296875,
      "learning_rate": 0.00019340292511810803,
      "loss": 0.2766,
      "step": 7496
    },
    {
      "epoch": 0.03526440068863656,
      "grad_norm": 3.2697927951812744,
      "learning_rate": 0.00019340198213999455,
      "loss": 0.6837,
      "step": 7497
    },
    {
      "epoch": 0.03526910449024902,
      "grad_norm": 0.5271300673484802,
      "learning_rate": 0.00019340103916188107,
      "loss": 0.0549,
      "step": 7498
    },
    {
      "epoch": 0.03527380829186148,
      "grad_norm": 1.606574296951294,
      "learning_rate": 0.0001934000961837676,
      "loss": 0.2886,
      "step": 7499
    },
    {
      "epoch": 0.03527851209347395,
      "grad_norm": 0.1887870728969574,
      "learning_rate": 0.0001933991532056541,
      "loss": 0.0133,
      "step": 7500
    },
    {
      "epoch": 0.03528321589508641,
      "grad_norm": 0.9696192741394043,
      "learning_rate": 0.00019339821022754062,
      "loss": 0.0711,
      "step": 7501
    },
    {
      "epoch": 0.03528791969669887,
      "grad_norm": 0.3959833085536957,
      "learning_rate": 0.00019339726724942714,
      "loss": 0.0217,
      "step": 7502
    },
    {
      "epoch": 0.03529262349831133,
      "grad_norm": 2.1973626613616943,
      "learning_rate": 0.00019339632427131366,
      "loss": 0.1785,
      "step": 7503
    },
    {
      "epoch": 0.0352973272999238,
      "grad_norm": 1.6390607357025146,
      "learning_rate": 0.00019339538129320018,
      "loss": 0.1062,
      "step": 7504
    },
    {
      "epoch": 0.03530203110153626,
      "grad_norm": 2.9077796936035156,
      "learning_rate": 0.00019339443831508673,
      "loss": 0.262,
      "step": 7505
    },
    {
      "epoch": 0.03530673490314872,
      "grad_norm": 2.5006957054138184,
      "learning_rate": 0.00019339349533697324,
      "loss": 0.547,
      "step": 7506
    },
    {
      "epoch": 0.03531143870476119,
      "grad_norm": 3.139051914215088,
      "learning_rate": 0.00019339255235885976,
      "loss": 0.3897,
      "step": 7507
    },
    {
      "epoch": 0.03531614250637365,
      "grad_norm": 2.5708773136138916,
      "learning_rate": 0.00019339160938074628,
      "loss": 0.43,
      "step": 7508
    },
    {
      "epoch": 0.03532084630798611,
      "grad_norm": 1.5614696741104126,
      "learning_rate": 0.00019339066640263283,
      "loss": 0.158,
      "step": 7509
    },
    {
      "epoch": 0.03532555010959858,
      "grad_norm": 2.0139882564544678,
      "learning_rate": 0.00019338972342451932,
      "loss": 0.2343,
      "step": 7510
    },
    {
      "epoch": 0.03533025391121104,
      "grad_norm": 2.688678503036499,
      "learning_rate": 0.00019338878044640584,
      "loss": 0.404,
      "step": 7511
    },
    {
      "epoch": 0.0353349577128235,
      "grad_norm": 1.1619311571121216,
      "learning_rate": 0.00019338783746829236,
      "loss": 0.0836,
      "step": 7512
    },
    {
      "epoch": 0.03533966151443597,
      "grad_norm": 3.1163506507873535,
      "learning_rate": 0.00019338689449017887,
      "loss": 0.3717,
      "step": 7513
    },
    {
      "epoch": 0.03534436531604843,
      "grad_norm": 2.4401330947875977,
      "learning_rate": 0.00019338595151206542,
      "loss": 0.2699,
      "step": 7514
    },
    {
      "epoch": 0.03534906911766089,
      "grad_norm": 0.26188960671424866,
      "learning_rate": 0.00019338500853395194,
      "loss": 0.0202,
      "step": 7515
    },
    {
      "epoch": 0.03535377291927336,
      "grad_norm": 2.1298670768737793,
      "learning_rate": 0.00019338406555583846,
      "loss": 0.194,
      "step": 7516
    },
    {
      "epoch": 0.03535847672088582,
      "grad_norm": 2.317162036895752,
      "learning_rate": 0.00019338312257772498,
      "loss": 0.2759,
      "step": 7517
    },
    {
      "epoch": 0.03536318052249828,
      "grad_norm": 2.8936595916748047,
      "learning_rate": 0.00019338217959961152,
      "loss": 0.5666,
      "step": 7518
    },
    {
      "epoch": 0.03536788432411075,
      "grad_norm": 1.707751989364624,
      "learning_rate": 0.00019338123662149804,
      "loss": 0.4505,
      "step": 7519
    },
    {
      "epoch": 0.035372588125723206,
      "grad_norm": 1.8990612030029297,
      "learning_rate": 0.00019338029364338456,
      "loss": 0.3856,
      "step": 7520
    },
    {
      "epoch": 0.03537729192733567,
      "grad_norm": 2.424220085144043,
      "learning_rate": 0.00019337935066527105,
      "loss": 0.5019,
      "step": 7521
    },
    {
      "epoch": 0.03538199572894814,
      "grad_norm": 2.4077630043029785,
      "learning_rate": 0.00019337840768715757,
      "loss": 0.4736,
      "step": 7522
    },
    {
      "epoch": 0.035386699530560596,
      "grad_norm": 3.5435562133789062,
      "learning_rate": 0.00019337746470904412,
      "loss": 0.562,
      "step": 7523
    },
    {
      "epoch": 0.03539140333217306,
      "grad_norm": 2.8527884483337402,
      "learning_rate": 0.00019337652173093063,
      "loss": 0.4064,
      "step": 7524
    },
    {
      "epoch": 0.03539610713378553,
      "grad_norm": 2.6769230365753174,
      "learning_rate": 0.00019337557875281715,
      "loss": 0.4708,
      "step": 7525
    },
    {
      "epoch": 0.035400810935397986,
      "grad_norm": 2.6367123126983643,
      "learning_rate": 0.00019337463577470367,
      "loss": 0.6219,
      "step": 7526
    },
    {
      "epoch": 0.03540551473701045,
      "grad_norm": 0.5129420757293701,
      "learning_rate": 0.00019337369279659022,
      "loss": 0.0944,
      "step": 7527
    },
    {
      "epoch": 0.03541021853862292,
      "grad_norm": 1.0642930269241333,
      "learning_rate": 0.00019337274981847674,
      "loss": 0.155,
      "step": 7528
    },
    {
      "epoch": 0.035414922340235376,
      "grad_norm": 1.0276992321014404,
      "learning_rate": 0.00019337180684036325,
      "loss": 0.1631,
      "step": 7529
    },
    {
      "epoch": 0.03541962614184784,
      "grad_norm": 1.9865405559539795,
      "learning_rate": 0.00019337086386224977,
      "loss": 0.2925,
      "step": 7530
    },
    {
      "epoch": 0.03542432994346031,
      "grad_norm": 2.3494017124176025,
      "learning_rate": 0.0001933699208841363,
      "loss": 0.2777,
      "step": 7531
    },
    {
      "epoch": 0.035429033745072766,
      "grad_norm": 1.4781724214553833,
      "learning_rate": 0.0001933689779060228,
      "loss": 0.1547,
      "step": 7532
    },
    {
      "epoch": 0.03543373754668523,
      "grad_norm": 2.4314956665039062,
      "learning_rate": 0.00019336803492790933,
      "loss": 0.2689,
      "step": 7533
    },
    {
      "epoch": 0.0354384413482977,
      "grad_norm": 3.0723958015441895,
      "learning_rate": 0.00019336709194979585,
      "loss": 0.2512,
      "step": 7534
    },
    {
      "epoch": 0.035443145149910156,
      "grad_norm": 1.5195083618164062,
      "learning_rate": 0.00019336614897168237,
      "loss": 0.2667,
      "step": 7535
    },
    {
      "epoch": 0.03544784895152262,
      "grad_norm": 0.8667975664138794,
      "learning_rate": 0.00019336520599356889,
      "loss": 0.2169,
      "step": 7536
    },
    {
      "epoch": 0.03545255275313508,
      "grad_norm": 0.6099021434783936,
      "learning_rate": 0.00019336426301545543,
      "loss": 0.0605,
      "step": 7537
    },
    {
      "epoch": 0.035457256554747546,
      "grad_norm": 2.4638280868530273,
      "learning_rate": 0.00019336332003734195,
      "loss": 0.4908,
      "step": 7538
    },
    {
      "epoch": 0.03546196035636001,
      "grad_norm": 1.5459414720535278,
      "learning_rate": 0.00019336237705922847,
      "loss": 0.1515,
      "step": 7539
    },
    {
      "epoch": 0.03546666415797247,
      "grad_norm": 1.4224873781204224,
      "learning_rate": 0.000193361434081115,
      "loss": 0.3824,
      "step": 7540
    },
    {
      "epoch": 0.035471367959584936,
      "grad_norm": 2.1605453491210938,
      "learning_rate": 0.0001933604911030015,
      "loss": 0.293,
      "step": 7541
    },
    {
      "epoch": 0.0354760717611974,
      "grad_norm": 3.0177996158599854,
      "learning_rate": 0.00019335954812488802,
      "loss": 0.5981,
      "step": 7542
    },
    {
      "epoch": 0.03548077556280986,
      "grad_norm": 1.9199965000152588,
      "learning_rate": 0.00019335860514677454,
      "loss": 0.4412,
      "step": 7543
    },
    {
      "epoch": 0.035485479364422326,
      "grad_norm": 1.3058478832244873,
      "learning_rate": 0.00019335766216866106,
      "loss": 0.165,
      "step": 7544
    },
    {
      "epoch": 0.03549018316603479,
      "grad_norm": 2.3384923934936523,
      "learning_rate": 0.00019335671919054758,
      "loss": 0.3035,
      "step": 7545
    },
    {
      "epoch": 0.03549488696764725,
      "grad_norm": 1.383186936378479,
      "learning_rate": 0.00019335577621243413,
      "loss": 0.3296,
      "step": 7546
    },
    {
      "epoch": 0.035499590769259716,
      "grad_norm": 1.1340705156326294,
      "learning_rate": 0.00019335483323432064,
      "loss": 0.2529,
      "step": 7547
    },
    {
      "epoch": 0.03550429457087218,
      "grad_norm": 0.8480856418609619,
      "learning_rate": 0.00019335389025620716,
      "loss": 0.0941,
      "step": 7548
    },
    {
      "epoch": 0.03550899837248464,
      "grad_norm": 1.6674952507019043,
      "learning_rate": 0.00019335294727809368,
      "loss": 0.2113,
      "step": 7549
    },
    {
      "epoch": 0.035513702174097106,
      "grad_norm": 1.5350788831710815,
      "learning_rate": 0.00019335200429998023,
      "loss": 0.2316,
      "step": 7550
    },
    {
      "epoch": 0.03551840597570957,
      "grad_norm": 1.089459776878357,
      "learning_rate": 0.00019335106132186675,
      "loss": 0.0821,
      "step": 7551
    },
    {
      "epoch": 0.03552310977732203,
      "grad_norm": 5.433326244354248,
      "learning_rate": 0.00019335011834375324,
      "loss": 0.9835,
      "step": 7552
    },
    {
      "epoch": 0.035527813578934496,
      "grad_norm": 1.3798893690109253,
      "learning_rate": 0.00019334917536563976,
      "loss": 0.1795,
      "step": 7553
    },
    {
      "epoch": 0.035532517380546955,
      "grad_norm": 4.746497631072998,
      "learning_rate": 0.00019334823238752627,
      "loss": 0.4484,
      "step": 7554
    },
    {
      "epoch": 0.03553722118215942,
      "grad_norm": 4.221977233886719,
      "learning_rate": 0.00019334728940941282,
      "loss": 0.3943,
      "step": 7555
    },
    {
      "epoch": 0.035541924983771886,
      "grad_norm": 2.884469985961914,
      "learning_rate": 0.00019334634643129934,
      "loss": 0.6118,
      "step": 7556
    },
    {
      "epoch": 0.035546628785384345,
      "grad_norm": 0.9146345257759094,
      "learning_rate": 0.00019334540345318586,
      "loss": 0.1362,
      "step": 7557
    },
    {
      "epoch": 0.03555133258699681,
      "grad_norm": 1.2061877250671387,
      "learning_rate": 0.00019334446047507238,
      "loss": 0.2315,
      "step": 7558
    },
    {
      "epoch": 0.035556036388609276,
      "grad_norm": 1.1595820188522339,
      "learning_rate": 0.00019334351749695892,
      "loss": 0.0826,
      "step": 7559
    },
    {
      "epoch": 0.035560740190221735,
      "grad_norm": 1.0497132539749146,
      "learning_rate": 0.00019334257451884544,
      "loss": 0.0922,
      "step": 7560
    },
    {
      "epoch": 0.0355654439918342,
      "grad_norm": 1.6553446054458618,
      "learning_rate": 0.00019334163154073196,
      "loss": 0.3373,
      "step": 7561
    },
    {
      "epoch": 0.035570147793446666,
      "grad_norm": 1.9570660591125488,
      "learning_rate": 0.00019334068856261848,
      "loss": 0.3065,
      "step": 7562
    },
    {
      "epoch": 0.035574851595059125,
      "grad_norm": 2.9826853275299072,
      "learning_rate": 0.00019333974558450497,
      "loss": 0.3265,
      "step": 7563
    },
    {
      "epoch": 0.03557955539667159,
      "grad_norm": 1.2931948900222778,
      "learning_rate": 0.00019333880260639152,
      "loss": 0.1082,
      "step": 7564
    },
    {
      "epoch": 0.035584259198284056,
      "grad_norm": 1.440552830696106,
      "learning_rate": 0.00019333785962827803,
      "loss": 0.2172,
      "step": 7565
    },
    {
      "epoch": 0.035588962999896515,
      "grad_norm": 2.424330711364746,
      "learning_rate": 0.00019333691665016455,
      "loss": 0.2121,
      "step": 7566
    },
    {
      "epoch": 0.03559366680150898,
      "grad_norm": 0.9239464402198792,
      "learning_rate": 0.00019333597367205107,
      "loss": 0.0596,
      "step": 7567
    },
    {
      "epoch": 0.035598370603121446,
      "grad_norm": 3.1094608306884766,
      "learning_rate": 0.00019333503069393762,
      "loss": 0.4267,
      "step": 7568
    },
    {
      "epoch": 0.035603074404733905,
      "grad_norm": 3.921217203140259,
      "learning_rate": 0.00019333408771582414,
      "loss": 0.3946,
      "step": 7569
    },
    {
      "epoch": 0.03560777820634637,
      "grad_norm": 1.3966082334518433,
      "learning_rate": 0.00019333314473771065,
      "loss": 0.1392,
      "step": 7570
    },
    {
      "epoch": 0.03561248200795883,
      "grad_norm": 0.9640213251113892,
      "learning_rate": 0.00019333220175959717,
      "loss": 0.0526,
      "step": 7571
    },
    {
      "epoch": 0.035617185809571295,
      "grad_norm": 1.0852383375167847,
      "learning_rate": 0.0001933312587814837,
      "loss": 0.1291,
      "step": 7572
    },
    {
      "epoch": 0.03562188961118376,
      "grad_norm": 1.9865721464157104,
      "learning_rate": 0.0001933303158033702,
      "loss": 0.1663,
      "step": 7573
    },
    {
      "epoch": 0.03562659341279622,
      "grad_norm": 3.9683635234832764,
      "learning_rate": 0.00019332937282525673,
      "loss": 0.5569,
      "step": 7574
    },
    {
      "epoch": 0.035631297214408685,
      "grad_norm": 2.362294912338257,
      "learning_rate": 0.00019332842984714325,
      "loss": 0.1866,
      "step": 7575
    },
    {
      "epoch": 0.03563600101602115,
      "grad_norm": 3.87424373626709,
      "learning_rate": 0.00019332748686902977,
      "loss": 0.6608,
      "step": 7576
    },
    {
      "epoch": 0.03564070481763361,
      "grad_norm": 2.0145978927612305,
      "learning_rate": 0.0001933265438909163,
      "loss": 0.1293,
      "step": 7577
    },
    {
      "epoch": 0.035645408619246075,
      "grad_norm": 2.409428358078003,
      "learning_rate": 0.00019332560091280283,
      "loss": 0.5832,
      "step": 7578
    },
    {
      "epoch": 0.03565011242085854,
      "grad_norm": 0.5268972516059875,
      "learning_rate": 0.00019332465793468935,
      "loss": 0.0474,
      "step": 7579
    },
    {
      "epoch": 0.035654816222471,
      "grad_norm": 0.8880640268325806,
      "learning_rate": 0.00019332371495657587,
      "loss": 0.0822,
      "step": 7580
    },
    {
      "epoch": 0.035659520024083464,
      "grad_norm": 6.532806873321533,
      "learning_rate": 0.0001933227719784624,
      "loss": 1.5535,
      "step": 7581
    },
    {
      "epoch": 0.03566422382569593,
      "grad_norm": 0.7976489067077637,
      "learning_rate": 0.00019332182900034893,
      "loss": 0.1237,
      "step": 7582
    },
    {
      "epoch": 0.03566892762730839,
      "grad_norm": 1.1691333055496216,
      "learning_rate": 0.00019332088602223542,
      "loss": 0.1006,
      "step": 7583
    },
    {
      "epoch": 0.035673631428920854,
      "grad_norm": 0.42065131664276123,
      "learning_rate": 0.00019331994304412194,
      "loss": 0.0329,
      "step": 7584
    },
    {
      "epoch": 0.03567833523053332,
      "grad_norm": 1.2036515474319458,
      "learning_rate": 0.00019331900006600846,
      "loss": 0.136,
      "step": 7585
    },
    {
      "epoch": 0.03568303903214578,
      "grad_norm": 2.187006950378418,
      "learning_rate": 0.00019331805708789498,
      "loss": 0.3502,
      "step": 7586
    },
    {
      "epoch": 0.035687742833758244,
      "grad_norm": 2.0958569049835205,
      "learning_rate": 0.00019331711410978153,
      "loss": 0.2818,
      "step": 7587
    },
    {
      "epoch": 0.0356924466353707,
      "grad_norm": 2.6305792331695557,
      "learning_rate": 0.00019331617113166804,
      "loss": 0.7295,
      "step": 7588
    },
    {
      "epoch": 0.03569715043698317,
      "grad_norm": 0.8956066370010376,
      "learning_rate": 0.00019331522815355456,
      "loss": 0.0622,
      "step": 7589
    },
    {
      "epoch": 0.035701854238595634,
      "grad_norm": 0.4505092203617096,
      "learning_rate": 0.00019331428517544108,
      "loss": 0.0381,
      "step": 7590
    },
    {
      "epoch": 0.03570655804020809,
      "grad_norm": 4.901854515075684,
      "learning_rate": 0.00019331334219732763,
      "loss": 0.3187,
      "step": 7591
    },
    {
      "epoch": 0.03571126184182056,
      "grad_norm": 2.310251474380493,
      "learning_rate": 0.00019331239921921415,
      "loss": 0.1859,
      "step": 7592
    },
    {
      "epoch": 0.035715965643433024,
      "grad_norm": 4.25996732711792,
      "learning_rate": 0.00019331145624110066,
      "loss": 0.9736,
      "step": 7593
    },
    {
      "epoch": 0.03572066944504548,
      "grad_norm": 0.4913555979728699,
      "learning_rate": 0.00019331051326298716,
      "loss": 0.0445,
      "step": 7594
    },
    {
      "epoch": 0.03572537324665795,
      "grad_norm": 1.377273440361023,
      "learning_rate": 0.00019330957028487367,
      "loss": 0.1065,
      "step": 7595
    },
    {
      "epoch": 0.035730077048270414,
      "grad_norm": 2.479434013366699,
      "learning_rate": 0.00019330862730676022,
      "loss": 0.2747,
      "step": 7596
    },
    {
      "epoch": 0.03573478084988287,
      "grad_norm": 4.5536041259765625,
      "learning_rate": 0.00019330768432864674,
      "loss": 0.9904,
      "step": 7597
    },
    {
      "epoch": 0.03573948465149534,
      "grad_norm": 0.3289385139942169,
      "learning_rate": 0.00019330674135053326,
      "loss": 0.0235,
      "step": 7598
    },
    {
      "epoch": 0.035744188453107804,
      "grad_norm": 2.6651296615600586,
      "learning_rate": 0.00019330579837241978,
      "loss": 0.5443,
      "step": 7599
    },
    {
      "epoch": 0.03574889225472026,
      "grad_norm": 1.6515014171600342,
      "learning_rate": 0.00019330485539430632,
      "loss": 0.1309,
      "step": 7600
    },
    {
      "epoch": 0.03575359605633273,
      "grad_norm": 2.3996641635894775,
      "learning_rate": 0.00019330391241619284,
      "loss": 0.2081,
      "step": 7601
    },
    {
      "epoch": 0.035758299857945194,
      "grad_norm": 2.044100522994995,
      "learning_rate": 0.00019330296943807936,
      "loss": 0.2052,
      "step": 7602
    },
    {
      "epoch": 0.03576300365955765,
      "grad_norm": 7.136459827423096,
      "learning_rate": 0.00019330202645996588,
      "loss": 0.4711,
      "step": 7603
    },
    {
      "epoch": 0.03576770746117012,
      "grad_norm": 1.3346816301345825,
      "learning_rate": 0.0001933010834818524,
      "loss": 0.1262,
      "step": 7604
    },
    {
      "epoch": 0.03577241126278258,
      "grad_norm": 2.266035795211792,
      "learning_rate": 0.00019330014050373892,
      "loss": 0.169,
      "step": 7605
    },
    {
      "epoch": 0.03577711506439504,
      "grad_norm": 4.811086177825928,
      "learning_rate": 0.00019329919752562543,
      "loss": 0.9238,
      "step": 7606
    },
    {
      "epoch": 0.03578181886600751,
      "grad_norm": 4.152270317077637,
      "learning_rate": 0.00019329825454751195,
      "loss": 0.7598,
      "step": 7607
    },
    {
      "epoch": 0.03578652266761997,
      "grad_norm": 1.8557727336883545,
      "learning_rate": 0.00019329731156939847,
      "loss": 0.2941,
      "step": 7608
    },
    {
      "epoch": 0.03579122646923243,
      "grad_norm": 0.6019214391708374,
      "learning_rate": 0.00019329636859128502,
      "loss": 0.0657,
      "step": 7609
    },
    {
      "epoch": 0.0357959302708449,
      "grad_norm": 0.48748448491096497,
      "learning_rate": 0.00019329542561317154,
      "loss": 0.0315,
      "step": 7610
    },
    {
      "epoch": 0.03580063407245736,
      "grad_norm": 3.1576483249664307,
      "learning_rate": 0.00019329448263505805,
      "loss": 0.3828,
      "step": 7611
    },
    {
      "epoch": 0.03580533787406982,
      "grad_norm": 1.9731276035308838,
      "learning_rate": 0.00019329353965694457,
      "loss": 0.1732,
      "step": 7612
    },
    {
      "epoch": 0.03581004167568229,
      "grad_norm": 4.015052318572998,
      "learning_rate": 0.0001932925966788311,
      "loss": 0.3928,
      "step": 7613
    },
    {
      "epoch": 0.03581474547729475,
      "grad_norm": 0.7951876521110535,
      "learning_rate": 0.0001932916537007176,
      "loss": 0.0549,
      "step": 7614
    },
    {
      "epoch": 0.03581944927890721,
      "grad_norm": 6.315249919891357,
      "learning_rate": 0.00019329071072260413,
      "loss": 0.6717,
      "step": 7615
    },
    {
      "epoch": 0.03582415308051968,
      "grad_norm": 1.6547870635986328,
      "learning_rate": 0.00019328976774449065,
      "loss": 0.3731,
      "step": 7616
    },
    {
      "epoch": 0.03582885688213214,
      "grad_norm": 2.824439287185669,
      "learning_rate": 0.00019328882476637717,
      "loss": 0.189,
      "step": 7617
    },
    {
      "epoch": 0.0358335606837446,
      "grad_norm": 3.2725813388824463,
      "learning_rate": 0.0001932878817882637,
      "loss": 0.7108,
      "step": 7618
    },
    {
      "epoch": 0.03583826448535707,
      "grad_norm": 2.062859058380127,
      "learning_rate": 0.00019328693881015023,
      "loss": 0.1672,
      "step": 7619
    },
    {
      "epoch": 0.03584296828696953,
      "grad_norm": 1.6097235679626465,
      "learning_rate": 0.00019328599583203675,
      "loss": 0.3417,
      "step": 7620
    },
    {
      "epoch": 0.03584767208858199,
      "grad_norm": 2.914911985397339,
      "learning_rate": 0.00019328505285392327,
      "loss": 0.5024,
      "step": 7621
    },
    {
      "epoch": 0.03585237589019445,
      "grad_norm": 3.3779397010803223,
      "learning_rate": 0.0001932841098758098,
      "loss": 0.6286,
      "step": 7622
    },
    {
      "epoch": 0.03585707969180692,
      "grad_norm": 1.6954116821289062,
      "learning_rate": 0.00019328316689769633,
      "loss": 0.2277,
      "step": 7623
    },
    {
      "epoch": 0.03586178349341938,
      "grad_norm": 1.4009696245193481,
      "learning_rate": 0.00019328222391958285,
      "loss": 0.1572,
      "step": 7624
    },
    {
      "epoch": 0.03586648729503184,
      "grad_norm": 0.47318384051322937,
      "learning_rate": 0.00019328128094146934,
      "loss": 0.0468,
      "step": 7625
    },
    {
      "epoch": 0.03587119109664431,
      "grad_norm": 2.130032777786255,
      "learning_rate": 0.00019328033796335586,
      "loss": 0.2112,
      "step": 7626
    },
    {
      "epoch": 0.03587589489825677,
      "grad_norm": 1.594287633895874,
      "learning_rate": 0.0001932793949852424,
      "loss": 0.1713,
      "step": 7627
    },
    {
      "epoch": 0.03588059869986923,
      "grad_norm": 2.5236988067626953,
      "learning_rate": 0.00019327845200712893,
      "loss": 0.4602,
      "step": 7628
    },
    {
      "epoch": 0.0358853025014817,
      "grad_norm": 2.221590280532837,
      "learning_rate": 0.00019327750902901544,
      "loss": 0.3634,
      "step": 7629
    },
    {
      "epoch": 0.03589000630309416,
      "grad_norm": 1.518350601196289,
      "learning_rate": 0.00019327656605090196,
      "loss": 0.1909,
      "step": 7630
    },
    {
      "epoch": 0.03589471010470662,
      "grad_norm": 1.683042049407959,
      "learning_rate": 0.00019327562307278848,
      "loss": 0.3742,
      "step": 7631
    },
    {
      "epoch": 0.03589941390631909,
      "grad_norm": 3.2107577323913574,
      "learning_rate": 0.00019327468009467503,
      "loss": 0.6259,
      "step": 7632
    },
    {
      "epoch": 0.03590411770793155,
      "grad_norm": 1.476239800453186,
      "learning_rate": 0.00019327373711656155,
      "loss": 0.3113,
      "step": 7633
    },
    {
      "epoch": 0.03590882150954401,
      "grad_norm": 1.7984813451766968,
      "learning_rate": 0.00019327279413844806,
      "loss": 0.1327,
      "step": 7634
    },
    {
      "epoch": 0.03591352531115648,
      "grad_norm": 0.8022615313529968,
      "learning_rate": 0.00019327185116033458,
      "loss": 0.0998,
      "step": 7635
    },
    {
      "epoch": 0.03591822911276894,
      "grad_norm": 2.1340086460113525,
      "learning_rate": 0.0001932709081822211,
      "loss": 0.2087,
      "step": 7636
    },
    {
      "epoch": 0.0359229329143814,
      "grad_norm": 0.7902393341064453,
      "learning_rate": 0.00019326996520410762,
      "loss": 0.1755,
      "step": 7637
    },
    {
      "epoch": 0.03592763671599387,
      "grad_norm": 0.8496509194374084,
      "learning_rate": 0.00019326902222599414,
      "loss": 0.162,
      "step": 7638
    },
    {
      "epoch": 0.035932340517606326,
      "grad_norm": 1.8103666305541992,
      "learning_rate": 0.00019326807924788066,
      "loss": 0.2471,
      "step": 7639
    },
    {
      "epoch": 0.03593704431921879,
      "grad_norm": 0.6111563444137573,
      "learning_rate": 0.00019326713626976718,
      "loss": 0.0839,
      "step": 7640
    },
    {
      "epoch": 0.03594174812083126,
      "grad_norm": 1.3913828134536743,
      "learning_rate": 0.00019326619329165372,
      "loss": 0.1792,
      "step": 7641
    },
    {
      "epoch": 0.035946451922443716,
      "grad_norm": 2.444122791290283,
      "learning_rate": 0.00019326525031354024,
      "loss": 0.4846,
      "step": 7642
    },
    {
      "epoch": 0.03595115572405618,
      "grad_norm": 1.4262994527816772,
      "learning_rate": 0.00019326430733542676,
      "loss": 0.198,
      "step": 7643
    },
    {
      "epoch": 0.03595585952566865,
      "grad_norm": 0.3766067922115326,
      "learning_rate": 0.00019326336435731328,
      "loss": 0.0381,
      "step": 7644
    },
    {
      "epoch": 0.035960563327281106,
      "grad_norm": 2.091651439666748,
      "learning_rate": 0.0001932624213791998,
      "loss": 0.3619,
      "step": 7645
    },
    {
      "epoch": 0.03596526712889357,
      "grad_norm": 3.373635768890381,
      "learning_rate": 0.00019326147840108632,
      "loss": 0.7961,
      "step": 7646
    },
    {
      "epoch": 0.03596997093050604,
      "grad_norm": 0.6835833191871643,
      "learning_rate": 0.00019326053542297283,
      "loss": 0.0707,
      "step": 7647
    },
    {
      "epoch": 0.035974674732118496,
      "grad_norm": 0.7528269290924072,
      "learning_rate": 0.00019325959244485935,
      "loss": 0.1006,
      "step": 7648
    },
    {
      "epoch": 0.03597937853373096,
      "grad_norm": 4.782341957092285,
      "learning_rate": 0.00019325864946674587,
      "loss": 0.7102,
      "step": 7649
    },
    {
      "epoch": 0.03598408233534343,
      "grad_norm": 1.0759377479553223,
      "learning_rate": 0.00019325770648863242,
      "loss": 0.1389,
      "step": 7650
    },
    {
      "epoch": 0.035988786136955886,
      "grad_norm": 2.360109806060791,
      "learning_rate": 0.00019325676351051894,
      "loss": 0.38,
      "step": 7651
    },
    {
      "epoch": 0.03599348993856835,
      "grad_norm": 0.7459149360656738,
      "learning_rate": 0.00019325582053240545,
      "loss": 0.0329,
      "step": 7652
    },
    {
      "epoch": 0.03599819374018082,
      "grad_norm": 1.0870440006256104,
      "learning_rate": 0.00019325487755429197,
      "loss": 0.1381,
      "step": 7653
    },
    {
      "epoch": 0.036002897541793276,
      "grad_norm": 0.7244410514831543,
      "learning_rate": 0.0001932539345761785,
      "loss": 0.053,
      "step": 7654
    },
    {
      "epoch": 0.03600760134340574,
      "grad_norm": 1.023335337638855,
      "learning_rate": 0.00019325299159806504,
      "loss": 0.0668,
      "step": 7655
    },
    {
      "epoch": 0.03601230514501821,
      "grad_norm": 0.48486265540122986,
      "learning_rate": 0.00019325204861995153,
      "loss": 0.0336,
      "step": 7656
    },
    {
      "epoch": 0.036017008946630666,
      "grad_norm": 3.298821449279785,
      "learning_rate": 0.00019325110564183805,
      "loss": 0.2799,
      "step": 7657
    },
    {
      "epoch": 0.03602171274824313,
      "grad_norm": 8.787105560302734,
      "learning_rate": 0.00019325016266372457,
      "loss": 0.4132,
      "step": 7658
    },
    {
      "epoch": 0.03602641654985559,
      "grad_norm": 1.5870879888534546,
      "learning_rate": 0.0001932492196856111,
      "loss": 0.1666,
      "step": 7659
    },
    {
      "epoch": 0.036031120351468056,
      "grad_norm": 0.7797830700874329,
      "learning_rate": 0.00019324827670749763,
      "loss": 0.0378,
      "step": 7660
    },
    {
      "epoch": 0.03603582415308052,
      "grad_norm": 2.817723512649536,
      "learning_rate": 0.00019324733372938415,
      "loss": 0.3696,
      "step": 7661
    },
    {
      "epoch": 0.03604052795469298,
      "grad_norm": 3.885369062423706,
      "learning_rate": 0.00019324639075127067,
      "loss": 0.5045,
      "step": 7662
    },
    {
      "epoch": 0.036045231756305446,
      "grad_norm": 3.173643112182617,
      "learning_rate": 0.0001932454477731572,
      "loss": 0.4109,
      "step": 7663
    },
    {
      "epoch": 0.03604993555791791,
      "grad_norm": 2.176795244216919,
      "learning_rate": 0.00019324450479504373,
      "loss": 0.3615,
      "step": 7664
    },
    {
      "epoch": 0.03605463935953037,
      "grad_norm": 1.5147074460983276,
      "learning_rate": 0.00019324356181693025,
      "loss": 0.1297,
      "step": 7665
    },
    {
      "epoch": 0.036059343161142836,
      "grad_norm": 1.0504093170166016,
      "learning_rate": 0.00019324261883881677,
      "loss": 0.0849,
      "step": 7666
    },
    {
      "epoch": 0.0360640469627553,
      "grad_norm": 3.520112991333008,
      "learning_rate": 0.0001932416758607033,
      "loss": 0.494,
      "step": 7667
    },
    {
      "epoch": 0.03606875076436776,
      "grad_norm": 3.9938883781433105,
      "learning_rate": 0.0001932407328825898,
      "loss": 0.3725,
      "step": 7668
    },
    {
      "epoch": 0.036073454565980226,
      "grad_norm": 0.9953289031982422,
      "learning_rate": 0.00019323978990447633,
      "loss": 0.0549,
      "step": 7669
    },
    {
      "epoch": 0.03607815836759269,
      "grad_norm": 0.989406168460846,
      "learning_rate": 0.00019323884692636284,
      "loss": 0.0723,
      "step": 7670
    },
    {
      "epoch": 0.03608286216920515,
      "grad_norm": 1.4039047956466675,
      "learning_rate": 0.00019323790394824936,
      "loss": 0.0564,
      "step": 7671
    },
    {
      "epoch": 0.036087565970817616,
      "grad_norm": 2.5189945697784424,
      "learning_rate": 0.00019323696097013588,
      "loss": 0.3326,
      "step": 7672
    },
    {
      "epoch": 0.03609226977243008,
      "grad_norm": 0.0931473970413208,
      "learning_rate": 0.00019323601799202243,
      "loss": 0.0056,
      "step": 7673
    },
    {
      "epoch": 0.03609697357404254,
      "grad_norm": 5.976694107055664,
      "learning_rate": 0.00019323507501390895,
      "loss": 0.8033,
      "step": 7674
    },
    {
      "epoch": 0.036101677375655006,
      "grad_norm": 3.5851964950561523,
      "learning_rate": 0.00019323413203579546,
      "loss": 0.7452,
      "step": 7675
    },
    {
      "epoch": 0.036106381177267464,
      "grad_norm": 0.12219684571027756,
      "learning_rate": 0.00019323318905768198,
      "loss": 0.0072,
      "step": 7676
    },
    {
      "epoch": 0.03611108497887993,
      "grad_norm": 5.809074401855469,
      "learning_rate": 0.0001932322460795685,
      "loss": 1.1399,
      "step": 7677
    },
    {
      "epoch": 0.036115788780492396,
      "grad_norm": 5.321526050567627,
      "learning_rate": 0.00019323130310145502,
      "loss": 1.0008,
      "step": 7678
    },
    {
      "epoch": 0.036120492582104854,
      "grad_norm": 4.421888828277588,
      "learning_rate": 0.00019323036012334154,
      "loss": 0.5215,
      "step": 7679
    },
    {
      "epoch": 0.03612519638371732,
      "grad_norm": 5.463748455047607,
      "learning_rate": 0.00019322941714522806,
      "loss": 1.107,
      "step": 7680
    },
    {
      "epoch": 0.036129900185329786,
      "grad_norm": 3.2263693809509277,
      "learning_rate": 0.00019322847416711458,
      "loss": 0.6774,
      "step": 7681
    },
    {
      "epoch": 0.036134603986942244,
      "grad_norm": 2.618079423904419,
      "learning_rate": 0.00019322753118900112,
      "loss": 0.1258,
      "step": 7682
    },
    {
      "epoch": 0.03613930778855471,
      "grad_norm": 2.574253559112549,
      "learning_rate": 0.00019322658821088764,
      "loss": 0.4639,
      "step": 7683
    },
    {
      "epoch": 0.036144011590167176,
      "grad_norm": 2.8809030055999756,
      "learning_rate": 0.00019322564523277416,
      "loss": 0.5727,
      "step": 7684
    },
    {
      "epoch": 0.036148715391779634,
      "grad_norm": 1.5648150444030762,
      "learning_rate": 0.00019322470225466068,
      "loss": 0.3295,
      "step": 7685
    },
    {
      "epoch": 0.0361534191933921,
      "grad_norm": 0.17225301265716553,
      "learning_rate": 0.00019322375927654722,
      "loss": 0.0139,
      "step": 7686
    },
    {
      "epoch": 0.036158122995004566,
      "grad_norm": 1.1656886339187622,
      "learning_rate": 0.00019322281629843372,
      "loss": 0.0906,
      "step": 7687
    },
    {
      "epoch": 0.036162826796617024,
      "grad_norm": 4.537817478179932,
      "learning_rate": 0.00019322187332032023,
      "loss": 1.0457,
      "step": 7688
    },
    {
      "epoch": 0.03616753059822949,
      "grad_norm": 0.9796514511108398,
      "learning_rate": 0.00019322093034220675,
      "loss": 0.1361,
      "step": 7689
    },
    {
      "epoch": 0.036172234399841956,
      "grad_norm": 0.6670960187911987,
      "learning_rate": 0.00019321998736409327,
      "loss": 0.0681,
      "step": 7690
    },
    {
      "epoch": 0.036176938201454414,
      "grad_norm": 1.6310728788375854,
      "learning_rate": 0.00019321904438597982,
      "loss": 0.1969,
      "step": 7691
    },
    {
      "epoch": 0.03618164200306688,
      "grad_norm": 1.309216022491455,
      "learning_rate": 0.00019321810140786634,
      "loss": 0.1682,
      "step": 7692
    },
    {
      "epoch": 0.03618634580467934,
      "grad_norm": 2.0758848190307617,
      "learning_rate": 0.00019321715842975285,
      "loss": 0.2814,
      "step": 7693
    },
    {
      "epoch": 0.036191049606291804,
      "grad_norm": 2.7258365154266357,
      "learning_rate": 0.00019321621545163937,
      "loss": 0.6986,
      "step": 7694
    },
    {
      "epoch": 0.03619575340790427,
      "grad_norm": 0.7277541160583496,
      "learning_rate": 0.0001932152724735259,
      "loss": 0.0892,
      "step": 7695
    },
    {
      "epoch": 0.03620045720951673,
      "grad_norm": 2.474559783935547,
      "learning_rate": 0.00019321432949541244,
      "loss": 0.4419,
      "step": 7696
    },
    {
      "epoch": 0.036205161011129194,
      "grad_norm": 3.8213956356048584,
      "learning_rate": 0.00019321338651729896,
      "loss": 0.6193,
      "step": 7697
    },
    {
      "epoch": 0.03620986481274166,
      "grad_norm": 2.442192316055298,
      "learning_rate": 0.00019321244353918547,
      "loss": 0.3293,
      "step": 7698
    },
    {
      "epoch": 0.03621456861435412,
      "grad_norm": 1.75432288646698,
      "learning_rate": 0.00019321150056107197,
      "loss": 0.1819,
      "step": 7699
    },
    {
      "epoch": 0.036219272415966584,
      "grad_norm": 1.6500880718231201,
      "learning_rate": 0.0001932105575829585,
      "loss": 0.1206,
      "step": 7700
    },
    {
      "epoch": 0.03622397621757905,
      "grad_norm": 1.4672062397003174,
      "learning_rate": 0.00019320961460484503,
      "loss": 0.1746,
      "step": 7701
    },
    {
      "epoch": 0.03622868001919151,
      "grad_norm": 10.020858764648438,
      "learning_rate": 0.00019320867162673155,
      "loss": 0.4444,
      "step": 7702
    },
    {
      "epoch": 0.036233383820803974,
      "grad_norm": 1.4460759162902832,
      "learning_rate": 0.00019320772864861807,
      "loss": 0.2432,
      "step": 7703
    },
    {
      "epoch": 0.03623808762241644,
      "grad_norm": 3.3720624446868896,
      "learning_rate": 0.0001932067856705046,
      "loss": 0.4854,
      "step": 7704
    },
    {
      "epoch": 0.0362427914240289,
      "grad_norm": 1.1526817083358765,
      "learning_rate": 0.00019320584269239113,
      "loss": 0.2835,
      "step": 7705
    },
    {
      "epoch": 0.036247495225641364,
      "grad_norm": 1.0048904418945312,
      "learning_rate": 0.00019320489971427765,
      "loss": 0.1014,
      "step": 7706
    },
    {
      "epoch": 0.03625219902725383,
      "grad_norm": 3.4770758152008057,
      "learning_rate": 0.00019320395673616417,
      "loss": 0.8072,
      "step": 7707
    },
    {
      "epoch": 0.03625690282886629,
      "grad_norm": 11.15910530090332,
      "learning_rate": 0.0001932030137580507,
      "loss": 1.0967,
      "step": 7708
    },
    {
      "epoch": 0.036261606630478754,
      "grad_norm": 0.8624027967453003,
      "learning_rate": 0.0001932020707799372,
      "loss": 0.11,
      "step": 7709
    },
    {
      "epoch": 0.03626631043209121,
      "grad_norm": 4.067207336425781,
      "learning_rate": 0.00019320112780182373,
      "loss": 0.4062,
      "step": 7710
    },
    {
      "epoch": 0.03627101423370368,
      "grad_norm": 2.217120885848999,
      "learning_rate": 0.00019320018482371024,
      "loss": 0.2755,
      "step": 7711
    },
    {
      "epoch": 0.036275718035316144,
      "grad_norm": 1.582794427871704,
      "learning_rate": 0.00019319924184559676,
      "loss": 0.1483,
      "step": 7712
    },
    {
      "epoch": 0.0362804218369286,
      "grad_norm": 1.9979429244995117,
      "learning_rate": 0.00019319829886748328,
      "loss": 0.198,
      "step": 7713
    },
    {
      "epoch": 0.03628512563854107,
      "grad_norm": 1.356534719467163,
      "learning_rate": 0.00019319735588936983,
      "loss": 0.2579,
      "step": 7714
    },
    {
      "epoch": 0.036289829440153534,
      "grad_norm": 8.015575408935547,
      "learning_rate": 0.00019319641291125635,
      "loss": 0.4928,
      "step": 7715
    },
    {
      "epoch": 0.03629453324176599,
      "grad_norm": 3.8844051361083984,
      "learning_rate": 0.00019319546993314286,
      "loss": 0.4802,
      "step": 7716
    },
    {
      "epoch": 0.03629923704337846,
      "grad_norm": 3.0363996028900146,
      "learning_rate": 0.00019319452695502938,
      "loss": 0.5919,
      "step": 7717
    },
    {
      "epoch": 0.036303940844990924,
      "grad_norm": 0.2854368984699249,
      "learning_rate": 0.0001931935839769159,
      "loss": 0.0414,
      "step": 7718
    },
    {
      "epoch": 0.03630864464660338,
      "grad_norm": 1.0625483989715576,
      "learning_rate": 0.00019319264099880242,
      "loss": 0.1048,
      "step": 7719
    },
    {
      "epoch": 0.03631334844821585,
      "grad_norm": 1.246559739112854,
      "learning_rate": 0.00019319169802068894,
      "loss": 0.2735,
      "step": 7720
    },
    {
      "epoch": 0.036318052249828314,
      "grad_norm": 2.1100008487701416,
      "learning_rate": 0.00019319075504257546,
      "loss": 0.4424,
      "step": 7721
    },
    {
      "epoch": 0.03632275605144077,
      "grad_norm": 2.0642929077148438,
      "learning_rate": 0.00019318981206446198,
      "loss": 0.2421,
      "step": 7722
    },
    {
      "epoch": 0.03632745985305324,
      "grad_norm": 0.4060322344303131,
      "learning_rate": 0.00019318886908634852,
      "loss": 0.037,
      "step": 7723
    },
    {
      "epoch": 0.036332163654665704,
      "grad_norm": 2.0303397178649902,
      "learning_rate": 0.00019318792610823504,
      "loss": 0.4436,
      "step": 7724
    },
    {
      "epoch": 0.03633686745627816,
      "grad_norm": 1.1454408168792725,
      "learning_rate": 0.00019318698313012156,
      "loss": 0.2417,
      "step": 7725
    },
    {
      "epoch": 0.03634157125789063,
      "grad_norm": 2.192091941833496,
      "learning_rate": 0.00019318604015200808,
      "loss": 0.6931,
      "step": 7726
    },
    {
      "epoch": 0.03634627505950309,
      "grad_norm": 1.3334887027740479,
      "learning_rate": 0.00019318509717389462,
      "loss": 0.3034,
      "step": 7727
    },
    {
      "epoch": 0.03635097886111555,
      "grad_norm": 0.8631740212440491,
      "learning_rate": 0.00019318415419578114,
      "loss": 0.1248,
      "step": 7728
    },
    {
      "epoch": 0.03635568266272802,
      "grad_norm": 1.0681442022323608,
      "learning_rate": 0.00019318321121766766,
      "loss": 0.3578,
      "step": 7729
    },
    {
      "epoch": 0.03636038646434048,
      "grad_norm": 3.019597053527832,
      "learning_rate": 0.00019318226823955415,
      "loss": 0.6432,
      "step": 7730
    },
    {
      "epoch": 0.03636509026595294,
      "grad_norm": 0.664807140827179,
      "learning_rate": 0.00019318132526144067,
      "loss": 0.0647,
      "step": 7731
    },
    {
      "epoch": 0.03636979406756541,
      "grad_norm": 0.9946608543395996,
      "learning_rate": 0.00019318038228332722,
      "loss": 0.1463,
      "step": 7732
    },
    {
      "epoch": 0.03637449786917787,
      "grad_norm": 0.8189358115196228,
      "learning_rate": 0.00019317943930521374,
      "loss": 0.0923,
      "step": 7733
    },
    {
      "epoch": 0.03637920167079033,
      "grad_norm": 1.2944591045379639,
      "learning_rate": 0.00019317849632710025,
      "loss": 0.158,
      "step": 7734
    },
    {
      "epoch": 0.0363839054724028,
      "grad_norm": 0.39278510212898254,
      "learning_rate": 0.00019317755334898677,
      "loss": 0.0324,
      "step": 7735
    },
    {
      "epoch": 0.03638860927401526,
      "grad_norm": 1.1830960512161255,
      "learning_rate": 0.00019317661037087332,
      "loss": 0.2265,
      "step": 7736
    },
    {
      "epoch": 0.03639331307562772,
      "grad_norm": 2.9263367652893066,
      "learning_rate": 0.00019317566739275984,
      "loss": 0.7877,
      "step": 7737
    },
    {
      "epoch": 0.03639801687724019,
      "grad_norm": 1.239307165145874,
      "learning_rate": 0.00019317472441464636,
      "loss": 0.1117,
      "step": 7738
    },
    {
      "epoch": 0.03640272067885265,
      "grad_norm": 0.5915355086326599,
      "learning_rate": 0.00019317378143653287,
      "loss": 0.0662,
      "step": 7739
    },
    {
      "epoch": 0.03640742448046511,
      "grad_norm": 2.864786386489868,
      "learning_rate": 0.0001931728384584194,
      "loss": 0.3465,
      "step": 7740
    },
    {
      "epoch": 0.03641212828207758,
      "grad_norm": 1.1494495868682861,
      "learning_rate": 0.0001931718954803059,
      "loss": 0.2178,
      "step": 7741
    },
    {
      "epoch": 0.03641683208369004,
      "grad_norm": 2.273684501647949,
      "learning_rate": 0.00019317095250219243,
      "loss": 0.2445,
      "step": 7742
    },
    {
      "epoch": 0.0364215358853025,
      "grad_norm": 1.5131397247314453,
      "learning_rate": 0.00019317000952407895,
      "loss": 0.323,
      "step": 7743
    },
    {
      "epoch": 0.03642623968691496,
      "grad_norm": 1.1637215614318848,
      "learning_rate": 0.00019316906654596547,
      "loss": 0.2254,
      "step": 7744
    },
    {
      "epoch": 0.03643094348852743,
      "grad_norm": 0.9379740357398987,
      "learning_rate": 0.000193168123567852,
      "loss": 0.1134,
      "step": 7745
    },
    {
      "epoch": 0.03643564729013989,
      "grad_norm": 2.004506826400757,
      "learning_rate": 0.00019316718058973853,
      "loss": 0.4214,
      "step": 7746
    },
    {
      "epoch": 0.03644035109175235,
      "grad_norm": 2.6786694526672363,
      "learning_rate": 0.00019316623761162505,
      "loss": 0.587,
      "step": 7747
    },
    {
      "epoch": 0.03644505489336482,
      "grad_norm": 1.2248798608779907,
      "learning_rate": 0.00019316529463351157,
      "loss": 0.1084,
      "step": 7748
    },
    {
      "epoch": 0.03644975869497728,
      "grad_norm": 1.5430554151535034,
      "learning_rate": 0.0001931643516553981,
      "loss": 0.2338,
      "step": 7749
    },
    {
      "epoch": 0.03645446249658974,
      "grad_norm": 1.9231764078140259,
      "learning_rate": 0.0001931634086772846,
      "loss": 0.4526,
      "step": 7750
    },
    {
      "epoch": 0.03645916629820221,
      "grad_norm": 3.381744146347046,
      "learning_rate": 0.00019316246569917113,
      "loss": 0.4631,
      "step": 7751
    },
    {
      "epoch": 0.03646387009981467,
      "grad_norm": 1.8515145778656006,
      "learning_rate": 0.00019316152272105764,
      "loss": 0.2173,
      "step": 7752
    },
    {
      "epoch": 0.03646857390142713,
      "grad_norm": 0.6216081976890564,
      "learning_rate": 0.00019316057974294416,
      "loss": 0.041,
      "step": 7753
    },
    {
      "epoch": 0.0364732777030396,
      "grad_norm": 1.3573862314224243,
      "learning_rate": 0.00019315963676483068,
      "loss": 0.2334,
      "step": 7754
    },
    {
      "epoch": 0.03647798150465206,
      "grad_norm": 1.0691224336624146,
      "learning_rate": 0.00019315869378671723,
      "loss": 0.0813,
      "step": 7755
    },
    {
      "epoch": 0.03648268530626452,
      "grad_norm": 0.7448350787162781,
      "learning_rate": 0.00019315775080860375,
      "loss": 0.1297,
      "step": 7756
    },
    {
      "epoch": 0.03648738910787699,
      "grad_norm": 1.63340163230896,
      "learning_rate": 0.00019315680783049026,
      "loss": 0.2109,
      "step": 7757
    },
    {
      "epoch": 0.03649209290948945,
      "grad_norm": 0.7582939863204956,
      "learning_rate": 0.00019315586485237678,
      "loss": 0.0739,
      "step": 7758
    },
    {
      "epoch": 0.03649679671110191,
      "grad_norm": 1.5794116258621216,
      "learning_rate": 0.00019315492187426333,
      "loss": 0.2246,
      "step": 7759
    },
    {
      "epoch": 0.03650150051271438,
      "grad_norm": 10.67240047454834,
      "learning_rate": 0.00019315397889614985,
      "loss": 0.3327,
      "step": 7760
    },
    {
      "epoch": 0.036506204314326836,
      "grad_norm": 1.6972929239273071,
      "learning_rate": 0.00019315303591803634,
      "loss": 0.1177,
      "step": 7761
    },
    {
      "epoch": 0.0365109081159393,
      "grad_norm": 1.8498857021331787,
      "learning_rate": 0.00019315209293992286,
      "loss": 0.3694,
      "step": 7762
    },
    {
      "epoch": 0.03651561191755177,
      "grad_norm": 0.30284589529037476,
      "learning_rate": 0.00019315114996180938,
      "loss": 0.0347,
      "step": 7763
    },
    {
      "epoch": 0.036520315719164226,
      "grad_norm": 3.345020055770874,
      "learning_rate": 0.00019315020698369592,
      "loss": 0.2496,
      "step": 7764
    },
    {
      "epoch": 0.03652501952077669,
      "grad_norm": 6.900015830993652,
      "learning_rate": 0.00019314926400558244,
      "loss": 0.2053,
      "step": 7765
    },
    {
      "epoch": 0.03652972332238916,
      "grad_norm": 0.2378566861152649,
      "learning_rate": 0.00019314832102746896,
      "loss": 0.0207,
      "step": 7766
    },
    {
      "epoch": 0.036534427124001616,
      "grad_norm": 1.688232421875,
      "learning_rate": 0.00019314737804935548,
      "loss": 0.2472,
      "step": 7767
    },
    {
      "epoch": 0.03653913092561408,
      "grad_norm": 3.562953472137451,
      "learning_rate": 0.00019314643507124202,
      "loss": 0.1026,
      "step": 7768
    },
    {
      "epoch": 0.03654383472722655,
      "grad_norm": 3.602471351623535,
      "learning_rate": 0.00019314549209312854,
      "loss": 0.3531,
      "step": 7769
    },
    {
      "epoch": 0.036548538528839006,
      "grad_norm": 1.4446525573730469,
      "learning_rate": 0.00019314454911501506,
      "loss": 0.1557,
      "step": 7770
    },
    {
      "epoch": 0.03655324233045147,
      "grad_norm": 0.8054254055023193,
      "learning_rate": 0.00019314360613690158,
      "loss": 0.0957,
      "step": 7771
    },
    {
      "epoch": 0.03655794613206394,
      "grad_norm": 2.735487937927246,
      "learning_rate": 0.00019314266315878807,
      "loss": 0.2096,
      "step": 7772
    },
    {
      "epoch": 0.036562649933676396,
      "grad_norm": 0.3624923825263977,
      "learning_rate": 0.00019314172018067462,
      "loss": 0.0324,
      "step": 7773
    },
    {
      "epoch": 0.03656735373528886,
      "grad_norm": 0.32001179456710815,
      "learning_rate": 0.00019314077720256114,
      "loss": 0.0226,
      "step": 7774
    },
    {
      "epoch": 0.03657205753690133,
      "grad_norm": 3.139319896697998,
      "learning_rate": 0.00019313983422444765,
      "loss": 0.3978,
      "step": 7775
    },
    {
      "epoch": 0.036576761338513786,
      "grad_norm": 2.6553092002868652,
      "learning_rate": 0.00019313889124633417,
      "loss": 0.4532,
      "step": 7776
    },
    {
      "epoch": 0.03658146514012625,
      "grad_norm": 2.363567590713501,
      "learning_rate": 0.00019313794826822072,
      "loss": 0.1059,
      "step": 7777
    },
    {
      "epoch": 0.03658616894173871,
      "grad_norm": 0.9737396836280823,
      "learning_rate": 0.00019313700529010724,
      "loss": 0.0536,
      "step": 7778
    },
    {
      "epoch": 0.036590872743351176,
      "grad_norm": 3.345316171646118,
      "learning_rate": 0.00019313606231199376,
      "loss": 0.1277,
      "step": 7779
    },
    {
      "epoch": 0.03659557654496364,
      "grad_norm": 8.441575050354004,
      "learning_rate": 0.00019313511933388027,
      "loss": 0.6369,
      "step": 7780
    },
    {
      "epoch": 0.0366002803465761,
      "grad_norm": 1.1373988389968872,
      "learning_rate": 0.0001931341763557668,
      "loss": 0.1796,
      "step": 7781
    },
    {
      "epoch": 0.036604984148188566,
      "grad_norm": 2.985720634460449,
      "learning_rate": 0.0001931332333776533,
      "loss": 0.4896,
      "step": 7782
    },
    {
      "epoch": 0.03660968794980103,
      "grad_norm": 5.8109450340271,
      "learning_rate": 0.00019313229039953983,
      "loss": 0.2688,
      "step": 7783
    },
    {
      "epoch": 0.03661439175141349,
      "grad_norm": 4.754164695739746,
      "learning_rate": 0.00019313134742142635,
      "loss": 0.4564,
      "step": 7784
    },
    {
      "epoch": 0.036619095553025956,
      "grad_norm": 0.41017594933509827,
      "learning_rate": 0.00019313040444331287,
      "loss": 0.0296,
      "step": 7785
    },
    {
      "epoch": 0.03662379935463842,
      "grad_norm": 1.810558795928955,
      "learning_rate": 0.00019312946146519941,
      "loss": 0.1459,
      "step": 7786
    },
    {
      "epoch": 0.03662850315625088,
      "grad_norm": 1.4624485969543457,
      "learning_rate": 0.00019312851848708593,
      "loss": 0.036,
      "step": 7787
    },
    {
      "epoch": 0.036633206957863346,
      "grad_norm": 0.43933603167533875,
      "learning_rate": 0.00019312757550897245,
      "loss": 0.0345,
      "step": 7788
    },
    {
      "epoch": 0.03663791075947581,
      "grad_norm": 5.554851531982422,
      "learning_rate": 0.00019312663253085897,
      "loss": 0.2544,
      "step": 7789
    },
    {
      "epoch": 0.03664261456108827,
      "grad_norm": 2.5843095779418945,
      "learning_rate": 0.0001931256895527455,
      "loss": 0.2294,
      "step": 7790
    },
    {
      "epoch": 0.036647318362700736,
      "grad_norm": 2.4992876052856445,
      "learning_rate": 0.00019312474657463203,
      "loss": 0.201,
      "step": 7791
    },
    {
      "epoch": 0.0366520221643132,
      "grad_norm": 3.048964262008667,
      "learning_rate": 0.00019312380359651853,
      "loss": 0.3776,
      "step": 7792
    },
    {
      "epoch": 0.03665672596592566,
      "grad_norm": 1.7987781763076782,
      "learning_rate": 0.00019312286061840504,
      "loss": 0.2433,
      "step": 7793
    },
    {
      "epoch": 0.036661429767538126,
      "grad_norm": 2.240090847015381,
      "learning_rate": 0.00019312191764029156,
      "loss": 0.1639,
      "step": 7794
    },
    {
      "epoch": 0.036666133569150584,
      "grad_norm": 1.2061476707458496,
      "learning_rate": 0.00019312097466217808,
      "loss": 0.13,
      "step": 7795
    },
    {
      "epoch": 0.03667083737076305,
      "grad_norm": 1.1650826930999756,
      "learning_rate": 0.00019312003168406463,
      "loss": 0.061,
      "step": 7796
    },
    {
      "epoch": 0.036675541172375516,
      "grad_norm": 0.6378370523452759,
      "learning_rate": 0.00019311908870595115,
      "loss": 0.0497,
      "step": 7797
    },
    {
      "epoch": 0.036680244973987974,
      "grad_norm": 2.698225498199463,
      "learning_rate": 0.00019311814572783766,
      "loss": 0.3798,
      "step": 7798
    },
    {
      "epoch": 0.03668494877560044,
      "grad_norm": 0.9596341848373413,
      "learning_rate": 0.00019311720274972418,
      "loss": 0.0736,
      "step": 7799
    },
    {
      "epoch": 0.036689652577212906,
      "grad_norm": 3.7749667167663574,
      "learning_rate": 0.00019311625977161073,
      "loss": 0.9212,
      "step": 7800
    },
    {
      "epoch": 0.036694356378825364,
      "grad_norm": 0.5022414326667786,
      "learning_rate": 0.00019311531679349725,
      "loss": 0.0206,
      "step": 7801
    },
    {
      "epoch": 0.03669906018043783,
      "grad_norm": 4.128026008605957,
      "learning_rate": 0.00019311437381538377,
      "loss": 0.1335,
      "step": 7802
    },
    {
      "epoch": 0.036703763982050296,
      "grad_norm": 5.02382755279541,
      "learning_rate": 0.00019311343083727026,
      "loss": 0.6456,
      "step": 7803
    },
    {
      "epoch": 0.036708467783662754,
      "grad_norm": 2.847858190536499,
      "learning_rate": 0.00019311248785915678,
      "loss": 0.1594,
      "step": 7804
    },
    {
      "epoch": 0.03671317158527522,
      "grad_norm": 4.342529773712158,
      "learning_rate": 0.00019311154488104332,
      "loss": 0.44,
      "step": 7805
    },
    {
      "epoch": 0.036717875386887686,
      "grad_norm": 4.086559295654297,
      "learning_rate": 0.00019311060190292984,
      "loss": 0.5982,
      "step": 7806
    },
    {
      "epoch": 0.036722579188500144,
      "grad_norm": 2.578385829925537,
      "learning_rate": 0.00019310965892481636,
      "loss": 0.0952,
      "step": 7807
    },
    {
      "epoch": 0.03672728299011261,
      "grad_norm": 5.554309844970703,
      "learning_rate": 0.00019310871594670288,
      "loss": 0.962,
      "step": 7808
    },
    {
      "epoch": 0.036731986791725076,
      "grad_norm": 2.872716188430786,
      "learning_rate": 0.00019310777296858942,
      "loss": 0.4662,
      "step": 7809
    },
    {
      "epoch": 0.036736690593337534,
      "grad_norm": 2.6913645267486572,
      "learning_rate": 0.00019310682999047594,
      "loss": 0.4408,
      "step": 7810
    },
    {
      "epoch": 0.03674139439495,
      "grad_norm": 0.30564236640930176,
      "learning_rate": 0.00019310588701236246,
      "loss": 0.0204,
      "step": 7811
    },
    {
      "epoch": 0.03674609819656246,
      "grad_norm": 1.2703685760498047,
      "learning_rate": 0.00019310494403424898,
      "loss": 0.1902,
      "step": 7812
    },
    {
      "epoch": 0.036750801998174924,
      "grad_norm": 0.47907572984695435,
      "learning_rate": 0.0001931040010561355,
      "loss": 0.0356,
      "step": 7813
    },
    {
      "epoch": 0.03675550579978739,
      "grad_norm": 0.7186143398284912,
      "learning_rate": 0.00019310305807802202,
      "loss": 0.0827,
      "step": 7814
    },
    {
      "epoch": 0.03676020960139985,
      "grad_norm": 0.5667698383331299,
      "learning_rate": 0.00019310211509990854,
      "loss": 0.0578,
      "step": 7815
    },
    {
      "epoch": 0.036764913403012314,
      "grad_norm": 3.487438201904297,
      "learning_rate": 0.00019310117212179505,
      "loss": 0.2129,
      "step": 7816
    },
    {
      "epoch": 0.03676961720462478,
      "grad_norm": 2.5637683868408203,
      "learning_rate": 0.00019310022914368157,
      "loss": 0.2385,
      "step": 7817
    },
    {
      "epoch": 0.03677432100623724,
      "grad_norm": 2.347989797592163,
      "learning_rate": 0.00019309928616556812,
      "loss": 0.2617,
      "step": 7818
    },
    {
      "epoch": 0.036779024807849704,
      "grad_norm": 2.4996016025543213,
      "learning_rate": 0.00019309834318745464,
      "loss": 0.1809,
      "step": 7819
    },
    {
      "epoch": 0.03678372860946217,
      "grad_norm": 0.38082489371299744,
      "learning_rate": 0.00019309740020934116,
      "loss": 0.0475,
      "step": 7820
    },
    {
      "epoch": 0.03678843241107463,
      "grad_norm": 1.7674925327301025,
      "learning_rate": 0.00019309645723122767,
      "loss": 0.168,
      "step": 7821
    },
    {
      "epoch": 0.036793136212687094,
      "grad_norm": 3.8618545532226562,
      "learning_rate": 0.0001930955142531142,
      "loss": 0.8387,
      "step": 7822
    },
    {
      "epoch": 0.03679784001429956,
      "grad_norm": 6.204516887664795,
      "learning_rate": 0.0001930945712750007,
      "loss": 0.6143,
      "step": 7823
    },
    {
      "epoch": 0.03680254381591202,
      "grad_norm": 3.3601596355438232,
      "learning_rate": 0.00019309362829688723,
      "loss": 0.6505,
      "step": 7824
    },
    {
      "epoch": 0.036807247617524484,
      "grad_norm": 1.1863278150558472,
      "learning_rate": 0.00019309268531877375,
      "loss": 0.1104,
      "step": 7825
    },
    {
      "epoch": 0.03681195141913695,
      "grad_norm": 0.5591681003570557,
      "learning_rate": 0.00019309174234066027,
      "loss": 0.0531,
      "step": 7826
    },
    {
      "epoch": 0.03681665522074941,
      "grad_norm": 2.2012505531311035,
      "learning_rate": 0.00019309079936254681,
      "loss": 0.5547,
      "step": 7827
    },
    {
      "epoch": 0.036821359022361874,
      "grad_norm": 0.5172024965286255,
      "learning_rate": 0.00019308985638443333,
      "loss": 0.0451,
      "step": 7828
    },
    {
      "epoch": 0.03682606282397433,
      "grad_norm": 2.5764060020446777,
      "learning_rate": 0.00019308891340631985,
      "loss": 0.1827,
      "step": 7829
    },
    {
      "epoch": 0.0368307666255868,
      "grad_norm": 1.7697380781173706,
      "learning_rate": 0.00019308797042820637,
      "loss": 0.1546,
      "step": 7830
    },
    {
      "epoch": 0.036835470427199264,
      "grad_norm": 2.2752106189727783,
      "learning_rate": 0.0001930870274500929,
      "loss": 0.1941,
      "step": 7831
    },
    {
      "epoch": 0.03684017422881172,
      "grad_norm": 3.040297746658325,
      "learning_rate": 0.00019308608447197943,
      "loss": 0.7245,
      "step": 7832
    },
    {
      "epoch": 0.03684487803042419,
      "grad_norm": 4.754310607910156,
      "learning_rate": 0.00019308514149386595,
      "loss": 0.5024,
      "step": 7833
    },
    {
      "epoch": 0.036849581832036654,
      "grad_norm": 2.6616482734680176,
      "learning_rate": 0.00019308419851575244,
      "loss": 0.2185,
      "step": 7834
    },
    {
      "epoch": 0.03685428563364911,
      "grad_norm": 0.4230595529079437,
      "learning_rate": 0.00019308325553763896,
      "loss": 0.0372,
      "step": 7835
    },
    {
      "epoch": 0.03685898943526158,
      "grad_norm": 0.6950669884681702,
      "learning_rate": 0.0001930823125595255,
      "loss": 0.0848,
      "step": 7836
    },
    {
      "epoch": 0.036863693236874044,
      "grad_norm": 2.677591562271118,
      "learning_rate": 0.00019308136958141203,
      "loss": 0.2431,
      "step": 7837
    },
    {
      "epoch": 0.0368683970384865,
      "grad_norm": 1.4576609134674072,
      "learning_rate": 0.00019308042660329855,
      "loss": 0.1152,
      "step": 7838
    },
    {
      "epoch": 0.03687310084009897,
      "grad_norm": 1.0848898887634277,
      "learning_rate": 0.00019307948362518506,
      "loss": 0.1201,
      "step": 7839
    },
    {
      "epoch": 0.036877804641711434,
      "grad_norm": 3.105311870574951,
      "learning_rate": 0.00019307854064707158,
      "loss": 0.4101,
      "step": 7840
    },
    {
      "epoch": 0.03688250844332389,
      "grad_norm": 1.5385233163833618,
      "learning_rate": 0.00019307759766895813,
      "loss": 0.1644,
      "step": 7841
    },
    {
      "epoch": 0.03688721224493636,
      "grad_norm": 0.27443310618400574,
      "learning_rate": 0.00019307665469084465,
      "loss": 0.0215,
      "step": 7842
    },
    {
      "epoch": 0.036891916046548824,
      "grad_norm": 4.403045654296875,
      "learning_rate": 0.00019307571171273117,
      "loss": 0.8053,
      "step": 7843
    },
    {
      "epoch": 0.03689661984816128,
      "grad_norm": 5.63460636138916,
      "learning_rate": 0.00019307476873461768,
      "loss": 0.4106,
      "step": 7844
    },
    {
      "epoch": 0.03690132364977375,
      "grad_norm": 1.3669072389602661,
      "learning_rate": 0.00019307382575650418,
      "loss": 0.128,
      "step": 7845
    },
    {
      "epoch": 0.03690602745138621,
      "grad_norm": 0.9041066765785217,
      "learning_rate": 0.00019307288277839072,
      "loss": 0.0811,
      "step": 7846
    },
    {
      "epoch": 0.03691073125299867,
      "grad_norm": 1.883234977722168,
      "learning_rate": 0.00019307193980027724,
      "loss": 0.3605,
      "step": 7847
    },
    {
      "epoch": 0.03691543505461114,
      "grad_norm": 0.16057033836841583,
      "learning_rate": 0.00019307099682216376,
      "loss": 0.0091,
      "step": 7848
    },
    {
      "epoch": 0.0369201388562236,
      "grad_norm": 0.5934469103813171,
      "learning_rate": 0.00019307005384405028,
      "loss": 0.0748,
      "step": 7849
    },
    {
      "epoch": 0.03692484265783606,
      "grad_norm": 8.741698265075684,
      "learning_rate": 0.00019306911086593682,
      "loss": 0.414,
      "step": 7850
    },
    {
      "epoch": 0.03692954645944853,
      "grad_norm": 1.7214125394821167,
      "learning_rate": 0.00019306816788782334,
      "loss": 0.1375,
      "step": 7851
    },
    {
      "epoch": 0.03693425026106099,
      "grad_norm": 0.13861200213432312,
      "learning_rate": 0.00019306722490970986,
      "loss": 0.0078,
      "step": 7852
    },
    {
      "epoch": 0.03693895406267345,
      "grad_norm": 2.2511792182922363,
      "learning_rate": 0.00019306628193159638,
      "loss": 0.1518,
      "step": 7853
    },
    {
      "epoch": 0.03694365786428592,
      "grad_norm": 0.14182424545288086,
      "learning_rate": 0.0001930653389534829,
      "loss": 0.0037,
      "step": 7854
    },
    {
      "epoch": 0.03694836166589838,
      "grad_norm": 4.396939754486084,
      "learning_rate": 0.00019306439597536942,
      "loss": 0.7113,
      "step": 7855
    },
    {
      "epoch": 0.03695306546751084,
      "grad_norm": 2.814868688583374,
      "learning_rate": 0.00019306345299725594,
      "loss": 0.3791,
      "step": 7856
    },
    {
      "epoch": 0.03695776926912331,
      "grad_norm": 7.77584171295166,
      "learning_rate": 0.00019306251001914245,
      "loss": 0.3167,
      "step": 7857
    },
    {
      "epoch": 0.03696247307073577,
      "grad_norm": 0.7464607954025269,
      "learning_rate": 0.00019306156704102897,
      "loss": 0.0385,
      "step": 7858
    },
    {
      "epoch": 0.03696717687234823,
      "grad_norm": 2.7918853759765625,
      "learning_rate": 0.00019306062406291552,
      "loss": 0.2081,
      "step": 7859
    },
    {
      "epoch": 0.0369718806739607,
      "grad_norm": 2.9305427074432373,
      "learning_rate": 0.00019305968108480204,
      "loss": 0.3547,
      "step": 7860
    },
    {
      "epoch": 0.03697658447557316,
      "grad_norm": 1.7386831045150757,
      "learning_rate": 0.00019305873810668856,
      "loss": 0.1808,
      "step": 7861
    },
    {
      "epoch": 0.03698128827718562,
      "grad_norm": 1.3404462337493896,
      "learning_rate": 0.00019305779512857507,
      "loss": 0.1006,
      "step": 7862
    },
    {
      "epoch": 0.03698599207879808,
      "grad_norm": 1.9362900257110596,
      "learning_rate": 0.0001930568521504616,
      "loss": 0.1817,
      "step": 7863
    },
    {
      "epoch": 0.03699069588041055,
      "grad_norm": 1.3637274503707886,
      "learning_rate": 0.00019305590917234814,
      "loss": 0.1228,
      "step": 7864
    },
    {
      "epoch": 0.03699539968202301,
      "grad_norm": 2.7772042751312256,
      "learning_rate": 0.00019305496619423463,
      "loss": 0.4337,
      "step": 7865
    },
    {
      "epoch": 0.03700010348363547,
      "grad_norm": 2.3043386936187744,
      "learning_rate": 0.00019305402321612115,
      "loss": 0.1488,
      "step": 7866
    },
    {
      "epoch": 0.03700480728524794,
      "grad_norm": 0.5029330849647522,
      "learning_rate": 0.00019305308023800767,
      "loss": 0.0527,
      "step": 7867
    },
    {
      "epoch": 0.0370095110868604,
      "grad_norm": 2.351219892501831,
      "learning_rate": 0.0001930521372598942,
      "loss": 0.1941,
      "step": 7868
    },
    {
      "epoch": 0.03701421488847286,
      "grad_norm": 1.565274953842163,
      "learning_rate": 0.00019305119428178073,
      "loss": 0.1275,
      "step": 7869
    },
    {
      "epoch": 0.03701891869008533,
      "grad_norm": 0.5941198468208313,
      "learning_rate": 0.00019305025130366725,
      "loss": 0.0438,
      "step": 7870
    },
    {
      "epoch": 0.03702362249169779,
      "grad_norm": 2.0959110260009766,
      "learning_rate": 0.00019304930832555377,
      "loss": 0.4865,
      "step": 7871
    },
    {
      "epoch": 0.03702832629331025,
      "grad_norm": 0.09520473331212997,
      "learning_rate": 0.0001930483653474403,
      "loss": 0.0069,
      "step": 7872
    },
    {
      "epoch": 0.03703303009492272,
      "grad_norm": 0.7486191391944885,
      "learning_rate": 0.00019304742236932683,
      "loss": 0.0486,
      "step": 7873
    },
    {
      "epoch": 0.03703773389653518,
      "grad_norm": 1.8187180757522583,
      "learning_rate": 0.00019304647939121335,
      "loss": 0.1396,
      "step": 7874
    },
    {
      "epoch": 0.03704243769814764,
      "grad_norm": 3.426711082458496,
      "learning_rate": 0.00019304553641309987,
      "loss": 0.2959,
      "step": 7875
    },
    {
      "epoch": 0.03704714149976011,
      "grad_norm": 0.5463305115699768,
      "learning_rate": 0.00019304459343498636,
      "loss": 0.0304,
      "step": 7876
    },
    {
      "epoch": 0.03705184530137257,
      "grad_norm": 1.4075751304626465,
      "learning_rate": 0.0001930436504568729,
      "loss": 0.0998,
      "step": 7877
    },
    {
      "epoch": 0.03705654910298503,
      "grad_norm": 4.091567516326904,
      "learning_rate": 0.00019304270747875943,
      "loss": 0.7479,
      "step": 7878
    },
    {
      "epoch": 0.0370612529045975,
      "grad_norm": 1.3223146200180054,
      "learning_rate": 0.00019304176450064595,
      "loss": 0.1102,
      "step": 7879
    },
    {
      "epoch": 0.037065956706209956,
      "grad_norm": 0.5677502751350403,
      "learning_rate": 0.00019304082152253246,
      "loss": 0.0265,
      "step": 7880
    },
    {
      "epoch": 0.03707066050782242,
      "grad_norm": 2.127915382385254,
      "learning_rate": 0.00019303987854441898,
      "loss": 0.3376,
      "step": 7881
    },
    {
      "epoch": 0.03707536430943489,
      "grad_norm": 2.35074782371521,
      "learning_rate": 0.00019303893556630553,
      "loss": 0.4045,
      "step": 7882
    },
    {
      "epoch": 0.037080068111047346,
      "grad_norm": 7.085258483886719,
      "learning_rate": 0.00019303799258819205,
      "loss": 1.2487,
      "step": 7883
    },
    {
      "epoch": 0.03708477191265981,
      "grad_norm": 3.130047082901001,
      "learning_rate": 0.00019303704961007857,
      "loss": 0.6236,
      "step": 7884
    },
    {
      "epoch": 0.03708947571427228,
      "grad_norm": 0.48853006958961487,
      "learning_rate": 0.00019303610663196508,
      "loss": 0.0624,
      "step": 7885
    },
    {
      "epoch": 0.037094179515884736,
      "grad_norm": 3.298179864883423,
      "learning_rate": 0.0001930351636538516,
      "loss": 0.6405,
      "step": 7886
    },
    {
      "epoch": 0.0370988833174972,
      "grad_norm": 2.136836051940918,
      "learning_rate": 0.00019303422067573812,
      "loss": 0.1156,
      "step": 7887
    },
    {
      "epoch": 0.03710358711910967,
      "grad_norm": 2.7492387294769287,
      "learning_rate": 0.00019303327769762464,
      "loss": 0.5428,
      "step": 7888
    },
    {
      "epoch": 0.037108290920722126,
      "grad_norm": 0.14730575680732727,
      "learning_rate": 0.00019303233471951116,
      "loss": 0.0098,
      "step": 7889
    },
    {
      "epoch": 0.03711299472233459,
      "grad_norm": 2.8630411624908447,
      "learning_rate": 0.00019303139174139768,
      "loss": 0.2375,
      "step": 7890
    },
    {
      "epoch": 0.03711769852394706,
      "grad_norm": 1.1502714157104492,
      "learning_rate": 0.00019303044876328422,
      "loss": 0.0986,
      "step": 7891
    },
    {
      "epoch": 0.037122402325559516,
      "grad_norm": 0.5734151005744934,
      "learning_rate": 0.00019302950578517074,
      "loss": 0.0374,
      "step": 7892
    },
    {
      "epoch": 0.03712710612717198,
      "grad_norm": 0.8290596008300781,
      "learning_rate": 0.00019302856280705726,
      "loss": 0.0698,
      "step": 7893
    },
    {
      "epoch": 0.03713180992878445,
      "grad_norm": 0.2265978306531906,
      "learning_rate": 0.00019302761982894378,
      "loss": 0.0154,
      "step": 7894
    },
    {
      "epoch": 0.037136513730396906,
      "grad_norm": 2.104846477508545,
      "learning_rate": 0.00019302667685083033,
      "loss": 0.3909,
      "step": 7895
    },
    {
      "epoch": 0.03714121753200937,
      "grad_norm": 1.3388465642929077,
      "learning_rate": 0.00019302573387271682,
      "loss": 0.3518,
      "step": 7896
    },
    {
      "epoch": 0.03714592133362183,
      "grad_norm": 2.4269967079162598,
      "learning_rate": 0.00019302479089460334,
      "loss": 0.4631,
      "step": 7897
    },
    {
      "epoch": 0.037150625135234296,
      "grad_norm": 4.162415981292725,
      "learning_rate": 0.00019302384791648985,
      "loss": 0.6235,
      "step": 7898
    },
    {
      "epoch": 0.03715532893684676,
      "grad_norm": 6.276314735412598,
      "learning_rate": 0.00019302290493837637,
      "loss": 0.6494,
      "step": 7899
    },
    {
      "epoch": 0.03716003273845922,
      "grad_norm": 1.4565799236297607,
      "learning_rate": 0.00019302196196026292,
      "loss": 0.2232,
      "step": 7900
    },
    {
      "epoch": 0.037164736540071686,
      "grad_norm": 3.1555733680725098,
      "learning_rate": 0.00019302101898214944,
      "loss": 0.1384,
      "step": 7901
    },
    {
      "epoch": 0.03716944034168415,
      "grad_norm": 0.9861579537391663,
      "learning_rate": 0.00019302007600403596,
      "loss": 0.0881,
      "step": 7902
    },
    {
      "epoch": 0.03717414414329661,
      "grad_norm": 4.421395778656006,
      "learning_rate": 0.00019301913302592247,
      "loss": 0.9267,
      "step": 7903
    },
    {
      "epoch": 0.037178847944909076,
      "grad_norm": 2.1840922832489014,
      "learning_rate": 0.000193018190047809,
      "loss": 0.2479,
      "step": 7904
    },
    {
      "epoch": 0.03718355174652154,
      "grad_norm": 2.310530662536621,
      "learning_rate": 0.00019301724706969554,
      "loss": 0.2342,
      "step": 7905
    },
    {
      "epoch": 0.037188255548134,
      "grad_norm": 2.938373327255249,
      "learning_rate": 0.00019301630409158206,
      "loss": 0.308,
      "step": 7906
    },
    {
      "epoch": 0.037192959349746466,
      "grad_norm": 1.5290518999099731,
      "learning_rate": 0.00019301536111346855,
      "loss": 0.1869,
      "step": 7907
    },
    {
      "epoch": 0.03719766315135893,
      "grad_norm": 0.7367951273918152,
      "learning_rate": 0.00019301441813535507,
      "loss": 0.0774,
      "step": 7908
    },
    {
      "epoch": 0.03720236695297139,
      "grad_norm": 1.108352541923523,
      "learning_rate": 0.0001930134751572416,
      "loss": 0.1522,
      "step": 7909
    },
    {
      "epoch": 0.037207070754583856,
      "grad_norm": 2.4608685970306396,
      "learning_rate": 0.00019301253217912813,
      "loss": 0.4427,
      "step": 7910
    },
    {
      "epoch": 0.03721177455619632,
      "grad_norm": 1.1764295101165771,
      "learning_rate": 0.00019301158920101465,
      "loss": 0.2242,
      "step": 7911
    },
    {
      "epoch": 0.03721647835780878,
      "grad_norm": 3.6528055667877197,
      "learning_rate": 0.00019301064622290117,
      "loss": 1.075,
      "step": 7912
    },
    {
      "epoch": 0.037221182159421246,
      "grad_norm": 2.409654140472412,
      "learning_rate": 0.0001930097032447877,
      "loss": 0.3407,
      "step": 7913
    },
    {
      "epoch": 0.037225885961033704,
      "grad_norm": 3.130274772644043,
      "learning_rate": 0.00019300876026667423,
      "loss": 0.4931,
      "step": 7914
    },
    {
      "epoch": 0.03723058976264617,
      "grad_norm": 3.3037452697753906,
      "learning_rate": 0.00019300781728856075,
      "loss": 0.3396,
      "step": 7915
    },
    {
      "epoch": 0.037235293564258636,
      "grad_norm": 3.3998026847839355,
      "learning_rate": 0.00019300687431044727,
      "loss": 0.4069,
      "step": 7916
    },
    {
      "epoch": 0.037239997365871094,
      "grad_norm": 2.886702299118042,
      "learning_rate": 0.0001930059313323338,
      "loss": 0.1953,
      "step": 7917
    },
    {
      "epoch": 0.03724470116748356,
      "grad_norm": 1.8410192728042603,
      "learning_rate": 0.0001930049883542203,
      "loss": 0.2672,
      "step": 7918
    },
    {
      "epoch": 0.037249404969096025,
      "grad_norm": 2.7808780670166016,
      "learning_rate": 0.00019300404537610683,
      "loss": 0.4647,
      "step": 7919
    },
    {
      "epoch": 0.037254108770708484,
      "grad_norm": 3.109656810760498,
      "learning_rate": 0.00019300310239799335,
      "loss": 0.9825,
      "step": 7920
    },
    {
      "epoch": 0.03725881257232095,
      "grad_norm": 3.2822539806365967,
      "learning_rate": 0.00019300215941987986,
      "loss": 0.6497,
      "step": 7921
    },
    {
      "epoch": 0.037263516373933415,
      "grad_norm": 2.7385199069976807,
      "learning_rate": 0.00019300121644176638,
      "loss": 0.5515,
      "step": 7922
    },
    {
      "epoch": 0.037268220175545874,
      "grad_norm": 2.6952762603759766,
      "learning_rate": 0.00019300027346365293,
      "loss": 0.4438,
      "step": 7923
    },
    {
      "epoch": 0.03727292397715834,
      "grad_norm": 2.5138237476348877,
      "learning_rate": 0.00019299933048553945,
      "loss": 0.2728,
      "step": 7924
    },
    {
      "epoch": 0.037277627778770805,
      "grad_norm": 1.0097298622131348,
      "learning_rate": 0.00019299838750742597,
      "loss": 0.1672,
      "step": 7925
    },
    {
      "epoch": 0.037282331580383264,
      "grad_norm": 3.451559543609619,
      "learning_rate": 0.00019299744452931248,
      "loss": 0.5895,
      "step": 7926
    },
    {
      "epoch": 0.03728703538199573,
      "grad_norm": 1.1471145153045654,
      "learning_rate": 0.000192996501551199,
      "loss": 0.2176,
      "step": 7927
    },
    {
      "epoch": 0.037291739183608195,
      "grad_norm": 1.0160470008850098,
      "learning_rate": 0.00019299555857308552,
      "loss": 0.1933,
      "step": 7928
    },
    {
      "epoch": 0.037296442985220654,
      "grad_norm": 1.1454461812973022,
      "learning_rate": 0.00019299461559497204,
      "loss": 0.1998,
      "step": 7929
    },
    {
      "epoch": 0.03730114678683312,
      "grad_norm": 1.1242884397506714,
      "learning_rate": 0.00019299367261685856,
      "loss": 0.137,
      "step": 7930
    },
    {
      "epoch": 0.03730585058844558,
      "grad_norm": 0.957055389881134,
      "learning_rate": 0.00019299272963874508,
      "loss": 0.1568,
      "step": 7931
    },
    {
      "epoch": 0.037310554390058044,
      "grad_norm": 1.7184914350509644,
      "learning_rate": 0.00019299178666063162,
      "loss": 0.3864,
      "step": 7932
    },
    {
      "epoch": 0.03731525819167051,
      "grad_norm": 2.5194098949432373,
      "learning_rate": 0.00019299084368251814,
      "loss": 0.4422,
      "step": 7933
    },
    {
      "epoch": 0.03731996199328297,
      "grad_norm": 2.152614116668701,
      "learning_rate": 0.00019298990070440466,
      "loss": 0.2751,
      "step": 7934
    },
    {
      "epoch": 0.037324665794895434,
      "grad_norm": 2.480008602142334,
      "learning_rate": 0.00019298895772629118,
      "loss": 0.3282,
      "step": 7935
    },
    {
      "epoch": 0.0373293695965079,
      "grad_norm": 0.4850766360759735,
      "learning_rate": 0.00019298801474817773,
      "loss": 0.0394,
      "step": 7936
    },
    {
      "epoch": 0.03733407339812036,
      "grad_norm": 0.8491914868354797,
      "learning_rate": 0.00019298707177006424,
      "loss": 0.1791,
      "step": 7937
    },
    {
      "epoch": 0.037338777199732824,
      "grad_norm": 1.8241280317306519,
      "learning_rate": 0.00019298612879195074,
      "loss": 0.1357,
      "step": 7938
    },
    {
      "epoch": 0.03734348100134529,
      "grad_norm": 0.3550434708595276,
      "learning_rate": 0.00019298518581383725,
      "loss": 0.0317,
      "step": 7939
    },
    {
      "epoch": 0.03734818480295775,
      "grad_norm": 0.7677097916603088,
      "learning_rate": 0.00019298424283572377,
      "loss": 0.1041,
      "step": 7940
    },
    {
      "epoch": 0.037352888604570214,
      "grad_norm": 4.9969801902771,
      "learning_rate": 0.00019298329985761032,
      "loss": 0.3795,
      "step": 7941
    },
    {
      "epoch": 0.03735759240618268,
      "grad_norm": 0.24610213935375214,
      "learning_rate": 0.00019298235687949684,
      "loss": 0.0191,
      "step": 7942
    },
    {
      "epoch": 0.03736229620779514,
      "grad_norm": 1.8223460912704468,
      "learning_rate": 0.00019298141390138336,
      "loss": 0.3169,
      "step": 7943
    },
    {
      "epoch": 0.037367000009407604,
      "grad_norm": 2.952793598175049,
      "learning_rate": 0.00019298047092326987,
      "loss": 0.5934,
      "step": 7944
    },
    {
      "epoch": 0.03737170381102007,
      "grad_norm": 3.1487739086151123,
      "learning_rate": 0.00019297952794515642,
      "loss": 0.3196,
      "step": 7945
    },
    {
      "epoch": 0.03737640761263253,
      "grad_norm": 0.6138777732849121,
      "learning_rate": 0.00019297858496704294,
      "loss": 0.0484,
      "step": 7946
    },
    {
      "epoch": 0.037381111414244994,
      "grad_norm": 0.3286796510219574,
      "learning_rate": 0.00019297764198892946,
      "loss": 0.0316,
      "step": 7947
    },
    {
      "epoch": 0.03738581521585745,
      "grad_norm": 1.3146553039550781,
      "learning_rate": 0.00019297669901081598,
      "loss": 0.2598,
      "step": 7948
    },
    {
      "epoch": 0.03739051901746992,
      "grad_norm": 1.3363817930221558,
      "learning_rate": 0.0001929757560327025,
      "loss": 0.1403,
      "step": 7949
    },
    {
      "epoch": 0.037395222819082384,
      "grad_norm": 3.0029194355010986,
      "learning_rate": 0.000192974813054589,
      "loss": 0.6073,
      "step": 7950
    },
    {
      "epoch": 0.03739992662069484,
      "grad_norm": 3.886185884475708,
      "learning_rate": 0.00019297387007647553,
      "loss": 0.3336,
      "step": 7951
    },
    {
      "epoch": 0.03740463042230731,
      "grad_norm": 3.9532339572906494,
      "learning_rate": 0.00019297292709836205,
      "loss": 0.4521,
      "step": 7952
    },
    {
      "epoch": 0.037409334223919774,
      "grad_norm": 2.380284547805786,
      "learning_rate": 0.00019297198412024857,
      "loss": 0.1895,
      "step": 7953
    },
    {
      "epoch": 0.03741403802553223,
      "grad_norm": 2.343229055404663,
      "learning_rate": 0.0001929710411421351,
      "loss": 0.3657,
      "step": 7954
    },
    {
      "epoch": 0.0374187418271447,
      "grad_norm": 3.617419481277466,
      "learning_rate": 0.00019297009816402163,
      "loss": 0.4371,
      "step": 7955
    },
    {
      "epoch": 0.037423445628757164,
      "grad_norm": 3.397735357284546,
      "learning_rate": 0.00019296915518590815,
      "loss": 0.3172,
      "step": 7956
    },
    {
      "epoch": 0.03742814943036962,
      "grad_norm": 2.116248846054077,
      "learning_rate": 0.00019296821220779467,
      "loss": 0.2655,
      "step": 7957
    },
    {
      "epoch": 0.03743285323198209,
      "grad_norm": 1.3981784582138062,
      "learning_rate": 0.0001929672692296812,
      "loss": 0.114,
      "step": 7958
    },
    {
      "epoch": 0.037437557033594554,
      "grad_norm": 1.7108078002929688,
      "learning_rate": 0.0001929663262515677,
      "loss": 0.1666,
      "step": 7959
    },
    {
      "epoch": 0.03744226083520701,
      "grad_norm": 3.278933525085449,
      "learning_rate": 0.00019296538327345423,
      "loss": 0.5426,
      "step": 7960
    },
    {
      "epoch": 0.03744696463681948,
      "grad_norm": 6.1155548095703125,
      "learning_rate": 0.00019296444029534075,
      "loss": 1.097,
      "step": 7961
    },
    {
      "epoch": 0.037451668438431944,
      "grad_norm": 2.392186403274536,
      "learning_rate": 0.00019296349731722726,
      "loss": 0.2571,
      "step": 7962
    },
    {
      "epoch": 0.0374563722400444,
      "grad_norm": 3.476336717605591,
      "learning_rate": 0.00019296255433911378,
      "loss": 0.1617,
      "step": 7963
    },
    {
      "epoch": 0.03746107604165687,
      "grad_norm": 2.1643571853637695,
      "learning_rate": 0.00019296161136100033,
      "loss": 0.2181,
      "step": 7964
    },
    {
      "epoch": 0.03746577984326933,
      "grad_norm": 1.3653756380081177,
      "learning_rate": 0.00019296066838288685,
      "loss": 0.1217,
      "step": 7965
    },
    {
      "epoch": 0.03747048364488179,
      "grad_norm": 1.529895544052124,
      "learning_rate": 0.00019295972540477337,
      "loss": 0.222,
      "step": 7966
    },
    {
      "epoch": 0.03747518744649426,
      "grad_norm": 1.8659322261810303,
      "learning_rate": 0.00019295878242665988,
      "loss": 0.2366,
      "step": 7967
    },
    {
      "epoch": 0.03747989124810672,
      "grad_norm": 4.953297138214111,
      "learning_rate": 0.00019295783944854643,
      "loss": 1.0495,
      "step": 7968
    },
    {
      "epoch": 0.03748459504971918,
      "grad_norm": 1.4258099794387817,
      "learning_rate": 0.00019295689647043292,
      "loss": 0.1446,
      "step": 7969
    },
    {
      "epoch": 0.03748929885133165,
      "grad_norm": 2.591294050216675,
      "learning_rate": 0.00019295595349231944,
      "loss": 0.5735,
      "step": 7970
    },
    {
      "epoch": 0.03749400265294411,
      "grad_norm": 0.3659820258617401,
      "learning_rate": 0.00019295501051420596,
      "loss": 0.0296,
      "step": 7971
    },
    {
      "epoch": 0.03749870645455657,
      "grad_norm": 1.898470163345337,
      "learning_rate": 0.00019295406753609248,
      "loss": 0.3924,
      "step": 7972
    },
    {
      "epoch": 0.03750341025616904,
      "grad_norm": 2.1099414825439453,
      "learning_rate": 0.00019295312455797902,
      "loss": 0.3353,
      "step": 7973
    },
    {
      "epoch": 0.0375081140577815,
      "grad_norm": 3.4958393573760986,
      "learning_rate": 0.00019295218157986554,
      "loss": 0.8278,
      "step": 7974
    },
    {
      "epoch": 0.03751281785939396,
      "grad_norm": 2.9053473472595215,
      "learning_rate": 0.00019295123860175206,
      "loss": 0.4873,
      "step": 7975
    },
    {
      "epoch": 0.03751752166100643,
      "grad_norm": 2.679421901702881,
      "learning_rate": 0.00019295029562363858,
      "loss": 0.3729,
      "step": 7976
    },
    {
      "epoch": 0.03752222546261889,
      "grad_norm": 0.8373465538024902,
      "learning_rate": 0.00019294935264552513,
      "loss": 0.1068,
      "step": 7977
    },
    {
      "epoch": 0.03752692926423135,
      "grad_norm": 1.4179086685180664,
      "learning_rate": 0.00019294840966741164,
      "loss": 0.2449,
      "step": 7978
    },
    {
      "epoch": 0.03753163306584382,
      "grad_norm": 1.765259861946106,
      "learning_rate": 0.00019294746668929816,
      "loss": 0.3158,
      "step": 7979
    },
    {
      "epoch": 0.03753633686745628,
      "grad_norm": 1.1436610221862793,
      "learning_rate": 0.00019294652371118468,
      "loss": 0.2198,
      "step": 7980
    },
    {
      "epoch": 0.03754104066906874,
      "grad_norm": 3.0891826152801514,
      "learning_rate": 0.00019294558073307117,
      "loss": 0.7327,
      "step": 7981
    },
    {
      "epoch": 0.0375457444706812,
      "grad_norm": 0.8940576314926147,
      "learning_rate": 0.00019294463775495772,
      "loss": 0.0972,
      "step": 7982
    },
    {
      "epoch": 0.03755044827229367,
      "grad_norm": 0.983468770980835,
      "learning_rate": 0.00019294369477684424,
      "loss": 0.1516,
      "step": 7983
    },
    {
      "epoch": 0.03755515207390613,
      "grad_norm": 1.1221808195114136,
      "learning_rate": 0.00019294275179873076,
      "loss": 0.2211,
      "step": 7984
    },
    {
      "epoch": 0.03755985587551859,
      "grad_norm": 0.4353387653827667,
      "learning_rate": 0.00019294180882061727,
      "loss": 0.0474,
      "step": 7985
    },
    {
      "epoch": 0.03756455967713106,
      "grad_norm": 2.4541897773742676,
      "learning_rate": 0.00019294086584250382,
      "loss": 0.54,
      "step": 7986
    },
    {
      "epoch": 0.03756926347874352,
      "grad_norm": 2.011622667312622,
      "learning_rate": 0.00019293992286439034,
      "loss": 0.5266,
      "step": 7987
    },
    {
      "epoch": 0.03757396728035598,
      "grad_norm": 1.6302920579910278,
      "learning_rate": 0.00019293897988627686,
      "loss": 0.2318,
      "step": 7988
    },
    {
      "epoch": 0.03757867108196845,
      "grad_norm": 1.039183497428894,
      "learning_rate": 0.00019293803690816338,
      "loss": 0.0962,
      "step": 7989
    },
    {
      "epoch": 0.03758337488358091,
      "grad_norm": 0.6944321393966675,
      "learning_rate": 0.0001929370939300499,
      "loss": 0.0497,
      "step": 7990
    },
    {
      "epoch": 0.03758807868519337,
      "grad_norm": 1.462079405784607,
      "learning_rate": 0.0001929361509519364,
      "loss": 0.1208,
      "step": 7991
    },
    {
      "epoch": 0.03759278248680584,
      "grad_norm": 0.45275968313217163,
      "learning_rate": 0.00019293520797382293,
      "loss": 0.0247,
      "step": 7992
    },
    {
      "epoch": 0.0375974862884183,
      "grad_norm": 4.881349563598633,
      "learning_rate": 0.00019293426499570945,
      "loss": 1.4226,
      "step": 7993
    },
    {
      "epoch": 0.03760219009003076,
      "grad_norm": 0.6710740327835083,
      "learning_rate": 0.00019293332201759597,
      "loss": 0.0775,
      "step": 7994
    },
    {
      "epoch": 0.03760689389164323,
      "grad_norm": 2.050264835357666,
      "learning_rate": 0.00019293237903948252,
      "loss": 0.234,
      "step": 7995
    },
    {
      "epoch": 0.03761159769325569,
      "grad_norm": 1.8232113122940063,
      "learning_rate": 0.00019293143606136903,
      "loss": 0.2307,
      "step": 7996
    },
    {
      "epoch": 0.03761630149486815,
      "grad_norm": 1.5855305194854736,
      "learning_rate": 0.00019293049308325555,
      "loss": 0.1665,
      "step": 7997
    },
    {
      "epoch": 0.03762100529648062,
      "grad_norm": 1.245339274406433,
      "learning_rate": 0.00019292955010514207,
      "loss": 0.2307,
      "step": 7998
    },
    {
      "epoch": 0.037625709098093076,
      "grad_norm": 0.35255518555641174,
      "learning_rate": 0.0001929286071270286,
      "loss": 0.0483,
      "step": 7999
    },
    {
      "epoch": 0.03763041289970554,
      "grad_norm": 3.242351531982422,
      "learning_rate": 0.0001929276641489151,
      "loss": 0.8322,
      "step": 8000
    },
    {
      "epoch": 0.03763511670131801,
      "grad_norm": 1.7215389013290405,
      "learning_rate": 0.00019292672117080163,
      "loss": 0.3505,
      "step": 8001
    },
    {
      "epoch": 0.037639820502930466,
      "grad_norm": 1.3889356851577759,
      "learning_rate": 0.00019292577819268815,
      "loss": 0.1313,
      "step": 8002
    },
    {
      "epoch": 0.03764452430454293,
      "grad_norm": 0.6634209156036377,
      "learning_rate": 0.00019292483521457466,
      "loss": 0.0618,
      "step": 8003
    },
    {
      "epoch": 0.0376492281061554,
      "grad_norm": 2.7417376041412354,
      "learning_rate": 0.00019292389223646118,
      "loss": 0.2604,
      "step": 8004
    },
    {
      "epoch": 0.037653931907767856,
      "grad_norm": 1.6861990690231323,
      "learning_rate": 0.00019292294925834773,
      "loss": 0.3074,
      "step": 8005
    },
    {
      "epoch": 0.03765863570938032,
      "grad_norm": 0.5983887910842896,
      "learning_rate": 0.00019292200628023425,
      "loss": 0.0953,
      "step": 8006
    },
    {
      "epoch": 0.03766333951099279,
      "grad_norm": 0.852727472782135,
      "learning_rate": 0.00019292106330212077,
      "loss": 0.0862,
      "step": 8007
    },
    {
      "epoch": 0.037668043312605246,
      "grad_norm": 8.330482482910156,
      "learning_rate": 0.00019292012032400728,
      "loss": 0.6713,
      "step": 8008
    },
    {
      "epoch": 0.03767274711421771,
      "grad_norm": 1.013413429260254,
      "learning_rate": 0.00019291917734589383,
      "loss": 0.1297,
      "step": 8009
    },
    {
      "epoch": 0.03767745091583018,
      "grad_norm": 2.5245158672332764,
      "learning_rate": 0.00019291823436778035,
      "loss": 0.3023,
      "step": 8010
    },
    {
      "epoch": 0.037682154717442636,
      "grad_norm": 5.747779369354248,
      "learning_rate": 0.00019291729138966687,
      "loss": 1.0131,
      "step": 8011
    },
    {
      "epoch": 0.0376868585190551,
      "grad_norm": 0.2568918466567993,
      "learning_rate": 0.00019291634841155336,
      "loss": 0.0222,
      "step": 8012
    },
    {
      "epoch": 0.03769156232066757,
      "grad_norm": 0.5892771482467651,
      "learning_rate": 0.00019291540543343988,
      "loss": 0.0613,
      "step": 8013
    },
    {
      "epoch": 0.037696266122280025,
      "grad_norm": 2.6926839351654053,
      "learning_rate": 0.00019291446245532642,
      "loss": 0.4458,
      "step": 8014
    },
    {
      "epoch": 0.03770096992389249,
      "grad_norm": 1.0922917127609253,
      "learning_rate": 0.00019291351947721294,
      "loss": 0.1878,
      "step": 8015
    },
    {
      "epoch": 0.03770567372550495,
      "grad_norm": 1.322798252105713,
      "learning_rate": 0.00019291257649909946,
      "loss": 0.0816,
      "step": 8016
    },
    {
      "epoch": 0.037710377527117415,
      "grad_norm": 1.905612587928772,
      "learning_rate": 0.00019291163352098598,
      "loss": 0.2272,
      "step": 8017
    },
    {
      "epoch": 0.03771508132872988,
      "grad_norm": 3.2613654136657715,
      "learning_rate": 0.00019291069054287253,
      "loss": 0.4974,
      "step": 8018
    },
    {
      "epoch": 0.03771978513034234,
      "grad_norm": 3.8626701831817627,
      "learning_rate": 0.00019290974756475904,
      "loss": 1.0144,
      "step": 8019
    },
    {
      "epoch": 0.037724488931954805,
      "grad_norm": 2.2853713035583496,
      "learning_rate": 0.00019290880458664556,
      "loss": 0.2445,
      "step": 8020
    },
    {
      "epoch": 0.03772919273356727,
      "grad_norm": 2.6638600826263428,
      "learning_rate": 0.00019290786160853208,
      "loss": 0.2182,
      "step": 8021
    },
    {
      "epoch": 0.03773389653517973,
      "grad_norm": 1.8548882007598877,
      "learning_rate": 0.0001929069186304186,
      "loss": 0.3064,
      "step": 8022
    },
    {
      "epoch": 0.037738600336792195,
      "grad_norm": 2.586080312728882,
      "learning_rate": 0.00019290597565230512,
      "loss": 0.5711,
      "step": 8023
    },
    {
      "epoch": 0.03774330413840466,
      "grad_norm": 1.742334008216858,
      "learning_rate": 0.00019290503267419164,
      "loss": 0.4235,
      "step": 8024
    },
    {
      "epoch": 0.03774800794001712,
      "grad_norm": 2.3356356620788574,
      "learning_rate": 0.00019290408969607816,
      "loss": 0.4148,
      "step": 8025
    },
    {
      "epoch": 0.037752711741629585,
      "grad_norm": 2.1479110717773438,
      "learning_rate": 0.00019290314671796467,
      "loss": 0.3529,
      "step": 8026
    },
    {
      "epoch": 0.03775741554324205,
      "grad_norm": 1.9615458250045776,
      "learning_rate": 0.00019290220373985122,
      "loss": 0.3106,
      "step": 8027
    },
    {
      "epoch": 0.03776211934485451,
      "grad_norm": 0.9848212003707886,
      "learning_rate": 0.00019290126076173774,
      "loss": 0.087,
      "step": 8028
    },
    {
      "epoch": 0.037766823146466975,
      "grad_norm": 1.4730960130691528,
      "learning_rate": 0.00019290031778362426,
      "loss": 0.2016,
      "step": 8029
    },
    {
      "epoch": 0.03777152694807944,
      "grad_norm": 2.0456204414367676,
      "learning_rate": 0.00019289937480551078,
      "loss": 0.2514,
      "step": 8030
    },
    {
      "epoch": 0.0377762307496919,
      "grad_norm": 2.2715117931365967,
      "learning_rate": 0.0001928984318273973,
      "loss": 0.2686,
      "step": 8031
    },
    {
      "epoch": 0.037780934551304365,
      "grad_norm": 1.3403754234313965,
      "learning_rate": 0.0001928974888492838,
      "loss": 0.186,
      "step": 8032
    },
    {
      "epoch": 0.037785638352916824,
      "grad_norm": 0.13668426871299744,
      "learning_rate": 0.00019289654587117033,
      "loss": 0.008,
      "step": 8033
    },
    {
      "epoch": 0.03779034215452929,
      "grad_norm": 1.4002896547317505,
      "learning_rate": 0.00019289560289305685,
      "loss": 0.2335,
      "step": 8034
    },
    {
      "epoch": 0.037795045956141755,
      "grad_norm": 4.259429931640625,
      "learning_rate": 0.00019289465991494337,
      "loss": 1.2264,
      "step": 8035
    },
    {
      "epoch": 0.037799749757754214,
      "grad_norm": 1.483908772468567,
      "learning_rate": 0.00019289371693682992,
      "loss": 0.4023,
      "step": 8036
    },
    {
      "epoch": 0.03780445355936668,
      "grad_norm": 1.7190886735916138,
      "learning_rate": 0.00019289277395871643,
      "loss": 0.1741,
      "step": 8037
    },
    {
      "epoch": 0.037809157360979145,
      "grad_norm": 1.5675302743911743,
      "learning_rate": 0.00019289183098060295,
      "loss": 0.4139,
      "step": 8038
    },
    {
      "epoch": 0.037813861162591604,
      "grad_norm": 1.2033770084381104,
      "learning_rate": 0.00019289088800248947,
      "loss": 0.1752,
      "step": 8039
    },
    {
      "epoch": 0.03781856496420407,
      "grad_norm": 1.434281587600708,
      "learning_rate": 0.000192889945024376,
      "loss": 0.2555,
      "step": 8040
    },
    {
      "epoch": 0.037823268765816535,
      "grad_norm": 1.7127020359039307,
      "learning_rate": 0.00019288900204626254,
      "loss": 0.1632,
      "step": 8041
    },
    {
      "epoch": 0.037827972567428994,
      "grad_norm": 1.0811916589736938,
      "learning_rate": 0.00019288805906814905,
      "loss": 0.1107,
      "step": 8042
    },
    {
      "epoch": 0.03783267636904146,
      "grad_norm": 0.618357241153717,
      "learning_rate": 0.00019288711609003555,
      "loss": 0.0759,
      "step": 8043
    },
    {
      "epoch": 0.037837380170653925,
      "grad_norm": 0.6200541853904724,
      "learning_rate": 0.00019288617311192206,
      "loss": 0.0816,
      "step": 8044
    },
    {
      "epoch": 0.037842083972266384,
      "grad_norm": 2.1456634998321533,
      "learning_rate": 0.0001928852301338086,
      "loss": 0.6232,
      "step": 8045
    },
    {
      "epoch": 0.03784678777387885,
      "grad_norm": 2.4357123374938965,
      "learning_rate": 0.00019288428715569513,
      "loss": 0.461,
      "step": 8046
    },
    {
      "epoch": 0.037851491575491315,
      "grad_norm": 2.9372475147247314,
      "learning_rate": 0.00019288334417758165,
      "loss": 0.8248,
      "step": 8047
    },
    {
      "epoch": 0.037856195377103774,
      "grad_norm": 1.7507506608963013,
      "learning_rate": 0.00019288240119946817,
      "loss": 0.3932,
      "step": 8048
    },
    {
      "epoch": 0.03786089917871624,
      "grad_norm": 2.879031181335449,
      "learning_rate": 0.00019288145822135468,
      "loss": 0.7201,
      "step": 8049
    },
    {
      "epoch": 0.0378656029803287,
      "grad_norm": 0.7175156474113464,
      "learning_rate": 0.00019288051524324123,
      "loss": 0.0777,
      "step": 8050
    },
    {
      "epoch": 0.037870306781941164,
      "grad_norm": 2.8801538944244385,
      "learning_rate": 0.00019287957226512775,
      "loss": 0.3317,
      "step": 8051
    },
    {
      "epoch": 0.03787501058355363,
      "grad_norm": 0.599754273891449,
      "learning_rate": 0.00019287862928701427,
      "loss": 0.0785,
      "step": 8052
    },
    {
      "epoch": 0.03787971438516609,
      "grad_norm": 0.9390575289726257,
      "learning_rate": 0.00019287768630890079,
      "loss": 0.049,
      "step": 8053
    },
    {
      "epoch": 0.037884418186778554,
      "grad_norm": 1.2701706886291504,
      "learning_rate": 0.00019287674333078728,
      "loss": 0.104,
      "step": 8054
    },
    {
      "epoch": 0.03788912198839102,
      "grad_norm": 2.137552261352539,
      "learning_rate": 0.00019287580035267382,
      "loss": 0.2964,
      "step": 8055
    },
    {
      "epoch": 0.03789382579000348,
      "grad_norm": 2.2626872062683105,
      "learning_rate": 0.00019287485737456034,
      "loss": 0.3097,
      "step": 8056
    },
    {
      "epoch": 0.037898529591615944,
      "grad_norm": 1.2172651290893555,
      "learning_rate": 0.00019287391439644686,
      "loss": 0.1142,
      "step": 8057
    },
    {
      "epoch": 0.03790323339322841,
      "grad_norm": 2.4211440086364746,
      "learning_rate": 0.00019287297141833338,
      "loss": 0.2628,
      "step": 8058
    },
    {
      "epoch": 0.03790793719484087,
      "grad_norm": 2.3840172290802,
      "learning_rate": 0.00019287202844021993,
      "loss": 0.4128,
      "step": 8059
    },
    {
      "epoch": 0.037912640996453334,
      "grad_norm": 2.036884307861328,
      "learning_rate": 0.00019287108546210644,
      "loss": 0.3082,
      "step": 8060
    },
    {
      "epoch": 0.0379173447980658,
      "grad_norm": 2.5022451877593994,
      "learning_rate": 0.00019287014248399296,
      "loss": 0.4353,
      "step": 8061
    },
    {
      "epoch": 0.03792204859967826,
      "grad_norm": 1.7209140062332153,
      "learning_rate": 0.00019286919950587948,
      "loss": 0.1365,
      "step": 8062
    },
    {
      "epoch": 0.037926752401290724,
      "grad_norm": 1.922632098197937,
      "learning_rate": 0.000192868256527766,
      "loss": 0.3448,
      "step": 8063
    },
    {
      "epoch": 0.03793145620290319,
      "grad_norm": 1.3279987573623657,
      "learning_rate": 0.00019286731354965252,
      "loss": 0.1302,
      "step": 8064
    },
    {
      "epoch": 0.03793616000451565,
      "grad_norm": 1.5124449729919434,
      "learning_rate": 0.00019286637057153904,
      "loss": 0.2043,
      "step": 8065
    },
    {
      "epoch": 0.037940863806128114,
      "grad_norm": 0.8843697309494019,
      "learning_rate": 0.00019286542759342556,
      "loss": 0.1221,
      "step": 8066
    },
    {
      "epoch": 0.03794556760774057,
      "grad_norm": 1.3618251085281372,
      "learning_rate": 0.00019286448461531207,
      "loss": 0.1801,
      "step": 8067
    },
    {
      "epoch": 0.03795027140935304,
      "grad_norm": 1.49722158908844,
      "learning_rate": 0.00019286354163719862,
      "loss": 0.188,
      "step": 8068
    },
    {
      "epoch": 0.037954975210965504,
      "grad_norm": 1.8199714422225952,
      "learning_rate": 0.00019286259865908514,
      "loss": 0.2063,
      "step": 8069
    },
    {
      "epoch": 0.03795967901257796,
      "grad_norm": 1.6725449562072754,
      "learning_rate": 0.00019286165568097166,
      "loss": 0.3305,
      "step": 8070
    },
    {
      "epoch": 0.03796438281419043,
      "grad_norm": 2.34240460395813,
      "learning_rate": 0.00019286071270285818,
      "loss": 0.5026,
      "step": 8071
    },
    {
      "epoch": 0.037969086615802894,
      "grad_norm": 0.3903462886810303,
      "learning_rate": 0.0001928597697247447,
      "loss": 0.0491,
      "step": 8072
    },
    {
      "epoch": 0.03797379041741535,
      "grad_norm": 3.5469937324523926,
      "learning_rate": 0.00019285882674663124,
      "loss": 0.619,
      "step": 8073
    },
    {
      "epoch": 0.03797849421902782,
      "grad_norm": 2.7512078285217285,
      "learning_rate": 0.00019285788376851773,
      "loss": 0.4755,
      "step": 8074
    },
    {
      "epoch": 0.037983198020640284,
      "grad_norm": 2.229102373123169,
      "learning_rate": 0.00019285694079040425,
      "loss": 0.3887,
      "step": 8075
    },
    {
      "epoch": 0.03798790182225274,
      "grad_norm": 1.8215481042861938,
      "learning_rate": 0.00019285599781229077,
      "loss": 0.0962,
      "step": 8076
    },
    {
      "epoch": 0.03799260562386521,
      "grad_norm": 2.5871195793151855,
      "learning_rate": 0.00019285505483417732,
      "loss": 0.4253,
      "step": 8077
    },
    {
      "epoch": 0.037997309425477674,
      "grad_norm": 2.3178293704986572,
      "learning_rate": 0.00019285411185606383,
      "loss": 0.2133,
      "step": 8078
    },
    {
      "epoch": 0.03800201322709013,
      "grad_norm": 1.0208648443222046,
      "learning_rate": 0.00019285316887795035,
      "loss": 0.1682,
      "step": 8079
    },
    {
      "epoch": 0.0380067170287026,
      "grad_norm": 1.626235842704773,
      "learning_rate": 0.00019285222589983687,
      "loss": 0.2972,
      "step": 8080
    },
    {
      "epoch": 0.038011420830315064,
      "grad_norm": 2.094109058380127,
      "learning_rate": 0.0001928512829217234,
      "loss": 0.1963,
      "step": 8081
    },
    {
      "epoch": 0.03801612463192752,
      "grad_norm": 0.45873311161994934,
      "learning_rate": 0.00019285033994360994,
      "loss": 0.0434,
      "step": 8082
    },
    {
      "epoch": 0.03802082843353999,
      "grad_norm": 4.500588417053223,
      "learning_rate": 0.00019284939696549645,
      "loss": 0.3411,
      "step": 8083
    },
    {
      "epoch": 0.03802553223515245,
      "grad_norm": 1.1977908611297607,
      "learning_rate": 0.00019284845398738297,
      "loss": 0.1596,
      "step": 8084
    },
    {
      "epoch": 0.03803023603676491,
      "grad_norm": 1.0778661966323853,
      "learning_rate": 0.00019284751100926946,
      "loss": 0.2159,
      "step": 8085
    },
    {
      "epoch": 0.03803493983837738,
      "grad_norm": 2.0005011558532715,
      "learning_rate": 0.000192846568031156,
      "loss": 0.187,
      "step": 8086
    },
    {
      "epoch": 0.03803964363998984,
      "grad_norm": 2.1733624935150146,
      "learning_rate": 0.00019284562505304253,
      "loss": 0.3524,
      "step": 8087
    },
    {
      "epoch": 0.0380443474416023,
      "grad_norm": 1.6104991436004639,
      "learning_rate": 0.00019284468207492905,
      "loss": 0.1765,
      "step": 8088
    },
    {
      "epoch": 0.03804905124321477,
      "grad_norm": 0.7243966460227966,
      "learning_rate": 0.00019284373909681557,
      "loss": 0.0576,
      "step": 8089
    },
    {
      "epoch": 0.03805375504482723,
      "grad_norm": 2.052373170852661,
      "learning_rate": 0.00019284279611870208,
      "loss": 0.5478,
      "step": 8090
    },
    {
      "epoch": 0.03805845884643969,
      "grad_norm": 1.1417824029922485,
      "learning_rate": 0.00019284185314058863,
      "loss": 0.3264,
      "step": 8091
    },
    {
      "epoch": 0.03806316264805216,
      "grad_norm": 5.428610324859619,
      "learning_rate": 0.00019284091016247515,
      "loss": 0.6554,
      "step": 8092
    },
    {
      "epoch": 0.03806786644966462,
      "grad_norm": 1.187828779220581,
      "learning_rate": 0.00019283996718436167,
      "loss": 0.0968,
      "step": 8093
    },
    {
      "epoch": 0.03807257025127708,
      "grad_norm": 0.9174568057060242,
      "learning_rate": 0.00019283902420624819,
      "loss": 0.1442,
      "step": 8094
    },
    {
      "epoch": 0.03807727405288955,
      "grad_norm": 1.7868856191635132,
      "learning_rate": 0.0001928380812281347,
      "loss": 0.2604,
      "step": 8095
    },
    {
      "epoch": 0.03808197785450201,
      "grad_norm": 0.9389485716819763,
      "learning_rate": 0.00019283713825002122,
      "loss": 0.1906,
      "step": 8096
    },
    {
      "epoch": 0.03808668165611447,
      "grad_norm": 2.433128833770752,
      "learning_rate": 0.00019283619527190774,
      "loss": 0.6036,
      "step": 8097
    },
    {
      "epoch": 0.03809138545772694,
      "grad_norm": 1.1791843175888062,
      "learning_rate": 0.00019283525229379426,
      "loss": 0.3419,
      "step": 8098
    },
    {
      "epoch": 0.0380960892593394,
      "grad_norm": 0.9919605851173401,
      "learning_rate": 0.00019283430931568078,
      "loss": 0.2128,
      "step": 8099
    },
    {
      "epoch": 0.03810079306095186,
      "grad_norm": 0.5231031775474548,
      "learning_rate": 0.00019283336633756733,
      "loss": 0.0562,
      "step": 8100
    },
    {
      "epoch": 0.03810549686256432,
      "grad_norm": 2.9736533164978027,
      "learning_rate": 0.00019283242335945384,
      "loss": 0.2702,
      "step": 8101
    },
    {
      "epoch": 0.03811020066417679,
      "grad_norm": 3.1733524799346924,
      "learning_rate": 0.00019283148038134036,
      "loss": 0.5695,
      "step": 8102
    },
    {
      "epoch": 0.03811490446578925,
      "grad_norm": 2.6455013751983643,
      "learning_rate": 0.00019283053740322688,
      "loss": 0.6556,
      "step": 8103
    },
    {
      "epoch": 0.03811960826740171,
      "grad_norm": 3.6684625148773193,
      "learning_rate": 0.00019282959442511343,
      "loss": 0.475,
      "step": 8104
    },
    {
      "epoch": 0.03812431206901418,
      "grad_norm": 1.9561827182769775,
      "learning_rate": 0.00019282865144699992,
      "loss": 0.3398,
      "step": 8105
    },
    {
      "epoch": 0.03812901587062664,
      "grad_norm": 5.110825061798096,
      "learning_rate": 0.00019282770846888644,
      "loss": 0.6959,
      "step": 8106
    },
    {
      "epoch": 0.0381337196722391,
      "grad_norm": 2.3640034198760986,
      "learning_rate": 0.00019282676549077296,
      "loss": 0.3285,
      "step": 8107
    },
    {
      "epoch": 0.03813842347385157,
      "grad_norm": 2.3218517303466797,
      "learning_rate": 0.00019282582251265947,
      "loss": 0.3528,
      "step": 8108
    },
    {
      "epoch": 0.03814312727546403,
      "grad_norm": 2.6629395484924316,
      "learning_rate": 0.00019282487953454602,
      "loss": 0.7212,
      "step": 8109
    },
    {
      "epoch": 0.03814783107707649,
      "grad_norm": 0.6864699721336365,
      "learning_rate": 0.00019282393655643254,
      "loss": 0.0742,
      "step": 8110
    },
    {
      "epoch": 0.03815253487868896,
      "grad_norm": 2.3099443912506104,
      "learning_rate": 0.00019282299357831906,
      "loss": 0.3582,
      "step": 8111
    },
    {
      "epoch": 0.03815723868030142,
      "grad_norm": 1.3305031061172485,
      "learning_rate": 0.00019282205060020558,
      "loss": 0.1982,
      "step": 8112
    },
    {
      "epoch": 0.03816194248191388,
      "grad_norm": 2.5165793895721436,
      "learning_rate": 0.0001928211076220921,
      "loss": 0.3152,
      "step": 8113
    },
    {
      "epoch": 0.03816664628352635,
      "grad_norm": 0.8893397450447083,
      "learning_rate": 0.00019282016464397864,
      "loss": 0.0671,
      "step": 8114
    },
    {
      "epoch": 0.03817135008513881,
      "grad_norm": 3.0117084980010986,
      "learning_rate": 0.00019281922166586516,
      "loss": 0.1516,
      "step": 8115
    },
    {
      "epoch": 0.03817605388675127,
      "grad_norm": 2.207473039627075,
      "learning_rate": 0.00019281827868775165,
      "loss": 0.4564,
      "step": 8116
    },
    {
      "epoch": 0.03818075768836374,
      "grad_norm": 1.3521242141723633,
      "learning_rate": 0.00019281733570963817,
      "loss": 0.1588,
      "step": 8117
    },
    {
      "epoch": 0.038185461489976195,
      "grad_norm": 2.80511736869812,
      "learning_rate": 0.00019281639273152472,
      "loss": 0.6032,
      "step": 8118
    },
    {
      "epoch": 0.03819016529158866,
      "grad_norm": 3.5206756591796875,
      "learning_rate": 0.00019281544975341123,
      "loss": 0.7333,
      "step": 8119
    },
    {
      "epoch": 0.03819486909320113,
      "grad_norm": 0.4542594254016876,
      "learning_rate": 0.00019281450677529775,
      "loss": 0.0459,
      "step": 8120
    },
    {
      "epoch": 0.038199572894813585,
      "grad_norm": 1.8133689165115356,
      "learning_rate": 0.00019281356379718427,
      "loss": 0.2574,
      "step": 8121
    },
    {
      "epoch": 0.03820427669642605,
      "grad_norm": 3.0015547275543213,
      "learning_rate": 0.0001928126208190708,
      "loss": 0.7172,
      "step": 8122
    },
    {
      "epoch": 0.03820898049803852,
      "grad_norm": 0.16362544894218445,
      "learning_rate": 0.00019281167784095734,
      "loss": 0.0129,
      "step": 8123
    },
    {
      "epoch": 0.038213684299650975,
      "grad_norm": 1.3823543787002563,
      "learning_rate": 0.00019281073486284385,
      "loss": 0.3072,
      "step": 8124
    },
    {
      "epoch": 0.03821838810126344,
      "grad_norm": 1.3942644596099854,
      "learning_rate": 0.00019280979188473037,
      "loss": 0.1412,
      "step": 8125
    },
    {
      "epoch": 0.03822309190287591,
      "grad_norm": 1.468568205833435,
      "learning_rate": 0.0001928088489066169,
      "loss": 0.3399,
      "step": 8126
    },
    {
      "epoch": 0.038227795704488365,
      "grad_norm": 1.412900686264038,
      "learning_rate": 0.0001928079059285034,
      "loss": 0.1839,
      "step": 8127
    },
    {
      "epoch": 0.03823249950610083,
      "grad_norm": 2.321408271789551,
      "learning_rate": 0.00019280696295038993,
      "loss": 0.5438,
      "step": 8128
    },
    {
      "epoch": 0.0382372033077133,
      "grad_norm": 1.1428074836730957,
      "learning_rate": 0.00019280601997227645,
      "loss": 0.1539,
      "step": 8129
    },
    {
      "epoch": 0.038241907109325755,
      "grad_norm": 1.5361764430999756,
      "learning_rate": 0.00019280507699416297,
      "loss": 0.3337,
      "step": 8130
    },
    {
      "epoch": 0.03824661091093822,
      "grad_norm": 1.9563226699829102,
      "learning_rate": 0.00019280413401604948,
      "loss": 0.4349,
      "step": 8131
    },
    {
      "epoch": 0.03825131471255069,
      "grad_norm": 1.113654613494873,
      "learning_rate": 0.00019280319103793603,
      "loss": 0.1541,
      "step": 8132
    },
    {
      "epoch": 0.038256018514163145,
      "grad_norm": 1.6387901306152344,
      "learning_rate": 0.00019280224805982255,
      "loss": 0.4625,
      "step": 8133
    },
    {
      "epoch": 0.03826072231577561,
      "grad_norm": 0.5461203455924988,
      "learning_rate": 0.00019280130508170907,
      "loss": 0.0608,
      "step": 8134
    },
    {
      "epoch": 0.03826542611738807,
      "grad_norm": 0.46730583906173706,
      "learning_rate": 0.00019280036210359559,
      "loss": 0.0386,
      "step": 8135
    },
    {
      "epoch": 0.038270129919000535,
      "grad_norm": 1.183956265449524,
      "learning_rate": 0.0001927994191254821,
      "loss": 0.1784,
      "step": 8136
    },
    {
      "epoch": 0.038274833720613,
      "grad_norm": 1.2851157188415527,
      "learning_rate": 0.00019279847614736862,
      "loss": 0.1586,
      "step": 8137
    },
    {
      "epoch": 0.03827953752222546,
      "grad_norm": 7.297983169555664,
      "learning_rate": 0.00019279753316925514,
      "loss": 0.9037,
      "step": 8138
    },
    {
      "epoch": 0.038284241323837925,
      "grad_norm": 1.9851874113082886,
      "learning_rate": 0.00019279659019114166,
      "loss": 0.3974,
      "step": 8139
    },
    {
      "epoch": 0.03828894512545039,
      "grad_norm": 1.5530098676681519,
      "learning_rate": 0.00019279564721302818,
      "loss": 0.371,
      "step": 8140
    },
    {
      "epoch": 0.03829364892706285,
      "grad_norm": 2.431302070617676,
      "learning_rate": 0.00019279470423491473,
      "loss": 0.2819,
      "step": 8141
    },
    {
      "epoch": 0.038298352728675315,
      "grad_norm": 4.696455478668213,
      "learning_rate": 0.00019279376125680124,
      "loss": 0.2884,
      "step": 8142
    },
    {
      "epoch": 0.03830305653028778,
      "grad_norm": 0.5339064002037048,
      "learning_rate": 0.00019279281827868776,
      "loss": 0.0564,
      "step": 8143
    },
    {
      "epoch": 0.03830776033190024,
      "grad_norm": 1.2892149686813354,
      "learning_rate": 0.00019279187530057428,
      "loss": 0.2353,
      "step": 8144
    },
    {
      "epoch": 0.038312464133512705,
      "grad_norm": 1.7602461576461792,
      "learning_rate": 0.00019279093232246083,
      "loss": 0.2374,
      "step": 8145
    },
    {
      "epoch": 0.03831716793512517,
      "grad_norm": 2.401264190673828,
      "learning_rate": 0.00019278998934434735,
      "loss": 0.6062,
      "step": 8146
    },
    {
      "epoch": 0.03832187173673763,
      "grad_norm": 1.4257164001464844,
      "learning_rate": 0.00019278904636623384,
      "loss": 0.2568,
      "step": 8147
    },
    {
      "epoch": 0.038326575538350095,
      "grad_norm": 0.44362780451774597,
      "learning_rate": 0.00019278810338812036,
      "loss": 0.0442,
      "step": 8148
    },
    {
      "epoch": 0.03833127933996256,
      "grad_norm": 1.6601861715316772,
      "learning_rate": 0.00019278716041000687,
      "loss": 0.2896,
      "step": 8149
    },
    {
      "epoch": 0.03833598314157502,
      "grad_norm": 0.7627170085906982,
      "learning_rate": 0.00019278621743189342,
      "loss": 0.1629,
      "step": 8150
    },
    {
      "epoch": 0.038340686943187485,
      "grad_norm": 0.43409794569015503,
      "learning_rate": 0.00019278527445377994,
      "loss": 0.0346,
      "step": 8151
    },
    {
      "epoch": 0.038345390744799944,
      "grad_norm": 1.6849308013916016,
      "learning_rate": 0.00019278433147566646,
      "loss": 0.2001,
      "step": 8152
    },
    {
      "epoch": 0.03835009454641241,
      "grad_norm": 2.571322441101074,
      "learning_rate": 0.00019278338849755298,
      "loss": 0.2324,
      "step": 8153
    },
    {
      "epoch": 0.038354798348024875,
      "grad_norm": 1.4025424718856812,
      "learning_rate": 0.00019278244551943952,
      "loss": 0.1146,
      "step": 8154
    },
    {
      "epoch": 0.038359502149637334,
      "grad_norm": 0.8048461079597473,
      "learning_rate": 0.00019278150254132604,
      "loss": 0.0715,
      "step": 8155
    },
    {
      "epoch": 0.0383642059512498,
      "grad_norm": 1.0848112106323242,
      "learning_rate": 0.00019278055956321256,
      "loss": 0.1009,
      "step": 8156
    },
    {
      "epoch": 0.038368909752862265,
      "grad_norm": 2.8394157886505127,
      "learning_rate": 0.00019277961658509908,
      "loss": 0.2845,
      "step": 8157
    },
    {
      "epoch": 0.038373613554474724,
      "grad_norm": 2.2669332027435303,
      "learning_rate": 0.00019277867360698557,
      "loss": 0.4798,
      "step": 8158
    },
    {
      "epoch": 0.03837831735608719,
      "grad_norm": 1.5817627906799316,
      "learning_rate": 0.00019277773062887211,
      "loss": 0.2029,
      "step": 8159
    },
    {
      "epoch": 0.038383021157699655,
      "grad_norm": 1.2488642930984497,
      "learning_rate": 0.00019277678765075863,
      "loss": 0.1928,
      "step": 8160
    },
    {
      "epoch": 0.038387724959312114,
      "grad_norm": 1.1269571781158447,
      "learning_rate": 0.00019277584467264515,
      "loss": 0.1818,
      "step": 8161
    },
    {
      "epoch": 0.03839242876092458,
      "grad_norm": 1.2478201389312744,
      "learning_rate": 0.00019277490169453167,
      "loss": 0.1486,
      "step": 8162
    },
    {
      "epoch": 0.038397132562537045,
      "grad_norm": 1.8074862957000732,
      "learning_rate": 0.0001927739587164182,
      "loss": 0.3153,
      "step": 8163
    },
    {
      "epoch": 0.038401836364149504,
      "grad_norm": 1.9047127962112427,
      "learning_rate": 0.00019277301573830474,
      "loss": 0.2467,
      "step": 8164
    },
    {
      "epoch": 0.03840654016576197,
      "grad_norm": 1.7602168321609497,
      "learning_rate": 0.00019277207276019125,
      "loss": 0.1504,
      "step": 8165
    },
    {
      "epoch": 0.038411243967374435,
      "grad_norm": 0.9833545088768005,
      "learning_rate": 0.00019277112978207777,
      "loss": 0.0949,
      "step": 8166
    },
    {
      "epoch": 0.038415947768986894,
      "grad_norm": 2.7208147048950195,
      "learning_rate": 0.0001927701868039643,
      "loss": 0.4882,
      "step": 8167
    },
    {
      "epoch": 0.03842065157059936,
      "grad_norm": 1.5259736776351929,
      "learning_rate": 0.0001927692438258508,
      "loss": 0.2144,
      "step": 8168
    },
    {
      "epoch": 0.03842535537221182,
      "grad_norm": 5.688972473144531,
      "learning_rate": 0.00019276830084773733,
      "loss": 0.9891,
      "step": 8169
    },
    {
      "epoch": 0.038430059173824284,
      "grad_norm": 1.227318286895752,
      "learning_rate": 0.00019276735786962385,
      "loss": 0.1943,
      "step": 8170
    },
    {
      "epoch": 0.03843476297543675,
      "grad_norm": 2.3114683628082275,
      "learning_rate": 0.00019276641489151037,
      "loss": 0.4399,
      "step": 8171
    },
    {
      "epoch": 0.03843946677704921,
      "grad_norm": 1.4535080194473267,
      "learning_rate": 0.00019276547191339688,
      "loss": 0.144,
      "step": 8172
    },
    {
      "epoch": 0.038444170578661674,
      "grad_norm": 0.5898130536079407,
      "learning_rate": 0.00019276452893528343,
      "loss": 0.0411,
      "step": 8173
    },
    {
      "epoch": 0.03844887438027414,
      "grad_norm": 2.117581605911255,
      "learning_rate": 0.00019276358595716995,
      "loss": 0.2623,
      "step": 8174
    },
    {
      "epoch": 0.0384535781818866,
      "grad_norm": 0.6662576794624329,
      "learning_rate": 0.00019276264297905647,
      "loss": 0.0692,
      "step": 8175
    },
    {
      "epoch": 0.038458281983499064,
      "grad_norm": 3.4143333435058594,
      "learning_rate": 0.00019276170000094299,
      "loss": 0.5986,
      "step": 8176
    },
    {
      "epoch": 0.03846298578511153,
      "grad_norm": 2.0086405277252197,
      "learning_rate": 0.00019276075702282953,
      "loss": 0.2289,
      "step": 8177
    },
    {
      "epoch": 0.03846768958672399,
      "grad_norm": 0.28089091181755066,
      "learning_rate": 0.00019275981404471602,
      "loss": 0.0178,
      "step": 8178
    },
    {
      "epoch": 0.038472393388336454,
      "grad_norm": 1.4975374937057495,
      "learning_rate": 0.00019275887106660254,
      "loss": 0.2124,
      "step": 8179
    },
    {
      "epoch": 0.03847709718994892,
      "grad_norm": 3.9611141681671143,
      "learning_rate": 0.00019275792808848906,
      "loss": 0.5827,
      "step": 8180
    },
    {
      "epoch": 0.03848180099156138,
      "grad_norm": 2.249242067337036,
      "learning_rate": 0.00019275698511037558,
      "loss": 0.3311,
      "step": 8181
    },
    {
      "epoch": 0.038486504793173844,
      "grad_norm": 2.0171849727630615,
      "learning_rate": 0.00019275604213226213,
      "loss": 0.268,
      "step": 8182
    },
    {
      "epoch": 0.03849120859478631,
      "grad_norm": 3.096508026123047,
      "learning_rate": 0.00019275509915414864,
      "loss": 0.5711,
      "step": 8183
    },
    {
      "epoch": 0.03849591239639877,
      "grad_norm": 1.4802387952804565,
      "learning_rate": 0.00019275415617603516,
      "loss": 0.2189,
      "step": 8184
    },
    {
      "epoch": 0.038500616198011234,
      "grad_norm": 1.2638524770736694,
      "learning_rate": 0.00019275321319792168,
      "loss": 0.0852,
      "step": 8185
    },
    {
      "epoch": 0.03850531999962369,
      "grad_norm": 2.7250571250915527,
      "learning_rate": 0.00019275227021980823,
      "loss": 0.6679,
      "step": 8186
    },
    {
      "epoch": 0.03851002380123616,
      "grad_norm": 0.800390899181366,
      "learning_rate": 0.00019275132724169475,
      "loss": 0.0782,
      "step": 8187
    },
    {
      "epoch": 0.038514727602848624,
      "grad_norm": 4.1076860427856445,
      "learning_rate": 0.00019275038426358126,
      "loss": 0.6815,
      "step": 8188
    },
    {
      "epoch": 0.03851943140446108,
      "grad_norm": 1.1485036611557007,
      "learning_rate": 0.00019274944128546776,
      "loss": 0.1138,
      "step": 8189
    },
    {
      "epoch": 0.03852413520607355,
      "grad_norm": 1.2862653732299805,
      "learning_rate": 0.00019274849830735427,
      "loss": 0.4009,
      "step": 8190
    },
    {
      "epoch": 0.038528839007686014,
      "grad_norm": 1.6786915063858032,
      "learning_rate": 0.00019274755532924082,
      "loss": 0.3702,
      "step": 8191
    },
    {
      "epoch": 0.03853354280929847,
      "grad_norm": 3.8540091514587402,
      "learning_rate": 0.00019274661235112734,
      "loss": 0.7417,
      "step": 8192
    },
    {
      "epoch": 0.03853824661091094,
      "grad_norm": 0.6420841813087463,
      "learning_rate": 0.00019274566937301386,
      "loss": 0.088,
      "step": 8193
    },
    {
      "epoch": 0.038542950412523404,
      "grad_norm": 3.4188785552978516,
      "learning_rate": 0.00019274472639490038,
      "loss": 0.5545,
      "step": 8194
    },
    {
      "epoch": 0.03854765421413586,
      "grad_norm": 1.7798042297363281,
      "learning_rate": 0.00019274378341678692,
      "loss": 0.3404,
      "step": 8195
    },
    {
      "epoch": 0.03855235801574833,
      "grad_norm": 3.4926249980926514,
      "learning_rate": 0.00019274284043867344,
      "loss": 0.2929,
      "step": 8196
    },
    {
      "epoch": 0.038557061817360794,
      "grad_norm": 1.749178409576416,
      "learning_rate": 0.00019274189746055996,
      "loss": 0.156,
      "step": 8197
    },
    {
      "epoch": 0.03856176561897325,
      "grad_norm": 1.5856513977050781,
      "learning_rate": 0.00019274095448244648,
      "loss": 0.3337,
      "step": 8198
    },
    {
      "epoch": 0.03856646942058572,
      "grad_norm": 0.5698688626289368,
      "learning_rate": 0.000192740011504333,
      "loss": 0.0562,
      "step": 8199
    },
    {
      "epoch": 0.038571173222198184,
      "grad_norm": 1.634266972541809,
      "learning_rate": 0.00019273906852621951,
      "loss": 0.3338,
      "step": 8200
    },
    {
      "epoch": 0.03857587702381064,
      "grad_norm": 0.32598432898521423,
      "learning_rate": 0.00019273812554810603,
      "loss": 0.05,
      "step": 8201
    },
    {
      "epoch": 0.03858058082542311,
      "grad_norm": 1.5112898349761963,
      "learning_rate": 0.00019273718256999255,
      "loss": 0.1427,
      "step": 8202
    },
    {
      "epoch": 0.03858528462703557,
      "grad_norm": 2.3028452396392822,
      "learning_rate": 0.00019273623959187907,
      "loss": 0.3742,
      "step": 8203
    },
    {
      "epoch": 0.03858998842864803,
      "grad_norm": 2.093444585800171,
      "learning_rate": 0.00019273529661376562,
      "loss": 0.1305,
      "step": 8204
    },
    {
      "epoch": 0.0385946922302605,
      "grad_norm": 5.658060550689697,
      "learning_rate": 0.00019273435363565214,
      "loss": 0.9416,
      "step": 8205
    },
    {
      "epoch": 0.03859939603187296,
      "grad_norm": 1.3484548330307007,
      "learning_rate": 0.00019273341065753865,
      "loss": 0.2246,
      "step": 8206
    },
    {
      "epoch": 0.03860409983348542,
      "grad_norm": 1.6402671337127686,
      "learning_rate": 0.00019273246767942517,
      "loss": 0.1722,
      "step": 8207
    },
    {
      "epoch": 0.03860880363509789,
      "grad_norm": 0.9945619106292725,
      "learning_rate": 0.0001927315247013117,
      "loss": 0.1155,
      "step": 8208
    },
    {
      "epoch": 0.03861350743671035,
      "grad_norm": 5.374368667602539,
      "learning_rate": 0.0001927305817231982,
      "loss": 1.3105,
      "step": 8209
    },
    {
      "epoch": 0.03861821123832281,
      "grad_norm": 2.721191644668579,
      "learning_rate": 0.00019272963874508473,
      "loss": 0.4086,
      "step": 8210
    },
    {
      "epoch": 0.03862291503993528,
      "grad_norm": 0.9515562653541565,
      "learning_rate": 0.00019272869576697125,
      "loss": 0.3068,
      "step": 8211
    },
    {
      "epoch": 0.03862761884154774,
      "grad_norm": 2.6296908855438232,
      "learning_rate": 0.00019272775278885777,
      "loss": 0.3155,
      "step": 8212
    },
    {
      "epoch": 0.0386323226431602,
      "grad_norm": 4.578728675842285,
      "learning_rate": 0.00019272680981074428,
      "loss": 0.7021,
      "step": 8213
    },
    {
      "epoch": 0.03863702644477267,
      "grad_norm": 1.3958309888839722,
      "learning_rate": 0.00019272586683263083,
      "loss": 0.3552,
      "step": 8214
    },
    {
      "epoch": 0.03864173024638513,
      "grad_norm": 0.7831143140792847,
      "learning_rate": 0.00019272492385451735,
      "loss": 0.1021,
      "step": 8215
    },
    {
      "epoch": 0.03864643404799759,
      "grad_norm": 2.687699794769287,
      "learning_rate": 0.00019272398087640387,
      "loss": 0.6352,
      "step": 8216
    },
    {
      "epoch": 0.03865113784961006,
      "grad_norm": 3.1434335708618164,
      "learning_rate": 0.00019272303789829039,
      "loss": 0.5502,
      "step": 8217
    },
    {
      "epoch": 0.03865584165122252,
      "grad_norm": 0.9672843217849731,
      "learning_rate": 0.00019272209492017693,
      "loss": 0.1843,
      "step": 8218
    },
    {
      "epoch": 0.03866054545283498,
      "grad_norm": 0.9696053862571716,
      "learning_rate": 0.00019272115194206345,
      "loss": 0.092,
      "step": 8219
    },
    {
      "epoch": 0.03866524925444744,
      "grad_norm": 1.0582389831542969,
      "learning_rate": 0.00019272020896394994,
      "loss": 0.1299,
      "step": 8220
    },
    {
      "epoch": 0.03866995305605991,
      "grad_norm": 1.4269804954528809,
      "learning_rate": 0.00019271926598583646,
      "loss": 0.4036,
      "step": 8221
    },
    {
      "epoch": 0.03867465685767237,
      "grad_norm": 1.175850749015808,
      "learning_rate": 0.00019271832300772298,
      "loss": 0.1984,
      "step": 8222
    },
    {
      "epoch": 0.03867936065928483,
      "grad_norm": 1.0784852504730225,
      "learning_rate": 0.00019271738002960953,
      "loss": 0.2553,
      "step": 8223
    },
    {
      "epoch": 0.0386840644608973,
      "grad_norm": 2.501527786254883,
      "learning_rate": 0.00019271643705149604,
      "loss": 0.7163,
      "step": 8224
    },
    {
      "epoch": 0.03868876826250976,
      "grad_norm": 2.9875752925872803,
      "learning_rate": 0.00019271549407338256,
      "loss": 0.2595,
      "step": 8225
    },
    {
      "epoch": 0.03869347206412222,
      "grad_norm": 0.7757128477096558,
      "learning_rate": 0.00019271455109526908,
      "loss": 0.0591,
      "step": 8226
    },
    {
      "epoch": 0.03869817586573469,
      "grad_norm": 0.9497467875480652,
      "learning_rate": 0.00019271360811715563,
      "loss": 0.28,
      "step": 8227
    },
    {
      "epoch": 0.03870287966734715,
      "grad_norm": 2.1960625648498535,
      "learning_rate": 0.00019271266513904215,
      "loss": 0.6152,
      "step": 8228
    },
    {
      "epoch": 0.03870758346895961,
      "grad_norm": 0.6491140723228455,
      "learning_rate": 0.00019271172216092866,
      "loss": 0.1053,
      "step": 8229
    },
    {
      "epoch": 0.03871228727057208,
      "grad_norm": 1.1516146659851074,
      "learning_rate": 0.00019271077918281518,
      "loss": 0.176,
      "step": 8230
    },
    {
      "epoch": 0.03871699107218454,
      "grad_norm": 1.9836702346801758,
      "learning_rate": 0.0001927098362047017,
      "loss": 0.2799,
      "step": 8231
    },
    {
      "epoch": 0.038721694873797,
      "grad_norm": 1.1420040130615234,
      "learning_rate": 0.00019270889322658822,
      "loss": 0.179,
      "step": 8232
    },
    {
      "epoch": 0.03872639867540947,
      "grad_norm": 1.8502334356307983,
      "learning_rate": 0.00019270795024847474,
      "loss": 0.395,
      "step": 8233
    },
    {
      "epoch": 0.03873110247702193,
      "grad_norm": 1.8301302194595337,
      "learning_rate": 0.00019270700727036126,
      "loss": 0.4344,
      "step": 8234
    },
    {
      "epoch": 0.03873580627863439,
      "grad_norm": 0.7464813590049744,
      "learning_rate": 0.00019270606429224778,
      "loss": 0.0772,
      "step": 8235
    },
    {
      "epoch": 0.03874051008024686,
      "grad_norm": 1.2962309122085571,
      "learning_rate": 0.00019270512131413432,
      "loss": 0.2931,
      "step": 8236
    },
    {
      "epoch": 0.038745213881859315,
      "grad_norm": 2.8637170791625977,
      "learning_rate": 0.00019270417833602084,
      "loss": 0.5847,
      "step": 8237
    },
    {
      "epoch": 0.03874991768347178,
      "grad_norm": 1.3364484310150146,
      "learning_rate": 0.00019270323535790736,
      "loss": 0.2423,
      "step": 8238
    },
    {
      "epoch": 0.03875462148508425,
      "grad_norm": 1.2188440561294556,
      "learning_rate": 0.00019270229237979388,
      "loss": 0.114,
      "step": 8239
    },
    {
      "epoch": 0.038759325286696705,
      "grad_norm": 0.8135616779327393,
      "learning_rate": 0.0001927013494016804,
      "loss": 0.0885,
      "step": 8240
    },
    {
      "epoch": 0.03876402908830917,
      "grad_norm": 0.5880018472671509,
      "learning_rate": 0.00019270040642356691,
      "loss": 0.0419,
      "step": 8241
    },
    {
      "epoch": 0.03876873288992164,
      "grad_norm": 1.0560038089752197,
      "learning_rate": 0.00019269946344545343,
      "loss": 0.1289,
      "step": 8242
    },
    {
      "epoch": 0.038773436691534095,
      "grad_norm": 0.9561461210250854,
      "learning_rate": 0.00019269852046733995,
      "loss": 0.1068,
      "step": 8243
    },
    {
      "epoch": 0.03877814049314656,
      "grad_norm": 3.5463671684265137,
      "learning_rate": 0.00019269757748922647,
      "loss": 0.5901,
      "step": 8244
    },
    {
      "epoch": 0.03878284429475903,
      "grad_norm": 0.8515180945396423,
      "learning_rate": 0.00019269663451111302,
      "loss": 0.1028,
      "step": 8245
    },
    {
      "epoch": 0.038787548096371485,
      "grad_norm": 2.440756320953369,
      "learning_rate": 0.00019269569153299954,
      "loss": 0.2431,
      "step": 8246
    },
    {
      "epoch": 0.03879225189798395,
      "grad_norm": 3.332970142364502,
      "learning_rate": 0.00019269474855488605,
      "loss": 0.2405,
      "step": 8247
    },
    {
      "epoch": 0.03879695569959642,
      "grad_norm": 2.1702826023101807,
      "learning_rate": 0.00019269380557677257,
      "loss": 0.1316,
      "step": 8248
    },
    {
      "epoch": 0.038801659501208875,
      "grad_norm": 1.2510168552398682,
      "learning_rate": 0.0001926928625986591,
      "loss": 0.2528,
      "step": 8249
    },
    {
      "epoch": 0.03880636330282134,
      "grad_norm": 0.6128829121589661,
      "learning_rate": 0.00019269191962054564,
      "loss": 0.0729,
      "step": 8250
    },
    {
      "epoch": 0.038811067104433807,
      "grad_norm": 2.9296016693115234,
      "learning_rate": 0.00019269097664243213,
      "loss": 0.2143,
      "step": 8251
    },
    {
      "epoch": 0.038815770906046265,
      "grad_norm": 0.6078921556472778,
      "learning_rate": 0.00019269003366431865,
      "loss": 0.032,
      "step": 8252
    },
    {
      "epoch": 0.03882047470765873,
      "grad_norm": 3.8651585578918457,
      "learning_rate": 0.00019268909068620517,
      "loss": 0.3952,
      "step": 8253
    },
    {
      "epoch": 0.03882517850927119,
      "grad_norm": 4.2615156173706055,
      "learning_rate": 0.0001926881477080917,
      "loss": 0.7354,
      "step": 8254
    },
    {
      "epoch": 0.038829882310883655,
      "grad_norm": 3.216465711593628,
      "learning_rate": 0.00019268720472997823,
      "loss": 0.2023,
      "step": 8255
    },
    {
      "epoch": 0.03883458611249612,
      "grad_norm": 2.671820878982544,
      "learning_rate": 0.00019268626175186475,
      "loss": 0.3344,
      "step": 8256
    },
    {
      "epoch": 0.03883928991410858,
      "grad_norm": 1.0601611137390137,
      "learning_rate": 0.00019268531877375127,
      "loss": 0.0983,
      "step": 8257
    },
    {
      "epoch": 0.038843993715721045,
      "grad_norm": 1.3705360889434814,
      "learning_rate": 0.00019268437579563779,
      "loss": 0.07,
      "step": 8258
    },
    {
      "epoch": 0.03884869751733351,
      "grad_norm": 8.664207458496094,
      "learning_rate": 0.00019268343281752433,
      "loss": 1.031,
      "step": 8259
    },
    {
      "epoch": 0.03885340131894597,
      "grad_norm": 2.065650701522827,
      "learning_rate": 0.00019268248983941085,
      "loss": 0.1711,
      "step": 8260
    },
    {
      "epoch": 0.038858105120558435,
      "grad_norm": 4.745749473571777,
      "learning_rate": 0.00019268154686129737,
      "loss": 0.7636,
      "step": 8261
    },
    {
      "epoch": 0.0388628089221709,
      "grad_norm": 0.094139963388443,
      "learning_rate": 0.0001926806038831839,
      "loss": 0.0053,
      "step": 8262
    },
    {
      "epoch": 0.03886751272378336,
      "grad_norm": 0.1496715098619461,
      "learning_rate": 0.00019267966090507038,
      "loss": 0.0135,
      "step": 8263
    },
    {
      "epoch": 0.038872216525395825,
      "grad_norm": 2.7357213497161865,
      "learning_rate": 0.00019267871792695693,
      "loss": 0.1903,
      "step": 8264
    },
    {
      "epoch": 0.03887692032700829,
      "grad_norm": 3.5162761211395264,
      "learning_rate": 0.00019267777494884344,
      "loss": 0.4503,
      "step": 8265
    },
    {
      "epoch": 0.03888162412862075,
      "grad_norm": 1.7859584093093872,
      "learning_rate": 0.00019267683197072996,
      "loss": 0.1414,
      "step": 8266
    },
    {
      "epoch": 0.038886327930233215,
      "grad_norm": 0.4376530349254608,
      "learning_rate": 0.00019267588899261648,
      "loss": 0.0287,
      "step": 8267
    },
    {
      "epoch": 0.03889103173184568,
      "grad_norm": 3.9541685581207275,
      "learning_rate": 0.00019267494601450303,
      "loss": 0.3565,
      "step": 8268
    },
    {
      "epoch": 0.03889573553345814,
      "grad_norm": 1.1756980419158936,
      "learning_rate": 0.00019267400303638955,
      "loss": 0.0432,
      "step": 8269
    },
    {
      "epoch": 0.038900439335070605,
      "grad_norm": 0.35509151220321655,
      "learning_rate": 0.00019267306005827606,
      "loss": 0.0222,
      "step": 8270
    },
    {
      "epoch": 0.038905143136683064,
      "grad_norm": 1.5375086069107056,
      "learning_rate": 0.00019267211708016258,
      "loss": 0.1243,
      "step": 8271
    },
    {
      "epoch": 0.03890984693829553,
      "grad_norm": 0.42270997166633606,
      "learning_rate": 0.0001926711741020491,
      "loss": 0.0282,
      "step": 8272
    },
    {
      "epoch": 0.038914550739907995,
      "grad_norm": 5.208208084106445,
      "learning_rate": 0.00019267023112393562,
      "loss": 0.2146,
      "step": 8273
    },
    {
      "epoch": 0.038919254541520454,
      "grad_norm": 2.257882833480835,
      "learning_rate": 0.00019266928814582214,
      "loss": 0.0964,
      "step": 8274
    },
    {
      "epoch": 0.03892395834313292,
      "grad_norm": 6.053847312927246,
      "learning_rate": 0.00019266834516770866,
      "loss": 0.8433,
      "step": 8275
    },
    {
      "epoch": 0.038928662144745385,
      "grad_norm": 0.12359323352575302,
      "learning_rate": 0.00019266740218959518,
      "loss": 0.0066,
      "step": 8276
    },
    {
      "epoch": 0.038933365946357844,
      "grad_norm": 2.9379515647888184,
      "learning_rate": 0.00019266645921148172,
      "loss": 0.3821,
      "step": 8277
    },
    {
      "epoch": 0.03893806974797031,
      "grad_norm": 3.7544608116149902,
      "learning_rate": 0.00019266551623336824,
      "loss": 0.6724,
      "step": 8278
    },
    {
      "epoch": 0.038942773549582775,
      "grad_norm": 7.955796718597412,
      "learning_rate": 0.00019266457325525476,
      "loss": 1.3776,
      "step": 8279
    },
    {
      "epoch": 0.038947477351195234,
      "grad_norm": 3.470789909362793,
      "learning_rate": 0.00019266363027714128,
      "loss": 0.2872,
      "step": 8280
    },
    {
      "epoch": 0.0389521811528077,
      "grad_norm": 5.19110107421875,
      "learning_rate": 0.0001926626872990278,
      "loss": 0.7565,
      "step": 8281
    },
    {
      "epoch": 0.038956884954420165,
      "grad_norm": 4.582918643951416,
      "learning_rate": 0.00019266174432091431,
      "loss": 0.5943,
      "step": 8282
    },
    {
      "epoch": 0.038961588756032624,
      "grad_norm": 0.18057093024253845,
      "learning_rate": 0.00019266080134280083,
      "loss": 0.0143,
      "step": 8283
    },
    {
      "epoch": 0.03896629255764509,
      "grad_norm": 5.131321907043457,
      "learning_rate": 0.00019265985836468735,
      "loss": 1.1707,
      "step": 8284
    },
    {
      "epoch": 0.038970996359257555,
      "grad_norm": 5.39626932144165,
      "learning_rate": 0.00019265891538657387,
      "loss": 0.2655,
      "step": 8285
    },
    {
      "epoch": 0.038975700160870014,
      "grad_norm": 1.2259528636932373,
      "learning_rate": 0.00019265797240846042,
      "loss": 0.0735,
      "step": 8286
    },
    {
      "epoch": 0.03898040396248248,
      "grad_norm": 2.725475311279297,
      "learning_rate": 0.00019265702943034694,
      "loss": 0.2181,
      "step": 8287
    },
    {
      "epoch": 0.03898510776409494,
      "grad_norm": 1.471390724182129,
      "learning_rate": 0.00019265608645223345,
      "loss": 0.2423,
      "step": 8288
    },
    {
      "epoch": 0.038989811565707404,
      "grad_norm": 1.9988093376159668,
      "learning_rate": 0.00019265514347411997,
      "loss": 0.1961,
      "step": 8289
    },
    {
      "epoch": 0.03899451536731987,
      "grad_norm": 2.2388343811035156,
      "learning_rate": 0.0001926542004960065,
      "loss": 0.3167,
      "step": 8290
    },
    {
      "epoch": 0.03899921916893233,
      "grad_norm": 0.5481727719306946,
      "learning_rate": 0.00019265325751789304,
      "loss": 0.055,
      "step": 8291
    },
    {
      "epoch": 0.039003922970544794,
      "grad_norm": 0.6960824131965637,
      "learning_rate": 0.00019265231453977956,
      "loss": 0.0611,
      "step": 8292
    },
    {
      "epoch": 0.03900862677215726,
      "grad_norm": 2.7531063556671143,
      "learning_rate": 0.00019265137156166607,
      "loss": 0.44,
      "step": 8293
    },
    {
      "epoch": 0.03901333057376972,
      "grad_norm": 1.4860681295394897,
      "learning_rate": 0.00019265042858355257,
      "loss": 0.1611,
      "step": 8294
    },
    {
      "epoch": 0.039018034375382184,
      "grad_norm": 6.465826988220215,
      "learning_rate": 0.0001926494856054391,
      "loss": 0.8048,
      "step": 8295
    },
    {
      "epoch": 0.03902273817699465,
      "grad_norm": 0.5081544518470764,
      "learning_rate": 0.00019264854262732563,
      "loss": 0.0334,
      "step": 8296
    },
    {
      "epoch": 0.03902744197860711,
      "grad_norm": 3.2881715297698975,
      "learning_rate": 0.00019264759964921215,
      "loss": 0.5536,
      "step": 8297
    },
    {
      "epoch": 0.039032145780219574,
      "grad_norm": 1.1245601177215576,
      "learning_rate": 0.00019264665667109867,
      "loss": 0.115,
      "step": 8298
    },
    {
      "epoch": 0.03903684958183204,
      "grad_norm": 1.6946003437042236,
      "learning_rate": 0.00019264571369298519,
      "loss": 0.1425,
      "step": 8299
    },
    {
      "epoch": 0.0390415533834445,
      "grad_norm": 1.3498437404632568,
      "learning_rate": 0.00019264477071487173,
      "loss": 0.2245,
      "step": 8300
    },
    {
      "epoch": 0.039046257185056964,
      "grad_norm": 0.6648370027542114,
      "learning_rate": 0.00019264382773675825,
      "loss": 0.0713,
      "step": 8301
    },
    {
      "epoch": 0.03905096098666943,
      "grad_norm": 2.411193609237671,
      "learning_rate": 0.00019264288475864477,
      "loss": 0.2659,
      "step": 8302
    },
    {
      "epoch": 0.03905566478828189,
      "grad_norm": 1.3670015335083008,
      "learning_rate": 0.0001926419417805313,
      "loss": 0.1287,
      "step": 8303
    },
    {
      "epoch": 0.039060368589894354,
      "grad_norm": 3.2216975688934326,
      "learning_rate": 0.0001926409988024178,
      "loss": 0.4556,
      "step": 8304
    },
    {
      "epoch": 0.03906507239150681,
      "grad_norm": 4.705864906311035,
      "learning_rate": 0.00019264005582430433,
      "loss": 0.3047,
      "step": 8305
    },
    {
      "epoch": 0.03906977619311928,
      "grad_norm": 1.9982950687408447,
      "learning_rate": 0.00019263911284619084,
      "loss": 0.287,
      "step": 8306
    },
    {
      "epoch": 0.039074479994731744,
      "grad_norm": 2.9971425533294678,
      "learning_rate": 0.00019263816986807736,
      "loss": 0.831,
      "step": 8307
    },
    {
      "epoch": 0.0390791837963442,
      "grad_norm": 1.962870478630066,
      "learning_rate": 0.00019263722688996388,
      "loss": 0.0796,
      "step": 8308
    },
    {
      "epoch": 0.03908388759795667,
      "grad_norm": 4.300082683563232,
      "learning_rate": 0.00019263628391185043,
      "loss": 0.6054,
      "step": 8309
    },
    {
      "epoch": 0.039088591399569134,
      "grad_norm": 2.277905225753784,
      "learning_rate": 0.00019263534093373695,
      "loss": 0.4172,
      "step": 8310
    },
    {
      "epoch": 0.03909329520118159,
      "grad_norm": 1.694332242012024,
      "learning_rate": 0.00019263439795562346,
      "loss": 0.1924,
      "step": 8311
    },
    {
      "epoch": 0.03909799900279406,
      "grad_norm": 0.7578747272491455,
      "learning_rate": 0.00019263345497750998,
      "loss": 0.0723,
      "step": 8312
    },
    {
      "epoch": 0.039102702804406524,
      "grad_norm": 1.5127646923065186,
      "learning_rate": 0.0001926325119993965,
      "loss": 0.2155,
      "step": 8313
    },
    {
      "epoch": 0.03910740660601898,
      "grad_norm": 1.795742154121399,
      "learning_rate": 0.00019263156902128302,
      "loss": 0.2538,
      "step": 8314
    },
    {
      "epoch": 0.03911211040763145,
      "grad_norm": 2.784797430038452,
      "learning_rate": 0.00019263062604316954,
      "loss": 0.2997,
      "step": 8315
    },
    {
      "epoch": 0.039116814209243914,
      "grad_norm": 0.5300264358520508,
      "learning_rate": 0.00019262968306505606,
      "loss": 0.0472,
      "step": 8316
    },
    {
      "epoch": 0.03912151801085637,
      "grad_norm": 2.291417360305786,
      "learning_rate": 0.00019262874008694258,
      "loss": 0.5026,
      "step": 8317
    },
    {
      "epoch": 0.03912622181246884,
      "grad_norm": 2.69628643989563,
      "learning_rate": 0.00019262779710882912,
      "loss": 0.5738,
      "step": 8318
    },
    {
      "epoch": 0.039130925614081304,
      "grad_norm": 1.510612964630127,
      "learning_rate": 0.00019262685413071564,
      "loss": 0.2613,
      "step": 8319
    },
    {
      "epoch": 0.03913562941569376,
      "grad_norm": 0.45541444420814514,
      "learning_rate": 0.00019262591115260216,
      "loss": 0.0291,
      "step": 8320
    },
    {
      "epoch": 0.03914033321730623,
      "grad_norm": 1.3317320346832275,
      "learning_rate": 0.00019262496817448868,
      "loss": 0.204,
      "step": 8321
    },
    {
      "epoch": 0.03914503701891869,
      "grad_norm": 1.3092395067214966,
      "learning_rate": 0.0001926240251963752,
      "loss": 0.1521,
      "step": 8322
    },
    {
      "epoch": 0.03914974082053115,
      "grad_norm": 1.6897270679473877,
      "learning_rate": 0.00019262308221826174,
      "loss": 0.319,
      "step": 8323
    },
    {
      "epoch": 0.03915444462214362,
      "grad_norm": 0.4266047775745392,
      "learning_rate": 0.00019262213924014826,
      "loss": 0.038,
      "step": 8324
    },
    {
      "epoch": 0.03915914842375608,
      "grad_norm": 2.8201098442077637,
      "learning_rate": 0.00019262119626203475,
      "loss": 0.4093,
      "step": 8325
    },
    {
      "epoch": 0.03916385222536854,
      "grad_norm": 2.5584769248962402,
      "learning_rate": 0.00019262025328392127,
      "loss": 0.4482,
      "step": 8326
    },
    {
      "epoch": 0.03916855602698101,
      "grad_norm": 0.7852883338928223,
      "learning_rate": 0.00019261931030580782,
      "loss": 0.0533,
      "step": 8327
    },
    {
      "epoch": 0.03917325982859347,
      "grad_norm": 1.7702306509017944,
      "learning_rate": 0.00019261836732769434,
      "loss": 0.2302,
      "step": 8328
    },
    {
      "epoch": 0.03917796363020593,
      "grad_norm": 3.0819103717803955,
      "learning_rate": 0.00019261742434958085,
      "loss": 0.5905,
      "step": 8329
    },
    {
      "epoch": 0.0391826674318184,
      "grad_norm": 0.5664777755737305,
      "learning_rate": 0.00019261648137146737,
      "loss": 0.0432,
      "step": 8330
    },
    {
      "epoch": 0.03918737123343086,
      "grad_norm": 3.1099233627319336,
      "learning_rate": 0.0001926155383933539,
      "loss": 0.6464,
      "step": 8331
    },
    {
      "epoch": 0.03919207503504332,
      "grad_norm": 1.6780513525009155,
      "learning_rate": 0.00019261459541524044,
      "loss": 0.1624,
      "step": 8332
    },
    {
      "epoch": 0.03919677883665579,
      "grad_norm": 0.8206750154495239,
      "learning_rate": 0.00019261365243712696,
      "loss": 0.0801,
      "step": 8333
    },
    {
      "epoch": 0.03920148263826825,
      "grad_norm": 2.0035791397094727,
      "learning_rate": 0.00019261270945901347,
      "loss": 0.3612,
      "step": 8334
    },
    {
      "epoch": 0.03920618643988071,
      "grad_norm": 0.7402475476264954,
      "learning_rate": 0.0001926117664809,
      "loss": 0.0821,
      "step": 8335
    },
    {
      "epoch": 0.03921089024149318,
      "grad_norm": 1.2956643104553223,
      "learning_rate": 0.0001926108235027865,
      "loss": 0.1395,
      "step": 8336
    },
    {
      "epoch": 0.03921559404310564,
      "grad_norm": 2.140707015991211,
      "learning_rate": 0.00019260988052467303,
      "loss": 0.4272,
      "step": 8337
    },
    {
      "epoch": 0.0392202978447181,
      "grad_norm": 0.798072338104248,
      "learning_rate": 0.00019260893754655955,
      "loss": 0.0762,
      "step": 8338
    },
    {
      "epoch": 0.03922500164633056,
      "grad_norm": 0.6089836955070496,
      "learning_rate": 0.00019260799456844607,
      "loss": 0.0619,
      "step": 8339
    },
    {
      "epoch": 0.03922970544794303,
      "grad_norm": 5.768218517303467,
      "learning_rate": 0.00019260705159033259,
      "loss": 0.7881,
      "step": 8340
    },
    {
      "epoch": 0.03923440924955549,
      "grad_norm": 3.9694254398345947,
      "learning_rate": 0.00019260610861221913,
      "loss": 1.0499,
      "step": 8341
    },
    {
      "epoch": 0.03923911305116795,
      "grad_norm": 1.7508327960968018,
      "learning_rate": 0.00019260516563410565,
      "loss": 0.3459,
      "step": 8342
    },
    {
      "epoch": 0.03924381685278042,
      "grad_norm": 3.1266355514526367,
      "learning_rate": 0.00019260422265599217,
      "loss": 0.555,
      "step": 8343
    },
    {
      "epoch": 0.03924852065439288,
      "grad_norm": 1.3799854516983032,
      "learning_rate": 0.0001926032796778787,
      "loss": 0.2703,
      "step": 8344
    },
    {
      "epoch": 0.03925322445600534,
      "grad_norm": 1.0122398138046265,
      "learning_rate": 0.0001926023366997652,
      "loss": 0.1318,
      "step": 8345
    },
    {
      "epoch": 0.039257928257617807,
      "grad_norm": 0.11775542795658112,
      "learning_rate": 0.00019260139372165172,
      "loss": 0.0079,
      "step": 8346
    },
    {
      "epoch": 0.03926263205923027,
      "grad_norm": 3.1797983646392822,
      "learning_rate": 0.00019260045074353824,
      "loss": 0.606,
      "step": 8347
    },
    {
      "epoch": 0.03926733586084273,
      "grad_norm": 2.2155110836029053,
      "learning_rate": 0.00019259950776542476,
      "loss": 0.4022,
      "step": 8348
    },
    {
      "epoch": 0.039272039662455197,
      "grad_norm": 1.477325439453125,
      "learning_rate": 0.00019259856478731128,
      "loss": 0.2118,
      "step": 8349
    },
    {
      "epoch": 0.03927674346406766,
      "grad_norm": 3.494156837463379,
      "learning_rate": 0.00019259762180919783,
      "loss": 0.8827,
      "step": 8350
    },
    {
      "epoch": 0.03928144726568012,
      "grad_norm": 3.7456207275390625,
      "learning_rate": 0.00019259667883108435,
      "loss": 0.6788,
      "step": 8351
    },
    {
      "epoch": 0.039286151067292586,
      "grad_norm": 0.95158851146698,
      "learning_rate": 0.00019259573585297086,
      "loss": 0.0794,
      "step": 8352
    },
    {
      "epoch": 0.03929085486890505,
      "grad_norm": 2.2548410892486572,
      "learning_rate": 0.00019259479287485738,
      "loss": 0.4025,
      "step": 8353
    },
    {
      "epoch": 0.03929555867051751,
      "grad_norm": 2.435377836227417,
      "learning_rate": 0.00019259384989674393,
      "loss": 0.2782,
      "step": 8354
    },
    {
      "epoch": 0.039300262472129976,
      "grad_norm": 2.9915950298309326,
      "learning_rate": 0.00019259290691863045,
      "loss": 0.4464,
      "step": 8355
    },
    {
      "epoch": 0.039304966273742435,
      "grad_norm": 2.512329578399658,
      "learning_rate": 0.00019259196394051694,
      "loss": 0.3659,
      "step": 8356
    },
    {
      "epoch": 0.0393096700753549,
      "grad_norm": 1.477439522743225,
      "learning_rate": 0.00019259102096240346,
      "loss": 0.1999,
      "step": 8357
    },
    {
      "epoch": 0.039314373876967366,
      "grad_norm": 1.908896803855896,
      "learning_rate": 0.00019259007798428998,
      "loss": 0.2625,
      "step": 8358
    },
    {
      "epoch": 0.039319077678579825,
      "grad_norm": 3.2195537090301514,
      "learning_rate": 0.00019258913500617652,
      "loss": 0.7073,
      "step": 8359
    },
    {
      "epoch": 0.03932378148019229,
      "grad_norm": 1.6023383140563965,
      "learning_rate": 0.00019258819202806304,
      "loss": 0.111,
      "step": 8360
    },
    {
      "epoch": 0.039328485281804756,
      "grad_norm": 0.5265437364578247,
      "learning_rate": 0.00019258724904994956,
      "loss": 0.0618,
      "step": 8361
    },
    {
      "epoch": 0.039333189083417215,
      "grad_norm": 1.2981789112091064,
      "learning_rate": 0.00019258630607183608,
      "loss": 0.2694,
      "step": 8362
    },
    {
      "epoch": 0.03933789288502968,
      "grad_norm": 2.0521297454833984,
      "learning_rate": 0.00019258536309372262,
      "loss": 0.4415,
      "step": 8363
    },
    {
      "epoch": 0.039342596686642146,
      "grad_norm": 1.088226318359375,
      "learning_rate": 0.00019258442011560914,
      "loss": 0.1271,
      "step": 8364
    },
    {
      "epoch": 0.039347300488254605,
      "grad_norm": 2.5727174282073975,
      "learning_rate": 0.00019258347713749566,
      "loss": 0.3568,
      "step": 8365
    },
    {
      "epoch": 0.03935200428986707,
      "grad_norm": 1.4505144357681274,
      "learning_rate": 0.00019258253415938218,
      "loss": 0.2432,
      "step": 8366
    },
    {
      "epoch": 0.039356708091479536,
      "grad_norm": 0.3461467921733856,
      "learning_rate": 0.00019258159118126867,
      "loss": 0.0316,
      "step": 8367
    },
    {
      "epoch": 0.039361411893091995,
      "grad_norm": 1.1849844455718994,
      "learning_rate": 0.00019258064820315522,
      "loss": 0.2935,
      "step": 8368
    },
    {
      "epoch": 0.03936611569470446,
      "grad_norm": 3.736927032470703,
      "learning_rate": 0.00019257970522504174,
      "loss": 0.4467,
      "step": 8369
    },
    {
      "epoch": 0.039370819496316926,
      "grad_norm": 0.924959123134613,
      "learning_rate": 0.00019257876224692825,
      "loss": 0.2032,
      "step": 8370
    },
    {
      "epoch": 0.039375523297929385,
      "grad_norm": 1.7215089797973633,
      "learning_rate": 0.00019257781926881477,
      "loss": 0.4485,
      "step": 8371
    },
    {
      "epoch": 0.03938022709954185,
      "grad_norm": 1.007056713104248,
      "learning_rate": 0.0001925768762907013,
      "loss": 0.284,
      "step": 8372
    },
    {
      "epoch": 0.039384930901154316,
      "grad_norm": 1.1789802312850952,
      "learning_rate": 0.00019257593331258784,
      "loss": 0.1327,
      "step": 8373
    },
    {
      "epoch": 0.039389634702766775,
      "grad_norm": 1.947964072227478,
      "learning_rate": 0.00019257499033447436,
      "loss": 0.44,
      "step": 8374
    },
    {
      "epoch": 0.03939433850437924,
      "grad_norm": 1.125516414642334,
      "learning_rate": 0.00019257404735636087,
      "loss": 0.1512,
      "step": 8375
    },
    {
      "epoch": 0.0393990423059917,
      "grad_norm": 1.8802013397216797,
      "learning_rate": 0.0001925731043782474,
      "loss": 0.4453,
      "step": 8376
    },
    {
      "epoch": 0.039403746107604165,
      "grad_norm": 1.4159821271896362,
      "learning_rate": 0.0001925721614001339,
      "loss": 0.2205,
      "step": 8377
    },
    {
      "epoch": 0.03940844990921663,
      "grad_norm": 1.7629399299621582,
      "learning_rate": 0.00019257121842202043,
      "loss": 0.0971,
      "step": 8378
    },
    {
      "epoch": 0.03941315371082909,
      "grad_norm": 2.8608477115631104,
      "learning_rate": 0.00019257027544390695,
      "loss": 0.3145,
      "step": 8379
    },
    {
      "epoch": 0.039417857512441555,
      "grad_norm": 2.0679686069488525,
      "learning_rate": 0.00019256933246579347,
      "loss": 0.416,
      "step": 8380
    },
    {
      "epoch": 0.03942256131405402,
      "grad_norm": 0.9832369685173035,
      "learning_rate": 0.00019256838948767999,
      "loss": 0.1185,
      "step": 8381
    },
    {
      "epoch": 0.03942726511566648,
      "grad_norm": 1.8890303373336792,
      "learning_rate": 0.00019256744650956653,
      "loss": 0.2895,
      "step": 8382
    },
    {
      "epoch": 0.039431968917278945,
      "grad_norm": 0.6731769442558289,
      "learning_rate": 0.00019256650353145305,
      "loss": 0.0706,
      "step": 8383
    },
    {
      "epoch": 0.03943667271889141,
      "grad_norm": 3.743239641189575,
      "learning_rate": 0.00019256556055333957,
      "loss": 0.3362,
      "step": 8384
    },
    {
      "epoch": 0.03944137652050387,
      "grad_norm": 0.7446731925010681,
      "learning_rate": 0.0001925646175752261,
      "loss": 0.1138,
      "step": 8385
    },
    {
      "epoch": 0.039446080322116335,
      "grad_norm": 1.1809810400009155,
      "learning_rate": 0.00019256367459711263,
      "loss": 0.2635,
      "step": 8386
    },
    {
      "epoch": 0.0394507841237288,
      "grad_norm": 2.234651565551758,
      "learning_rate": 0.00019256273161899912,
      "loss": 0.3316,
      "step": 8387
    },
    {
      "epoch": 0.03945548792534126,
      "grad_norm": 3.47369122505188,
      "learning_rate": 0.00019256178864088564,
      "loss": 0.5389,
      "step": 8388
    },
    {
      "epoch": 0.039460191726953725,
      "grad_norm": 0.9133585691452026,
      "learning_rate": 0.00019256084566277216,
      "loss": 0.1659,
      "step": 8389
    },
    {
      "epoch": 0.03946489552856619,
      "grad_norm": 2.3618836402893066,
      "learning_rate": 0.00019255990268465868,
      "loss": 0.2888,
      "step": 8390
    },
    {
      "epoch": 0.03946959933017865,
      "grad_norm": 3.0370655059814453,
      "learning_rate": 0.00019255895970654523,
      "loss": 0.7646,
      "step": 8391
    },
    {
      "epoch": 0.039474303131791115,
      "grad_norm": 1.1618832349777222,
      "learning_rate": 0.00019255801672843175,
      "loss": 0.1428,
      "step": 8392
    },
    {
      "epoch": 0.039479006933403574,
      "grad_norm": 0.5935649871826172,
      "learning_rate": 0.00019255707375031826,
      "loss": 0.0705,
      "step": 8393
    },
    {
      "epoch": 0.03948371073501604,
      "grad_norm": 1.8200161457061768,
      "learning_rate": 0.00019255613077220478,
      "loss": 0.4632,
      "step": 8394
    },
    {
      "epoch": 0.039488414536628505,
      "grad_norm": 0.6888193488121033,
      "learning_rate": 0.00019255518779409133,
      "loss": 0.078,
      "step": 8395
    },
    {
      "epoch": 0.039493118338240964,
      "grad_norm": 3.8087830543518066,
      "learning_rate": 0.00019255424481597785,
      "loss": 0.9851,
      "step": 8396
    },
    {
      "epoch": 0.03949782213985343,
      "grad_norm": 0.9298715591430664,
      "learning_rate": 0.00019255330183786437,
      "loss": 0.0945,
      "step": 8397
    },
    {
      "epoch": 0.039502525941465895,
      "grad_norm": 0.8306034803390503,
      "learning_rate": 0.00019255235885975086,
      "loss": 0.0903,
      "step": 8398
    },
    {
      "epoch": 0.039507229743078354,
      "grad_norm": 1.2242519855499268,
      "learning_rate": 0.00019255141588163738,
      "loss": 0.1817,
      "step": 8399
    },
    {
      "epoch": 0.03951193354469082,
      "grad_norm": 1.5099906921386719,
      "learning_rate": 0.00019255047290352392,
      "loss": 0.2163,
      "step": 8400
    },
    {
      "epoch": 0.039516637346303285,
      "grad_norm": 1.3517026901245117,
      "learning_rate": 0.00019254952992541044,
      "loss": 0.1326,
      "step": 8401
    },
    {
      "epoch": 0.039521341147915744,
      "grad_norm": 1.5120102167129517,
      "learning_rate": 0.00019254858694729696,
      "loss": 0.1421,
      "step": 8402
    },
    {
      "epoch": 0.03952604494952821,
      "grad_norm": 1.8452759981155396,
      "learning_rate": 0.00019254764396918348,
      "loss": 0.2923,
      "step": 8403
    },
    {
      "epoch": 0.039530748751140675,
      "grad_norm": 1.1786611080169678,
      "learning_rate": 0.00019254670099107002,
      "loss": 0.2126,
      "step": 8404
    },
    {
      "epoch": 0.039535452552753134,
      "grad_norm": 1.6856340169906616,
      "learning_rate": 0.00019254575801295654,
      "loss": 0.1522,
      "step": 8405
    },
    {
      "epoch": 0.0395401563543656,
      "grad_norm": 7.092334747314453,
      "learning_rate": 0.00019254481503484306,
      "loss": 0.6449,
      "step": 8406
    },
    {
      "epoch": 0.039544860155978065,
      "grad_norm": 0.9310954213142395,
      "learning_rate": 0.00019254387205672958,
      "loss": 0.0604,
      "step": 8407
    },
    {
      "epoch": 0.039549563957590524,
      "grad_norm": 3.248211622238159,
      "learning_rate": 0.0001925429290786161,
      "loss": 0.6263,
      "step": 8408
    },
    {
      "epoch": 0.03955426775920299,
      "grad_norm": 2.3232457637786865,
      "learning_rate": 0.00019254198610050262,
      "loss": 0.1771,
      "step": 8409
    },
    {
      "epoch": 0.03955897156081545,
      "grad_norm": 3.0780930519104004,
      "learning_rate": 0.00019254104312238914,
      "loss": 0.3898,
      "step": 8410
    },
    {
      "epoch": 0.039563675362427914,
      "grad_norm": 0.9314543008804321,
      "learning_rate": 0.00019254010014427565,
      "loss": 0.0977,
      "step": 8411
    },
    {
      "epoch": 0.03956837916404038,
      "grad_norm": 0.9902701377868652,
      "learning_rate": 0.00019253915716616217,
      "loss": 0.1057,
      "step": 8412
    },
    {
      "epoch": 0.03957308296565284,
      "grad_norm": 0.48700109124183655,
      "learning_rate": 0.00019253821418804872,
      "loss": 0.0504,
      "step": 8413
    },
    {
      "epoch": 0.039577786767265304,
      "grad_norm": 2.0909347534179688,
      "learning_rate": 0.00019253727120993524,
      "loss": 0.2718,
      "step": 8414
    },
    {
      "epoch": 0.03958249056887777,
      "grad_norm": 1.2883856296539307,
      "learning_rate": 0.00019253632823182176,
      "loss": 0.1519,
      "step": 8415
    },
    {
      "epoch": 0.03958719437049023,
      "grad_norm": 1.141886591911316,
      "learning_rate": 0.00019253538525370827,
      "loss": 0.128,
      "step": 8416
    },
    {
      "epoch": 0.039591898172102694,
      "grad_norm": 1.8577121496200562,
      "learning_rate": 0.0001925344422755948,
      "loss": 0.2629,
      "step": 8417
    },
    {
      "epoch": 0.03959660197371516,
      "grad_norm": 0.20344239473342896,
      "learning_rate": 0.0001925334992974813,
      "loss": 0.0199,
      "step": 8418
    },
    {
      "epoch": 0.03960130577532762,
      "grad_norm": 2.3246970176696777,
      "learning_rate": 0.00019253255631936783,
      "loss": 0.2279,
      "step": 8419
    },
    {
      "epoch": 0.039606009576940084,
      "grad_norm": 1.0333032608032227,
      "learning_rate": 0.00019253161334125435,
      "loss": 0.0647,
      "step": 8420
    },
    {
      "epoch": 0.03961071337855255,
      "grad_norm": 1.4486998319625854,
      "learning_rate": 0.00019253067036314087,
      "loss": 0.1852,
      "step": 8421
    },
    {
      "epoch": 0.03961541718016501,
      "grad_norm": 0.8082633018493652,
      "learning_rate": 0.00019252972738502739,
      "loss": 0.1121,
      "step": 8422
    },
    {
      "epoch": 0.039620120981777474,
      "grad_norm": 0.9758472442626953,
      "learning_rate": 0.00019252878440691393,
      "loss": 0.0999,
      "step": 8423
    },
    {
      "epoch": 0.03962482478338994,
      "grad_norm": 2.7748453617095947,
      "learning_rate": 0.00019252784142880045,
      "loss": 0.3325,
      "step": 8424
    },
    {
      "epoch": 0.0396295285850024,
      "grad_norm": 0.4079236388206482,
      "learning_rate": 0.00019252689845068697,
      "loss": 0.0296,
      "step": 8425
    },
    {
      "epoch": 0.039634232386614864,
      "grad_norm": 3.451627016067505,
      "learning_rate": 0.0001925259554725735,
      "loss": 0.6763,
      "step": 8426
    },
    {
      "epoch": 0.03963893618822732,
      "grad_norm": 3.936542510986328,
      "learning_rate": 0.00019252501249446003,
      "loss": 0.7434,
      "step": 8427
    },
    {
      "epoch": 0.03964363998983979,
      "grad_norm": 1.2369023561477661,
      "learning_rate": 0.00019252406951634655,
      "loss": 0.0859,
      "step": 8428
    },
    {
      "epoch": 0.039648343791452254,
      "grad_norm": 3.7418384552001953,
      "learning_rate": 0.00019252312653823304,
      "loss": 0.6527,
      "step": 8429
    },
    {
      "epoch": 0.03965304759306471,
      "grad_norm": 2.5916104316711426,
      "learning_rate": 0.00019252218356011956,
      "loss": 0.2945,
      "step": 8430
    },
    {
      "epoch": 0.03965775139467718,
      "grad_norm": 1.438329815864563,
      "learning_rate": 0.00019252124058200608,
      "loss": 0.188,
      "step": 8431
    },
    {
      "epoch": 0.039662455196289643,
      "grad_norm": 0.7055890560150146,
      "learning_rate": 0.00019252029760389263,
      "loss": 0.0726,
      "step": 8432
    },
    {
      "epoch": 0.0396671589979021,
      "grad_norm": 0.7272335886955261,
      "learning_rate": 0.00019251935462577915,
      "loss": 0.0643,
      "step": 8433
    },
    {
      "epoch": 0.03967186279951457,
      "grad_norm": 1.955968976020813,
      "learning_rate": 0.00019251841164766566,
      "loss": 0.092,
      "step": 8434
    },
    {
      "epoch": 0.039676566601127033,
      "grad_norm": 2.2538843154907227,
      "learning_rate": 0.00019251746866955218,
      "loss": 0.1512,
      "step": 8435
    },
    {
      "epoch": 0.03968127040273949,
      "grad_norm": 1.141589879989624,
      "learning_rate": 0.00019251652569143873,
      "loss": 0.1402,
      "step": 8436
    },
    {
      "epoch": 0.03968597420435196,
      "grad_norm": 1.097933053970337,
      "learning_rate": 0.00019251558271332525,
      "loss": 0.0869,
      "step": 8437
    },
    {
      "epoch": 0.03969067800596442,
      "grad_norm": 0.43279942870140076,
      "learning_rate": 0.00019251463973521177,
      "loss": 0.0308,
      "step": 8438
    },
    {
      "epoch": 0.03969538180757688,
      "grad_norm": 5.199399948120117,
      "learning_rate": 0.00019251369675709828,
      "loss": 1.1051,
      "step": 8439
    },
    {
      "epoch": 0.03970008560918935,
      "grad_norm": 0.6975630521774292,
      "learning_rate": 0.00019251275377898478,
      "loss": 0.043,
      "step": 8440
    },
    {
      "epoch": 0.03970478941080181,
      "grad_norm": 4.144469738006592,
      "learning_rate": 0.00019251181080087132,
      "loss": 0.3965,
      "step": 8441
    },
    {
      "epoch": 0.03970949321241427,
      "grad_norm": 4.103987216949463,
      "learning_rate": 0.00019251086782275784,
      "loss": 0.4881,
      "step": 8442
    },
    {
      "epoch": 0.03971419701402674,
      "grad_norm": 2.8657023906707764,
      "learning_rate": 0.00019250992484464436,
      "loss": 0.644,
      "step": 8443
    },
    {
      "epoch": 0.039718900815639196,
      "grad_norm": 2.0105137825012207,
      "learning_rate": 0.00019250898186653088,
      "loss": 0.2934,
      "step": 8444
    },
    {
      "epoch": 0.03972360461725166,
      "grad_norm": 0.9325845837593079,
      "learning_rate": 0.00019250803888841742,
      "loss": 0.0647,
      "step": 8445
    },
    {
      "epoch": 0.03972830841886413,
      "grad_norm": 2.2320451736450195,
      "learning_rate": 0.00019250709591030394,
      "loss": 0.3838,
      "step": 8446
    },
    {
      "epoch": 0.039733012220476586,
      "grad_norm": 0.7168571352958679,
      "learning_rate": 0.00019250615293219046,
      "loss": 0.0476,
      "step": 8447
    },
    {
      "epoch": 0.03973771602208905,
      "grad_norm": 1.0454075336456299,
      "learning_rate": 0.00019250520995407698,
      "loss": 0.1261,
      "step": 8448
    },
    {
      "epoch": 0.03974241982370152,
      "grad_norm": 6.583555698394775,
      "learning_rate": 0.0001925042669759635,
      "loss": 0.5554,
      "step": 8449
    },
    {
      "epoch": 0.039747123625313976,
      "grad_norm": 3.0792014598846436,
      "learning_rate": 0.00019250332399785002,
      "loss": 0.4622,
      "step": 8450
    },
    {
      "epoch": 0.03975182742692644,
      "grad_norm": 2.3323328495025635,
      "learning_rate": 0.00019250238101973654,
      "loss": 0.1141,
      "step": 8451
    },
    {
      "epoch": 0.03975653122853891,
      "grad_norm": 6.0024943351745605,
      "learning_rate": 0.00019250143804162305,
      "loss": 0.8519,
      "step": 8452
    },
    {
      "epoch": 0.039761235030151366,
      "grad_norm": 0.5943161845207214,
      "learning_rate": 0.00019250049506350957,
      "loss": 0.0358,
      "step": 8453
    },
    {
      "epoch": 0.03976593883176383,
      "grad_norm": 3.6241118907928467,
      "learning_rate": 0.00019249955208539612,
      "loss": 0.626,
      "step": 8454
    },
    {
      "epoch": 0.0397706426333763,
      "grad_norm": 5.851267337799072,
      "learning_rate": 0.00019249860910728264,
      "loss": 0.5842,
      "step": 8455
    },
    {
      "epoch": 0.039775346434988756,
      "grad_norm": 6.355515956878662,
      "learning_rate": 0.00019249766612916916,
      "loss": 0.605,
      "step": 8456
    },
    {
      "epoch": 0.03978005023660122,
      "grad_norm": 1.186300277709961,
      "learning_rate": 0.00019249672315105567,
      "loss": 0.0801,
      "step": 8457
    },
    {
      "epoch": 0.03978475403821369,
      "grad_norm": 1.2394704818725586,
      "learning_rate": 0.0001924957801729422,
      "loss": 0.1148,
      "step": 8458
    },
    {
      "epoch": 0.039789457839826146,
      "grad_norm": 1.460675835609436,
      "learning_rate": 0.00019249483719482874,
      "loss": 0.1647,
      "step": 8459
    },
    {
      "epoch": 0.03979416164143861,
      "grad_norm": 1.0920305252075195,
      "learning_rate": 0.00019249389421671523,
      "loss": 0.1536,
      "step": 8460
    },
    {
      "epoch": 0.03979886544305107,
      "grad_norm": 2.2010397911071777,
      "learning_rate": 0.00019249295123860175,
      "loss": 0.3767,
      "step": 8461
    },
    {
      "epoch": 0.039803569244663536,
      "grad_norm": 2.2684519290924072,
      "learning_rate": 0.00019249200826048827,
      "loss": 0.2318,
      "step": 8462
    },
    {
      "epoch": 0.039808273046276,
      "grad_norm": 1.2133982181549072,
      "learning_rate": 0.0001924910652823748,
      "loss": 0.1736,
      "step": 8463
    },
    {
      "epoch": 0.03981297684788846,
      "grad_norm": 0.8066197037696838,
      "learning_rate": 0.00019249012230426133,
      "loss": 0.0774,
      "step": 8464
    },
    {
      "epoch": 0.039817680649500926,
      "grad_norm": 2.7504637241363525,
      "learning_rate": 0.00019248917932614785,
      "loss": 0.4232,
      "step": 8465
    },
    {
      "epoch": 0.03982238445111339,
      "grad_norm": 9.96021556854248,
      "learning_rate": 0.00019248823634803437,
      "loss": 0.1494,
      "step": 8466
    },
    {
      "epoch": 0.03982708825272585,
      "grad_norm": 1.87837815284729,
      "learning_rate": 0.0001924872933699209,
      "loss": 0.1954,
      "step": 8467
    },
    {
      "epoch": 0.039831792054338316,
      "grad_norm": 2.9759063720703125,
      "learning_rate": 0.00019248635039180743,
      "loss": 0.6894,
      "step": 8468
    },
    {
      "epoch": 0.03983649585595078,
      "grad_norm": 0.46272316575050354,
      "learning_rate": 0.00019248540741369395,
      "loss": 0.048,
      "step": 8469
    },
    {
      "epoch": 0.03984119965756324,
      "grad_norm": 6.072597503662109,
      "learning_rate": 0.00019248446443558047,
      "loss": 0.1585,
      "step": 8470
    },
    {
      "epoch": 0.039845903459175706,
      "grad_norm": 1.8908228874206543,
      "learning_rate": 0.00019248352145746696,
      "loss": 0.2344,
      "step": 8471
    },
    {
      "epoch": 0.03985060726078817,
      "grad_norm": 1.3425347805023193,
      "learning_rate": 0.00019248257847935348,
      "loss": 0.0863,
      "step": 8472
    },
    {
      "epoch": 0.03985531106240063,
      "grad_norm": 2.0850045680999756,
      "learning_rate": 0.00019248163550124003,
      "loss": 0.1937,
      "step": 8473
    },
    {
      "epoch": 0.039860014864013096,
      "grad_norm": 2.1941752433776855,
      "learning_rate": 0.00019248069252312655,
      "loss": 0.4224,
      "step": 8474
    },
    {
      "epoch": 0.03986471866562556,
      "grad_norm": 2.136333703994751,
      "learning_rate": 0.00019247974954501306,
      "loss": 0.1682,
      "step": 8475
    },
    {
      "epoch": 0.03986942246723802,
      "grad_norm": 2.5628271102905273,
      "learning_rate": 0.00019247880656689958,
      "loss": 0.4024,
      "step": 8476
    },
    {
      "epoch": 0.039874126268850486,
      "grad_norm": 3.439603567123413,
      "learning_rate": 0.00019247786358878613,
      "loss": 0.3919,
      "step": 8477
    },
    {
      "epoch": 0.039878830070462945,
      "grad_norm": 2.9017581939697266,
      "learning_rate": 0.00019247692061067265,
      "loss": 0.2992,
      "step": 8478
    },
    {
      "epoch": 0.03988353387207541,
      "grad_norm": 2.2869179248809814,
      "learning_rate": 0.00019247597763255917,
      "loss": 0.4177,
      "step": 8479
    },
    {
      "epoch": 0.039888237673687876,
      "grad_norm": 1.1432291269302368,
      "learning_rate": 0.00019247503465444568,
      "loss": 0.3243,
      "step": 8480
    },
    {
      "epoch": 0.039892941475300335,
      "grad_norm": 1.9563649892807007,
      "learning_rate": 0.0001924740916763322,
      "loss": 0.1652,
      "step": 8481
    },
    {
      "epoch": 0.0398976452769128,
      "grad_norm": 0.6340417861938477,
      "learning_rate": 0.00019247314869821872,
      "loss": 0.0568,
      "step": 8482
    },
    {
      "epoch": 0.039902349078525266,
      "grad_norm": 3.21822452545166,
      "learning_rate": 0.00019247220572010524,
      "loss": 0.2181,
      "step": 8483
    },
    {
      "epoch": 0.039907052880137725,
      "grad_norm": 2.0071001052856445,
      "learning_rate": 0.00019247126274199176,
      "loss": 0.5743,
      "step": 8484
    },
    {
      "epoch": 0.03991175668175019,
      "grad_norm": 0.8388619422912598,
      "learning_rate": 0.00019247031976387828,
      "loss": 0.0848,
      "step": 8485
    },
    {
      "epoch": 0.039916460483362656,
      "grad_norm": 1.3209388256072998,
      "learning_rate": 0.00019246937678576482,
      "loss": 0.2011,
      "step": 8486
    },
    {
      "epoch": 0.039921164284975115,
      "grad_norm": 6.0503010749816895,
      "learning_rate": 0.00019246843380765134,
      "loss": 0.3625,
      "step": 8487
    },
    {
      "epoch": 0.03992586808658758,
      "grad_norm": 1.8859554529190063,
      "learning_rate": 0.00019246749082953786,
      "loss": 0.1346,
      "step": 8488
    },
    {
      "epoch": 0.039930571888200046,
      "grad_norm": 5.930914878845215,
      "learning_rate": 0.00019246654785142438,
      "loss": 0.8534,
      "step": 8489
    },
    {
      "epoch": 0.039935275689812505,
      "grad_norm": 2.9049296379089355,
      "learning_rate": 0.0001924656048733109,
      "loss": 0.697,
      "step": 8490
    },
    {
      "epoch": 0.03993997949142497,
      "grad_norm": 1.245950698852539,
      "learning_rate": 0.00019246466189519742,
      "loss": 0.1711,
      "step": 8491
    },
    {
      "epoch": 0.039944683293037436,
      "grad_norm": 2.535877227783203,
      "learning_rate": 0.00019246371891708394,
      "loss": 0.4165,
      "step": 8492
    },
    {
      "epoch": 0.039949387094649895,
      "grad_norm": 2.7129485607147217,
      "learning_rate": 0.00019246277593897045,
      "loss": 0.4005,
      "step": 8493
    },
    {
      "epoch": 0.03995409089626236,
      "grad_norm": 2.527815341949463,
      "learning_rate": 0.00019246183296085697,
      "loss": 0.309,
      "step": 8494
    },
    {
      "epoch": 0.03995879469787482,
      "grad_norm": 2.028165578842163,
      "learning_rate": 0.00019246088998274352,
      "loss": 0.3315,
      "step": 8495
    },
    {
      "epoch": 0.039963498499487285,
      "grad_norm": 2.6440789699554443,
      "learning_rate": 0.00019245994700463004,
      "loss": 0.8233,
      "step": 8496
    },
    {
      "epoch": 0.03996820230109975,
      "grad_norm": 0.5250505208969116,
      "learning_rate": 0.00019245900402651656,
      "loss": 0.0643,
      "step": 8497
    },
    {
      "epoch": 0.03997290610271221,
      "grad_norm": 1.0169618129730225,
      "learning_rate": 0.00019245806104840307,
      "loss": 0.0928,
      "step": 8498
    },
    {
      "epoch": 0.039977609904324675,
      "grad_norm": 4.708625316619873,
      "learning_rate": 0.0001924571180702896,
      "loss": 0.7383,
      "step": 8499
    },
    {
      "epoch": 0.03998231370593714,
      "grad_norm": 1.407179355621338,
      "learning_rate": 0.00019245617509217614,
      "loss": 0.3995,
      "step": 8500
    },
    {
      "epoch": 0.0399870175075496,
      "grad_norm": 1.5506726503372192,
      "learning_rate": 0.00019245523211406266,
      "loss": 0.1776,
      "step": 8501
    },
    {
      "epoch": 0.039991721309162065,
      "grad_norm": 0.5874794721603394,
      "learning_rate": 0.00019245428913594915,
      "loss": 0.0415,
      "step": 8502
    },
    {
      "epoch": 0.03999642511077453,
      "grad_norm": 5.8656086921691895,
      "learning_rate": 0.00019245334615783567,
      "loss": 1.313,
      "step": 8503
    },
    {
      "epoch": 0.04000112891238699,
      "grad_norm": 4.819539546966553,
      "learning_rate": 0.0001924524031797222,
      "loss": 1.0106,
      "step": 8504
    },
    {
      "epoch": 0.040005832713999455,
      "grad_norm": 4.728869438171387,
      "learning_rate": 0.00019245146020160873,
      "loss": 0.348,
      "step": 8505
    },
    {
      "epoch": 0.04001053651561192,
      "grad_norm": 1.4352085590362549,
      "learning_rate": 0.00019245051722349525,
      "loss": 0.1486,
      "step": 8506
    },
    {
      "epoch": 0.04001524031722438,
      "grad_norm": 4.099085807800293,
      "learning_rate": 0.00019244957424538177,
      "loss": 0.7569,
      "step": 8507
    },
    {
      "epoch": 0.040019944118836845,
      "grad_norm": 2.900683641433716,
      "learning_rate": 0.0001924486312672683,
      "loss": 0.3724,
      "step": 8508
    },
    {
      "epoch": 0.04002464792044931,
      "grad_norm": 0.6841486692428589,
      "learning_rate": 0.00019244768828915483,
      "loss": 0.0627,
      "step": 8509
    },
    {
      "epoch": 0.04002935172206177,
      "grad_norm": 2.3771543502807617,
      "learning_rate": 0.00019244674531104135,
      "loss": 0.2881,
      "step": 8510
    },
    {
      "epoch": 0.040034055523674235,
      "grad_norm": 1.7796592712402344,
      "learning_rate": 0.00019244580233292787,
      "loss": 0.2687,
      "step": 8511
    },
    {
      "epoch": 0.040038759325286694,
      "grad_norm": 1.8362890481948853,
      "learning_rate": 0.0001924448593548144,
      "loss": 0.2279,
      "step": 8512
    },
    {
      "epoch": 0.04004346312689916,
      "grad_norm": 2.5128896236419678,
      "learning_rate": 0.0001924439163767009,
      "loss": 0.356,
      "step": 8513
    },
    {
      "epoch": 0.040048166928511625,
      "grad_norm": 1.2239720821380615,
      "learning_rate": 0.00019244297339858743,
      "loss": 0.1061,
      "step": 8514
    },
    {
      "epoch": 0.040052870730124084,
      "grad_norm": 2.1528358459472656,
      "learning_rate": 0.00019244203042047395,
      "loss": 0.3683,
      "step": 8515
    },
    {
      "epoch": 0.04005757453173655,
      "grad_norm": 2.0147149562835693,
      "learning_rate": 0.00019244108744236046,
      "loss": 0.2706,
      "step": 8516
    },
    {
      "epoch": 0.040062278333349015,
      "grad_norm": 0.8406578302383423,
      "learning_rate": 0.00019244014446424698,
      "loss": 0.0809,
      "step": 8517
    },
    {
      "epoch": 0.040066982134961474,
      "grad_norm": 3.2306761741638184,
      "learning_rate": 0.00019243920148613353,
      "loss": 0.5077,
      "step": 8518
    },
    {
      "epoch": 0.04007168593657394,
      "grad_norm": 1.1226800680160522,
      "learning_rate": 0.00019243825850802005,
      "loss": 0.2053,
      "step": 8519
    },
    {
      "epoch": 0.040076389738186405,
      "grad_norm": 2.092864751815796,
      "learning_rate": 0.00019243731552990657,
      "loss": 0.3489,
      "step": 8520
    },
    {
      "epoch": 0.040081093539798864,
      "grad_norm": 1.060040831565857,
      "learning_rate": 0.00019243637255179308,
      "loss": 0.1278,
      "step": 8521
    },
    {
      "epoch": 0.04008579734141133,
      "grad_norm": 2.0265860557556152,
      "learning_rate": 0.0001924354295736796,
      "loss": 0.3547,
      "step": 8522
    },
    {
      "epoch": 0.040090501143023795,
      "grad_norm": 4.0470380783081055,
      "learning_rate": 0.00019243448659556612,
      "loss": 0.8001,
      "step": 8523
    },
    {
      "epoch": 0.040095204944636254,
      "grad_norm": 1.125756025314331,
      "learning_rate": 0.00019243354361745264,
      "loss": 0.1638,
      "step": 8524
    },
    {
      "epoch": 0.04009990874624872,
      "grad_norm": 1.8942949771881104,
      "learning_rate": 0.00019243260063933916,
      "loss": 0.4075,
      "step": 8525
    },
    {
      "epoch": 0.040104612547861185,
      "grad_norm": 1.5931836366653442,
      "learning_rate": 0.00019243165766122568,
      "loss": 0.193,
      "step": 8526
    },
    {
      "epoch": 0.040109316349473643,
      "grad_norm": 1.894547462463379,
      "learning_rate": 0.00019243071468311222,
      "loss": 0.4365,
      "step": 8527
    },
    {
      "epoch": 0.04011402015108611,
      "grad_norm": 1.9658194780349731,
      "learning_rate": 0.00019242977170499874,
      "loss": 0.2942,
      "step": 8528
    },
    {
      "epoch": 0.04011872395269857,
      "grad_norm": 0.6535565257072449,
      "learning_rate": 0.00019242882872688526,
      "loss": 0.0905,
      "step": 8529
    },
    {
      "epoch": 0.040123427754311033,
      "grad_norm": 1.3420500755310059,
      "learning_rate": 0.00019242788574877178,
      "loss": 0.2016,
      "step": 8530
    },
    {
      "epoch": 0.0401281315559235,
      "grad_norm": 3.9125239849090576,
      "learning_rate": 0.0001924269427706583,
      "loss": 0.3945,
      "step": 8531
    },
    {
      "epoch": 0.04013283535753596,
      "grad_norm": 3.240635395050049,
      "learning_rate": 0.00019242599979254484,
      "loss": 0.5238,
      "step": 8532
    },
    {
      "epoch": 0.04013753915914842,
      "grad_norm": 5.296669006347656,
      "learning_rate": 0.00019242505681443134,
      "loss": 0.6771,
      "step": 8533
    },
    {
      "epoch": 0.04014224296076089,
      "grad_norm": 1.5673383474349976,
      "learning_rate": 0.00019242411383631785,
      "loss": 0.3475,
      "step": 8534
    },
    {
      "epoch": 0.04014694676237335,
      "grad_norm": 4.463374614715576,
      "learning_rate": 0.00019242317085820437,
      "loss": 0.2297,
      "step": 8535
    },
    {
      "epoch": 0.04015165056398581,
      "grad_norm": 1.1439791917800903,
      "learning_rate": 0.00019242222788009092,
      "loss": 0.167,
      "step": 8536
    },
    {
      "epoch": 0.04015635436559828,
      "grad_norm": 1.4192830324172974,
      "learning_rate": 0.00019242128490197744,
      "loss": 0.1803,
      "step": 8537
    },
    {
      "epoch": 0.04016105816721074,
      "grad_norm": 2.929356813430786,
      "learning_rate": 0.00019242034192386396,
      "loss": 0.4512,
      "step": 8538
    },
    {
      "epoch": 0.0401657619688232,
      "grad_norm": 0.8178502321243286,
      "learning_rate": 0.00019241939894575047,
      "loss": 0.0844,
      "step": 8539
    },
    {
      "epoch": 0.04017046577043567,
      "grad_norm": 2.1079952716827393,
      "learning_rate": 0.000192418455967637,
      "loss": 0.2079,
      "step": 8540
    },
    {
      "epoch": 0.04017516957204813,
      "grad_norm": 0.9469717144966125,
      "learning_rate": 0.00019241751298952354,
      "loss": 0.1114,
      "step": 8541
    },
    {
      "epoch": 0.04017987337366059,
      "grad_norm": 1.060713529586792,
      "learning_rate": 0.00019241657001141006,
      "loss": 0.1532,
      "step": 8542
    },
    {
      "epoch": 0.04018457717527306,
      "grad_norm": 1.9266865253448486,
      "learning_rate": 0.00019241562703329658,
      "loss": 0.4702,
      "step": 8543
    },
    {
      "epoch": 0.04018928097688552,
      "grad_norm": 1.1639987230300903,
      "learning_rate": 0.0001924146840551831,
      "loss": 0.1943,
      "step": 8544
    },
    {
      "epoch": 0.04019398477849798,
      "grad_norm": 4.224987506866455,
      "learning_rate": 0.0001924137410770696,
      "loss": 0.6332,
      "step": 8545
    },
    {
      "epoch": 0.04019868858011044,
      "grad_norm": 1.866158127784729,
      "learning_rate": 0.00019241279809895613,
      "loss": 0.1524,
      "step": 8546
    },
    {
      "epoch": 0.04020339238172291,
      "grad_norm": 2.286269426345825,
      "learning_rate": 0.00019241185512084265,
      "loss": 0.3765,
      "step": 8547
    },
    {
      "epoch": 0.04020809618333537,
      "grad_norm": 0.29207339882850647,
      "learning_rate": 0.00019241091214272917,
      "loss": 0.0214,
      "step": 8548
    },
    {
      "epoch": 0.04021279998494783,
      "grad_norm": 2.1443238258361816,
      "learning_rate": 0.0001924099691646157,
      "loss": 0.2812,
      "step": 8549
    },
    {
      "epoch": 0.0402175037865603,
      "grad_norm": 3.3666200637817383,
      "learning_rate": 0.00019240902618650223,
      "loss": 0.6823,
      "step": 8550
    },
    {
      "epoch": 0.04022220758817276,
      "grad_norm": 2.2846930027008057,
      "learning_rate": 0.00019240808320838875,
      "loss": 0.3434,
      "step": 8551
    },
    {
      "epoch": 0.04022691138978522,
      "grad_norm": 0.9147999286651611,
      "learning_rate": 0.00019240714023027527,
      "loss": 0.0743,
      "step": 8552
    },
    {
      "epoch": 0.04023161519139769,
      "grad_norm": 1.822290062904358,
      "learning_rate": 0.0001924061972521618,
      "loss": 0.2855,
      "step": 8553
    },
    {
      "epoch": 0.04023631899301015,
      "grad_norm": 3.3277828693389893,
      "learning_rate": 0.0001924052542740483,
      "loss": 0.5298,
      "step": 8554
    },
    {
      "epoch": 0.04024102279462261,
      "grad_norm": 5.2334136962890625,
      "learning_rate": 0.00019240431129593483,
      "loss": 0.6761,
      "step": 8555
    },
    {
      "epoch": 0.04024572659623508,
      "grad_norm": 2.637190341949463,
      "learning_rate": 0.00019240336831782135,
      "loss": 0.3612,
      "step": 8556
    },
    {
      "epoch": 0.04025043039784754,
      "grad_norm": 1.156858205795288,
      "learning_rate": 0.00019240242533970786,
      "loss": 0.1296,
      "step": 8557
    },
    {
      "epoch": 0.04025513419946,
      "grad_norm": 0.6705741882324219,
      "learning_rate": 0.00019240148236159438,
      "loss": 0.0766,
      "step": 8558
    },
    {
      "epoch": 0.04025983800107247,
      "grad_norm": 2.72248911857605,
      "learning_rate": 0.00019240053938348093,
      "loss": 0.3033,
      "step": 8559
    },
    {
      "epoch": 0.04026454180268493,
      "grad_norm": 0.950799822807312,
      "learning_rate": 0.00019239959640536745,
      "loss": 0.1318,
      "step": 8560
    },
    {
      "epoch": 0.04026924560429739,
      "grad_norm": 1.5978060960769653,
      "learning_rate": 0.00019239865342725397,
      "loss": 0.1754,
      "step": 8561
    },
    {
      "epoch": 0.04027394940590986,
      "grad_norm": 3.5657081604003906,
      "learning_rate": 0.00019239771044914048,
      "loss": 0.4185,
      "step": 8562
    },
    {
      "epoch": 0.040278653207522316,
      "grad_norm": 0.13736452162265778,
      "learning_rate": 0.00019239676747102703,
      "loss": 0.0086,
      "step": 8563
    },
    {
      "epoch": 0.04028335700913478,
      "grad_norm": 1.4804750680923462,
      "learning_rate": 0.00019239582449291352,
      "loss": 0.1429,
      "step": 8564
    },
    {
      "epoch": 0.04028806081074725,
      "grad_norm": 2.396620273590088,
      "learning_rate": 0.00019239488151480004,
      "loss": 0.4655,
      "step": 8565
    },
    {
      "epoch": 0.040292764612359706,
      "grad_norm": 0.9701055288314819,
      "learning_rate": 0.00019239393853668656,
      "loss": 0.0806,
      "step": 8566
    },
    {
      "epoch": 0.04029746841397217,
      "grad_norm": 1.1890285015106201,
      "learning_rate": 0.00019239299555857308,
      "loss": 0.1021,
      "step": 8567
    },
    {
      "epoch": 0.04030217221558464,
      "grad_norm": 4.5913262367248535,
      "learning_rate": 0.00019239205258045962,
      "loss": 0.6325,
      "step": 8568
    },
    {
      "epoch": 0.040306876017197096,
      "grad_norm": 5.526054859161377,
      "learning_rate": 0.00019239110960234614,
      "loss": 0.8372,
      "step": 8569
    },
    {
      "epoch": 0.04031157981880956,
      "grad_norm": 0.6501899361610413,
      "learning_rate": 0.00019239016662423266,
      "loss": 0.0329,
      "step": 8570
    },
    {
      "epoch": 0.04031628362042203,
      "grad_norm": 2.330857276916504,
      "learning_rate": 0.00019238922364611918,
      "loss": 0.2933,
      "step": 8571
    },
    {
      "epoch": 0.040320987422034486,
      "grad_norm": 1.6972883939743042,
      "learning_rate": 0.0001923882806680057,
      "loss": 0.2827,
      "step": 8572
    },
    {
      "epoch": 0.04032569122364695,
      "grad_norm": 1.388602375984192,
      "learning_rate": 0.00019238733768989224,
      "loss": 0.1733,
      "step": 8573
    },
    {
      "epoch": 0.04033039502525942,
      "grad_norm": 5.248016357421875,
      "learning_rate": 0.00019238639471177876,
      "loss": 0.8428,
      "step": 8574
    },
    {
      "epoch": 0.040335098826871876,
      "grad_norm": 0.3165748715400696,
      "learning_rate": 0.00019238545173366528,
      "loss": 0.0295,
      "step": 8575
    },
    {
      "epoch": 0.04033980262848434,
      "grad_norm": 0.6701427102088928,
      "learning_rate": 0.00019238450875555177,
      "loss": 0.0828,
      "step": 8576
    },
    {
      "epoch": 0.04034450643009681,
      "grad_norm": 3.3012735843658447,
      "learning_rate": 0.00019238356577743832,
      "loss": 0.3791,
      "step": 8577
    },
    {
      "epoch": 0.040349210231709266,
      "grad_norm": 3.006913185119629,
      "learning_rate": 0.00019238262279932484,
      "loss": 0.197,
      "step": 8578
    },
    {
      "epoch": 0.04035391403332173,
      "grad_norm": 1.664878249168396,
      "learning_rate": 0.00019238167982121136,
      "loss": 0.1818,
      "step": 8579
    },
    {
      "epoch": 0.04035861783493419,
      "grad_norm": 1.02957022190094,
      "learning_rate": 0.00019238073684309787,
      "loss": 0.0647,
      "step": 8580
    },
    {
      "epoch": 0.040363321636546656,
      "grad_norm": 1.52695894241333,
      "learning_rate": 0.0001923797938649844,
      "loss": 0.1463,
      "step": 8581
    },
    {
      "epoch": 0.04036802543815912,
      "grad_norm": 0.4398084580898285,
      "learning_rate": 0.00019237885088687094,
      "loss": 0.0272,
      "step": 8582
    },
    {
      "epoch": 0.04037272923977158,
      "grad_norm": 2.4275693893432617,
      "learning_rate": 0.00019237790790875746,
      "loss": 0.2026,
      "step": 8583
    },
    {
      "epoch": 0.040377433041384046,
      "grad_norm": 3.7321159839630127,
      "learning_rate": 0.00019237696493064398,
      "loss": 0.8378,
      "step": 8584
    },
    {
      "epoch": 0.04038213684299651,
      "grad_norm": 2.1763479709625244,
      "learning_rate": 0.0001923760219525305,
      "loss": 0.2893,
      "step": 8585
    },
    {
      "epoch": 0.04038684064460897,
      "grad_norm": 2.4146101474761963,
      "learning_rate": 0.000192375078974417,
      "loss": 0.1785,
      "step": 8586
    },
    {
      "epoch": 0.040391544446221436,
      "grad_norm": 1.1316254138946533,
      "learning_rate": 0.00019237413599630353,
      "loss": 0.0709,
      "step": 8587
    },
    {
      "epoch": 0.0403962482478339,
      "grad_norm": 8.74500560760498,
      "learning_rate": 0.00019237319301819005,
      "loss": 0.5121,
      "step": 8588
    },
    {
      "epoch": 0.04040095204944636,
      "grad_norm": 0.3675036132335663,
      "learning_rate": 0.00019237225004007657,
      "loss": 0.0336,
      "step": 8589
    },
    {
      "epoch": 0.040405655851058826,
      "grad_norm": 3.145693302154541,
      "learning_rate": 0.0001923713070619631,
      "loss": 0.6845,
      "step": 8590
    },
    {
      "epoch": 0.04041035965267129,
      "grad_norm": 1.9615315198898315,
      "learning_rate": 0.00019237036408384963,
      "loss": 0.1555,
      "step": 8591
    },
    {
      "epoch": 0.04041506345428375,
      "grad_norm": 3.7143290042877197,
      "learning_rate": 0.00019236942110573615,
      "loss": 0.494,
      "step": 8592
    },
    {
      "epoch": 0.040419767255896216,
      "grad_norm": 0.48627790808677673,
      "learning_rate": 0.00019236847812762267,
      "loss": 0.0323,
      "step": 8593
    },
    {
      "epoch": 0.04042447105750868,
      "grad_norm": 0.5537363290786743,
      "learning_rate": 0.0001923675351495092,
      "loss": 0.0273,
      "step": 8594
    },
    {
      "epoch": 0.04042917485912114,
      "grad_norm": 2.2202260494232178,
      "learning_rate": 0.0001923665921713957,
      "loss": 0.5488,
      "step": 8595
    },
    {
      "epoch": 0.040433878660733606,
      "grad_norm": 1.957729697227478,
      "learning_rate": 0.00019236564919328223,
      "loss": 0.3425,
      "step": 8596
    },
    {
      "epoch": 0.040438582462346065,
      "grad_norm": 0.6682220101356506,
      "learning_rate": 0.00019236470621516875,
      "loss": 0.0845,
      "step": 8597
    },
    {
      "epoch": 0.04044328626395853,
      "grad_norm": 0.9679937362670898,
      "learning_rate": 0.00019236376323705526,
      "loss": 0.1013,
      "step": 8598
    },
    {
      "epoch": 0.040447990065570996,
      "grad_norm": 0.781504213809967,
      "learning_rate": 0.00019236282025894178,
      "loss": 0.0601,
      "step": 8599
    },
    {
      "epoch": 0.040452693867183455,
      "grad_norm": 0.5090154409408569,
      "learning_rate": 0.00019236187728082833,
      "loss": 0.0654,
      "step": 8600
    },
    {
      "epoch": 0.04045739766879592,
      "grad_norm": 2.6796109676361084,
      "learning_rate": 0.00019236093430271485,
      "loss": 0.2829,
      "step": 8601
    },
    {
      "epoch": 0.040462101470408386,
      "grad_norm": 2.106985569000244,
      "learning_rate": 0.00019235999132460137,
      "loss": 0.1721,
      "step": 8602
    },
    {
      "epoch": 0.040466805272020845,
      "grad_norm": 3.7029244899749756,
      "learning_rate": 0.00019235904834648788,
      "loss": 0.4803,
      "step": 8603
    },
    {
      "epoch": 0.04047150907363331,
      "grad_norm": 1.9516396522521973,
      "learning_rate": 0.00019235810536837443,
      "loss": 0.1213,
      "step": 8604
    },
    {
      "epoch": 0.040476212875245776,
      "grad_norm": 0.4680396616458893,
      "learning_rate": 0.00019235716239026095,
      "loss": 0.0597,
      "step": 8605
    },
    {
      "epoch": 0.040480916676858235,
      "grad_norm": 3.0084025859832764,
      "learning_rate": 0.00019235621941214747,
      "loss": 0.2629,
      "step": 8606
    },
    {
      "epoch": 0.0404856204784707,
      "grad_norm": 1.30913507938385,
      "learning_rate": 0.00019235527643403396,
      "loss": 0.2361,
      "step": 8607
    },
    {
      "epoch": 0.040490324280083166,
      "grad_norm": 2.885072708129883,
      "learning_rate": 0.00019235433345592048,
      "loss": 0.5953,
      "step": 8608
    },
    {
      "epoch": 0.040495028081695625,
      "grad_norm": 1.729387879371643,
      "learning_rate": 0.00019235339047780702,
      "loss": 0.0965,
      "step": 8609
    },
    {
      "epoch": 0.04049973188330809,
      "grad_norm": 1.930640697479248,
      "learning_rate": 0.00019235244749969354,
      "loss": 0.1363,
      "step": 8610
    },
    {
      "epoch": 0.040504435684920556,
      "grad_norm": 0.8703027963638306,
      "learning_rate": 0.00019235150452158006,
      "loss": 0.079,
      "step": 8611
    },
    {
      "epoch": 0.040509139486533015,
      "grad_norm": 0.6577091813087463,
      "learning_rate": 0.00019235056154346658,
      "loss": 0.0418,
      "step": 8612
    },
    {
      "epoch": 0.04051384328814548,
      "grad_norm": 3.274676561355591,
      "learning_rate": 0.00019234961856535312,
      "loss": 0.3562,
      "step": 8613
    },
    {
      "epoch": 0.04051854708975794,
      "grad_norm": 5.986823081970215,
      "learning_rate": 0.00019234867558723964,
      "loss": 0.4474,
      "step": 8614
    },
    {
      "epoch": 0.040523250891370405,
      "grad_norm": 10.45773983001709,
      "learning_rate": 0.00019234773260912616,
      "loss": 0.8313,
      "step": 8615
    },
    {
      "epoch": 0.04052795469298287,
      "grad_norm": 1.7139989137649536,
      "learning_rate": 0.00019234678963101268,
      "loss": 0.2661,
      "step": 8616
    },
    {
      "epoch": 0.04053265849459533,
      "grad_norm": 4.083181858062744,
      "learning_rate": 0.0001923458466528992,
      "loss": 0.5376,
      "step": 8617
    },
    {
      "epoch": 0.040537362296207795,
      "grad_norm": 3.1836202144622803,
      "learning_rate": 0.00019234490367478572,
      "loss": 0.4336,
      "step": 8618
    },
    {
      "epoch": 0.04054206609782026,
      "grad_norm": 2.0764060020446777,
      "learning_rate": 0.00019234396069667224,
      "loss": 0.3467,
      "step": 8619
    },
    {
      "epoch": 0.04054676989943272,
      "grad_norm": 1.848124623298645,
      "learning_rate": 0.00019234301771855876,
      "loss": 0.2156,
      "step": 8620
    },
    {
      "epoch": 0.040551473701045185,
      "grad_norm": 2.148836851119995,
      "learning_rate": 0.00019234207474044527,
      "loss": 0.1849,
      "step": 8621
    },
    {
      "epoch": 0.04055617750265765,
      "grad_norm": 2.4940335750579834,
      "learning_rate": 0.00019234113176233182,
      "loss": 0.55,
      "step": 8622
    },
    {
      "epoch": 0.04056088130427011,
      "grad_norm": 1.4888405799865723,
      "learning_rate": 0.00019234018878421834,
      "loss": 0.1377,
      "step": 8623
    },
    {
      "epoch": 0.040565585105882575,
      "grad_norm": 1.9804893732070923,
      "learning_rate": 0.00019233924580610486,
      "loss": 0.2706,
      "step": 8624
    },
    {
      "epoch": 0.04057028890749504,
      "grad_norm": 0.6845867037773132,
      "learning_rate": 0.00019233830282799138,
      "loss": 0.0653,
      "step": 8625
    },
    {
      "epoch": 0.0405749927091075,
      "grad_norm": 3.5122644901275635,
      "learning_rate": 0.0001923373598498779,
      "loss": 0.3923,
      "step": 8626
    },
    {
      "epoch": 0.040579696510719965,
      "grad_norm": 2.2936441898345947,
      "learning_rate": 0.0001923364168717644,
      "loss": 0.3062,
      "step": 8627
    },
    {
      "epoch": 0.04058440031233243,
      "grad_norm": 1.750501036643982,
      "learning_rate": 0.00019233547389365093,
      "loss": 0.168,
      "step": 8628
    },
    {
      "epoch": 0.04058910411394489,
      "grad_norm": 2.9899752140045166,
      "learning_rate": 0.00019233453091553745,
      "loss": 0.4354,
      "step": 8629
    },
    {
      "epoch": 0.040593807915557355,
      "grad_norm": 4.063110828399658,
      "learning_rate": 0.00019233358793742397,
      "loss": 1.0935,
      "step": 8630
    },
    {
      "epoch": 0.04059851171716981,
      "grad_norm": 5.416355133056641,
      "learning_rate": 0.0001923326449593105,
      "loss": 1.1135,
      "step": 8631
    },
    {
      "epoch": 0.04060321551878228,
      "grad_norm": 0.6683500409126282,
      "learning_rate": 0.00019233170198119703,
      "loss": 0.0503,
      "step": 8632
    },
    {
      "epoch": 0.040607919320394745,
      "grad_norm": 0.859964907169342,
      "learning_rate": 0.00019233075900308355,
      "loss": 0.0906,
      "step": 8633
    },
    {
      "epoch": 0.0406126231220072,
      "grad_norm": 0.7174992561340332,
      "learning_rate": 0.00019232981602497007,
      "loss": 0.0602,
      "step": 8634
    },
    {
      "epoch": 0.04061732692361967,
      "grad_norm": 1.136413812637329,
      "learning_rate": 0.0001923288730468566,
      "loss": 0.1717,
      "step": 8635
    },
    {
      "epoch": 0.040622030725232135,
      "grad_norm": 2.0348258018493652,
      "learning_rate": 0.00019232793006874313,
      "loss": 0.355,
      "step": 8636
    },
    {
      "epoch": 0.04062673452684459,
      "grad_norm": 0.38400959968566895,
      "learning_rate": 0.00019232698709062965,
      "loss": 0.0314,
      "step": 8637
    },
    {
      "epoch": 0.04063143832845706,
      "grad_norm": 2.3205175399780273,
      "learning_rate": 0.00019232604411251615,
      "loss": 0.3444,
      "step": 8638
    },
    {
      "epoch": 0.040636142130069525,
      "grad_norm": 1.2071055173873901,
      "learning_rate": 0.00019232510113440266,
      "loss": 0.0837,
      "step": 8639
    },
    {
      "epoch": 0.04064084593168198,
      "grad_norm": 1.705337643623352,
      "learning_rate": 0.00019232415815628918,
      "loss": 0.2107,
      "step": 8640
    },
    {
      "epoch": 0.04064554973329445,
      "grad_norm": 1.301374912261963,
      "learning_rate": 0.00019232321517817573,
      "loss": 0.2218,
      "step": 8641
    },
    {
      "epoch": 0.040650253534906915,
      "grad_norm": 3.6495492458343506,
      "learning_rate": 0.00019232227220006225,
      "loss": 0.7006,
      "step": 8642
    },
    {
      "epoch": 0.04065495733651937,
      "grad_norm": 0.854912519454956,
      "learning_rate": 0.00019232132922194877,
      "loss": 0.0887,
      "step": 8643
    },
    {
      "epoch": 0.04065966113813184,
      "grad_norm": 2.0799307823181152,
      "learning_rate": 0.00019232038624383528,
      "loss": 0.4269,
      "step": 8644
    },
    {
      "epoch": 0.040664364939744305,
      "grad_norm": 0.8929483890533447,
      "learning_rate": 0.00019231944326572183,
      "loss": 0.071,
      "step": 8645
    },
    {
      "epoch": 0.04066906874135676,
      "grad_norm": 4.037342548370361,
      "learning_rate": 0.00019231850028760835,
      "loss": 0.832,
      "step": 8646
    },
    {
      "epoch": 0.04067377254296923,
      "grad_norm": 1.7242664098739624,
      "learning_rate": 0.00019231755730949487,
      "loss": 0.339,
      "step": 8647
    },
    {
      "epoch": 0.04067847634458169,
      "grad_norm": 1.0268933773040771,
      "learning_rate": 0.00019231661433138139,
      "loss": 0.0769,
      "step": 8648
    },
    {
      "epoch": 0.04068318014619415,
      "grad_norm": 4.040668964385986,
      "learning_rate": 0.00019231567135326788,
      "loss": 0.2913,
      "step": 8649
    },
    {
      "epoch": 0.04068788394780662,
      "grad_norm": 3.1993722915649414,
      "learning_rate": 0.00019231472837515442,
      "loss": 0.9034,
      "step": 8650
    },
    {
      "epoch": 0.04069258774941908,
      "grad_norm": 0.02448333613574505,
      "learning_rate": 0.00019231378539704094,
      "loss": 0.0008,
      "step": 8651
    },
    {
      "epoch": 0.04069729155103154,
      "grad_norm": 9.825760841369629,
      "learning_rate": 0.00019231284241892746,
      "loss": 0.5524,
      "step": 8652
    },
    {
      "epoch": 0.04070199535264401,
      "grad_norm": 1.7805536985397339,
      "learning_rate": 0.00019231189944081398,
      "loss": 0.1274,
      "step": 8653
    },
    {
      "epoch": 0.04070669915425647,
      "grad_norm": 2.182295799255371,
      "learning_rate": 0.00019231095646270052,
      "loss": 0.2036,
      "step": 8654
    },
    {
      "epoch": 0.04071140295586893,
      "grad_norm": 1.2863965034484863,
      "learning_rate": 0.00019231001348458704,
      "loss": 0.174,
      "step": 8655
    },
    {
      "epoch": 0.0407161067574814,
      "grad_norm": 3.3966751098632812,
      "learning_rate": 0.00019230907050647356,
      "loss": 0.3669,
      "step": 8656
    },
    {
      "epoch": 0.04072081055909386,
      "grad_norm": 2.0286664962768555,
      "learning_rate": 0.00019230812752836008,
      "loss": 0.3353,
      "step": 8657
    },
    {
      "epoch": 0.04072551436070632,
      "grad_norm": 0.6712675094604492,
      "learning_rate": 0.0001923071845502466,
      "loss": 0.0554,
      "step": 8658
    },
    {
      "epoch": 0.04073021816231879,
      "grad_norm": 1.3706551790237427,
      "learning_rate": 0.00019230624157213312,
      "loss": 0.1667,
      "step": 8659
    },
    {
      "epoch": 0.04073492196393125,
      "grad_norm": 1.3780200481414795,
      "learning_rate": 0.00019230529859401964,
      "loss": 0.1187,
      "step": 8660
    },
    {
      "epoch": 0.04073962576554371,
      "grad_norm": 2.978424310684204,
      "learning_rate": 0.00019230435561590616,
      "loss": 0.2801,
      "step": 8661
    },
    {
      "epoch": 0.04074432956715618,
      "grad_norm": 0.23857466876506805,
      "learning_rate": 0.00019230341263779267,
      "loss": 0.0149,
      "step": 8662
    },
    {
      "epoch": 0.04074903336876864,
      "grad_norm": 0.5351563692092896,
      "learning_rate": 0.00019230246965967922,
      "loss": 0.0635,
      "step": 8663
    },
    {
      "epoch": 0.0407537371703811,
      "grad_norm": 1.1213139295578003,
      "learning_rate": 0.00019230152668156574,
      "loss": 0.1484,
      "step": 8664
    },
    {
      "epoch": 0.04075844097199356,
      "grad_norm": 1.5797119140625,
      "learning_rate": 0.00019230058370345226,
      "loss": 0.2213,
      "step": 8665
    },
    {
      "epoch": 0.04076314477360603,
      "grad_norm": 3.315645456314087,
      "learning_rate": 0.00019229964072533878,
      "loss": 0.5852,
      "step": 8666
    },
    {
      "epoch": 0.04076784857521849,
      "grad_norm": 1.2226462364196777,
      "learning_rate": 0.0001922986977472253,
      "loss": 0.0928,
      "step": 8667
    },
    {
      "epoch": 0.04077255237683095,
      "grad_norm": 2.0433735847473145,
      "learning_rate": 0.0001922977547691118,
      "loss": 0.4627,
      "step": 8668
    },
    {
      "epoch": 0.04077725617844342,
      "grad_norm": 2.9713525772094727,
      "learning_rate": 0.00019229681179099833,
      "loss": 0.4782,
      "step": 8669
    },
    {
      "epoch": 0.04078195998005588,
      "grad_norm": 2.2385826110839844,
      "learning_rate": 0.00019229586881288485,
      "loss": 0.3696,
      "step": 8670
    },
    {
      "epoch": 0.04078666378166834,
      "grad_norm": 1.9836324453353882,
      "learning_rate": 0.00019229492583477137,
      "loss": 0.3541,
      "step": 8671
    },
    {
      "epoch": 0.04079136758328081,
      "grad_norm": 1.1099028587341309,
      "learning_rate": 0.00019229398285665791,
      "loss": 0.0994,
      "step": 8672
    },
    {
      "epoch": 0.04079607138489327,
      "grad_norm": 3.7970550060272217,
      "learning_rate": 0.00019229303987854443,
      "loss": 0.1578,
      "step": 8673
    },
    {
      "epoch": 0.04080077518650573,
      "grad_norm": 5.3348164558410645,
      "learning_rate": 0.00019229209690043095,
      "loss": 1.0935,
      "step": 8674
    },
    {
      "epoch": 0.0408054789881182,
      "grad_norm": 0.8805802464485168,
      "learning_rate": 0.00019229115392231747,
      "loss": 0.122,
      "step": 8675
    },
    {
      "epoch": 0.04081018278973066,
      "grad_norm": 1.1127160787582397,
      "learning_rate": 0.000192290210944204,
      "loss": 0.0627,
      "step": 8676
    },
    {
      "epoch": 0.04081488659134312,
      "grad_norm": 0.9956444501876831,
      "learning_rate": 0.00019228926796609053,
      "loss": 0.1583,
      "step": 8677
    },
    {
      "epoch": 0.04081959039295559,
      "grad_norm": 0.9671980738639832,
      "learning_rate": 0.00019228832498797705,
      "loss": 0.1535,
      "step": 8678
    },
    {
      "epoch": 0.04082429419456805,
      "grad_norm": 0.992807924747467,
      "learning_rate": 0.00019228738200986357,
      "loss": 0.183,
      "step": 8679
    },
    {
      "epoch": 0.04082899799618051,
      "grad_norm": 1.3248499631881714,
      "learning_rate": 0.00019228643903175006,
      "loss": 0.2384,
      "step": 8680
    },
    {
      "epoch": 0.04083370179779298,
      "grad_norm": 1.0947626829147339,
      "learning_rate": 0.00019228549605363658,
      "loss": 0.2664,
      "step": 8681
    },
    {
      "epoch": 0.040838405599405436,
      "grad_norm": 0.5764153003692627,
      "learning_rate": 0.00019228455307552313,
      "loss": 0.048,
      "step": 8682
    },
    {
      "epoch": 0.0408431094010179,
      "grad_norm": 2.333996295928955,
      "learning_rate": 0.00019228361009740965,
      "loss": 0.3046,
      "step": 8683
    },
    {
      "epoch": 0.04084781320263037,
      "grad_norm": 2.171337366104126,
      "learning_rate": 0.00019228266711929617,
      "loss": 0.2019,
      "step": 8684
    },
    {
      "epoch": 0.040852517004242826,
      "grad_norm": 0.9338300824165344,
      "learning_rate": 0.00019228172414118268,
      "loss": 0.1097,
      "step": 8685
    },
    {
      "epoch": 0.04085722080585529,
      "grad_norm": 1.9028695821762085,
      "learning_rate": 0.00019228078116306923,
      "loss": 0.1884,
      "step": 8686
    },
    {
      "epoch": 0.04086192460746776,
      "grad_norm": 1.573479175567627,
      "learning_rate": 0.00019227983818495575,
      "loss": 0.1923,
      "step": 8687
    },
    {
      "epoch": 0.040866628409080216,
      "grad_norm": 3.0073952674865723,
      "learning_rate": 0.00019227889520684227,
      "loss": 0.685,
      "step": 8688
    },
    {
      "epoch": 0.04087133221069268,
      "grad_norm": 1.4072556495666504,
      "learning_rate": 0.00019227795222872879,
      "loss": 0.3076,
      "step": 8689
    },
    {
      "epoch": 0.04087603601230515,
      "grad_norm": 2.6285109519958496,
      "learning_rate": 0.0001922770092506153,
      "loss": 0.3301,
      "step": 8690
    },
    {
      "epoch": 0.040880739813917606,
      "grad_norm": 3.544612169265747,
      "learning_rate": 0.00019227606627250182,
      "loss": 0.3482,
      "step": 8691
    },
    {
      "epoch": 0.04088544361553007,
      "grad_norm": 2.604928970336914,
      "learning_rate": 0.00019227512329438834,
      "loss": 0.4349,
      "step": 8692
    },
    {
      "epoch": 0.04089014741714254,
      "grad_norm": 1.4328752756118774,
      "learning_rate": 0.00019227418031627486,
      "loss": 0.143,
      "step": 8693
    },
    {
      "epoch": 0.040894851218754996,
      "grad_norm": 2.0568277835845947,
      "learning_rate": 0.00019227323733816138,
      "loss": 0.3289,
      "step": 8694
    },
    {
      "epoch": 0.04089955502036746,
      "grad_norm": 4.283332347869873,
      "learning_rate": 0.00019227229436004792,
      "loss": 0.9617,
      "step": 8695
    },
    {
      "epoch": 0.04090425882197993,
      "grad_norm": 0.8163945078849792,
      "learning_rate": 0.00019227135138193444,
      "loss": 0.0679,
      "step": 8696
    },
    {
      "epoch": 0.040908962623592386,
      "grad_norm": 2.1149840354919434,
      "learning_rate": 0.00019227040840382096,
      "loss": 0.3089,
      "step": 8697
    },
    {
      "epoch": 0.04091366642520485,
      "grad_norm": 2.6252951622009277,
      "learning_rate": 0.00019226946542570748,
      "loss": 0.4137,
      "step": 8698
    },
    {
      "epoch": 0.04091837022681731,
      "grad_norm": 2.8582239151000977,
      "learning_rate": 0.000192268522447594,
      "loss": 0.3543,
      "step": 8699
    },
    {
      "epoch": 0.040923074028429776,
      "grad_norm": 1.695859432220459,
      "learning_rate": 0.00019226757946948052,
      "loss": 0.5185,
      "step": 8700
    },
    {
      "epoch": 0.04092777783004224,
      "grad_norm": 2.2186074256896973,
      "learning_rate": 0.00019226663649136704,
      "loss": 0.1807,
      "step": 8701
    },
    {
      "epoch": 0.0409324816316547,
      "grad_norm": 7.703974723815918,
      "learning_rate": 0.00019226569351325356,
      "loss": 0.7474,
      "step": 8702
    },
    {
      "epoch": 0.040937185433267166,
      "grad_norm": 1.2908021211624146,
      "learning_rate": 0.00019226475053514007,
      "loss": 0.1514,
      "step": 8703
    },
    {
      "epoch": 0.04094188923487963,
      "grad_norm": 4.1995849609375,
      "learning_rate": 0.00019226380755702662,
      "loss": 0.3595,
      "step": 8704
    },
    {
      "epoch": 0.04094659303649209,
      "grad_norm": 1.8846780061721802,
      "learning_rate": 0.00019226286457891314,
      "loss": 0.3057,
      "step": 8705
    },
    {
      "epoch": 0.040951296838104556,
      "grad_norm": 4.689868450164795,
      "learning_rate": 0.00019226192160079966,
      "loss": 0.6719,
      "step": 8706
    },
    {
      "epoch": 0.04095600063971702,
      "grad_norm": 1.049634337425232,
      "learning_rate": 0.00019226097862268618,
      "loss": 0.1531,
      "step": 8707
    },
    {
      "epoch": 0.04096070444132948,
      "grad_norm": 2.6683032512664795,
      "learning_rate": 0.0001922600356445727,
      "loss": 0.1122,
      "step": 8708
    },
    {
      "epoch": 0.040965408242941946,
      "grad_norm": 2.8377647399902344,
      "learning_rate": 0.00019225909266645924,
      "loss": 0.1829,
      "step": 8709
    },
    {
      "epoch": 0.04097011204455441,
      "grad_norm": 1.567319631576538,
      "learning_rate": 0.00019225814968834576,
      "loss": 0.194,
      "step": 8710
    },
    {
      "epoch": 0.04097481584616687,
      "grad_norm": 2.563037872314453,
      "learning_rate": 0.00019225720671023225,
      "loss": 0.4535,
      "step": 8711
    },
    {
      "epoch": 0.040979519647779336,
      "grad_norm": 0.5330788493156433,
      "learning_rate": 0.00019225626373211877,
      "loss": 0.0843,
      "step": 8712
    },
    {
      "epoch": 0.0409842234493918,
      "grad_norm": 1.9304938316345215,
      "learning_rate": 0.00019225532075400531,
      "loss": 0.2388,
      "step": 8713
    },
    {
      "epoch": 0.04098892725100426,
      "grad_norm": 2.0175416469573975,
      "learning_rate": 0.00019225437777589183,
      "loss": 0.4664,
      "step": 8714
    },
    {
      "epoch": 0.040993631052616726,
      "grad_norm": 2.0443944931030273,
      "learning_rate": 0.00019225343479777835,
      "loss": 0.2453,
      "step": 8715
    },
    {
      "epoch": 0.040998334854229185,
      "grad_norm": 1.127183437347412,
      "learning_rate": 0.00019225249181966487,
      "loss": 0.128,
      "step": 8716
    },
    {
      "epoch": 0.04100303865584165,
      "grad_norm": 2.6274900436401367,
      "learning_rate": 0.0001922515488415514,
      "loss": 0.4013,
      "step": 8717
    },
    {
      "epoch": 0.041007742457454116,
      "grad_norm": 1.94554603099823,
      "learning_rate": 0.00019225060586343793,
      "loss": 0.2507,
      "step": 8718
    },
    {
      "epoch": 0.041012446259066575,
      "grad_norm": 0.7725791931152344,
      "learning_rate": 0.00019224966288532445,
      "loss": 0.1492,
      "step": 8719
    },
    {
      "epoch": 0.04101715006067904,
      "grad_norm": 1.314498782157898,
      "learning_rate": 0.00019224871990721097,
      "loss": 0.0941,
      "step": 8720
    },
    {
      "epoch": 0.041021853862291506,
      "grad_norm": 0.8122530579566956,
      "learning_rate": 0.0001922477769290975,
      "loss": 0.0808,
      "step": 8721
    },
    {
      "epoch": 0.041026557663903965,
      "grad_norm": 3.306661605834961,
      "learning_rate": 0.000192246833950984,
      "loss": 0.5918,
      "step": 8722
    },
    {
      "epoch": 0.04103126146551643,
      "grad_norm": 1.9676519632339478,
      "learning_rate": 0.00019224589097287053,
      "loss": 0.1835,
      "step": 8723
    },
    {
      "epoch": 0.041035965267128896,
      "grad_norm": 0.3845830261707306,
      "learning_rate": 0.00019224494799475705,
      "loss": 0.0318,
      "step": 8724
    },
    {
      "epoch": 0.041040669068741355,
      "grad_norm": 0.3010634183883667,
      "learning_rate": 0.00019224400501664357,
      "loss": 0.0285,
      "step": 8725
    },
    {
      "epoch": 0.04104537287035382,
      "grad_norm": 1.6922756433486938,
      "learning_rate": 0.00019224306203853008,
      "loss": 0.191,
      "step": 8726
    },
    {
      "epoch": 0.041050076671966286,
      "grad_norm": 1.259914517402649,
      "learning_rate": 0.00019224211906041663,
      "loss": 0.1341,
      "step": 8727
    },
    {
      "epoch": 0.041054780473578745,
      "grad_norm": 3.859379291534424,
      "learning_rate": 0.00019224117608230315,
      "loss": 0.4673,
      "step": 8728
    },
    {
      "epoch": 0.04105948427519121,
      "grad_norm": 1.7788320779800415,
      "learning_rate": 0.00019224023310418967,
      "loss": 0.18,
      "step": 8729
    },
    {
      "epoch": 0.041064188076803676,
      "grad_norm": 1.3043025732040405,
      "learning_rate": 0.00019223929012607619,
      "loss": 0.1884,
      "step": 8730
    },
    {
      "epoch": 0.041068891878416135,
      "grad_norm": 0.8482586741447449,
      "learning_rate": 0.0001922383471479627,
      "loss": 0.0786,
      "step": 8731
    },
    {
      "epoch": 0.0410735956800286,
      "grad_norm": 1.9669352769851685,
      "learning_rate": 0.00019223740416984922,
      "loss": 0.4462,
      "step": 8732
    },
    {
      "epoch": 0.04107829948164106,
      "grad_norm": 2.368706226348877,
      "learning_rate": 0.00019223646119173574,
      "loss": 0.2009,
      "step": 8733
    },
    {
      "epoch": 0.041083003283253525,
      "grad_norm": 2.6774587631225586,
      "learning_rate": 0.00019223551821362226,
      "loss": 0.3122,
      "step": 8734
    },
    {
      "epoch": 0.04108770708486599,
      "grad_norm": 1.9752460718154907,
      "learning_rate": 0.00019223457523550878,
      "loss": 0.1888,
      "step": 8735
    },
    {
      "epoch": 0.04109241088647845,
      "grad_norm": 3.181703567504883,
      "learning_rate": 0.00019223363225739532,
      "loss": 0.535,
      "step": 8736
    },
    {
      "epoch": 0.041097114688090915,
      "grad_norm": 2.749133348464966,
      "learning_rate": 0.00019223268927928184,
      "loss": 0.2303,
      "step": 8737
    },
    {
      "epoch": 0.04110181848970338,
      "grad_norm": 0.9871482253074646,
      "learning_rate": 0.00019223174630116836,
      "loss": 0.0768,
      "step": 8738
    },
    {
      "epoch": 0.04110652229131584,
      "grad_norm": 4.673251628875732,
      "learning_rate": 0.00019223080332305488,
      "loss": 0.6768,
      "step": 8739
    },
    {
      "epoch": 0.041111226092928305,
      "grad_norm": 1.9699519872665405,
      "learning_rate": 0.0001922298603449414,
      "loss": 0.3318,
      "step": 8740
    },
    {
      "epoch": 0.04111592989454077,
      "grad_norm": 0.42851296067237854,
      "learning_rate": 0.00019222891736682794,
      "loss": 0.0293,
      "step": 8741
    },
    {
      "epoch": 0.04112063369615323,
      "grad_norm": 1.3040268421173096,
      "learning_rate": 0.00019222797438871444,
      "loss": 0.124,
      "step": 8742
    },
    {
      "epoch": 0.041125337497765695,
      "grad_norm": 1.7978980541229248,
      "learning_rate": 0.00019222703141060096,
      "loss": 0.1777,
      "step": 8743
    },
    {
      "epoch": 0.04113004129937816,
      "grad_norm": 3.251220226287842,
      "learning_rate": 0.00019222608843248747,
      "loss": 0.4564,
      "step": 8744
    },
    {
      "epoch": 0.04113474510099062,
      "grad_norm": 1.6124545335769653,
      "learning_rate": 0.00019222514545437402,
      "loss": 0.1552,
      "step": 8745
    },
    {
      "epoch": 0.041139448902603085,
      "grad_norm": 0.7483356595039368,
      "learning_rate": 0.00019222420247626054,
      "loss": 0.062,
      "step": 8746
    },
    {
      "epoch": 0.04114415270421555,
      "grad_norm": 3.5468575954437256,
      "learning_rate": 0.00019222325949814706,
      "loss": 0.5835,
      "step": 8747
    },
    {
      "epoch": 0.04114885650582801,
      "grad_norm": 1.6608396768569946,
      "learning_rate": 0.00019222231652003358,
      "loss": 0.2377,
      "step": 8748
    },
    {
      "epoch": 0.041153560307440475,
      "grad_norm": 1.255338430404663,
      "learning_rate": 0.0001922213735419201,
      "loss": 0.1503,
      "step": 8749
    },
    {
      "epoch": 0.04115826410905293,
      "grad_norm": 0.8083533048629761,
      "learning_rate": 0.00019222043056380664,
      "loss": 0.0909,
      "step": 8750
    },
    {
      "epoch": 0.0411629679106654,
      "grad_norm": 1.056355357170105,
      "learning_rate": 0.00019221948758569316,
      "loss": 0.0669,
      "step": 8751
    },
    {
      "epoch": 0.041167671712277865,
      "grad_norm": 1.5700385570526123,
      "learning_rate": 0.00019221854460757968,
      "loss": 0.084,
      "step": 8752
    },
    {
      "epoch": 0.04117237551389032,
      "grad_norm": 3.7443485260009766,
      "learning_rate": 0.00019221760162946617,
      "loss": 0.4296,
      "step": 8753
    },
    {
      "epoch": 0.04117707931550279,
      "grad_norm": 0.40534770488739014,
      "learning_rate": 0.00019221665865135271,
      "loss": 0.0309,
      "step": 8754
    },
    {
      "epoch": 0.041181783117115255,
      "grad_norm": 4.702043533325195,
      "learning_rate": 0.00019221571567323923,
      "loss": 0.6372,
      "step": 8755
    },
    {
      "epoch": 0.04118648691872771,
      "grad_norm": 5.294105529785156,
      "learning_rate": 0.00019221477269512575,
      "loss": 0.4166,
      "step": 8756
    },
    {
      "epoch": 0.04119119072034018,
      "grad_norm": 3.1775336265563965,
      "learning_rate": 0.00019221382971701227,
      "loss": 0.2519,
      "step": 8757
    },
    {
      "epoch": 0.041195894521952645,
      "grad_norm": 3.1424343585968018,
      "learning_rate": 0.0001922128867388988,
      "loss": 0.2985,
      "step": 8758
    },
    {
      "epoch": 0.0412005983235651,
      "grad_norm": 2.6130104064941406,
      "learning_rate": 0.00019221194376078533,
      "loss": 0.1684,
      "step": 8759
    },
    {
      "epoch": 0.04120530212517757,
      "grad_norm": 0.35424885153770447,
      "learning_rate": 0.00019221100078267185,
      "loss": 0.019,
      "step": 8760
    },
    {
      "epoch": 0.041210005926790035,
      "grad_norm": 0.8400391936302185,
      "learning_rate": 0.00019221005780455837,
      "loss": 0.0316,
      "step": 8761
    },
    {
      "epoch": 0.04121470972840249,
      "grad_norm": 3.814976215362549,
      "learning_rate": 0.0001922091148264449,
      "loss": 0.3041,
      "step": 8762
    },
    {
      "epoch": 0.04121941353001496,
      "grad_norm": 4.500102519989014,
      "learning_rate": 0.0001922081718483314,
      "loss": 0.2597,
      "step": 8763
    },
    {
      "epoch": 0.041224117331627425,
      "grad_norm": 3.2350564002990723,
      "learning_rate": 0.00019220722887021793,
      "loss": 0.3006,
      "step": 8764
    },
    {
      "epoch": 0.04122882113323988,
      "grad_norm": 3.1523091793060303,
      "learning_rate": 0.00019220628589210445,
      "loss": 0.255,
      "step": 8765
    },
    {
      "epoch": 0.04123352493485235,
      "grad_norm": 3.122248649597168,
      "learning_rate": 0.00019220534291399097,
      "loss": 0.2419,
      "step": 8766
    },
    {
      "epoch": 0.04123822873646481,
      "grad_norm": 0.7434117197990417,
      "learning_rate": 0.00019220439993587748,
      "loss": 0.0444,
      "step": 8767
    },
    {
      "epoch": 0.04124293253807727,
      "grad_norm": 4.4525532722473145,
      "learning_rate": 0.00019220345695776403,
      "loss": 0.6797,
      "step": 8768
    },
    {
      "epoch": 0.04124763633968974,
      "grad_norm": 2.7673375606536865,
      "learning_rate": 0.00019220251397965055,
      "loss": 0.3288,
      "step": 8769
    },
    {
      "epoch": 0.0412523401413022,
      "grad_norm": 2.261461019515991,
      "learning_rate": 0.00019220157100153707,
      "loss": 0.4158,
      "step": 8770
    },
    {
      "epoch": 0.04125704394291466,
      "grad_norm": 0.4611170291900635,
      "learning_rate": 0.00019220062802342359,
      "loss": 0.0314,
      "step": 8771
    },
    {
      "epoch": 0.04126174774452713,
      "grad_norm": 1.316996693611145,
      "learning_rate": 0.00019219968504531013,
      "loss": 0.1432,
      "step": 8772
    },
    {
      "epoch": 0.04126645154613959,
      "grad_norm": 3.623964309692383,
      "learning_rate": 0.00019219874206719662,
      "loss": 0.5462,
      "step": 8773
    },
    {
      "epoch": 0.04127115534775205,
      "grad_norm": 3.9764974117279053,
      "learning_rate": 0.00019219779908908314,
      "loss": 0.4188,
      "step": 8774
    },
    {
      "epoch": 0.04127585914936452,
      "grad_norm": 3.760171413421631,
      "learning_rate": 0.00019219685611096966,
      "loss": 0.3777,
      "step": 8775
    },
    {
      "epoch": 0.04128056295097698,
      "grad_norm": 1.4340147972106934,
      "learning_rate": 0.00019219591313285618,
      "loss": 0.0872,
      "step": 8776
    },
    {
      "epoch": 0.04128526675258944,
      "grad_norm": 1.4825519323349,
      "learning_rate": 0.00019219497015474272,
      "loss": 0.1702,
      "step": 8777
    },
    {
      "epoch": 0.04128997055420191,
      "grad_norm": 2.255316972732544,
      "learning_rate": 0.00019219402717662924,
      "loss": 0.3478,
      "step": 8778
    },
    {
      "epoch": 0.04129467435581437,
      "grad_norm": 6.690969467163086,
      "learning_rate": 0.00019219308419851576,
      "loss": 0.624,
      "step": 8779
    },
    {
      "epoch": 0.04129937815742683,
      "grad_norm": 4.577845096588135,
      "learning_rate": 0.00019219214122040228,
      "loss": 0.3882,
      "step": 8780
    },
    {
      "epoch": 0.0413040819590393,
      "grad_norm": 2.783108949661255,
      "learning_rate": 0.0001921911982422888,
      "loss": 0.4939,
      "step": 8781
    },
    {
      "epoch": 0.04130878576065176,
      "grad_norm": 1.8552249670028687,
      "learning_rate": 0.00019219025526417534,
      "loss": 0.2339,
      "step": 8782
    },
    {
      "epoch": 0.04131348956226422,
      "grad_norm": 3.459105968475342,
      "learning_rate": 0.00019218931228606186,
      "loss": 0.4312,
      "step": 8783
    },
    {
      "epoch": 0.04131819336387668,
      "grad_norm": 1.5136646032333374,
      "learning_rate": 0.00019218836930794836,
      "loss": 0.1173,
      "step": 8784
    },
    {
      "epoch": 0.04132289716548915,
      "grad_norm": 0.731908917427063,
      "learning_rate": 0.00019218742632983487,
      "loss": 0.0417,
      "step": 8785
    },
    {
      "epoch": 0.04132760096710161,
      "grad_norm": 2.1880438327789307,
      "learning_rate": 0.00019218648335172142,
      "loss": 0.4404,
      "step": 8786
    },
    {
      "epoch": 0.04133230476871407,
      "grad_norm": 3.464022636413574,
      "learning_rate": 0.00019218554037360794,
      "loss": 0.3039,
      "step": 8787
    },
    {
      "epoch": 0.04133700857032654,
      "grad_norm": 4.378656387329102,
      "learning_rate": 0.00019218459739549446,
      "loss": 0.5055,
      "step": 8788
    },
    {
      "epoch": 0.041341712371939,
      "grad_norm": 2.7777841091156006,
      "learning_rate": 0.00019218365441738098,
      "loss": 0.4024,
      "step": 8789
    },
    {
      "epoch": 0.04134641617355146,
      "grad_norm": 1.8736001253128052,
      "learning_rate": 0.0001921827114392675,
      "loss": 0.1715,
      "step": 8790
    },
    {
      "epoch": 0.04135111997516393,
      "grad_norm": 3.8089187145233154,
      "learning_rate": 0.00019218176846115404,
      "loss": 0.5279,
      "step": 8791
    },
    {
      "epoch": 0.04135582377677639,
      "grad_norm": 1.7708759307861328,
      "learning_rate": 0.00019218082548304056,
      "loss": 0.4347,
      "step": 8792
    },
    {
      "epoch": 0.04136052757838885,
      "grad_norm": 3.560356855392456,
      "learning_rate": 0.00019217988250492708,
      "loss": 0.3186,
      "step": 8793
    },
    {
      "epoch": 0.04136523138000132,
      "grad_norm": 1.4250682592391968,
      "learning_rate": 0.0001921789395268136,
      "loss": 0.1437,
      "step": 8794
    },
    {
      "epoch": 0.04136993518161378,
      "grad_norm": 1.4253244400024414,
      "learning_rate": 0.00019217799654870011,
      "loss": 0.1934,
      "step": 8795
    },
    {
      "epoch": 0.04137463898322624,
      "grad_norm": 4.02288818359375,
      "learning_rate": 0.00019217705357058663,
      "loss": 0.5261,
      "step": 8796
    },
    {
      "epoch": 0.04137934278483871,
      "grad_norm": 2.33172869682312,
      "learning_rate": 0.00019217611059247315,
      "loss": 0.3614,
      "step": 8797
    },
    {
      "epoch": 0.04138404658645117,
      "grad_norm": 2.7682533264160156,
      "learning_rate": 0.00019217516761435967,
      "loss": 0.5536,
      "step": 8798
    },
    {
      "epoch": 0.04138875038806363,
      "grad_norm": 2.112460136413574,
      "learning_rate": 0.0001921742246362462,
      "loss": 0.241,
      "step": 8799
    },
    {
      "epoch": 0.0413934541896761,
      "grad_norm": 1.7406445741653442,
      "learning_rate": 0.00019217328165813273,
      "loss": 0.421,
      "step": 8800
    },
    {
      "epoch": 0.041398157991288556,
      "grad_norm": 3.6634132862091064,
      "learning_rate": 0.00019217233868001925,
      "loss": 0.5273,
      "step": 8801
    },
    {
      "epoch": 0.04140286179290102,
      "grad_norm": 3.669147253036499,
      "learning_rate": 0.00019217139570190577,
      "loss": 0.4606,
      "step": 8802
    },
    {
      "epoch": 0.04140756559451349,
      "grad_norm": 4.360439300537109,
      "learning_rate": 0.0001921704527237923,
      "loss": 0.6149,
      "step": 8803
    },
    {
      "epoch": 0.041412269396125946,
      "grad_norm": 1.6877058744430542,
      "learning_rate": 0.0001921695097456788,
      "loss": 0.156,
      "step": 8804
    },
    {
      "epoch": 0.04141697319773841,
      "grad_norm": 2.5748531818389893,
      "learning_rate": 0.00019216856676756533,
      "loss": 0.2898,
      "step": 8805
    },
    {
      "epoch": 0.04142167699935088,
      "grad_norm": 0.5201444029808044,
      "learning_rate": 0.00019216762378945185,
      "loss": 0.0709,
      "step": 8806
    },
    {
      "epoch": 0.041426380800963336,
      "grad_norm": 2.8980910778045654,
      "learning_rate": 0.00019216668081133837,
      "loss": 0.2465,
      "step": 8807
    },
    {
      "epoch": 0.0414310846025758,
      "grad_norm": 1.034975290298462,
      "learning_rate": 0.00019216573783322488,
      "loss": 0.1051,
      "step": 8808
    },
    {
      "epoch": 0.04143578840418827,
      "grad_norm": 2.5329787731170654,
      "learning_rate": 0.00019216479485511143,
      "loss": 0.2262,
      "step": 8809
    },
    {
      "epoch": 0.041440492205800726,
      "grad_norm": 2.4458744525909424,
      "learning_rate": 0.00019216385187699795,
      "loss": 0.1752,
      "step": 8810
    },
    {
      "epoch": 0.04144519600741319,
      "grad_norm": 1.6648327112197876,
      "learning_rate": 0.00019216290889888447,
      "loss": 0.2558,
      "step": 8811
    },
    {
      "epoch": 0.04144989980902566,
      "grad_norm": 2.640026807785034,
      "learning_rate": 0.00019216196592077099,
      "loss": 0.4801,
      "step": 8812
    },
    {
      "epoch": 0.041454603610638116,
      "grad_norm": 1.2339063882827759,
      "learning_rate": 0.00019216102294265753,
      "loss": 0.1402,
      "step": 8813
    },
    {
      "epoch": 0.04145930741225058,
      "grad_norm": 0.5464963912963867,
      "learning_rate": 0.00019216007996454405,
      "loss": 0.0428,
      "step": 8814
    },
    {
      "epoch": 0.04146401121386305,
      "grad_norm": 1.5272138118743896,
      "learning_rate": 0.00019215913698643054,
      "loss": 0.1856,
      "step": 8815
    },
    {
      "epoch": 0.041468715015475506,
      "grad_norm": 7.956870079040527,
      "learning_rate": 0.00019215819400831706,
      "loss": 0.79,
      "step": 8816
    },
    {
      "epoch": 0.04147341881708797,
      "grad_norm": 2.1656923294067383,
      "learning_rate": 0.00019215725103020358,
      "loss": 0.2911,
      "step": 8817
    },
    {
      "epoch": 0.04147812261870043,
      "grad_norm": 1.930829405784607,
      "learning_rate": 0.00019215630805209012,
      "loss": 0.2435,
      "step": 8818
    },
    {
      "epoch": 0.041482826420312896,
      "grad_norm": 2.078213691711426,
      "learning_rate": 0.00019215536507397664,
      "loss": 0.2644,
      "step": 8819
    },
    {
      "epoch": 0.04148753022192536,
      "grad_norm": 2.3583974838256836,
      "learning_rate": 0.00019215442209586316,
      "loss": 0.5417,
      "step": 8820
    },
    {
      "epoch": 0.04149223402353782,
      "grad_norm": 1.5155847072601318,
      "learning_rate": 0.00019215347911774968,
      "loss": 0.2039,
      "step": 8821
    },
    {
      "epoch": 0.041496937825150286,
      "grad_norm": 2.3154900074005127,
      "learning_rate": 0.00019215253613963623,
      "loss": 0.2605,
      "step": 8822
    },
    {
      "epoch": 0.04150164162676275,
      "grad_norm": 0.7778782844543457,
      "learning_rate": 0.00019215159316152274,
      "loss": 0.0726,
      "step": 8823
    },
    {
      "epoch": 0.04150634542837521,
      "grad_norm": 0.8672562837600708,
      "learning_rate": 0.00019215065018340926,
      "loss": 0.0822,
      "step": 8824
    },
    {
      "epoch": 0.041511049229987676,
      "grad_norm": 3.114758253097534,
      "learning_rate": 0.00019214970720529578,
      "loss": 0.5327,
      "step": 8825
    },
    {
      "epoch": 0.04151575303160014,
      "grad_norm": 1.991539716720581,
      "learning_rate": 0.0001921487642271823,
      "loss": 0.2114,
      "step": 8826
    },
    {
      "epoch": 0.0415204568332126,
      "grad_norm": 1.0056664943695068,
      "learning_rate": 0.00019214782124906882,
      "loss": 0.1003,
      "step": 8827
    },
    {
      "epoch": 0.041525160634825066,
      "grad_norm": 3.135605812072754,
      "learning_rate": 0.00019214687827095534,
      "loss": 0.4403,
      "step": 8828
    },
    {
      "epoch": 0.04152986443643753,
      "grad_norm": 3.29382061958313,
      "learning_rate": 0.00019214593529284186,
      "loss": 0.1673,
      "step": 8829
    },
    {
      "epoch": 0.04153456823804999,
      "grad_norm": 0.9747568368911743,
      "learning_rate": 0.00019214499231472838,
      "loss": 0.1023,
      "step": 8830
    },
    {
      "epoch": 0.041539272039662456,
      "grad_norm": 0.5274363160133362,
      "learning_rate": 0.00019214404933661492,
      "loss": 0.0671,
      "step": 8831
    },
    {
      "epoch": 0.04154397584127492,
      "grad_norm": 5.57018518447876,
      "learning_rate": 0.00019214310635850144,
      "loss": 0.956,
      "step": 8832
    },
    {
      "epoch": 0.04154867964288738,
      "grad_norm": 1.6870890855789185,
      "learning_rate": 0.00019214216338038796,
      "loss": 0.2502,
      "step": 8833
    },
    {
      "epoch": 0.041553383444499846,
      "grad_norm": 0.15666508674621582,
      "learning_rate": 0.00019214122040227448,
      "loss": 0.0106,
      "step": 8834
    },
    {
      "epoch": 0.041558087246112305,
      "grad_norm": 2.165543794631958,
      "learning_rate": 0.000192140277424161,
      "loss": 0.2072,
      "step": 8835
    },
    {
      "epoch": 0.04156279104772477,
      "grad_norm": 4.7358198165893555,
      "learning_rate": 0.00019213933444604751,
      "loss": 1.1285,
      "step": 8836
    },
    {
      "epoch": 0.041567494849337236,
      "grad_norm": 1.3908393383026123,
      "learning_rate": 0.00019213839146793403,
      "loss": 0.1033,
      "step": 8837
    },
    {
      "epoch": 0.041572198650949695,
      "grad_norm": 3.019326686859131,
      "learning_rate": 0.00019213744848982055,
      "loss": 0.4173,
      "step": 8838
    },
    {
      "epoch": 0.04157690245256216,
      "grad_norm": 2.9634904861450195,
      "learning_rate": 0.00019213650551170707,
      "loss": 0.3583,
      "step": 8839
    },
    {
      "epoch": 0.041581606254174626,
      "grad_norm": 1.1119283437728882,
      "learning_rate": 0.0001921355625335936,
      "loss": 0.1168,
      "step": 8840
    },
    {
      "epoch": 0.041586310055787085,
      "grad_norm": 2.973301649093628,
      "learning_rate": 0.00019213461955548013,
      "loss": 0.526,
      "step": 8841
    },
    {
      "epoch": 0.04159101385739955,
      "grad_norm": 4.4184699058532715,
      "learning_rate": 0.00019213367657736665,
      "loss": 0.8223,
      "step": 8842
    },
    {
      "epoch": 0.041595717659012016,
      "grad_norm": 7.350369930267334,
      "learning_rate": 0.00019213273359925317,
      "loss": 1.0858,
      "step": 8843
    },
    {
      "epoch": 0.041600421460624475,
      "grad_norm": 5.631178855895996,
      "learning_rate": 0.0001921317906211397,
      "loss": 0.8213,
      "step": 8844
    },
    {
      "epoch": 0.04160512526223694,
      "grad_norm": 0.139458566904068,
      "learning_rate": 0.00019213084764302624,
      "loss": 0.009,
      "step": 8845
    },
    {
      "epoch": 0.041609829063849406,
      "grad_norm": 1.541206955909729,
      "learning_rate": 0.00019212990466491273,
      "loss": 0.3765,
      "step": 8846
    },
    {
      "epoch": 0.041614532865461865,
      "grad_norm": 3.161527156829834,
      "learning_rate": 0.00019212896168679925,
      "loss": 0.5964,
      "step": 8847
    },
    {
      "epoch": 0.04161923666707433,
      "grad_norm": 0.7594240307807922,
      "learning_rate": 0.00019212801870868577,
      "loss": 0.2106,
      "step": 8848
    },
    {
      "epoch": 0.041623940468686796,
      "grad_norm": 0.9604368209838867,
      "learning_rate": 0.00019212707573057228,
      "loss": 0.1616,
      "step": 8849
    },
    {
      "epoch": 0.041628644270299255,
      "grad_norm": 0.7758048176765442,
      "learning_rate": 0.00019212613275245883,
      "loss": 0.1893,
      "step": 8850
    },
    {
      "epoch": 0.04163334807191172,
      "grad_norm": 1.4085026979446411,
      "learning_rate": 0.00019212518977434535,
      "loss": 0.0779,
      "step": 8851
    },
    {
      "epoch": 0.04163805187352418,
      "grad_norm": 2.5203654766082764,
      "learning_rate": 0.00019212424679623187,
      "loss": 0.2433,
      "step": 8852
    },
    {
      "epoch": 0.041642755675136645,
      "grad_norm": 0.7716912627220154,
      "learning_rate": 0.00019212330381811839,
      "loss": 0.0942,
      "step": 8853
    },
    {
      "epoch": 0.04164745947674911,
      "grad_norm": 0.7582952976226807,
      "learning_rate": 0.00019212236084000493,
      "loss": 0.0779,
      "step": 8854
    },
    {
      "epoch": 0.04165216327836157,
      "grad_norm": 11.142412185668945,
      "learning_rate": 0.00019212141786189145,
      "loss": 0.4728,
      "step": 8855
    },
    {
      "epoch": 0.041656867079974035,
      "grad_norm": 1.3991165161132812,
      "learning_rate": 0.00019212047488377797,
      "loss": 0.1866,
      "step": 8856
    },
    {
      "epoch": 0.0416615708815865,
      "grad_norm": 0.9117549061775208,
      "learning_rate": 0.0001921195319056645,
      "loss": 0.0867,
      "step": 8857
    },
    {
      "epoch": 0.04166627468319896,
      "grad_norm": 2.755279064178467,
      "learning_rate": 0.00019211858892755098,
      "loss": 0.2654,
      "step": 8858
    },
    {
      "epoch": 0.041670978484811425,
      "grad_norm": 1.334689974784851,
      "learning_rate": 0.00019211764594943752,
      "loss": 0.1471,
      "step": 8859
    },
    {
      "epoch": 0.04167568228642389,
      "grad_norm": 2.0789108276367188,
      "learning_rate": 0.00019211670297132404,
      "loss": 0.3153,
      "step": 8860
    },
    {
      "epoch": 0.04168038608803635,
      "grad_norm": 3.5275745391845703,
      "learning_rate": 0.00019211575999321056,
      "loss": 0.456,
      "step": 8861
    },
    {
      "epoch": 0.041685089889648815,
      "grad_norm": 2.4813079833984375,
      "learning_rate": 0.00019211481701509708,
      "loss": 0.2393,
      "step": 8862
    },
    {
      "epoch": 0.04168979369126128,
      "grad_norm": 3.781450033187866,
      "learning_rate": 0.00019211387403698363,
      "loss": 0.616,
      "step": 8863
    },
    {
      "epoch": 0.04169449749287374,
      "grad_norm": 1.5393435955047607,
      "learning_rate": 0.00019211293105887014,
      "loss": 0.1963,
      "step": 8864
    },
    {
      "epoch": 0.041699201294486204,
      "grad_norm": 2.3298771381378174,
      "learning_rate": 0.00019211198808075666,
      "loss": 0.2313,
      "step": 8865
    },
    {
      "epoch": 0.04170390509609867,
      "grad_norm": 1.3713703155517578,
      "learning_rate": 0.00019211104510264318,
      "loss": 0.081,
      "step": 8866
    },
    {
      "epoch": 0.04170860889771113,
      "grad_norm": 2.0790865421295166,
      "learning_rate": 0.0001921101021245297,
      "loss": 0.2243,
      "step": 8867
    },
    {
      "epoch": 0.041713312699323594,
      "grad_norm": 2.7932522296905518,
      "learning_rate": 0.00019210915914641622,
      "loss": 0.2983,
      "step": 8868
    },
    {
      "epoch": 0.04171801650093605,
      "grad_norm": 0.8555548787117004,
      "learning_rate": 0.00019210821616830274,
      "loss": 0.0721,
      "step": 8869
    },
    {
      "epoch": 0.04172272030254852,
      "grad_norm": 3.3294460773468018,
      "learning_rate": 0.00019210727319018926,
      "loss": 0.3179,
      "step": 8870
    },
    {
      "epoch": 0.041727424104160984,
      "grad_norm": 0.7274549603462219,
      "learning_rate": 0.00019210633021207578,
      "loss": 0.0772,
      "step": 8871
    },
    {
      "epoch": 0.04173212790577344,
      "grad_norm": 0.931623101234436,
      "learning_rate": 0.00019210538723396232,
      "loss": 0.1093,
      "step": 8872
    },
    {
      "epoch": 0.04173683170738591,
      "grad_norm": 3.0816569328308105,
      "learning_rate": 0.00019210444425584884,
      "loss": 0.5957,
      "step": 8873
    },
    {
      "epoch": 0.041741535508998374,
      "grad_norm": 3.9442129135131836,
      "learning_rate": 0.00019210350127773536,
      "loss": 0.5371,
      "step": 8874
    },
    {
      "epoch": 0.04174623931061083,
      "grad_norm": 1.5106562376022339,
      "learning_rate": 0.00019210255829962188,
      "loss": 0.1824,
      "step": 8875
    },
    {
      "epoch": 0.0417509431122233,
      "grad_norm": 5.217294216156006,
      "learning_rate": 0.0001921016153215084,
      "loss": 1.2256,
      "step": 8876
    },
    {
      "epoch": 0.041755646913835764,
      "grad_norm": 3.848653554916382,
      "learning_rate": 0.00019210067234339491,
      "loss": 0.2183,
      "step": 8877
    },
    {
      "epoch": 0.04176035071544822,
      "grad_norm": 1.8407385349273682,
      "learning_rate": 0.00019209972936528143,
      "loss": 0.2271,
      "step": 8878
    },
    {
      "epoch": 0.04176505451706069,
      "grad_norm": 5.5954508781433105,
      "learning_rate": 0.00019209878638716795,
      "loss": 0.5748,
      "step": 8879
    },
    {
      "epoch": 0.041769758318673154,
      "grad_norm": 1.0752875804901123,
      "learning_rate": 0.00019209784340905447,
      "loss": 0.1382,
      "step": 8880
    },
    {
      "epoch": 0.04177446212028561,
      "grad_norm": 0.4424646198749542,
      "learning_rate": 0.00019209690043094102,
      "loss": 0.0502,
      "step": 8881
    },
    {
      "epoch": 0.04177916592189808,
      "grad_norm": 1.6595063209533691,
      "learning_rate": 0.00019209595745282753,
      "loss": 0.1559,
      "step": 8882
    },
    {
      "epoch": 0.041783869723510544,
      "grad_norm": 4.149900436401367,
      "learning_rate": 0.00019209501447471405,
      "loss": 0.7613,
      "step": 8883
    },
    {
      "epoch": 0.041788573525123,
      "grad_norm": 1.9483919143676758,
      "learning_rate": 0.00019209407149660057,
      "loss": 0.2196,
      "step": 8884
    },
    {
      "epoch": 0.04179327732673547,
      "grad_norm": 0.46308615803718567,
      "learning_rate": 0.0001920931285184871,
      "loss": 0.0524,
      "step": 8885
    },
    {
      "epoch": 0.04179798112834793,
      "grad_norm": 0.15358874201774597,
      "learning_rate": 0.00019209218554037364,
      "loss": 0.0083,
      "step": 8886
    },
    {
      "epoch": 0.04180268492996039,
      "grad_norm": 2.2607970237731934,
      "learning_rate": 0.00019209124256226016,
      "loss": 0.3075,
      "step": 8887
    },
    {
      "epoch": 0.04180738873157286,
      "grad_norm": 3.6839048862457275,
      "learning_rate": 0.00019209029958414667,
      "loss": 0.6116,
      "step": 8888
    },
    {
      "epoch": 0.04181209253318532,
      "grad_norm": 0.3243733048439026,
      "learning_rate": 0.00019208935660603317,
      "loss": 0.025,
      "step": 8889
    },
    {
      "epoch": 0.04181679633479778,
      "grad_norm": 1.1493867635726929,
      "learning_rate": 0.00019208841362791968,
      "loss": 0.1116,
      "step": 8890
    },
    {
      "epoch": 0.04182150013641025,
      "grad_norm": 0.36156657338142395,
      "learning_rate": 0.00019208747064980623,
      "loss": 0.0295,
      "step": 8891
    },
    {
      "epoch": 0.04182620393802271,
      "grad_norm": 0.18131910264492035,
      "learning_rate": 0.00019208652767169275,
      "loss": 0.0093,
      "step": 8892
    },
    {
      "epoch": 0.04183090773963517,
      "grad_norm": 3.128384590148926,
      "learning_rate": 0.00019208558469357927,
      "loss": 0.4069,
      "step": 8893
    },
    {
      "epoch": 0.04183561154124764,
      "grad_norm": 2.217055559158325,
      "learning_rate": 0.00019208464171546579,
      "loss": 0.3353,
      "step": 8894
    },
    {
      "epoch": 0.0418403153428601,
      "grad_norm": 0.5685828924179077,
      "learning_rate": 0.00019208369873735233,
      "loss": 0.0425,
      "step": 8895
    },
    {
      "epoch": 0.04184501914447256,
      "grad_norm": 0.3270133435726166,
      "learning_rate": 0.00019208275575923885,
      "loss": 0.02,
      "step": 8896
    },
    {
      "epoch": 0.04184972294608503,
      "grad_norm": 1.1386359930038452,
      "learning_rate": 0.00019208181278112537,
      "loss": 0.1002,
      "step": 8897
    },
    {
      "epoch": 0.04185442674769749,
      "grad_norm": 1.5659512281417847,
      "learning_rate": 0.0001920808698030119,
      "loss": 0.3256,
      "step": 8898
    },
    {
      "epoch": 0.04185913054930995,
      "grad_norm": 1.4545494318008423,
      "learning_rate": 0.0001920799268248984,
      "loss": 0.2181,
      "step": 8899
    },
    {
      "epoch": 0.04186383435092242,
      "grad_norm": 0.4639129936695099,
      "learning_rate": 0.00019207898384678492,
      "loss": 0.0515,
      "step": 8900
    },
    {
      "epoch": 0.04186853815253488,
      "grad_norm": 5.8357391357421875,
      "learning_rate": 0.00019207804086867144,
      "loss": 1.0585,
      "step": 8901
    },
    {
      "epoch": 0.04187324195414734,
      "grad_norm": 0.12559640407562256,
      "learning_rate": 0.00019207709789055796,
      "loss": 0.0067,
      "step": 8902
    },
    {
      "epoch": 0.0418779457557598,
      "grad_norm": 1.256882667541504,
      "learning_rate": 0.00019207615491244448,
      "loss": 0.0798,
      "step": 8903
    },
    {
      "epoch": 0.04188264955737227,
      "grad_norm": 20.120922088623047,
      "learning_rate": 0.00019207521193433103,
      "loss": 0.9286,
      "step": 8904
    },
    {
      "epoch": 0.04188735335898473,
      "grad_norm": 9.011049270629883,
      "learning_rate": 0.00019207426895621754,
      "loss": 0.4361,
      "step": 8905
    },
    {
      "epoch": 0.04189205716059719,
      "grad_norm": 4.6279096603393555,
      "learning_rate": 0.00019207332597810406,
      "loss": 0.379,
      "step": 8906
    },
    {
      "epoch": 0.04189676096220966,
      "grad_norm": 10.763167381286621,
      "learning_rate": 0.00019207238299999058,
      "loss": 0.7839,
      "step": 8907
    },
    {
      "epoch": 0.04190146476382212,
      "grad_norm": 0.38524600863456726,
      "learning_rate": 0.0001920714400218771,
      "loss": 0.0265,
      "step": 8908
    },
    {
      "epoch": 0.04190616856543458,
      "grad_norm": 4.057327747344971,
      "learning_rate": 0.00019207049704376362,
      "loss": 0.4957,
      "step": 8909
    },
    {
      "epoch": 0.04191087236704705,
      "grad_norm": 3.3639442920684814,
      "learning_rate": 0.00019206955406565014,
      "loss": 0.2467,
      "step": 8910
    },
    {
      "epoch": 0.04191557616865951,
      "grad_norm": 0.8433426022529602,
      "learning_rate": 0.00019206861108753666,
      "loss": 0.0749,
      "step": 8911
    },
    {
      "epoch": 0.04192027997027197,
      "grad_norm": 4.932240962982178,
      "learning_rate": 0.00019206766810942318,
      "loss": 0.6818,
      "step": 8912
    },
    {
      "epoch": 0.04192498377188444,
      "grad_norm": 0.5661478042602539,
      "learning_rate": 0.00019206672513130972,
      "loss": 0.0357,
      "step": 8913
    },
    {
      "epoch": 0.0419296875734969,
      "grad_norm": 3.26123309135437,
      "learning_rate": 0.00019206578215319624,
      "loss": 0.5948,
      "step": 8914
    },
    {
      "epoch": 0.04193439137510936,
      "grad_norm": 3.0566494464874268,
      "learning_rate": 0.00019206483917508276,
      "loss": 0.2556,
      "step": 8915
    },
    {
      "epoch": 0.04193909517672183,
      "grad_norm": 1.749050498008728,
      "learning_rate": 0.00019206389619696928,
      "loss": 0.3051,
      "step": 8916
    },
    {
      "epoch": 0.04194379897833429,
      "grad_norm": 2.387014389038086,
      "learning_rate": 0.0001920629532188558,
      "loss": 0.3409,
      "step": 8917
    },
    {
      "epoch": 0.04194850277994675,
      "grad_norm": 6.896724224090576,
      "learning_rate": 0.00019206201024074234,
      "loss": 1.3748,
      "step": 8918
    },
    {
      "epoch": 0.04195320658155922,
      "grad_norm": 2.48993182182312,
      "learning_rate": 0.00019206106726262883,
      "loss": 0.1827,
      "step": 8919
    },
    {
      "epoch": 0.041957910383171676,
      "grad_norm": 2.6322250366210938,
      "learning_rate": 0.00019206012428451535,
      "loss": 0.2089,
      "step": 8920
    },
    {
      "epoch": 0.04196261418478414,
      "grad_norm": 3.413382053375244,
      "learning_rate": 0.00019205918130640187,
      "loss": 0.5735,
      "step": 8921
    },
    {
      "epoch": 0.04196731798639661,
      "grad_norm": 2.090259313583374,
      "learning_rate": 0.00019205823832828842,
      "loss": 0.2358,
      "step": 8922
    },
    {
      "epoch": 0.041972021788009066,
      "grad_norm": 1.8675868511199951,
      "learning_rate": 0.00019205729535017493,
      "loss": 0.2419,
      "step": 8923
    },
    {
      "epoch": 0.04197672558962153,
      "grad_norm": 0.7671390175819397,
      "learning_rate": 0.00019205635237206145,
      "loss": 0.0596,
      "step": 8924
    },
    {
      "epoch": 0.041981429391234,
      "grad_norm": 2.141643524169922,
      "learning_rate": 0.00019205540939394797,
      "loss": 0.1633,
      "step": 8925
    },
    {
      "epoch": 0.041986133192846456,
      "grad_norm": 1.5245811939239502,
      "learning_rate": 0.0001920544664158345,
      "loss": 0.1723,
      "step": 8926
    },
    {
      "epoch": 0.04199083699445892,
      "grad_norm": 13.701902389526367,
      "learning_rate": 0.00019205352343772104,
      "loss": 0.6473,
      "step": 8927
    },
    {
      "epoch": 0.04199554079607139,
      "grad_norm": 0.4273076057434082,
      "learning_rate": 0.00019205258045960756,
      "loss": 0.0476,
      "step": 8928
    },
    {
      "epoch": 0.042000244597683846,
      "grad_norm": 2.01672625541687,
      "learning_rate": 0.00019205163748149407,
      "loss": 0.3092,
      "step": 8929
    },
    {
      "epoch": 0.04200494839929631,
      "grad_norm": 4.164422035217285,
      "learning_rate": 0.0001920506945033806,
      "loss": 0.8578,
      "step": 8930
    },
    {
      "epoch": 0.04200965220090878,
      "grad_norm": 0.5940179824829102,
      "learning_rate": 0.0001920497515252671,
      "loss": 0.0646,
      "step": 8931
    },
    {
      "epoch": 0.042014356002521236,
      "grad_norm": 0.6836553812026978,
      "learning_rate": 0.00019204880854715363,
      "loss": 0.0574,
      "step": 8932
    },
    {
      "epoch": 0.0420190598041337,
      "grad_norm": 0.4045109450817108,
      "learning_rate": 0.00019204786556904015,
      "loss": 0.0438,
      "step": 8933
    },
    {
      "epoch": 0.04202376360574617,
      "grad_norm": 0.8312796950340271,
      "learning_rate": 0.00019204692259092667,
      "loss": 0.0311,
      "step": 8934
    },
    {
      "epoch": 0.042028467407358626,
      "grad_norm": 6.34565544128418,
      "learning_rate": 0.00019204597961281319,
      "loss": 0.5622,
      "step": 8935
    },
    {
      "epoch": 0.04203317120897109,
      "grad_norm": 0.4778319299221039,
      "learning_rate": 0.00019204503663469973,
      "loss": 0.0609,
      "step": 8936
    },
    {
      "epoch": 0.04203787501058355,
      "grad_norm": 1.950605869293213,
      "learning_rate": 0.00019204409365658625,
      "loss": 0.2371,
      "step": 8937
    },
    {
      "epoch": 0.042042578812196016,
      "grad_norm": 1.5508646965026855,
      "learning_rate": 0.00019204315067847277,
      "loss": 0.3285,
      "step": 8938
    },
    {
      "epoch": 0.04204728261380848,
      "grad_norm": 2.00996470451355,
      "learning_rate": 0.0001920422077003593,
      "loss": 0.0769,
      "step": 8939
    },
    {
      "epoch": 0.04205198641542094,
      "grad_norm": 0.6124427318572998,
      "learning_rate": 0.0001920412647222458,
      "loss": 0.064,
      "step": 8940
    },
    {
      "epoch": 0.042056690217033406,
      "grad_norm": 1.674461841583252,
      "learning_rate": 0.00019204032174413232,
      "loss": 0.4074,
      "step": 8941
    },
    {
      "epoch": 0.04206139401864587,
      "grad_norm": 1.5062947273254395,
      "learning_rate": 0.00019203937876601884,
      "loss": 0.1232,
      "step": 8942
    },
    {
      "epoch": 0.04206609782025833,
      "grad_norm": 1.0934158563613892,
      "learning_rate": 0.00019203843578790536,
      "loss": 0.0606,
      "step": 8943
    },
    {
      "epoch": 0.042070801621870796,
      "grad_norm": 1.6679211854934692,
      "learning_rate": 0.00019203749280979188,
      "loss": 0.1574,
      "step": 8944
    },
    {
      "epoch": 0.04207550542348326,
      "grad_norm": 1.7289302349090576,
      "learning_rate": 0.00019203654983167843,
      "loss": 0.1721,
      "step": 8945
    },
    {
      "epoch": 0.04208020922509572,
      "grad_norm": 1.4612938165664673,
      "learning_rate": 0.00019203560685356494,
      "loss": 0.2521,
      "step": 8946
    },
    {
      "epoch": 0.042084913026708186,
      "grad_norm": 2.6545679569244385,
      "learning_rate": 0.00019203466387545146,
      "loss": 0.3357,
      "step": 8947
    },
    {
      "epoch": 0.04208961682832065,
      "grad_norm": 1.2294585704803467,
      "learning_rate": 0.00019203372089733798,
      "loss": 0.1623,
      "step": 8948
    },
    {
      "epoch": 0.04209432062993311,
      "grad_norm": 3.637816905975342,
      "learning_rate": 0.0001920327779192245,
      "loss": 0.8694,
      "step": 8949
    },
    {
      "epoch": 0.042099024431545576,
      "grad_norm": 0.622117280960083,
      "learning_rate": 0.00019203183494111102,
      "loss": 0.0715,
      "step": 8950
    },
    {
      "epoch": 0.04210372823315804,
      "grad_norm": 4.622574329376221,
      "learning_rate": 0.00019203089196299754,
      "loss": 0.4234,
      "step": 8951
    },
    {
      "epoch": 0.0421084320347705,
      "grad_norm": 0.9022159576416016,
      "learning_rate": 0.00019202994898488406,
      "loss": 0.0748,
      "step": 8952
    },
    {
      "epoch": 0.042113135836382966,
      "grad_norm": 3.081754207611084,
      "learning_rate": 0.00019202900600677058,
      "loss": 0.4783,
      "step": 8953
    },
    {
      "epoch": 0.042117839637995425,
      "grad_norm": 4.499273300170898,
      "learning_rate": 0.00019202806302865712,
      "loss": 0.8294,
      "step": 8954
    },
    {
      "epoch": 0.04212254343960789,
      "grad_norm": 2.982858896255493,
      "learning_rate": 0.00019202712005054364,
      "loss": 0.3006,
      "step": 8955
    },
    {
      "epoch": 0.042127247241220356,
      "grad_norm": 2.208923101425171,
      "learning_rate": 0.00019202617707243016,
      "loss": 0.1804,
      "step": 8956
    },
    {
      "epoch": 0.042131951042832815,
      "grad_norm": 2.6118645668029785,
      "learning_rate": 0.00019202523409431668,
      "loss": 0.2133,
      "step": 8957
    },
    {
      "epoch": 0.04213665484444528,
      "grad_norm": 1.0301645994186401,
      "learning_rate": 0.0001920242911162032,
      "loss": 0.1135,
      "step": 8958
    },
    {
      "epoch": 0.042141358646057746,
      "grad_norm": 2.851280927658081,
      "learning_rate": 0.00019202334813808974,
      "loss": 0.3528,
      "step": 8959
    },
    {
      "epoch": 0.042146062447670204,
      "grad_norm": 1.7511029243469238,
      "learning_rate": 0.00019202240515997626,
      "loss": 0.2117,
      "step": 8960
    },
    {
      "epoch": 0.04215076624928267,
      "grad_norm": 0.5054875016212463,
      "learning_rate": 0.00019202146218186278,
      "loss": 0.0487,
      "step": 8961
    },
    {
      "epoch": 0.042155470050895136,
      "grad_norm": 3.1819212436676025,
      "learning_rate": 0.00019202051920374927,
      "loss": 0.3898,
      "step": 8962
    },
    {
      "epoch": 0.042160173852507594,
      "grad_norm": 0.8684318661689758,
      "learning_rate": 0.00019201957622563582,
      "loss": 0.0832,
      "step": 8963
    },
    {
      "epoch": 0.04216487765412006,
      "grad_norm": 2.3973467350006104,
      "learning_rate": 0.00019201863324752233,
      "loss": 0.3239,
      "step": 8964
    },
    {
      "epoch": 0.042169581455732526,
      "grad_norm": 1.3058019876480103,
      "learning_rate": 0.00019201769026940885,
      "loss": 0.1313,
      "step": 8965
    },
    {
      "epoch": 0.042174285257344984,
      "grad_norm": 1.972265601158142,
      "learning_rate": 0.00019201674729129537,
      "loss": 0.1618,
      "step": 8966
    },
    {
      "epoch": 0.04217898905895745,
      "grad_norm": 0.669843316078186,
      "learning_rate": 0.0001920158043131819,
      "loss": 0.0845,
      "step": 8967
    },
    {
      "epoch": 0.042183692860569916,
      "grad_norm": 5.775795936584473,
      "learning_rate": 0.00019201486133506844,
      "loss": 0.4538,
      "step": 8968
    },
    {
      "epoch": 0.042188396662182374,
      "grad_norm": 4.055261135101318,
      "learning_rate": 0.00019201391835695495,
      "loss": 0.3611,
      "step": 8969
    },
    {
      "epoch": 0.04219310046379484,
      "grad_norm": 4.398081302642822,
      "learning_rate": 0.00019201297537884147,
      "loss": 0.2746,
      "step": 8970
    },
    {
      "epoch": 0.0421978042654073,
      "grad_norm": 4.0525054931640625,
      "learning_rate": 0.000192012032400728,
      "loss": 0.4725,
      "step": 8971
    },
    {
      "epoch": 0.042202508067019764,
      "grad_norm": 4.458251476287842,
      "learning_rate": 0.0001920110894226145,
      "loss": 0.3681,
      "step": 8972
    },
    {
      "epoch": 0.04220721186863223,
      "grad_norm": 1.445433259010315,
      "learning_rate": 0.00019201014644450103,
      "loss": 0.1524,
      "step": 8973
    },
    {
      "epoch": 0.04221191567024469,
      "grad_norm": 1.9957911968231201,
      "learning_rate": 0.00019200920346638755,
      "loss": 0.3792,
      "step": 8974
    },
    {
      "epoch": 0.042216619471857154,
      "grad_norm": 2.081951856613159,
      "learning_rate": 0.00019200826048827407,
      "loss": 0.1891,
      "step": 8975
    },
    {
      "epoch": 0.04222132327346962,
      "grad_norm": 0.6825717091560364,
      "learning_rate": 0.00019200731751016059,
      "loss": 0.0921,
      "step": 8976
    },
    {
      "epoch": 0.04222602707508208,
      "grad_norm": 2.5382649898529053,
      "learning_rate": 0.00019200637453204713,
      "loss": 0.3083,
      "step": 8977
    },
    {
      "epoch": 0.042230730876694544,
      "grad_norm": 2.7501795291900635,
      "learning_rate": 0.00019200543155393365,
      "loss": 0.4368,
      "step": 8978
    },
    {
      "epoch": 0.04223543467830701,
      "grad_norm": 3.8464255332946777,
      "learning_rate": 0.00019200448857582017,
      "loss": 0.3747,
      "step": 8979
    },
    {
      "epoch": 0.04224013847991947,
      "grad_norm": 1.3790806531906128,
      "learning_rate": 0.0001920035455977067,
      "loss": 0.1386,
      "step": 8980
    },
    {
      "epoch": 0.042244842281531934,
      "grad_norm": 2.034407138824463,
      "learning_rate": 0.0001920026026195932,
      "loss": 0.1982,
      "step": 8981
    },
    {
      "epoch": 0.0422495460831444,
      "grad_norm": 1.0755807161331177,
      "learning_rate": 0.00019200165964147972,
      "loss": 0.0798,
      "step": 8982
    },
    {
      "epoch": 0.04225424988475686,
      "grad_norm": 0.791412353515625,
      "learning_rate": 0.00019200071666336624,
      "loss": 0.0695,
      "step": 8983
    },
    {
      "epoch": 0.042258953686369324,
      "grad_norm": 1.8063920736312866,
      "learning_rate": 0.00019199977368525276,
      "loss": 0.2255,
      "step": 8984
    },
    {
      "epoch": 0.04226365748798179,
      "grad_norm": 4.465608596801758,
      "learning_rate": 0.00019199883070713928,
      "loss": 0.9694,
      "step": 8985
    },
    {
      "epoch": 0.04226836128959425,
      "grad_norm": 1.9528107643127441,
      "learning_rate": 0.00019199788772902583,
      "loss": 0.1931,
      "step": 8986
    },
    {
      "epoch": 0.042273065091206714,
      "grad_norm": 2.5010383129119873,
      "learning_rate": 0.00019199694475091234,
      "loss": 0.3732,
      "step": 8987
    },
    {
      "epoch": 0.04227776889281917,
      "grad_norm": 1.2970458269119263,
      "learning_rate": 0.00019199600177279886,
      "loss": 0.1857,
      "step": 8988
    },
    {
      "epoch": 0.04228247269443164,
      "grad_norm": 0.8173375129699707,
      "learning_rate": 0.00019199505879468538,
      "loss": 0.0646,
      "step": 8989
    },
    {
      "epoch": 0.042287176496044104,
      "grad_norm": 2.89178466796875,
      "learning_rate": 0.0001919941158165719,
      "loss": 0.4287,
      "step": 8990
    },
    {
      "epoch": 0.04229188029765656,
      "grad_norm": 3.4497580528259277,
      "learning_rate": 0.00019199317283845845,
      "loss": 0.7037,
      "step": 8991
    },
    {
      "epoch": 0.04229658409926903,
      "grad_norm": 1.0449244976043701,
      "learning_rate": 0.00019199222986034497,
      "loss": 0.0755,
      "step": 8992
    },
    {
      "epoch": 0.042301287900881494,
      "grad_norm": 0.3585472106933594,
      "learning_rate": 0.00019199128688223146,
      "loss": 0.029,
      "step": 8993
    },
    {
      "epoch": 0.04230599170249395,
      "grad_norm": 1.8380138874053955,
      "learning_rate": 0.00019199034390411798,
      "loss": 0.2126,
      "step": 8994
    },
    {
      "epoch": 0.04231069550410642,
      "grad_norm": 4.483469486236572,
      "learning_rate": 0.00019198940092600452,
      "loss": 1.2242,
      "step": 8995
    },
    {
      "epoch": 0.042315399305718884,
      "grad_norm": 2.09123158454895,
      "learning_rate": 0.00019198845794789104,
      "loss": 0.2775,
      "step": 8996
    },
    {
      "epoch": 0.04232010310733134,
      "grad_norm": 2.5780956745147705,
      "learning_rate": 0.00019198751496977756,
      "loss": 0.2922,
      "step": 8997
    },
    {
      "epoch": 0.04232480690894381,
      "grad_norm": 0.34732767939567566,
      "learning_rate": 0.00019198657199166408,
      "loss": 0.025,
      "step": 8998
    },
    {
      "epoch": 0.042329510710556274,
      "grad_norm": 1.5608316659927368,
      "learning_rate": 0.0001919856290135506,
      "loss": 0.1617,
      "step": 8999
    },
    {
      "epoch": 0.04233421451216873,
      "grad_norm": 2.8951566219329834,
      "learning_rate": 0.00019198468603543714,
      "loss": 0.5347,
      "step": 9000
    },
    {
      "epoch": 0.0423389183137812,
      "grad_norm": 0.7040378451347351,
      "learning_rate": 0.00019198374305732366,
      "loss": 0.0636,
      "step": 9001
    },
    {
      "epoch": 0.042343622115393664,
      "grad_norm": 9.9533052444458,
      "learning_rate": 0.00019198280007921018,
      "loss": 0.3204,
      "step": 9002
    },
    {
      "epoch": 0.04234832591700612,
      "grad_norm": 1.7794268131256104,
      "learning_rate": 0.0001919818571010967,
      "loss": 0.1771,
      "step": 9003
    },
    {
      "epoch": 0.04235302971861859,
      "grad_norm": 2.232877016067505,
      "learning_rate": 0.00019198091412298322,
      "loss": 0.2207,
      "step": 9004
    },
    {
      "epoch": 0.04235773352023105,
      "grad_norm": 0.8809033036231995,
      "learning_rate": 0.00019197997114486973,
      "loss": 0.0845,
      "step": 9005
    },
    {
      "epoch": 0.04236243732184351,
      "grad_norm": 1.2651464939117432,
      "learning_rate": 0.00019197902816675625,
      "loss": 0.1274,
      "step": 9006
    },
    {
      "epoch": 0.04236714112345598,
      "grad_norm": 2.8762435913085938,
      "learning_rate": 0.00019197808518864277,
      "loss": 0.5508,
      "step": 9007
    },
    {
      "epoch": 0.04237184492506844,
      "grad_norm": 2.0067853927612305,
      "learning_rate": 0.0001919771422105293,
      "loss": 0.2471,
      "step": 9008
    },
    {
      "epoch": 0.0423765487266809,
      "grad_norm": 3.3068130016326904,
      "learning_rate": 0.00019197619923241584,
      "loss": 0.4139,
      "step": 9009
    },
    {
      "epoch": 0.04238125252829337,
      "grad_norm": 1.1684765815734863,
      "learning_rate": 0.00019197525625430235,
      "loss": 0.1027,
      "step": 9010
    },
    {
      "epoch": 0.04238595632990583,
      "grad_norm": 4.511866092681885,
      "learning_rate": 0.00019197431327618887,
      "loss": 0.5444,
      "step": 9011
    },
    {
      "epoch": 0.04239066013151829,
      "grad_norm": 1.281876802444458,
      "learning_rate": 0.0001919733702980754,
      "loss": 0.0975,
      "step": 9012
    },
    {
      "epoch": 0.04239536393313076,
      "grad_norm": 3.3770484924316406,
      "learning_rate": 0.0001919724273199619,
      "loss": 0.2733,
      "step": 9013
    },
    {
      "epoch": 0.04240006773474322,
      "grad_norm": 3.8046352863311768,
      "learning_rate": 0.00019197148434184843,
      "loss": 0.4149,
      "step": 9014
    },
    {
      "epoch": 0.04240477153635568,
      "grad_norm": 1.2987288236618042,
      "learning_rate": 0.00019197054136373495,
      "loss": 0.1756,
      "step": 9015
    },
    {
      "epoch": 0.04240947533796815,
      "grad_norm": 2.7066805362701416,
      "learning_rate": 0.00019196959838562147,
      "loss": 0.2135,
      "step": 9016
    },
    {
      "epoch": 0.04241417913958061,
      "grad_norm": 1.912939429283142,
      "learning_rate": 0.00019196865540750799,
      "loss": 0.1829,
      "step": 9017
    },
    {
      "epoch": 0.04241888294119307,
      "grad_norm": 4.8186845779418945,
      "learning_rate": 0.00019196771242939453,
      "loss": 0.7618,
      "step": 9018
    },
    {
      "epoch": 0.04242358674280554,
      "grad_norm": 2.553589344024658,
      "learning_rate": 0.00019196676945128105,
      "loss": 0.1913,
      "step": 9019
    },
    {
      "epoch": 0.042428290544418,
      "grad_norm": 0.2135622352361679,
      "learning_rate": 0.00019196582647316757,
      "loss": 0.0152,
      "step": 9020
    },
    {
      "epoch": 0.04243299434603046,
      "grad_norm": 2.6804847717285156,
      "learning_rate": 0.0001919648834950541,
      "loss": 0.3179,
      "step": 9021
    },
    {
      "epoch": 0.04243769814764292,
      "grad_norm": 1.0261690616607666,
      "learning_rate": 0.00019196394051694063,
      "loss": 0.0583,
      "step": 9022
    },
    {
      "epoch": 0.04244240194925539,
      "grad_norm": 2.9559903144836426,
      "learning_rate": 0.00019196299753882715,
      "loss": 0.3113,
      "step": 9023
    },
    {
      "epoch": 0.04244710575086785,
      "grad_norm": 3.2867631912231445,
      "learning_rate": 0.00019196205456071364,
      "loss": 0.4603,
      "step": 9024
    },
    {
      "epoch": 0.04245180955248031,
      "grad_norm": 4.295552730560303,
      "learning_rate": 0.00019196111158260016,
      "loss": 0.625,
      "step": 9025
    },
    {
      "epoch": 0.04245651335409278,
      "grad_norm": 0.39532139897346497,
      "learning_rate": 0.00019196016860448668,
      "loss": 0.0226,
      "step": 9026
    },
    {
      "epoch": 0.04246121715570524,
      "grad_norm": 2.7190358638763428,
      "learning_rate": 0.00019195922562637323,
      "loss": 0.1929,
      "step": 9027
    },
    {
      "epoch": 0.0424659209573177,
      "grad_norm": 2.7511391639709473,
      "learning_rate": 0.00019195828264825974,
      "loss": 0.4257,
      "step": 9028
    },
    {
      "epoch": 0.04247062475893017,
      "grad_norm": 3.5214121341705322,
      "learning_rate": 0.00019195733967014626,
      "loss": 0.4735,
      "step": 9029
    },
    {
      "epoch": 0.04247532856054263,
      "grad_norm": 1.078679084777832,
      "learning_rate": 0.00019195639669203278,
      "loss": 0.0735,
      "step": 9030
    },
    {
      "epoch": 0.04248003236215509,
      "grad_norm": 1.9608306884765625,
      "learning_rate": 0.00019195545371391933,
      "loss": 0.3685,
      "step": 9031
    },
    {
      "epoch": 0.04248473616376756,
      "grad_norm": 8.35302448272705,
      "learning_rate": 0.00019195451073580585,
      "loss": 0.2209,
      "step": 9032
    },
    {
      "epoch": 0.04248943996538002,
      "grad_norm": 2.018199920654297,
      "learning_rate": 0.00019195356775769237,
      "loss": 0.4128,
      "step": 9033
    },
    {
      "epoch": 0.04249414376699248,
      "grad_norm": 3.1497979164123535,
      "learning_rate": 0.00019195262477957888,
      "loss": 0.3034,
      "step": 9034
    },
    {
      "epoch": 0.04249884756860495,
      "grad_norm": 1.9820702075958252,
      "learning_rate": 0.00019195168180146538,
      "loss": 0.329,
      "step": 9035
    },
    {
      "epoch": 0.04250355137021741,
      "grad_norm": 1.5432542562484741,
      "learning_rate": 0.00019195073882335192,
      "loss": 0.0985,
      "step": 9036
    },
    {
      "epoch": 0.04250825517182987,
      "grad_norm": 1.1813173294067383,
      "learning_rate": 0.00019194979584523844,
      "loss": 0.1722,
      "step": 9037
    },
    {
      "epoch": 0.04251295897344234,
      "grad_norm": 0.8686546087265015,
      "learning_rate": 0.00019194885286712496,
      "loss": 0.092,
      "step": 9038
    },
    {
      "epoch": 0.042517662775054796,
      "grad_norm": 1.241163969039917,
      "learning_rate": 0.00019194790988901148,
      "loss": 0.3292,
      "step": 9039
    },
    {
      "epoch": 0.04252236657666726,
      "grad_norm": 2.218322277069092,
      "learning_rate": 0.000191946966910898,
      "loss": 0.3162,
      "step": 9040
    },
    {
      "epoch": 0.04252707037827973,
      "grad_norm": 26.79888916015625,
      "learning_rate": 0.00019194602393278454,
      "loss": 0.6085,
      "step": 9041
    },
    {
      "epoch": 0.042531774179892186,
      "grad_norm": 0.7882053256034851,
      "learning_rate": 0.00019194508095467106,
      "loss": 0.1229,
      "step": 9042
    },
    {
      "epoch": 0.04253647798150465,
      "grad_norm": 1.2368582487106323,
      "learning_rate": 0.00019194413797655758,
      "loss": 0.1966,
      "step": 9043
    },
    {
      "epoch": 0.04254118178311712,
      "grad_norm": 3.3419697284698486,
      "learning_rate": 0.0001919431949984441,
      "loss": 0.6115,
      "step": 9044
    },
    {
      "epoch": 0.042545885584729576,
      "grad_norm": 1.883420705795288,
      "learning_rate": 0.00019194225202033062,
      "loss": 0.1787,
      "step": 9045
    },
    {
      "epoch": 0.04255058938634204,
      "grad_norm": 1.7403372526168823,
      "learning_rate": 0.00019194130904221713,
      "loss": 0.1895,
      "step": 9046
    },
    {
      "epoch": 0.04255529318795451,
      "grad_norm": 1.091841697692871,
      "learning_rate": 0.00019194036606410365,
      "loss": 0.187,
      "step": 9047
    },
    {
      "epoch": 0.042559996989566966,
      "grad_norm": 0.5082851052284241,
      "learning_rate": 0.00019193942308599017,
      "loss": 0.0418,
      "step": 9048
    },
    {
      "epoch": 0.04256470079117943,
      "grad_norm": 1.449602484703064,
      "learning_rate": 0.0001919384801078767,
      "loss": 0.1719,
      "step": 9049
    },
    {
      "epoch": 0.0425694045927919,
      "grad_norm": 0.9170349836349487,
      "learning_rate": 0.00019193753712976324,
      "loss": 0.1565,
      "step": 9050
    },
    {
      "epoch": 0.042574108394404356,
      "grad_norm": 2.3484017848968506,
      "learning_rate": 0.00019193659415164975,
      "loss": 0.1754,
      "step": 9051
    },
    {
      "epoch": 0.04257881219601682,
      "grad_norm": 2.538360595703125,
      "learning_rate": 0.00019193565117353627,
      "loss": 0.1872,
      "step": 9052
    },
    {
      "epoch": 0.04258351599762929,
      "grad_norm": 0.5117705464363098,
      "learning_rate": 0.0001919347081954228,
      "loss": 0.0568,
      "step": 9053
    },
    {
      "epoch": 0.042588219799241746,
      "grad_norm": 8.827414512634277,
      "learning_rate": 0.00019193376521730934,
      "loss": 1.3743,
      "step": 9054
    },
    {
      "epoch": 0.04259292360085421,
      "grad_norm": 2.133798122406006,
      "learning_rate": 0.00019193282223919583,
      "loss": 0.2279,
      "step": 9055
    },
    {
      "epoch": 0.04259762740246667,
      "grad_norm": 4.265261650085449,
      "learning_rate": 0.00019193187926108235,
      "loss": 0.2082,
      "step": 9056
    },
    {
      "epoch": 0.042602331204079136,
      "grad_norm": 4.125725746154785,
      "learning_rate": 0.00019193093628296887,
      "loss": 0.698,
      "step": 9057
    },
    {
      "epoch": 0.0426070350056916,
      "grad_norm": 2.885256052017212,
      "learning_rate": 0.00019192999330485539,
      "loss": 0.3326,
      "step": 9058
    },
    {
      "epoch": 0.04261173880730406,
      "grad_norm": 0.24433769285678864,
      "learning_rate": 0.00019192905032674193,
      "loss": 0.0152,
      "step": 9059
    },
    {
      "epoch": 0.042616442608916526,
      "grad_norm": 3.7096381187438965,
      "learning_rate": 0.00019192810734862845,
      "loss": 0.2917,
      "step": 9060
    },
    {
      "epoch": 0.04262114641052899,
      "grad_norm": 0.2660598158836365,
      "learning_rate": 0.00019192716437051497,
      "loss": 0.0155,
      "step": 9061
    },
    {
      "epoch": 0.04262585021214145,
      "grad_norm": 3.245861530303955,
      "learning_rate": 0.0001919262213924015,
      "loss": 0.5651,
      "step": 9062
    },
    {
      "epoch": 0.042630554013753916,
      "grad_norm": 1.1386125087738037,
      "learning_rate": 0.00019192527841428803,
      "loss": 0.0883,
      "step": 9063
    },
    {
      "epoch": 0.04263525781536638,
      "grad_norm": 0.8662121295928955,
      "learning_rate": 0.00019192433543617455,
      "loss": 0.0993,
      "step": 9064
    },
    {
      "epoch": 0.04263996161697884,
      "grad_norm": 4.280734062194824,
      "learning_rate": 0.00019192339245806107,
      "loss": 0.6957,
      "step": 9065
    },
    {
      "epoch": 0.042644665418591306,
      "grad_norm": 2.8881375789642334,
      "learning_rate": 0.00019192244947994756,
      "loss": 0.4627,
      "step": 9066
    },
    {
      "epoch": 0.04264936922020377,
      "grad_norm": 2.739168167114258,
      "learning_rate": 0.00019192150650183408,
      "loss": 0.2341,
      "step": 9067
    },
    {
      "epoch": 0.04265407302181623,
      "grad_norm": 2.471086025238037,
      "learning_rate": 0.00019192056352372063,
      "loss": 0.2177,
      "step": 9068
    },
    {
      "epoch": 0.042658776823428696,
      "grad_norm": 1.692669153213501,
      "learning_rate": 0.00019191962054560714,
      "loss": 0.2768,
      "step": 9069
    },
    {
      "epoch": 0.04266348062504116,
      "grad_norm": 0.6927549839019775,
      "learning_rate": 0.00019191867756749366,
      "loss": 0.0397,
      "step": 9070
    },
    {
      "epoch": 0.04266818442665362,
      "grad_norm": 0.5234314203262329,
      "learning_rate": 0.00019191773458938018,
      "loss": 0.0442,
      "step": 9071
    },
    {
      "epoch": 0.042672888228266086,
      "grad_norm": 1.4302091598510742,
      "learning_rate": 0.00019191679161126673,
      "loss": 0.1312,
      "step": 9072
    },
    {
      "epoch": 0.042677592029878544,
      "grad_norm": 0.3544746935367584,
      "learning_rate": 0.00019191584863315325,
      "loss": 0.0342,
      "step": 9073
    },
    {
      "epoch": 0.04268229583149101,
      "grad_norm": 0.6194011569023132,
      "learning_rate": 0.00019191490565503977,
      "loss": 0.046,
      "step": 9074
    },
    {
      "epoch": 0.042686999633103476,
      "grad_norm": 0.8488043546676636,
      "learning_rate": 0.00019191396267692628,
      "loss": 0.0759,
      "step": 9075
    },
    {
      "epoch": 0.042691703434715934,
      "grad_norm": 0.2625703811645508,
      "learning_rate": 0.0001919130196988128,
      "loss": 0.0146,
      "step": 9076
    },
    {
      "epoch": 0.0426964072363284,
      "grad_norm": 4.60905122756958,
      "learning_rate": 0.00019191207672069932,
      "loss": 0.975,
      "step": 9077
    },
    {
      "epoch": 0.042701111037940866,
      "grad_norm": 1.0581384897232056,
      "learning_rate": 0.00019191113374258584,
      "loss": 0.0812,
      "step": 9078
    },
    {
      "epoch": 0.042705814839553324,
      "grad_norm": 1.4918557405471802,
      "learning_rate": 0.00019191019076447236,
      "loss": 0.0937,
      "step": 9079
    },
    {
      "epoch": 0.04271051864116579,
      "grad_norm": 0.37440064549446106,
      "learning_rate": 0.00019190924778635888,
      "loss": 0.0439,
      "step": 9080
    },
    {
      "epoch": 0.042715222442778256,
      "grad_norm": 0.020839039236307144,
      "learning_rate": 0.00019190830480824542,
      "loss": 0.001,
      "step": 9081
    },
    {
      "epoch": 0.042719926244390714,
      "grad_norm": 0.3633616864681244,
      "learning_rate": 0.00019190736183013194,
      "loss": 0.0202,
      "step": 9082
    },
    {
      "epoch": 0.04272463004600318,
      "grad_norm": 2.044513702392578,
      "learning_rate": 0.00019190641885201846,
      "loss": 0.1297,
      "step": 9083
    },
    {
      "epoch": 0.042729333847615646,
      "grad_norm": 4.488750457763672,
      "learning_rate": 0.00019190547587390498,
      "loss": 0.6658,
      "step": 9084
    },
    {
      "epoch": 0.042734037649228104,
      "grad_norm": 1.0283617973327637,
      "learning_rate": 0.0001919045328957915,
      "loss": 0.0545,
      "step": 9085
    },
    {
      "epoch": 0.04273874145084057,
      "grad_norm": 1.2199033498764038,
      "learning_rate": 0.00019190358991767802,
      "loss": 0.106,
      "step": 9086
    },
    {
      "epoch": 0.042743445252453036,
      "grad_norm": 2.2641921043395996,
      "learning_rate": 0.00019190264693956453,
      "loss": 0.2815,
      "step": 9087
    },
    {
      "epoch": 0.042748149054065494,
      "grad_norm": 3.6806254386901855,
      "learning_rate": 0.00019190170396145105,
      "loss": 0.8327,
      "step": 9088
    },
    {
      "epoch": 0.04275285285567796,
      "grad_norm": 3.000545024871826,
      "learning_rate": 0.00019190076098333757,
      "loss": 0.5067,
      "step": 9089
    },
    {
      "epoch": 0.04275755665729042,
      "grad_norm": 2.8715131282806396,
      "learning_rate": 0.00019189981800522412,
      "loss": 0.551,
      "step": 9090
    },
    {
      "epoch": 0.042762260458902884,
      "grad_norm": 2.0966241359710693,
      "learning_rate": 0.00019189887502711064,
      "loss": 0.1913,
      "step": 9091
    },
    {
      "epoch": 0.04276696426051535,
      "grad_norm": 0.37859487533569336,
      "learning_rate": 0.00019189793204899715,
      "loss": 0.0252,
      "step": 9092
    },
    {
      "epoch": 0.04277166806212781,
      "grad_norm": 2.7855100631713867,
      "learning_rate": 0.00019189698907088367,
      "loss": 0.2323,
      "step": 9093
    },
    {
      "epoch": 0.042776371863740274,
      "grad_norm": 0.5104357600212097,
      "learning_rate": 0.0001918960460927702,
      "loss": 0.0683,
      "step": 9094
    },
    {
      "epoch": 0.04278107566535274,
      "grad_norm": 0.273762971162796,
      "learning_rate": 0.00019189510311465674,
      "loss": 0.0163,
      "step": 9095
    },
    {
      "epoch": 0.0427857794669652,
      "grad_norm": 4.751072883605957,
      "learning_rate": 0.00019189416013654326,
      "loss": 0.8062,
      "step": 9096
    },
    {
      "epoch": 0.042790483268577664,
      "grad_norm": 0.3972630202770233,
      "learning_rate": 0.00019189321715842975,
      "loss": 0.0271,
      "step": 9097
    },
    {
      "epoch": 0.04279518707019013,
      "grad_norm": 1.9265530109405518,
      "learning_rate": 0.00019189227418031627,
      "loss": 0.1902,
      "step": 9098
    },
    {
      "epoch": 0.04279989087180259,
      "grad_norm": 1.493445873260498,
      "learning_rate": 0.00019189133120220279,
      "loss": 0.1454,
      "step": 9099
    },
    {
      "epoch": 0.042804594673415054,
      "grad_norm": 0.5954023599624634,
      "learning_rate": 0.00019189038822408933,
      "loss": 0.0836,
      "step": 9100
    },
    {
      "epoch": 0.04280929847502752,
      "grad_norm": 1.077817440032959,
      "learning_rate": 0.00019188944524597585,
      "loss": 0.0446,
      "step": 9101
    },
    {
      "epoch": 0.04281400227663998,
      "grad_norm": 2.907914161682129,
      "learning_rate": 0.00019188850226786237,
      "loss": 0.2058,
      "step": 9102
    },
    {
      "epoch": 0.042818706078252444,
      "grad_norm": 0.020951183512806892,
      "learning_rate": 0.0001918875592897489,
      "loss": 0.001,
      "step": 9103
    },
    {
      "epoch": 0.04282340987986491,
      "grad_norm": 3.147036552429199,
      "learning_rate": 0.00019188661631163543,
      "loss": 0.3693,
      "step": 9104
    },
    {
      "epoch": 0.04282811368147737,
      "grad_norm": 0.939933717250824,
      "learning_rate": 0.00019188567333352195,
      "loss": 0.0854,
      "step": 9105
    },
    {
      "epoch": 0.042832817483089834,
      "grad_norm": 1.6435902118682861,
      "learning_rate": 0.00019188473035540847,
      "loss": 0.2553,
      "step": 9106
    },
    {
      "epoch": 0.0428375212847023,
      "grad_norm": 0.7176346778869629,
      "learning_rate": 0.000191883787377295,
      "loss": 0.0635,
      "step": 9107
    },
    {
      "epoch": 0.04284222508631476,
      "grad_norm": 0.8701654076576233,
      "learning_rate": 0.0001918828443991815,
      "loss": 0.0924,
      "step": 9108
    },
    {
      "epoch": 0.042846928887927224,
      "grad_norm": 0.40818652510643005,
      "learning_rate": 0.00019188190142106803,
      "loss": 0.0301,
      "step": 9109
    },
    {
      "epoch": 0.04285163268953968,
      "grad_norm": 1.652811884880066,
      "learning_rate": 0.00019188095844295454,
      "loss": 0.192,
      "step": 9110
    },
    {
      "epoch": 0.04285633649115215,
      "grad_norm": 3.4375662803649902,
      "learning_rate": 0.00019188001546484106,
      "loss": 0.3844,
      "step": 9111
    },
    {
      "epoch": 0.042861040292764614,
      "grad_norm": 3.411561965942383,
      "learning_rate": 0.00019187907248672758,
      "loss": 0.4478,
      "step": 9112
    },
    {
      "epoch": 0.04286574409437707,
      "grad_norm": 3.2145466804504395,
      "learning_rate": 0.00019187812950861413,
      "loss": 0.6847,
      "step": 9113
    },
    {
      "epoch": 0.04287044789598954,
      "grad_norm": 1.5757365226745605,
      "learning_rate": 0.00019187718653050065,
      "loss": 0.1567,
      "step": 9114
    },
    {
      "epoch": 0.042875151697602004,
      "grad_norm": 2.7065491676330566,
      "learning_rate": 0.00019187624355238717,
      "loss": 0.3456,
      "step": 9115
    },
    {
      "epoch": 0.04287985549921446,
      "grad_norm": 2.857149124145508,
      "learning_rate": 0.00019187530057427368,
      "loss": 0.5679,
      "step": 9116
    },
    {
      "epoch": 0.04288455930082693,
      "grad_norm": 3.7408299446105957,
      "learning_rate": 0.0001918743575961602,
      "loss": 0.3701,
      "step": 9117
    },
    {
      "epoch": 0.042889263102439394,
      "grad_norm": 3.4460177421569824,
      "learning_rate": 0.00019187341461804672,
      "loss": 0.1672,
      "step": 9118
    },
    {
      "epoch": 0.04289396690405185,
      "grad_norm": 3.103182554244995,
      "learning_rate": 0.00019187247163993324,
      "loss": 0.1254,
      "step": 9119
    },
    {
      "epoch": 0.04289867070566432,
      "grad_norm": 0.6456543803215027,
      "learning_rate": 0.00019187152866181976,
      "loss": 0.0608,
      "step": 9120
    },
    {
      "epoch": 0.042903374507276784,
      "grad_norm": 1.4968644380569458,
      "learning_rate": 0.00019187058568370628,
      "loss": 0.1517,
      "step": 9121
    },
    {
      "epoch": 0.04290807830888924,
      "grad_norm": 2.066673994064331,
      "learning_rate": 0.00019186964270559282,
      "loss": 0.1319,
      "step": 9122
    },
    {
      "epoch": 0.04291278211050171,
      "grad_norm": 2.0647101402282715,
      "learning_rate": 0.00019186869972747934,
      "loss": 0.161,
      "step": 9123
    },
    {
      "epoch": 0.042917485912114174,
      "grad_norm": 2.055046558380127,
      "learning_rate": 0.00019186775674936586,
      "loss": 0.2515,
      "step": 9124
    },
    {
      "epoch": 0.04292218971372663,
      "grad_norm": 0.3766365945339203,
      "learning_rate": 0.00019186681377125238,
      "loss": 0.0274,
      "step": 9125
    },
    {
      "epoch": 0.0429268935153391,
      "grad_norm": 0.6655224561691284,
      "learning_rate": 0.0001918658707931389,
      "loss": 0.0501,
      "step": 9126
    },
    {
      "epoch": 0.04293159731695156,
      "grad_norm": 1.6872503757476807,
      "learning_rate": 0.00019186492781502544,
      "loss": 0.2825,
      "step": 9127
    },
    {
      "epoch": 0.04293630111856402,
      "grad_norm": 1.8611629009246826,
      "learning_rate": 0.00019186398483691193,
      "loss": 0.4074,
      "step": 9128
    },
    {
      "epoch": 0.04294100492017649,
      "grad_norm": 0.8845288753509521,
      "learning_rate": 0.00019186304185879845,
      "loss": 0.0561,
      "step": 9129
    },
    {
      "epoch": 0.04294570872178895,
      "grad_norm": 2.9939403533935547,
      "learning_rate": 0.00019186209888068497,
      "loss": 0.5957,
      "step": 9130
    },
    {
      "epoch": 0.04295041252340141,
      "grad_norm": 1.765494704246521,
      "learning_rate": 0.00019186115590257152,
      "loss": 0.1759,
      "step": 9131
    },
    {
      "epoch": 0.04295511632501388,
      "grad_norm": 0.32259753346443176,
      "learning_rate": 0.00019186021292445804,
      "loss": 0.0206,
      "step": 9132
    },
    {
      "epoch": 0.04295982012662634,
      "grad_norm": 2.112860679626465,
      "learning_rate": 0.00019185926994634455,
      "loss": 0.1972,
      "step": 9133
    },
    {
      "epoch": 0.0429645239282388,
      "grad_norm": 0.42555657029151917,
      "learning_rate": 0.00019185832696823107,
      "loss": 0.0339,
      "step": 9134
    },
    {
      "epoch": 0.04296922772985127,
      "grad_norm": 0.9231711030006409,
      "learning_rate": 0.0001918573839901176,
      "loss": 0.0493,
      "step": 9135
    },
    {
      "epoch": 0.04297393153146373,
      "grad_norm": 2.987046241760254,
      "learning_rate": 0.00019185644101200414,
      "loss": 0.2726,
      "step": 9136
    },
    {
      "epoch": 0.04297863533307619,
      "grad_norm": 1.5075539350509644,
      "learning_rate": 0.00019185549803389066,
      "loss": 0.1426,
      "step": 9137
    },
    {
      "epoch": 0.04298333913468866,
      "grad_norm": 2.216732978820801,
      "learning_rate": 0.00019185455505577718,
      "loss": 0.3291,
      "step": 9138
    },
    {
      "epoch": 0.04298804293630112,
      "grad_norm": 0.45020097494125366,
      "learning_rate": 0.0001918536120776637,
      "loss": 0.0325,
      "step": 9139
    },
    {
      "epoch": 0.04299274673791358,
      "grad_norm": 1.9474204778671265,
      "learning_rate": 0.0001918526690995502,
      "loss": 0.4293,
      "step": 9140
    },
    {
      "epoch": 0.04299745053952605,
      "grad_norm": 2.692418336868286,
      "learning_rate": 0.00019185172612143673,
      "loss": 0.3243,
      "step": 9141
    },
    {
      "epoch": 0.04300215434113851,
      "grad_norm": 1.3824353218078613,
      "learning_rate": 0.00019185078314332325,
      "loss": 0.2196,
      "step": 9142
    },
    {
      "epoch": 0.04300685814275097,
      "grad_norm": 0.6314926147460938,
      "learning_rate": 0.00019184984016520977,
      "loss": 0.0482,
      "step": 9143
    },
    {
      "epoch": 0.04301156194436343,
      "grad_norm": 2.033527135848999,
      "learning_rate": 0.0001918488971870963,
      "loss": 0.1403,
      "step": 9144
    },
    {
      "epoch": 0.0430162657459759,
      "grad_norm": 1.7780518531799316,
      "learning_rate": 0.00019184795420898283,
      "loss": 0.363,
      "step": 9145
    },
    {
      "epoch": 0.04302096954758836,
      "grad_norm": 3.6544694900512695,
      "learning_rate": 0.00019184701123086935,
      "loss": 0.4441,
      "step": 9146
    },
    {
      "epoch": 0.04302567334920082,
      "grad_norm": 1.1724631786346436,
      "learning_rate": 0.00019184606825275587,
      "loss": 0.1556,
      "step": 9147
    },
    {
      "epoch": 0.04303037715081329,
      "grad_norm": 2.037278413772583,
      "learning_rate": 0.0001918451252746424,
      "loss": 0.0963,
      "step": 9148
    },
    {
      "epoch": 0.04303508095242575,
      "grad_norm": 1.9128576517105103,
      "learning_rate": 0.0001918441822965289,
      "loss": 0.2308,
      "step": 9149
    },
    {
      "epoch": 0.04303978475403821,
      "grad_norm": 0.6217485666275024,
      "learning_rate": 0.00019184323931841543,
      "loss": 0.0604,
      "step": 9150
    },
    {
      "epoch": 0.04304448855565068,
      "grad_norm": 1.7250996828079224,
      "learning_rate": 0.00019184229634030194,
      "loss": 0.09,
      "step": 9151
    },
    {
      "epoch": 0.04304919235726314,
      "grad_norm": 7.103452682495117,
      "learning_rate": 0.00019184135336218846,
      "loss": 0.6103,
      "step": 9152
    },
    {
      "epoch": 0.0430538961588756,
      "grad_norm": 1.9862133264541626,
      "learning_rate": 0.00019184041038407498,
      "loss": 0.1224,
      "step": 9153
    },
    {
      "epoch": 0.04305859996048807,
      "grad_norm": 4.976305961608887,
      "learning_rate": 0.00019183946740596153,
      "loss": 0.3387,
      "step": 9154
    },
    {
      "epoch": 0.04306330376210053,
      "grad_norm": 1.6871970891952515,
      "learning_rate": 0.00019183852442784805,
      "loss": 0.1444,
      "step": 9155
    },
    {
      "epoch": 0.04306800756371299,
      "grad_norm": 2.657074451446533,
      "learning_rate": 0.00019183758144973456,
      "loss": 0.2789,
      "step": 9156
    },
    {
      "epoch": 0.04307271136532546,
      "grad_norm": 10.100461959838867,
      "learning_rate": 0.00019183663847162108,
      "loss": 1.2436,
      "step": 9157
    },
    {
      "epoch": 0.04307741516693792,
      "grad_norm": 4.117556095123291,
      "learning_rate": 0.0001918356954935076,
      "loss": 0.6387,
      "step": 9158
    },
    {
      "epoch": 0.04308211896855038,
      "grad_norm": 8.29261302947998,
      "learning_rate": 0.00019183475251539412,
      "loss": 1.2626,
      "step": 9159
    },
    {
      "epoch": 0.04308682277016285,
      "grad_norm": 2.7478370666503906,
      "learning_rate": 0.00019183380953728064,
      "loss": 0.3344,
      "step": 9160
    },
    {
      "epoch": 0.043091526571775306,
      "grad_norm": 7.342888832092285,
      "learning_rate": 0.00019183286655916716,
      "loss": 1.4314,
      "step": 9161
    },
    {
      "epoch": 0.04309623037338777,
      "grad_norm": 2.7045278549194336,
      "learning_rate": 0.00019183192358105368,
      "loss": 0.1158,
      "step": 9162
    },
    {
      "epoch": 0.04310093417500024,
      "grad_norm": 5.263622760772705,
      "learning_rate": 0.00019183098060294022,
      "loss": 0.8813,
      "step": 9163
    },
    {
      "epoch": 0.043105637976612696,
      "grad_norm": 2.105015277862549,
      "learning_rate": 0.00019183003762482674,
      "loss": 0.4401,
      "step": 9164
    },
    {
      "epoch": 0.04311034177822516,
      "grad_norm": 1.457027792930603,
      "learning_rate": 0.00019182909464671326,
      "loss": 0.1562,
      "step": 9165
    },
    {
      "epoch": 0.04311504557983763,
      "grad_norm": 1.2768375873565674,
      "learning_rate": 0.00019182815166859978,
      "loss": 0.1088,
      "step": 9166
    },
    {
      "epoch": 0.043119749381450086,
      "grad_norm": 2.8282618522644043,
      "learning_rate": 0.0001918272086904863,
      "loss": 0.2659,
      "step": 9167
    },
    {
      "epoch": 0.04312445318306255,
      "grad_norm": 2.1433029174804688,
      "learning_rate": 0.00019182626571237284,
      "loss": 0.1018,
      "step": 9168
    },
    {
      "epoch": 0.04312915698467502,
      "grad_norm": 2.261789321899414,
      "learning_rate": 0.00019182532273425936,
      "loss": 0.1449,
      "step": 9169
    },
    {
      "epoch": 0.043133860786287476,
      "grad_norm": 2.2996113300323486,
      "learning_rate": 0.00019182437975614585,
      "loss": 0.3972,
      "step": 9170
    },
    {
      "epoch": 0.04313856458789994,
      "grad_norm": 2.6213021278381348,
      "learning_rate": 0.00019182343677803237,
      "loss": 0.2524,
      "step": 9171
    },
    {
      "epoch": 0.04314326838951241,
      "grad_norm": 0.6224321722984314,
      "learning_rate": 0.00019182249379991892,
      "loss": 0.0525,
      "step": 9172
    },
    {
      "epoch": 0.043147972191124866,
      "grad_norm": 1.1620639562606812,
      "learning_rate": 0.00019182155082180544,
      "loss": 0.133,
      "step": 9173
    },
    {
      "epoch": 0.04315267599273733,
      "grad_norm": 3.2713139057159424,
      "learning_rate": 0.00019182060784369195,
      "loss": 0.3171,
      "step": 9174
    },
    {
      "epoch": 0.0431573797943498,
      "grad_norm": 2.369760513305664,
      "learning_rate": 0.00019181966486557847,
      "loss": 0.2353,
      "step": 9175
    },
    {
      "epoch": 0.043162083595962256,
      "grad_norm": 2.5764262676239014,
      "learning_rate": 0.000191818721887465,
      "loss": 0.3029,
      "step": 9176
    },
    {
      "epoch": 0.04316678739757472,
      "grad_norm": 0.8080867528915405,
      "learning_rate": 0.00019181777890935154,
      "loss": 0.0746,
      "step": 9177
    },
    {
      "epoch": 0.04317149119918718,
      "grad_norm": 1.6568881273269653,
      "learning_rate": 0.00019181683593123806,
      "loss": 0.2343,
      "step": 9178
    },
    {
      "epoch": 0.043176195000799646,
      "grad_norm": 2.4705681800842285,
      "learning_rate": 0.00019181589295312458,
      "loss": 0.3741,
      "step": 9179
    },
    {
      "epoch": 0.04318089880241211,
      "grad_norm": 0.6364278793334961,
      "learning_rate": 0.0001918149499750111,
      "loss": 0.0884,
      "step": 9180
    },
    {
      "epoch": 0.04318560260402457,
      "grad_norm": 0.7809053659439087,
      "learning_rate": 0.0001918140069968976,
      "loss": 0.1166,
      "step": 9181
    },
    {
      "epoch": 0.043190306405637036,
      "grad_norm": 3.1072802543640137,
      "learning_rate": 0.00019181306401878413,
      "loss": 0.5621,
      "step": 9182
    },
    {
      "epoch": 0.0431950102072495,
      "grad_norm": 0.803885281085968,
      "learning_rate": 0.00019181212104067065,
      "loss": 0.101,
      "step": 9183
    },
    {
      "epoch": 0.04319971400886196,
      "grad_norm": 0.9178580641746521,
      "learning_rate": 0.00019181117806255717,
      "loss": 0.1071,
      "step": 9184
    },
    {
      "epoch": 0.043204417810474426,
      "grad_norm": 1.996290683746338,
      "learning_rate": 0.0001918102350844437,
      "loss": 0.3247,
      "step": 9185
    },
    {
      "epoch": 0.04320912161208689,
      "grad_norm": 1.5022410154342651,
      "learning_rate": 0.00019180929210633023,
      "loss": 0.149,
      "step": 9186
    },
    {
      "epoch": 0.04321382541369935,
      "grad_norm": 3.728121280670166,
      "learning_rate": 0.00019180834912821675,
      "loss": 0.3255,
      "step": 9187
    },
    {
      "epoch": 0.043218529215311816,
      "grad_norm": 3.5834741592407227,
      "learning_rate": 0.00019180740615010327,
      "loss": 0.6482,
      "step": 9188
    },
    {
      "epoch": 0.04322323301692428,
      "grad_norm": 0.9892247319221497,
      "learning_rate": 0.0001918064631719898,
      "loss": 0.1261,
      "step": 9189
    },
    {
      "epoch": 0.04322793681853674,
      "grad_norm": 1.2441282272338867,
      "learning_rate": 0.0001918055201938763,
      "loss": 0.0843,
      "step": 9190
    },
    {
      "epoch": 0.043232640620149206,
      "grad_norm": 2.234787940979004,
      "learning_rate": 0.00019180457721576283,
      "loss": 0.2718,
      "step": 9191
    },
    {
      "epoch": 0.04323734442176167,
      "grad_norm": 1.057862401008606,
      "learning_rate": 0.00019180363423764934,
      "loss": 0.1057,
      "step": 9192
    },
    {
      "epoch": 0.04324204822337413,
      "grad_norm": 0.49939295649528503,
      "learning_rate": 0.00019180269125953586,
      "loss": 0.0332,
      "step": 9193
    },
    {
      "epoch": 0.043246752024986596,
      "grad_norm": 2.2389228343963623,
      "learning_rate": 0.00019180174828142238,
      "loss": 0.2344,
      "step": 9194
    },
    {
      "epoch": 0.043251455826599054,
      "grad_norm": 1.0964412689208984,
      "learning_rate": 0.00019180080530330893,
      "loss": 0.1701,
      "step": 9195
    },
    {
      "epoch": 0.04325615962821152,
      "grad_norm": 2.838376998901367,
      "learning_rate": 0.00019179986232519545,
      "loss": 0.6109,
      "step": 9196
    },
    {
      "epoch": 0.043260863429823986,
      "grad_norm": 3.1730611324310303,
      "learning_rate": 0.00019179891934708196,
      "loss": 0.2935,
      "step": 9197
    },
    {
      "epoch": 0.043265567231436444,
      "grad_norm": 5.341832637786865,
      "learning_rate": 0.00019179797636896848,
      "loss": 0.3041,
      "step": 9198
    },
    {
      "epoch": 0.04327027103304891,
      "grad_norm": 1.7263288497924805,
      "learning_rate": 0.000191797033390855,
      "loss": 0.4144,
      "step": 9199
    },
    {
      "epoch": 0.043274974834661376,
      "grad_norm": 1.437741756439209,
      "learning_rate": 0.00019179609041274155,
      "loss": 0.3213,
      "step": 9200
    },
    {
      "epoch": 0.043279678636273834,
      "grad_norm": 0.5968946814537048,
      "learning_rate": 0.00019179514743462804,
      "loss": 0.0387,
      "step": 9201
    },
    {
      "epoch": 0.0432843824378863,
      "grad_norm": 1.522158145904541,
      "learning_rate": 0.00019179420445651456,
      "loss": 0.0972,
      "step": 9202
    },
    {
      "epoch": 0.043289086239498765,
      "grad_norm": 1.4256905317306519,
      "learning_rate": 0.00019179326147840108,
      "loss": 0.0535,
      "step": 9203
    },
    {
      "epoch": 0.043293790041111224,
      "grad_norm": 3.728513240814209,
      "learning_rate": 0.00019179231850028762,
      "loss": 0.3215,
      "step": 9204
    },
    {
      "epoch": 0.04329849384272369,
      "grad_norm": 4.15350341796875,
      "learning_rate": 0.00019179137552217414,
      "loss": 0.3101,
      "step": 9205
    },
    {
      "epoch": 0.043303197644336155,
      "grad_norm": 1.2437565326690674,
      "learning_rate": 0.00019179043254406066,
      "loss": 0.0609,
      "step": 9206
    },
    {
      "epoch": 0.043307901445948614,
      "grad_norm": 0.9897750020027161,
      "learning_rate": 0.00019178948956594718,
      "loss": 0.0753,
      "step": 9207
    },
    {
      "epoch": 0.04331260524756108,
      "grad_norm": 2.132280111312866,
      "learning_rate": 0.0001917885465878337,
      "loss": 0.1197,
      "step": 9208
    },
    {
      "epoch": 0.043317309049173545,
      "grad_norm": 2.601198196411133,
      "learning_rate": 0.00019178760360972024,
      "loss": 0.2183,
      "step": 9209
    },
    {
      "epoch": 0.043322012850786004,
      "grad_norm": 3.165663003921509,
      "learning_rate": 0.00019178666063160676,
      "loss": 0.495,
      "step": 9210
    },
    {
      "epoch": 0.04332671665239847,
      "grad_norm": 1.6790251731872559,
      "learning_rate": 0.00019178571765349328,
      "loss": 0.1379,
      "step": 9211
    },
    {
      "epoch": 0.04333142045401093,
      "grad_norm": 0.40554389357566833,
      "learning_rate": 0.0001917847746753798,
      "loss": 0.0227,
      "step": 9212
    },
    {
      "epoch": 0.043336124255623394,
      "grad_norm": 2.9473485946655273,
      "learning_rate": 0.00019178383169726632,
      "loss": 0.2316,
      "step": 9213
    },
    {
      "epoch": 0.04334082805723586,
      "grad_norm": 3.3351047039031982,
      "learning_rate": 0.00019178288871915284,
      "loss": 0.3303,
      "step": 9214
    },
    {
      "epoch": 0.04334553185884832,
      "grad_norm": 2.990194082260132,
      "learning_rate": 0.00019178194574103935,
      "loss": 0.1199,
      "step": 9215
    },
    {
      "epoch": 0.043350235660460784,
      "grad_norm": 4.988463878631592,
      "learning_rate": 0.00019178100276292587,
      "loss": 0.9135,
      "step": 9216
    },
    {
      "epoch": 0.04335493946207325,
      "grad_norm": 0.9375431537628174,
      "learning_rate": 0.0001917800597848124,
      "loss": 0.0999,
      "step": 9217
    },
    {
      "epoch": 0.04335964326368571,
      "grad_norm": 4.1678900718688965,
      "learning_rate": 0.00019177911680669894,
      "loss": 0.5519,
      "step": 9218
    },
    {
      "epoch": 0.043364347065298174,
      "grad_norm": 3.1775293350219727,
      "learning_rate": 0.00019177817382858546,
      "loss": 0.2261,
      "step": 9219
    },
    {
      "epoch": 0.04336905086691064,
      "grad_norm": 6.572775363922119,
      "learning_rate": 0.00019177723085047198,
      "loss": 0.7788,
      "step": 9220
    },
    {
      "epoch": 0.0433737546685231,
      "grad_norm": 2.4194958209991455,
      "learning_rate": 0.0001917762878723585,
      "loss": 0.1874,
      "step": 9221
    },
    {
      "epoch": 0.043378458470135564,
      "grad_norm": 6.961133003234863,
      "learning_rate": 0.000191775344894245,
      "loss": 1.8894,
      "step": 9222
    },
    {
      "epoch": 0.04338316227174803,
      "grad_norm": 9.180593490600586,
      "learning_rate": 0.00019177440191613153,
      "loss": 0.6782,
      "step": 9223
    },
    {
      "epoch": 0.04338786607336049,
      "grad_norm": 1.5899744033813477,
      "learning_rate": 0.00019177345893801805,
      "loss": 0.1171,
      "step": 9224
    },
    {
      "epoch": 0.043392569874972954,
      "grad_norm": 4.128016471862793,
      "learning_rate": 0.00019177251595990457,
      "loss": 0.5241,
      "step": 9225
    },
    {
      "epoch": 0.04339727367658542,
      "grad_norm": 1.8292237520217896,
      "learning_rate": 0.0001917715729817911,
      "loss": 0.3565,
      "step": 9226
    },
    {
      "epoch": 0.04340197747819788,
      "grad_norm": 1.4991459846496582,
      "learning_rate": 0.00019177063000367763,
      "loss": 0.0658,
      "step": 9227
    },
    {
      "epoch": 0.043406681279810344,
      "grad_norm": 3.3988590240478516,
      "learning_rate": 0.00019176968702556415,
      "loss": 0.2718,
      "step": 9228
    },
    {
      "epoch": 0.0434113850814228,
      "grad_norm": 3.802093029022217,
      "learning_rate": 0.00019176874404745067,
      "loss": 0.686,
      "step": 9229
    },
    {
      "epoch": 0.04341608888303527,
      "grad_norm": 1.3679066896438599,
      "learning_rate": 0.0001917678010693372,
      "loss": 0.3303,
      "step": 9230
    },
    {
      "epoch": 0.043420792684647734,
      "grad_norm": 0.8678988218307495,
      "learning_rate": 0.00019176685809122373,
      "loss": 0.0977,
      "step": 9231
    },
    {
      "epoch": 0.04342549648626019,
      "grad_norm": 1.4171440601348877,
      "learning_rate": 0.00019176591511311023,
      "loss": 0.165,
      "step": 9232
    },
    {
      "epoch": 0.04343020028787266,
      "grad_norm": 2.1421291828155518,
      "learning_rate": 0.00019176497213499674,
      "loss": 0.1995,
      "step": 9233
    },
    {
      "epoch": 0.043434904089485124,
      "grad_norm": 1.1439491510391235,
      "learning_rate": 0.00019176402915688326,
      "loss": 0.1191,
      "step": 9234
    },
    {
      "epoch": 0.04343960789109758,
      "grad_norm": 0.4225955009460449,
      "learning_rate": 0.00019176308617876978,
      "loss": 0.0318,
      "step": 9235
    },
    {
      "epoch": 0.04344431169271005,
      "grad_norm": 1.1053695678710938,
      "learning_rate": 0.00019176214320065633,
      "loss": 0.1013,
      "step": 9236
    },
    {
      "epoch": 0.043449015494322514,
      "grad_norm": 1.2842997312545776,
      "learning_rate": 0.00019176120022254285,
      "loss": 0.1191,
      "step": 9237
    },
    {
      "epoch": 0.04345371929593497,
      "grad_norm": 1.6822268962860107,
      "learning_rate": 0.00019176025724442936,
      "loss": 0.1601,
      "step": 9238
    },
    {
      "epoch": 0.04345842309754744,
      "grad_norm": 1.91187584400177,
      "learning_rate": 0.00019175931426631588,
      "loss": 0.2129,
      "step": 9239
    },
    {
      "epoch": 0.043463126899159904,
      "grad_norm": 1.1761375665664673,
      "learning_rate": 0.00019175837128820243,
      "loss": 0.16,
      "step": 9240
    },
    {
      "epoch": 0.04346783070077236,
      "grad_norm": 3.2119698524475098,
      "learning_rate": 0.00019175742831008895,
      "loss": 0.4766,
      "step": 9241
    },
    {
      "epoch": 0.04347253450238483,
      "grad_norm": 1.7825183868408203,
      "learning_rate": 0.00019175648533197547,
      "loss": 0.2997,
      "step": 9242
    },
    {
      "epoch": 0.043477238303997294,
      "grad_norm": 1.6230189800262451,
      "learning_rate": 0.00019175554235386199,
      "loss": 0.2127,
      "step": 9243
    },
    {
      "epoch": 0.04348194210560975,
      "grad_norm": 1.4730952978134155,
      "learning_rate": 0.00019175459937574848,
      "loss": 0.2202,
      "step": 9244
    },
    {
      "epoch": 0.04348664590722222,
      "grad_norm": 1.1411280632019043,
      "learning_rate": 0.00019175365639763502,
      "loss": 0.1888,
      "step": 9245
    },
    {
      "epoch": 0.04349134970883468,
      "grad_norm": 1.7758766412734985,
      "learning_rate": 0.00019175271341952154,
      "loss": 0.2945,
      "step": 9246
    },
    {
      "epoch": 0.04349605351044714,
      "grad_norm": 1.0852793455123901,
      "learning_rate": 0.00019175177044140806,
      "loss": 0.0881,
      "step": 9247
    },
    {
      "epoch": 0.04350075731205961,
      "grad_norm": 2.041433334350586,
      "learning_rate": 0.00019175082746329458,
      "loss": 0.3004,
      "step": 9248
    },
    {
      "epoch": 0.04350546111367207,
      "grad_norm": 2.7032644748687744,
      "learning_rate": 0.0001917498844851811,
      "loss": 0.6131,
      "step": 9249
    },
    {
      "epoch": 0.04351016491528453,
      "grad_norm": 1.2545324563980103,
      "learning_rate": 0.00019174894150706764,
      "loss": 0.1974,
      "step": 9250
    },
    {
      "epoch": 0.043514868716897,
      "grad_norm": 1.6654386520385742,
      "learning_rate": 0.00019174799852895416,
      "loss": 0.1789,
      "step": 9251
    },
    {
      "epoch": 0.04351957251850946,
      "grad_norm": 8.582505226135254,
      "learning_rate": 0.00019174705555084068,
      "loss": 0.8491,
      "step": 9252
    },
    {
      "epoch": 0.04352427632012192,
      "grad_norm": 0.06022757664322853,
      "learning_rate": 0.0001917461125727272,
      "loss": 0.0017,
      "step": 9253
    },
    {
      "epoch": 0.04352898012173439,
      "grad_norm": 1.4226360321044922,
      "learning_rate": 0.00019174516959461372,
      "loss": 0.1207,
      "step": 9254
    },
    {
      "epoch": 0.04353368392334685,
      "grad_norm": 2.857036590576172,
      "learning_rate": 0.00019174422661650024,
      "loss": 0.2379,
      "step": 9255
    },
    {
      "epoch": 0.04353838772495931,
      "grad_norm": 3.2799198627471924,
      "learning_rate": 0.00019174328363838675,
      "loss": 0.349,
      "step": 9256
    },
    {
      "epoch": 0.04354309152657178,
      "grad_norm": 7.667892932891846,
      "learning_rate": 0.00019174234066027327,
      "loss": 0.4368,
      "step": 9257
    },
    {
      "epoch": 0.04354779532818424,
      "grad_norm": 3.9155192375183105,
      "learning_rate": 0.0001917413976821598,
      "loss": 0.6241,
      "step": 9258
    },
    {
      "epoch": 0.0435524991297967,
      "grad_norm": 0.6615258455276489,
      "learning_rate": 0.00019174045470404634,
      "loss": 0.0715,
      "step": 9259
    },
    {
      "epoch": 0.04355720293140917,
      "grad_norm": 3.0776705741882324,
      "learning_rate": 0.00019173951172593286,
      "loss": 0.6471,
      "step": 9260
    },
    {
      "epoch": 0.04356190673302163,
      "grad_norm": 2.298302412033081,
      "learning_rate": 0.00019173856874781938,
      "loss": 0.2947,
      "step": 9261
    },
    {
      "epoch": 0.04356661053463409,
      "grad_norm": 1.6611125469207764,
      "learning_rate": 0.0001917376257697059,
      "loss": 0.1263,
      "step": 9262
    },
    {
      "epoch": 0.04357131433624655,
      "grad_norm": 0.8063905239105225,
      "learning_rate": 0.0001917366827915924,
      "loss": 0.0904,
      "step": 9263
    },
    {
      "epoch": 0.04357601813785902,
      "grad_norm": 0.8203085064888,
      "learning_rate": 0.00019173573981347893,
      "loss": 0.036,
      "step": 9264
    },
    {
      "epoch": 0.04358072193947148,
      "grad_norm": 1.400359034538269,
      "learning_rate": 0.00019173479683536545,
      "loss": 0.0878,
      "step": 9265
    },
    {
      "epoch": 0.04358542574108394,
      "grad_norm": 3.9121272563934326,
      "learning_rate": 0.00019173385385725197,
      "loss": 0.2178,
      "step": 9266
    },
    {
      "epoch": 0.04359012954269641,
      "grad_norm": 3.375641107559204,
      "learning_rate": 0.0001917329108791385,
      "loss": 0.5158,
      "step": 9267
    },
    {
      "epoch": 0.04359483334430887,
      "grad_norm": 0.4452948570251465,
      "learning_rate": 0.00019173196790102503,
      "loss": 0.0528,
      "step": 9268
    },
    {
      "epoch": 0.04359953714592133,
      "grad_norm": 4.497988224029541,
      "learning_rate": 0.00019173102492291155,
      "loss": 0.4448,
      "step": 9269
    },
    {
      "epoch": 0.0436042409475338,
      "grad_norm": 1.4627060890197754,
      "learning_rate": 0.00019173008194479807,
      "loss": 0.0888,
      "step": 9270
    },
    {
      "epoch": 0.04360894474914626,
      "grad_norm": 2.357328414916992,
      "learning_rate": 0.0001917291389666846,
      "loss": 0.2298,
      "step": 9271
    },
    {
      "epoch": 0.04361364855075872,
      "grad_norm": 5.700252056121826,
      "learning_rate": 0.00019172819598857113,
      "loss": 0.54,
      "step": 9272
    },
    {
      "epoch": 0.04361835235237119,
      "grad_norm": 1.7551974058151245,
      "learning_rate": 0.00019172725301045765,
      "loss": 0.0826,
      "step": 9273
    },
    {
      "epoch": 0.04362305615398365,
      "grad_norm": 2.600297689437866,
      "learning_rate": 0.00019172631003234417,
      "loss": 0.4718,
      "step": 9274
    },
    {
      "epoch": 0.04362775995559611,
      "grad_norm": 16.30457305908203,
      "learning_rate": 0.00019172536705423066,
      "loss": 0.4027,
      "step": 9275
    },
    {
      "epoch": 0.04363246375720858,
      "grad_norm": 4.582874298095703,
      "learning_rate": 0.00019172442407611718,
      "loss": 0.4995,
      "step": 9276
    },
    {
      "epoch": 0.04363716755882104,
      "grad_norm": 1.7980514764785767,
      "learning_rate": 0.00019172348109800373,
      "loss": 0.2572,
      "step": 9277
    },
    {
      "epoch": 0.0436418713604335,
      "grad_norm": 1.9509937763214111,
      "learning_rate": 0.00019172253811989025,
      "loss": 0.1571,
      "step": 9278
    },
    {
      "epoch": 0.04364657516204597,
      "grad_norm": 0.7420340776443481,
      "learning_rate": 0.00019172159514177676,
      "loss": 0.0569,
      "step": 9279
    },
    {
      "epoch": 0.043651278963658426,
      "grad_norm": 0.5108239054679871,
      "learning_rate": 0.00019172065216366328,
      "loss": 0.0567,
      "step": 9280
    },
    {
      "epoch": 0.04365598276527089,
      "grad_norm": 3.8604190349578857,
      "learning_rate": 0.00019171970918554983,
      "loss": 0.3554,
      "step": 9281
    },
    {
      "epoch": 0.04366068656688336,
      "grad_norm": 4.024404048919678,
      "learning_rate": 0.00019171876620743635,
      "loss": 0.6538,
      "step": 9282
    },
    {
      "epoch": 0.043665390368495816,
      "grad_norm": 2.3370814323425293,
      "learning_rate": 0.00019171782322932287,
      "loss": 0.4729,
      "step": 9283
    },
    {
      "epoch": 0.04367009417010828,
      "grad_norm": 2.110917568206787,
      "learning_rate": 0.00019171688025120939,
      "loss": 0.3369,
      "step": 9284
    },
    {
      "epoch": 0.04367479797172075,
      "grad_norm": 0.518868088722229,
      "learning_rate": 0.0001917159372730959,
      "loss": 0.042,
      "step": 9285
    },
    {
      "epoch": 0.043679501773333206,
      "grad_norm": 2.1154260635375977,
      "learning_rate": 0.00019171499429498242,
      "loss": 0.1831,
      "step": 9286
    },
    {
      "epoch": 0.04368420557494567,
      "grad_norm": 0.47845277190208435,
      "learning_rate": 0.00019171405131686894,
      "loss": 0.0512,
      "step": 9287
    },
    {
      "epoch": 0.04368890937655814,
      "grad_norm": 5.116977214813232,
      "learning_rate": 0.00019171310833875546,
      "loss": 1.2843,
      "step": 9288
    },
    {
      "epoch": 0.043693613178170596,
      "grad_norm": 5.918869972229004,
      "learning_rate": 0.00019171216536064198,
      "loss": 0.9867,
      "step": 9289
    },
    {
      "epoch": 0.04369831697978306,
      "grad_norm": 4.2173380851745605,
      "learning_rate": 0.00019171122238252852,
      "loss": 0.4197,
      "step": 9290
    },
    {
      "epoch": 0.04370302078139553,
      "grad_norm": 2.270606756210327,
      "learning_rate": 0.00019171027940441504,
      "loss": 0.5176,
      "step": 9291
    },
    {
      "epoch": 0.043707724583007986,
      "grad_norm": 1.2548046112060547,
      "learning_rate": 0.00019170933642630156,
      "loss": 0.136,
      "step": 9292
    },
    {
      "epoch": 0.04371242838462045,
      "grad_norm": 1.2995803356170654,
      "learning_rate": 0.00019170839344818808,
      "loss": 0.2893,
      "step": 9293
    },
    {
      "epoch": 0.04371713218623292,
      "grad_norm": 1.4729496240615845,
      "learning_rate": 0.0001917074504700746,
      "loss": 0.3474,
      "step": 9294
    },
    {
      "epoch": 0.043721835987845376,
      "grad_norm": 0.8881381154060364,
      "learning_rate": 0.00019170650749196112,
      "loss": 0.1182,
      "step": 9295
    },
    {
      "epoch": 0.04372653978945784,
      "grad_norm": 3.3589119911193848,
      "learning_rate": 0.00019170556451384764,
      "loss": 0.736,
      "step": 9296
    },
    {
      "epoch": 0.0437312435910703,
      "grad_norm": 2.4385488033294678,
      "learning_rate": 0.00019170462153573415,
      "loss": 0.2876,
      "step": 9297
    },
    {
      "epoch": 0.043735947392682765,
      "grad_norm": 0.8647207021713257,
      "learning_rate": 0.00019170367855762067,
      "loss": 0.0556,
      "step": 9298
    },
    {
      "epoch": 0.04374065119429523,
      "grad_norm": 0.6941716074943542,
      "learning_rate": 0.0001917027355795072,
      "loss": 0.1497,
      "step": 9299
    },
    {
      "epoch": 0.04374535499590769,
      "grad_norm": 5.1026177406311035,
      "learning_rate": 0.00019170179260139374,
      "loss": 0.3641,
      "step": 9300
    },
    {
      "epoch": 0.043750058797520155,
      "grad_norm": 0.7187612056732178,
      "learning_rate": 0.00019170084962328026,
      "loss": 0.0714,
      "step": 9301
    },
    {
      "epoch": 0.04375476259913262,
      "grad_norm": 0.6395995616912842,
      "learning_rate": 0.00019169990664516678,
      "loss": 0.0393,
      "step": 9302
    },
    {
      "epoch": 0.04375946640074508,
      "grad_norm": 2.3181982040405273,
      "learning_rate": 0.0001916989636670533,
      "loss": 0.2342,
      "step": 9303
    },
    {
      "epoch": 0.043764170202357545,
      "grad_norm": 0.7770088315010071,
      "learning_rate": 0.00019169802068893984,
      "loss": 0.0651,
      "step": 9304
    },
    {
      "epoch": 0.04376887400397001,
      "grad_norm": 0.5439664125442505,
      "learning_rate": 0.00019169707771082636,
      "loss": 0.0617,
      "step": 9305
    },
    {
      "epoch": 0.04377357780558247,
      "grad_norm": 4.172003746032715,
      "learning_rate": 0.00019169613473271285,
      "loss": 0.3516,
      "step": 9306
    },
    {
      "epoch": 0.043778281607194935,
      "grad_norm": 2.3069112300872803,
      "learning_rate": 0.00019169519175459937,
      "loss": 0.234,
      "step": 9307
    },
    {
      "epoch": 0.0437829854088074,
      "grad_norm": 0.7784387469291687,
      "learning_rate": 0.0001916942487764859,
      "loss": 0.0551,
      "step": 9308
    },
    {
      "epoch": 0.04378768921041986,
      "grad_norm": 2.6700236797332764,
      "learning_rate": 0.00019169330579837243,
      "loss": 0.2209,
      "step": 9309
    },
    {
      "epoch": 0.043792393012032325,
      "grad_norm": 0.6413134336471558,
      "learning_rate": 0.00019169236282025895,
      "loss": 0.0412,
      "step": 9310
    },
    {
      "epoch": 0.04379709681364479,
      "grad_norm": 7.016850471496582,
      "learning_rate": 0.00019169141984214547,
      "loss": 0.1615,
      "step": 9311
    },
    {
      "epoch": 0.04380180061525725,
      "grad_norm": 0.1802523285150528,
      "learning_rate": 0.000191690476864032,
      "loss": 0.0105,
      "step": 9312
    },
    {
      "epoch": 0.043806504416869715,
      "grad_norm": 0.8118467926979065,
      "learning_rate": 0.00019168953388591853,
      "loss": 0.0579,
      "step": 9313
    },
    {
      "epoch": 0.043811208218482174,
      "grad_norm": 0.5418464541435242,
      "learning_rate": 0.00019168859090780505,
      "loss": 0.0361,
      "step": 9314
    },
    {
      "epoch": 0.04381591202009464,
      "grad_norm": 6.490591526031494,
      "learning_rate": 0.00019168764792969157,
      "loss": 0.5221,
      "step": 9315
    },
    {
      "epoch": 0.043820615821707105,
      "grad_norm": 1.382360577583313,
      "learning_rate": 0.0001916867049515781,
      "loss": 0.106,
      "step": 9316
    },
    {
      "epoch": 0.043825319623319564,
      "grad_norm": 0.6083800196647644,
      "learning_rate": 0.00019168576197346458,
      "loss": 0.0436,
      "step": 9317
    },
    {
      "epoch": 0.04383002342493203,
      "grad_norm": 1.7347387075424194,
      "learning_rate": 0.00019168481899535113,
      "loss": 0.3183,
      "step": 9318
    },
    {
      "epoch": 0.043834727226544495,
      "grad_norm": 2.5763165950775146,
      "learning_rate": 0.00019168387601723765,
      "loss": 0.5603,
      "step": 9319
    },
    {
      "epoch": 0.043839431028156954,
      "grad_norm": 0.20641037821769714,
      "learning_rate": 0.00019168293303912416,
      "loss": 0.0126,
      "step": 9320
    },
    {
      "epoch": 0.04384413482976942,
      "grad_norm": 2.327669143676758,
      "learning_rate": 0.00019168199006101068,
      "loss": 0.3599,
      "step": 9321
    },
    {
      "epoch": 0.043848838631381885,
      "grad_norm": 2.4326601028442383,
      "learning_rate": 0.00019168104708289723,
      "loss": 0.1753,
      "step": 9322
    },
    {
      "epoch": 0.043853542432994344,
      "grad_norm": 0.2388429343700409,
      "learning_rate": 0.00019168010410478375,
      "loss": 0.0175,
      "step": 9323
    },
    {
      "epoch": 0.04385824623460681,
      "grad_norm": 3.8979358673095703,
      "learning_rate": 0.00019167916112667027,
      "loss": 0.4046,
      "step": 9324
    },
    {
      "epoch": 0.043862950036219275,
      "grad_norm": 1.9737162590026855,
      "learning_rate": 0.00019167821814855679,
      "loss": 0.2747,
      "step": 9325
    },
    {
      "epoch": 0.043867653837831734,
      "grad_norm": 1.216312289237976,
      "learning_rate": 0.0001916772751704433,
      "loss": 0.1066,
      "step": 9326
    },
    {
      "epoch": 0.0438723576394442,
      "grad_norm": 0.4312979280948639,
      "learning_rate": 0.00019167633219232982,
      "loss": 0.0314,
      "step": 9327
    },
    {
      "epoch": 0.043877061441056665,
      "grad_norm": 2.796703338623047,
      "learning_rate": 0.00019167538921421634,
      "loss": 0.2289,
      "step": 9328
    },
    {
      "epoch": 0.043881765242669124,
      "grad_norm": 3.6900699138641357,
      "learning_rate": 0.00019167444623610286,
      "loss": 0.6481,
      "step": 9329
    },
    {
      "epoch": 0.04388646904428159,
      "grad_norm": 2.3662970066070557,
      "learning_rate": 0.00019167350325798938,
      "loss": 0.4474,
      "step": 9330
    },
    {
      "epoch": 0.04389117284589405,
      "grad_norm": 1.8185640573501587,
      "learning_rate": 0.00019167256027987592,
      "loss": 0.3703,
      "step": 9331
    },
    {
      "epoch": 0.043895876647506514,
      "grad_norm": 2.082608699798584,
      "learning_rate": 0.00019167161730176244,
      "loss": 0.0911,
      "step": 9332
    },
    {
      "epoch": 0.04390058044911898,
      "grad_norm": 2.3988709449768066,
      "learning_rate": 0.00019167067432364896,
      "loss": 0.4548,
      "step": 9333
    },
    {
      "epoch": 0.04390528425073144,
      "grad_norm": 1.3507490158081055,
      "learning_rate": 0.00019166973134553548,
      "loss": 0.1602,
      "step": 9334
    },
    {
      "epoch": 0.043909988052343904,
      "grad_norm": 1.7375154495239258,
      "learning_rate": 0.000191668788367422,
      "loss": 0.4609,
      "step": 9335
    },
    {
      "epoch": 0.04391469185395637,
      "grad_norm": 1.9819310903549194,
      "learning_rate": 0.00019166784538930854,
      "loss": 0.2803,
      "step": 9336
    },
    {
      "epoch": 0.04391939565556883,
      "grad_norm": 0.20858468115329742,
      "learning_rate": 0.00019166690241119504,
      "loss": 0.0173,
      "step": 9337
    },
    {
      "epoch": 0.043924099457181294,
      "grad_norm": 2.3596489429473877,
      "learning_rate": 0.00019166595943308155,
      "loss": 0.3254,
      "step": 9338
    },
    {
      "epoch": 0.04392880325879376,
      "grad_norm": 2.155372142791748,
      "learning_rate": 0.00019166501645496807,
      "loss": 0.3052,
      "step": 9339
    },
    {
      "epoch": 0.04393350706040622,
      "grad_norm": 2.7551748752593994,
      "learning_rate": 0.00019166407347685462,
      "loss": 0.5948,
      "step": 9340
    },
    {
      "epoch": 0.043938210862018684,
      "grad_norm": 0.9818763136863708,
      "learning_rate": 0.00019166313049874114,
      "loss": 0.1188,
      "step": 9341
    },
    {
      "epoch": 0.04394291466363115,
      "grad_norm": 0.5632927417755127,
      "learning_rate": 0.00019166218752062766,
      "loss": 0.0506,
      "step": 9342
    },
    {
      "epoch": 0.04394761846524361,
      "grad_norm": 2.132756471633911,
      "learning_rate": 0.00019166124454251418,
      "loss": 0.3706,
      "step": 9343
    },
    {
      "epoch": 0.043952322266856074,
      "grad_norm": 1.126611351966858,
      "learning_rate": 0.0001916603015644007,
      "loss": 0.1208,
      "step": 9344
    },
    {
      "epoch": 0.04395702606846854,
      "grad_norm": 1.382706880569458,
      "learning_rate": 0.00019165935858628724,
      "loss": 0.3211,
      "step": 9345
    },
    {
      "epoch": 0.043961729870081,
      "grad_norm": 0.8910264372825623,
      "learning_rate": 0.00019165841560817376,
      "loss": 0.1086,
      "step": 9346
    },
    {
      "epoch": 0.043966433671693464,
      "grad_norm": 0.6239649057388306,
      "learning_rate": 0.00019165747263006028,
      "loss": 0.102,
      "step": 9347
    },
    {
      "epoch": 0.04397113747330592,
      "grad_norm": 1.2067322731018066,
      "learning_rate": 0.00019165652965194677,
      "loss": 0.1166,
      "step": 9348
    },
    {
      "epoch": 0.04397584127491839,
      "grad_norm": 2.7342708110809326,
      "learning_rate": 0.00019165558667383331,
      "loss": 0.7567,
      "step": 9349
    },
    {
      "epoch": 0.043980545076530854,
      "grad_norm": 1.2275309562683105,
      "learning_rate": 0.00019165464369571983,
      "loss": 0.1332,
      "step": 9350
    },
    {
      "epoch": 0.04398524887814331,
      "grad_norm": 1.737412452697754,
      "learning_rate": 0.00019165370071760635,
      "loss": 0.1325,
      "step": 9351
    },
    {
      "epoch": 0.04398995267975578,
      "grad_norm": 5.254212379455566,
      "learning_rate": 0.00019165275773949287,
      "loss": 0.8705,
      "step": 9352
    },
    {
      "epoch": 0.043994656481368244,
      "grad_norm": 0.6922941207885742,
      "learning_rate": 0.0001916518147613794,
      "loss": 0.0356,
      "step": 9353
    },
    {
      "epoch": 0.0439993602829807,
      "grad_norm": 4.028919219970703,
      "learning_rate": 0.00019165087178326593,
      "loss": 0.467,
      "step": 9354
    },
    {
      "epoch": 0.04400406408459317,
      "grad_norm": 1.669677972793579,
      "learning_rate": 0.00019164992880515245,
      "loss": 0.2328,
      "step": 9355
    },
    {
      "epoch": 0.044008767886205634,
      "grad_norm": 3.45621657371521,
      "learning_rate": 0.00019164898582703897,
      "loss": 0.3747,
      "step": 9356
    },
    {
      "epoch": 0.04401347168781809,
      "grad_norm": 0.5042614936828613,
      "learning_rate": 0.0001916480428489255,
      "loss": 0.0546,
      "step": 9357
    },
    {
      "epoch": 0.04401817548943056,
      "grad_norm": 2.2702622413635254,
      "learning_rate": 0.000191647099870812,
      "loss": 0.2285,
      "step": 9358
    },
    {
      "epoch": 0.044022879291043024,
      "grad_norm": 2.6290712356567383,
      "learning_rate": 0.00019164615689269853,
      "loss": 0.2439,
      "step": 9359
    },
    {
      "epoch": 0.04402758309265548,
      "grad_norm": 0.7413787245750427,
      "learning_rate": 0.00019164521391458505,
      "loss": 0.0542,
      "step": 9360
    },
    {
      "epoch": 0.04403228689426795,
      "grad_norm": 1.292283296585083,
      "learning_rate": 0.00019164427093647156,
      "loss": 0.0377,
      "step": 9361
    },
    {
      "epoch": 0.044036990695880414,
      "grad_norm": 3.494255781173706,
      "learning_rate": 0.00019164332795835808,
      "loss": 0.3386,
      "step": 9362
    },
    {
      "epoch": 0.04404169449749287,
      "grad_norm": 0.7715829610824585,
      "learning_rate": 0.00019164238498024463,
      "loss": 0.061,
      "step": 9363
    },
    {
      "epoch": 0.04404639829910534,
      "grad_norm": 2.9506101608276367,
      "learning_rate": 0.00019164144200213115,
      "loss": 0.43,
      "step": 9364
    },
    {
      "epoch": 0.0440511021007178,
      "grad_norm": 2.9800167083740234,
      "learning_rate": 0.00019164049902401767,
      "loss": 0.2257,
      "step": 9365
    },
    {
      "epoch": 0.04405580590233026,
      "grad_norm": 2.1934468746185303,
      "learning_rate": 0.00019163955604590419,
      "loss": 0.4655,
      "step": 9366
    },
    {
      "epoch": 0.04406050970394273,
      "grad_norm": 2.791057825088501,
      "learning_rate": 0.0001916386130677907,
      "loss": 0.3735,
      "step": 9367
    },
    {
      "epoch": 0.04406521350555519,
      "grad_norm": 0.4517587125301361,
      "learning_rate": 0.00019163767008967722,
      "loss": 0.0344,
      "step": 9368
    },
    {
      "epoch": 0.04406991730716765,
      "grad_norm": 0.05480532720685005,
      "learning_rate": 0.00019163672711156374,
      "loss": 0.0033,
      "step": 9369
    },
    {
      "epoch": 0.04407462110878012,
      "grad_norm": 1.8244071006774902,
      "learning_rate": 0.00019163578413345026,
      "loss": 0.1736,
      "step": 9370
    },
    {
      "epoch": 0.04407932491039258,
      "grad_norm": 5.5536885261535645,
      "learning_rate": 0.00019163484115533678,
      "loss": 0.4886,
      "step": 9371
    },
    {
      "epoch": 0.04408402871200504,
      "grad_norm": 1.185307264328003,
      "learning_rate": 0.00019163389817722332,
      "loss": 0.1102,
      "step": 9372
    },
    {
      "epoch": 0.04408873251361751,
      "grad_norm": 0.20955584943294525,
      "learning_rate": 0.00019163295519910984,
      "loss": 0.0165,
      "step": 9373
    },
    {
      "epoch": 0.04409343631522997,
      "grad_norm": 2.805203676223755,
      "learning_rate": 0.00019163201222099636,
      "loss": 0.1084,
      "step": 9374
    },
    {
      "epoch": 0.04409814011684243,
      "grad_norm": 3.059166431427002,
      "learning_rate": 0.00019163106924288288,
      "loss": 0.335,
      "step": 9375
    },
    {
      "epoch": 0.0441028439184549,
      "grad_norm": 3.752664089202881,
      "learning_rate": 0.0001916301262647694,
      "loss": 0.3393,
      "step": 9376
    },
    {
      "epoch": 0.04410754772006736,
      "grad_norm": 1.321770191192627,
      "learning_rate": 0.00019162918328665594,
      "loss": 0.1339,
      "step": 9377
    },
    {
      "epoch": 0.04411225152167982,
      "grad_norm": 5.615309715270996,
      "learning_rate": 0.00019162824030854246,
      "loss": 0.3999,
      "step": 9378
    },
    {
      "epoch": 0.04411695532329229,
      "grad_norm": 3.1863389015197754,
      "learning_rate": 0.00019162729733042895,
      "loss": 0.5181,
      "step": 9379
    },
    {
      "epoch": 0.04412165912490475,
      "grad_norm": 3.6940338611602783,
      "learning_rate": 0.00019162635435231547,
      "loss": 0.2052,
      "step": 9380
    },
    {
      "epoch": 0.04412636292651721,
      "grad_norm": 1.6242820024490356,
      "learning_rate": 0.00019162541137420202,
      "loss": 0.2706,
      "step": 9381
    },
    {
      "epoch": 0.04413106672812967,
      "grad_norm": 2.6671597957611084,
      "learning_rate": 0.00019162446839608854,
      "loss": 0.2355,
      "step": 9382
    },
    {
      "epoch": 0.04413577052974214,
      "grad_norm": 9.006525039672852,
      "learning_rate": 0.00019162352541797506,
      "loss": 0.5154,
      "step": 9383
    },
    {
      "epoch": 0.0441404743313546,
      "grad_norm": 2.7522292137145996,
      "learning_rate": 0.00019162258243986157,
      "loss": 0.8255,
      "step": 9384
    },
    {
      "epoch": 0.04414517813296706,
      "grad_norm": 0.6059134602546692,
      "learning_rate": 0.0001916216394617481,
      "loss": 0.0451,
      "step": 9385
    },
    {
      "epoch": 0.04414988193457953,
      "grad_norm": 0.5153697729110718,
      "learning_rate": 0.00019162069648363464,
      "loss": 0.0647,
      "step": 9386
    },
    {
      "epoch": 0.04415458573619199,
      "grad_norm": 1.0308870077133179,
      "learning_rate": 0.00019161975350552116,
      "loss": 0.0973,
      "step": 9387
    },
    {
      "epoch": 0.04415928953780445,
      "grad_norm": 3.613924980163574,
      "learning_rate": 0.00019161881052740768,
      "loss": 0.3343,
      "step": 9388
    },
    {
      "epoch": 0.04416399333941692,
      "grad_norm": 3.4070746898651123,
      "learning_rate": 0.0001916178675492942,
      "loss": 0.5917,
      "step": 9389
    },
    {
      "epoch": 0.04416869714102938,
      "grad_norm": 0.9806150197982788,
      "learning_rate": 0.00019161692457118071,
      "loss": 0.0651,
      "step": 9390
    },
    {
      "epoch": 0.04417340094264184,
      "grad_norm": 5.757155895233154,
      "learning_rate": 0.00019161598159306723,
      "loss": 1.1085,
      "step": 9391
    },
    {
      "epoch": 0.04417810474425431,
      "grad_norm": 8.838415145874023,
      "learning_rate": 0.00019161503861495375,
      "loss": 0.577,
      "step": 9392
    },
    {
      "epoch": 0.04418280854586677,
      "grad_norm": 2.165205955505371,
      "learning_rate": 0.00019161409563684027,
      "loss": 0.1733,
      "step": 9393
    },
    {
      "epoch": 0.04418751234747923,
      "grad_norm": 1.1086280345916748,
      "learning_rate": 0.0001916131526587268,
      "loss": 0.0493,
      "step": 9394
    },
    {
      "epoch": 0.0441922161490917,
      "grad_norm": 1.2467597723007202,
      "learning_rate": 0.00019161220968061333,
      "loss": 0.109,
      "step": 9395
    },
    {
      "epoch": 0.04419691995070416,
      "grad_norm": 3.1693661212921143,
      "learning_rate": 0.00019161126670249985,
      "loss": 0.3939,
      "step": 9396
    },
    {
      "epoch": 0.04420162375231662,
      "grad_norm": 4.962555885314941,
      "learning_rate": 0.00019161032372438637,
      "loss": 0.8062,
      "step": 9397
    },
    {
      "epoch": 0.04420632755392909,
      "grad_norm": 7.731510162353516,
      "learning_rate": 0.0001916093807462729,
      "loss": 0.6978,
      "step": 9398
    },
    {
      "epoch": 0.044211031355541545,
      "grad_norm": 2.3841261863708496,
      "learning_rate": 0.0001916084377681594,
      "loss": 0.4813,
      "step": 9399
    },
    {
      "epoch": 0.04421573515715401,
      "grad_norm": 5.1165947914123535,
      "learning_rate": 0.00019160749479004593,
      "loss": 1.1647,
      "step": 9400
    },
    {
      "epoch": 0.04422043895876648,
      "grad_norm": 6.873956680297852,
      "learning_rate": 0.00019160655181193245,
      "loss": 1.2628,
      "step": 9401
    },
    {
      "epoch": 0.044225142760378935,
      "grad_norm": 5.752326965332031,
      "learning_rate": 0.00019160560883381896,
      "loss": 0.8768,
      "step": 9402
    },
    {
      "epoch": 0.0442298465619914,
      "grad_norm": 3.76576828956604,
      "learning_rate": 0.00019160466585570548,
      "loss": 0.4747,
      "step": 9403
    },
    {
      "epoch": 0.04423455036360387,
      "grad_norm": 1.20946204662323,
      "learning_rate": 0.00019160372287759203,
      "loss": 0.0912,
      "step": 9404
    },
    {
      "epoch": 0.044239254165216325,
      "grad_norm": 1.7818111181259155,
      "learning_rate": 0.00019160277989947855,
      "loss": 0.087,
      "step": 9405
    },
    {
      "epoch": 0.04424395796682879,
      "grad_norm": 3.208615779876709,
      "learning_rate": 0.00019160183692136507,
      "loss": 0.4217,
      "step": 9406
    },
    {
      "epoch": 0.04424866176844126,
      "grad_norm": 2.0485243797302246,
      "learning_rate": 0.00019160089394325159,
      "loss": 0.2539,
      "step": 9407
    },
    {
      "epoch": 0.044253365570053715,
      "grad_norm": 2.492368698120117,
      "learning_rate": 0.0001915999509651381,
      "loss": 0.4266,
      "step": 9408
    },
    {
      "epoch": 0.04425806937166618,
      "grad_norm": 4.610877990722656,
      "learning_rate": 0.00019159900798702465,
      "loss": 0.9156,
      "step": 9409
    },
    {
      "epoch": 0.04426277317327865,
      "grad_norm": 2.2256672382354736,
      "learning_rate": 0.00019159806500891114,
      "loss": 0.3388,
      "step": 9410
    },
    {
      "epoch": 0.044267476974891105,
      "grad_norm": 2.020789623260498,
      "learning_rate": 0.00019159712203079766,
      "loss": 0.1709,
      "step": 9411
    },
    {
      "epoch": 0.04427218077650357,
      "grad_norm": 1.6820166110992432,
      "learning_rate": 0.00019159617905268418,
      "loss": 0.2872,
      "step": 9412
    },
    {
      "epoch": 0.04427688457811604,
      "grad_norm": 3.304776906967163,
      "learning_rate": 0.00019159523607457072,
      "loss": 0.3353,
      "step": 9413
    },
    {
      "epoch": 0.044281588379728495,
      "grad_norm": 1.6678013801574707,
      "learning_rate": 0.00019159429309645724,
      "loss": 0.2061,
      "step": 9414
    },
    {
      "epoch": 0.04428629218134096,
      "grad_norm": 1.097835659980774,
      "learning_rate": 0.00019159335011834376,
      "loss": 0.1303,
      "step": 9415
    },
    {
      "epoch": 0.04429099598295342,
      "grad_norm": 1.1347090005874634,
      "learning_rate": 0.00019159240714023028,
      "loss": 0.1527,
      "step": 9416
    },
    {
      "epoch": 0.044295699784565885,
      "grad_norm": 1.701738715171814,
      "learning_rate": 0.0001915914641621168,
      "loss": 0.2387,
      "step": 9417
    },
    {
      "epoch": 0.04430040358617835,
      "grad_norm": 1.4368598461151123,
      "learning_rate": 0.00019159052118400334,
      "loss": 0.1099,
      "step": 9418
    },
    {
      "epoch": 0.04430510738779081,
      "grad_norm": 2.8638360500335693,
      "learning_rate": 0.00019158957820588986,
      "loss": 0.366,
      "step": 9419
    },
    {
      "epoch": 0.044309811189403275,
      "grad_norm": 1.855458378791809,
      "learning_rate": 0.00019158863522777638,
      "loss": 0.3104,
      "step": 9420
    },
    {
      "epoch": 0.04431451499101574,
      "grad_norm": 2.304020404815674,
      "learning_rate": 0.00019158769224966287,
      "loss": 0.2084,
      "step": 9421
    },
    {
      "epoch": 0.0443192187926282,
      "grad_norm": 1.7630268335342407,
      "learning_rate": 0.00019158674927154942,
      "loss": 0.1968,
      "step": 9422
    },
    {
      "epoch": 0.044323922594240665,
      "grad_norm": 1.6149375438690186,
      "learning_rate": 0.00019158580629343594,
      "loss": 0.172,
      "step": 9423
    },
    {
      "epoch": 0.04432862639585313,
      "grad_norm": 1.9532530307769775,
      "learning_rate": 0.00019158486331532246,
      "loss": 0.504,
      "step": 9424
    },
    {
      "epoch": 0.04433333019746559,
      "grad_norm": 1.6762967109680176,
      "learning_rate": 0.00019158392033720897,
      "loss": 0.2198,
      "step": 9425
    },
    {
      "epoch": 0.044338033999078055,
      "grad_norm": 2.3937461376190186,
      "learning_rate": 0.0001915829773590955,
      "loss": 0.4449,
      "step": 9426
    },
    {
      "epoch": 0.04434273780069052,
      "grad_norm": 1.10542893409729,
      "learning_rate": 0.00019158203438098204,
      "loss": 0.1283,
      "step": 9427
    },
    {
      "epoch": 0.04434744160230298,
      "grad_norm": 1.4201914072036743,
      "learning_rate": 0.00019158109140286856,
      "loss": 0.3096,
      "step": 9428
    },
    {
      "epoch": 0.044352145403915445,
      "grad_norm": 2.3118014335632324,
      "learning_rate": 0.00019158014842475508,
      "loss": 0.4394,
      "step": 9429
    },
    {
      "epoch": 0.04435684920552791,
      "grad_norm": 5.793317794799805,
      "learning_rate": 0.0001915792054466416,
      "loss": 0.9171,
      "step": 9430
    },
    {
      "epoch": 0.04436155300714037,
      "grad_norm": 1.0451878309249878,
      "learning_rate": 0.00019157826246852811,
      "loss": 0.099,
      "step": 9431
    },
    {
      "epoch": 0.044366256808752835,
      "grad_norm": 2.114229202270508,
      "learning_rate": 0.00019157731949041463,
      "loss": 0.4838,
      "step": 9432
    },
    {
      "epoch": 0.044370960610365294,
      "grad_norm": 1.39629065990448,
      "learning_rate": 0.00019157637651230115,
      "loss": 0.1699,
      "step": 9433
    },
    {
      "epoch": 0.04437566441197776,
      "grad_norm": 1.5828428268432617,
      "learning_rate": 0.00019157543353418767,
      "loss": 0.2625,
      "step": 9434
    },
    {
      "epoch": 0.044380368213590225,
      "grad_norm": 1.2142423391342163,
      "learning_rate": 0.0001915744905560742,
      "loss": 0.3084,
      "step": 9435
    },
    {
      "epoch": 0.044385072015202684,
      "grad_norm": 0.7307363152503967,
      "learning_rate": 0.00019157354757796073,
      "loss": 0.1219,
      "step": 9436
    },
    {
      "epoch": 0.04438977581681515,
      "grad_norm": 3.7753076553344727,
      "learning_rate": 0.00019157260459984725,
      "loss": 0.4136,
      "step": 9437
    },
    {
      "epoch": 0.044394479618427615,
      "grad_norm": 2.9579710960388184,
      "learning_rate": 0.00019157166162173377,
      "loss": 0.7373,
      "step": 9438
    },
    {
      "epoch": 0.044399183420040074,
      "grad_norm": 0.9064165949821472,
      "learning_rate": 0.0001915707186436203,
      "loss": 0.1895,
      "step": 9439
    },
    {
      "epoch": 0.04440388722165254,
      "grad_norm": 2.1114234924316406,
      "learning_rate": 0.00019156977566550684,
      "loss": 0.4638,
      "step": 9440
    },
    {
      "epoch": 0.044408591023265005,
      "grad_norm": 1.3676347732543945,
      "learning_rate": 0.00019156883268739333,
      "loss": 0.2286,
      "step": 9441
    },
    {
      "epoch": 0.044413294824877464,
      "grad_norm": 1.2973324060440063,
      "learning_rate": 0.00019156788970927985,
      "loss": 0.1827,
      "step": 9442
    },
    {
      "epoch": 0.04441799862648993,
      "grad_norm": 1.2881479263305664,
      "learning_rate": 0.00019156694673116636,
      "loss": 0.2488,
      "step": 9443
    },
    {
      "epoch": 0.044422702428102395,
      "grad_norm": 1.049483299255371,
      "learning_rate": 0.00019156600375305288,
      "loss": 0.211,
      "step": 9444
    },
    {
      "epoch": 0.044427406229714854,
      "grad_norm": 0.8739486336708069,
      "learning_rate": 0.00019156506077493943,
      "loss": 0.3034,
      "step": 9445
    },
    {
      "epoch": 0.04443211003132732,
      "grad_norm": 1.307159662246704,
      "learning_rate": 0.00019156411779682595,
      "loss": 0.2342,
      "step": 9446
    },
    {
      "epoch": 0.044436813832939785,
      "grad_norm": 2.3734934329986572,
      "learning_rate": 0.00019156317481871247,
      "loss": 0.3926,
      "step": 9447
    },
    {
      "epoch": 0.044441517634552244,
      "grad_norm": 1.1524193286895752,
      "learning_rate": 0.00019156223184059899,
      "loss": 0.2636,
      "step": 9448
    },
    {
      "epoch": 0.04444622143616471,
      "grad_norm": 0.9223279356956482,
      "learning_rate": 0.00019156128886248553,
      "loss": 0.1547,
      "step": 9449
    },
    {
      "epoch": 0.04445092523777717,
      "grad_norm": 1.1668765544891357,
      "learning_rate": 0.00019156034588437205,
      "loss": 0.1779,
      "step": 9450
    },
    {
      "epoch": 0.044455629039389634,
      "grad_norm": 2.8676841259002686,
      "learning_rate": 0.00019155940290625857,
      "loss": 0.4764,
      "step": 9451
    },
    {
      "epoch": 0.0444603328410021,
      "grad_norm": 6.426552772521973,
      "learning_rate": 0.00019155845992814506,
      "loss": 0.6594,
      "step": 9452
    },
    {
      "epoch": 0.04446503664261456,
      "grad_norm": 1.230494737625122,
      "learning_rate": 0.00019155751695003158,
      "loss": 0.164,
      "step": 9453
    },
    {
      "epoch": 0.044469740444227024,
      "grad_norm": 3.4591434001922607,
      "learning_rate": 0.00019155657397191812,
      "loss": 0.5012,
      "step": 9454
    },
    {
      "epoch": 0.04447444424583949,
      "grad_norm": 0.5889024138450623,
      "learning_rate": 0.00019155563099380464,
      "loss": 0.0896,
      "step": 9455
    },
    {
      "epoch": 0.04447914804745195,
      "grad_norm": 0.7039512991905212,
      "learning_rate": 0.00019155468801569116,
      "loss": 0.0527,
      "step": 9456
    },
    {
      "epoch": 0.044483851849064414,
      "grad_norm": 3.0745482444763184,
      "learning_rate": 0.00019155374503757768,
      "loss": 0.5773,
      "step": 9457
    },
    {
      "epoch": 0.04448855565067688,
      "grad_norm": 0.933165431022644,
      "learning_rate": 0.0001915528020594642,
      "loss": 0.1005,
      "step": 9458
    },
    {
      "epoch": 0.04449325945228934,
      "grad_norm": 1.4477218389511108,
      "learning_rate": 0.00019155185908135074,
      "loss": 0.1151,
      "step": 9459
    },
    {
      "epoch": 0.044497963253901804,
      "grad_norm": 0.6964730620384216,
      "learning_rate": 0.00019155091610323726,
      "loss": 0.0808,
      "step": 9460
    },
    {
      "epoch": 0.04450266705551427,
      "grad_norm": 1.7628964185714722,
      "learning_rate": 0.00019154997312512378,
      "loss": 0.1314,
      "step": 9461
    },
    {
      "epoch": 0.04450737085712673,
      "grad_norm": 0.25163882970809937,
      "learning_rate": 0.0001915490301470103,
      "loss": 0.0232,
      "step": 9462
    },
    {
      "epoch": 0.044512074658739194,
      "grad_norm": 3.749833345413208,
      "learning_rate": 0.00019154808716889682,
      "loss": 0.7418,
      "step": 9463
    },
    {
      "epoch": 0.04451677846035166,
      "grad_norm": 1.0702769756317139,
      "learning_rate": 0.00019154714419078334,
      "loss": 0.1292,
      "step": 9464
    },
    {
      "epoch": 0.04452148226196412,
      "grad_norm": 2.1230411529541016,
      "learning_rate": 0.00019154620121266986,
      "loss": 0.3156,
      "step": 9465
    },
    {
      "epoch": 0.044526186063576584,
      "grad_norm": 3.0262842178344727,
      "learning_rate": 0.00019154525823455637,
      "loss": 0.3895,
      "step": 9466
    },
    {
      "epoch": 0.04453088986518904,
      "grad_norm": 2.334258794784546,
      "learning_rate": 0.0001915443152564429,
      "loss": 0.2362,
      "step": 9467
    },
    {
      "epoch": 0.04453559366680151,
      "grad_norm": 1.53510582447052,
      "learning_rate": 0.00019154337227832944,
      "loss": 0.1425,
      "step": 9468
    },
    {
      "epoch": 0.044540297468413974,
      "grad_norm": 0.31540003418922424,
      "learning_rate": 0.00019154242930021596,
      "loss": 0.0291,
      "step": 9469
    },
    {
      "epoch": 0.04454500127002643,
      "grad_norm": 1.7271381616592407,
      "learning_rate": 0.00019154148632210248,
      "loss": 0.0807,
      "step": 9470
    },
    {
      "epoch": 0.0445497050716389,
      "grad_norm": 3.449650764465332,
      "learning_rate": 0.000191540543343989,
      "loss": 0.6377,
      "step": 9471
    },
    {
      "epoch": 0.044554408873251364,
      "grad_norm": 1.827063798904419,
      "learning_rate": 0.00019153960036587551,
      "loss": 0.2898,
      "step": 9472
    },
    {
      "epoch": 0.04455911267486382,
      "grad_norm": 2.2858574390411377,
      "learning_rate": 0.00019153865738776203,
      "loss": 0.4398,
      "step": 9473
    },
    {
      "epoch": 0.04456381647647629,
      "grad_norm": 0.7946672439575195,
      "learning_rate": 0.00019153771440964855,
      "loss": 0.0616,
      "step": 9474
    },
    {
      "epoch": 0.044568520278088754,
      "grad_norm": 0.7103960514068604,
      "learning_rate": 0.00019153677143153507,
      "loss": 0.0765,
      "step": 9475
    },
    {
      "epoch": 0.04457322407970121,
      "grad_norm": 1.028159260749817,
      "learning_rate": 0.0001915358284534216,
      "loss": 0.1226,
      "step": 9476
    },
    {
      "epoch": 0.04457792788131368,
      "grad_norm": 3.898463726043701,
      "learning_rate": 0.00019153488547530813,
      "loss": 0.2898,
      "step": 9477
    },
    {
      "epoch": 0.044582631682926144,
      "grad_norm": 0.5810986161231995,
      "learning_rate": 0.00019153394249719465,
      "loss": 0.0527,
      "step": 9478
    },
    {
      "epoch": 0.0445873354845386,
      "grad_norm": 1.6916435956954956,
      "learning_rate": 0.00019153299951908117,
      "loss": 0.3293,
      "step": 9479
    },
    {
      "epoch": 0.04459203928615107,
      "grad_norm": 3.700207471847534,
      "learning_rate": 0.0001915320565409677,
      "loss": 0.3232,
      "step": 9480
    },
    {
      "epoch": 0.044596743087763534,
      "grad_norm": 0.3125889301300049,
      "learning_rate": 0.00019153111356285424,
      "loss": 0.0347,
      "step": 9481
    },
    {
      "epoch": 0.04460144688937599,
      "grad_norm": 8.352777481079102,
      "learning_rate": 0.00019153017058474075,
      "loss": 0.4653,
      "step": 9482
    },
    {
      "epoch": 0.04460615069098846,
      "grad_norm": 1.650004267692566,
      "learning_rate": 0.00019152922760662725,
      "loss": 0.2039,
      "step": 9483
    },
    {
      "epoch": 0.04461085449260092,
      "grad_norm": 3.762859582901001,
      "learning_rate": 0.00019152828462851376,
      "loss": 0.6,
      "step": 9484
    },
    {
      "epoch": 0.04461555829421338,
      "grad_norm": 2.9199674129486084,
      "learning_rate": 0.00019152734165040028,
      "loss": 0.5287,
      "step": 9485
    },
    {
      "epoch": 0.04462026209582585,
      "grad_norm": 1.2894511222839355,
      "learning_rate": 0.00019152639867228683,
      "loss": 0.1407,
      "step": 9486
    },
    {
      "epoch": 0.04462496589743831,
      "grad_norm": 0.5348854660987854,
      "learning_rate": 0.00019152545569417335,
      "loss": 0.0648,
      "step": 9487
    },
    {
      "epoch": 0.04462966969905077,
      "grad_norm": 1.6658704280853271,
      "learning_rate": 0.00019152451271605987,
      "loss": 0.2063,
      "step": 9488
    },
    {
      "epoch": 0.04463437350066324,
      "grad_norm": 5.508810997009277,
      "learning_rate": 0.00019152356973794639,
      "loss": 0.8646,
      "step": 9489
    },
    {
      "epoch": 0.0446390773022757,
      "grad_norm": 2.3673055171966553,
      "learning_rate": 0.00019152262675983293,
      "loss": 0.2795,
      "step": 9490
    },
    {
      "epoch": 0.04464378110388816,
      "grad_norm": 3.6816790103912354,
      "learning_rate": 0.00019152168378171945,
      "loss": 0.5547,
      "step": 9491
    },
    {
      "epoch": 0.04464848490550063,
      "grad_norm": 1.07089102268219,
      "learning_rate": 0.00019152074080360597,
      "loss": 0.0672,
      "step": 9492
    },
    {
      "epoch": 0.04465318870711309,
      "grad_norm": 2.904729127883911,
      "learning_rate": 0.0001915197978254925,
      "loss": 0.43,
      "step": 9493
    },
    {
      "epoch": 0.04465789250872555,
      "grad_norm": 1.641719937324524,
      "learning_rate": 0.000191518854847379,
      "loss": 0.1736,
      "step": 9494
    },
    {
      "epoch": 0.04466259631033802,
      "grad_norm": 4.2221903800964355,
      "learning_rate": 0.00019151791186926552,
      "loss": 1.0363,
      "step": 9495
    },
    {
      "epoch": 0.04466730011195048,
      "grad_norm": 0.4254668354988098,
      "learning_rate": 0.00019151696889115204,
      "loss": 0.0516,
      "step": 9496
    },
    {
      "epoch": 0.04467200391356294,
      "grad_norm": 2.729271650314331,
      "learning_rate": 0.00019151602591303856,
      "loss": 0.4279,
      "step": 9497
    },
    {
      "epoch": 0.04467670771517541,
      "grad_norm": 2.6028032302856445,
      "learning_rate": 0.00019151508293492508,
      "loss": 0.2453,
      "step": 9498
    },
    {
      "epoch": 0.04468141151678787,
      "grad_norm": 1.4296092987060547,
      "learning_rate": 0.00019151413995681163,
      "loss": 0.1374,
      "step": 9499
    },
    {
      "epoch": 0.04468611531840033,
      "grad_norm": 1.8525527715682983,
      "learning_rate": 0.00019151319697869814,
      "loss": 0.3746,
      "step": 9500
    },
    {
      "epoch": 0.04469081912001279,
      "grad_norm": 1.0045181512832642,
      "learning_rate": 0.00019151225400058466,
      "loss": 0.0613,
      "step": 9501
    },
    {
      "epoch": 0.04469552292162526,
      "grad_norm": 2.25417423248291,
      "learning_rate": 0.00019151131102247118,
      "loss": 0.2594,
      "step": 9502
    },
    {
      "epoch": 0.04470022672323772,
      "grad_norm": 0.3501725196838379,
      "learning_rate": 0.0001915103680443577,
      "loss": 0.0434,
      "step": 9503
    },
    {
      "epoch": 0.04470493052485018,
      "grad_norm": 1.9270659685134888,
      "learning_rate": 0.00019150942506624422,
      "loss": 0.2412,
      "step": 9504
    },
    {
      "epoch": 0.04470963432646265,
      "grad_norm": 0.7014861702919006,
      "learning_rate": 0.00019150848208813074,
      "loss": 0.0572,
      "step": 9505
    },
    {
      "epoch": 0.04471433812807511,
      "grad_norm": 3.493929624557495,
      "learning_rate": 0.00019150753911001726,
      "loss": 0.6664,
      "step": 9506
    },
    {
      "epoch": 0.04471904192968757,
      "grad_norm": 2.436882734298706,
      "learning_rate": 0.00019150659613190377,
      "loss": 0.397,
      "step": 9507
    },
    {
      "epoch": 0.04472374573130004,
      "grad_norm": 1.7580534219741821,
      "learning_rate": 0.0001915056531537903,
      "loss": 0.1839,
      "step": 9508
    },
    {
      "epoch": 0.0447284495329125,
      "grad_norm": 0.5435834527015686,
      "learning_rate": 0.00019150471017567684,
      "loss": 0.0382,
      "step": 9509
    },
    {
      "epoch": 0.04473315333452496,
      "grad_norm": 2.354473114013672,
      "learning_rate": 0.00019150376719756336,
      "loss": 0.269,
      "step": 9510
    },
    {
      "epoch": 0.04473785713613743,
      "grad_norm": 1.432510495185852,
      "learning_rate": 0.00019150282421944988,
      "loss": 0.1413,
      "step": 9511
    },
    {
      "epoch": 0.04474256093774989,
      "grad_norm": 3.704261302947998,
      "learning_rate": 0.0001915018812413364,
      "loss": 0.5346,
      "step": 9512
    },
    {
      "epoch": 0.04474726473936235,
      "grad_norm": 1.93244206905365,
      "learning_rate": 0.00019150093826322294,
      "loss": 0.3845,
      "step": 9513
    },
    {
      "epoch": 0.04475196854097482,
      "grad_norm": 1.4671881198883057,
      "learning_rate": 0.00019149999528510943,
      "loss": 0.1136,
      "step": 9514
    },
    {
      "epoch": 0.04475667234258728,
      "grad_norm": 2.3782286643981934,
      "learning_rate": 0.00019149905230699595,
      "loss": 0.2995,
      "step": 9515
    },
    {
      "epoch": 0.04476137614419974,
      "grad_norm": 0.9962984323501587,
      "learning_rate": 0.00019149810932888247,
      "loss": 0.0776,
      "step": 9516
    },
    {
      "epoch": 0.04476607994581221,
      "grad_norm": 2.0800392627716064,
      "learning_rate": 0.000191497166350769,
      "loss": 0.1987,
      "step": 9517
    },
    {
      "epoch": 0.044770783747424665,
      "grad_norm": 0.7375709414482117,
      "learning_rate": 0.00019149622337265553,
      "loss": 0.0712,
      "step": 9518
    },
    {
      "epoch": 0.04477548754903713,
      "grad_norm": 3.466390609741211,
      "learning_rate": 0.00019149528039454205,
      "loss": 0.3504,
      "step": 9519
    },
    {
      "epoch": 0.0447801913506496,
      "grad_norm": 1.859695553779602,
      "learning_rate": 0.00019149433741642857,
      "loss": 0.1771,
      "step": 9520
    },
    {
      "epoch": 0.044784895152262055,
      "grad_norm": 1.2208752632141113,
      "learning_rate": 0.0001914933944383151,
      "loss": 0.1667,
      "step": 9521
    },
    {
      "epoch": 0.04478959895387452,
      "grad_norm": 3.4495456218719482,
      "learning_rate": 0.00019149245146020164,
      "loss": 0.5522,
      "step": 9522
    },
    {
      "epoch": 0.04479430275548699,
      "grad_norm": 1.120222568511963,
      "learning_rate": 0.00019149150848208815,
      "loss": 0.1021,
      "step": 9523
    },
    {
      "epoch": 0.044799006557099445,
      "grad_norm": 2.5539584159851074,
      "learning_rate": 0.00019149056550397467,
      "loss": 0.3528,
      "step": 9524
    },
    {
      "epoch": 0.04480371035871191,
      "grad_norm": 2.240122079849243,
      "learning_rate": 0.0001914896225258612,
      "loss": 0.2829,
      "step": 9525
    },
    {
      "epoch": 0.04480841416032438,
      "grad_norm": 1.6538630723953247,
      "learning_rate": 0.00019148867954774768,
      "loss": 0.1381,
      "step": 9526
    },
    {
      "epoch": 0.044813117961936835,
      "grad_norm": 1.4064197540283203,
      "learning_rate": 0.00019148773656963423,
      "loss": 0.247,
      "step": 9527
    },
    {
      "epoch": 0.0448178217635493,
      "grad_norm": 5.8737592697143555,
      "learning_rate": 0.00019148679359152075,
      "loss": 0.5798,
      "step": 9528
    },
    {
      "epoch": 0.04482252556516177,
      "grad_norm": 1.184112548828125,
      "learning_rate": 0.00019148585061340727,
      "loss": 0.0559,
      "step": 9529
    },
    {
      "epoch": 0.044827229366774225,
      "grad_norm": 1.017643928527832,
      "learning_rate": 0.00019148490763529379,
      "loss": 0.0905,
      "step": 9530
    },
    {
      "epoch": 0.04483193316838669,
      "grad_norm": 1.8578709363937378,
      "learning_rate": 0.00019148396465718033,
      "loss": 0.4309,
      "step": 9531
    },
    {
      "epoch": 0.04483663696999916,
      "grad_norm": 0.9298057556152344,
      "learning_rate": 0.00019148302167906685,
      "loss": 0.0829,
      "step": 9532
    },
    {
      "epoch": 0.044841340771611615,
      "grad_norm": 1.6319903135299683,
      "learning_rate": 0.00019148207870095337,
      "loss": 0.0961,
      "step": 9533
    },
    {
      "epoch": 0.04484604457322408,
      "grad_norm": 1.3387360572814941,
      "learning_rate": 0.0001914811357228399,
      "loss": 0.1971,
      "step": 9534
    },
    {
      "epoch": 0.04485074837483654,
      "grad_norm": 0.23198209702968597,
      "learning_rate": 0.0001914801927447264,
      "loss": 0.0136,
      "step": 9535
    },
    {
      "epoch": 0.044855452176449005,
      "grad_norm": 0.8246077299118042,
      "learning_rate": 0.00019147924976661292,
      "loss": 0.0911,
      "step": 9536
    },
    {
      "epoch": 0.04486015597806147,
      "grad_norm": 1.6378506422042847,
      "learning_rate": 0.00019147830678849944,
      "loss": 0.2613,
      "step": 9537
    },
    {
      "epoch": 0.04486485977967393,
      "grad_norm": 1.1529600620269775,
      "learning_rate": 0.00019147736381038596,
      "loss": 0.0824,
      "step": 9538
    },
    {
      "epoch": 0.044869563581286395,
      "grad_norm": 0.894176185131073,
      "learning_rate": 0.00019147642083227248,
      "loss": 0.0809,
      "step": 9539
    },
    {
      "epoch": 0.04487426738289886,
      "grad_norm": 2.539034843444824,
      "learning_rate": 0.00019147547785415903,
      "loss": 0.2318,
      "step": 9540
    },
    {
      "epoch": 0.04487897118451132,
      "grad_norm": 2.408963441848755,
      "learning_rate": 0.00019147453487604554,
      "loss": 0.4591,
      "step": 9541
    },
    {
      "epoch": 0.044883674986123785,
      "grad_norm": 4.750478267669678,
      "learning_rate": 0.00019147359189793206,
      "loss": 0.5149,
      "step": 9542
    },
    {
      "epoch": 0.04488837878773625,
      "grad_norm": 1.0114398002624512,
      "learning_rate": 0.00019147264891981858,
      "loss": 0.0771,
      "step": 9543
    },
    {
      "epoch": 0.04489308258934871,
      "grad_norm": 3.732712984085083,
      "learning_rate": 0.0001914717059417051,
      "loss": 0.512,
      "step": 9544
    },
    {
      "epoch": 0.044897786390961175,
      "grad_norm": 10.706838607788086,
      "learning_rate": 0.00019147076296359162,
      "loss": 0.0938,
      "step": 9545
    },
    {
      "epoch": 0.04490249019257364,
      "grad_norm": 7.208127975463867,
      "learning_rate": 0.00019146981998547814,
      "loss": 0.8203,
      "step": 9546
    },
    {
      "epoch": 0.0449071939941861,
      "grad_norm": 4.307689666748047,
      "learning_rate": 0.00019146887700736466,
      "loss": 0.7102,
      "step": 9547
    },
    {
      "epoch": 0.044911897795798565,
      "grad_norm": 2.166926383972168,
      "learning_rate": 0.00019146793402925117,
      "loss": 0.4554,
      "step": 9548
    },
    {
      "epoch": 0.04491660159741103,
      "grad_norm": 2.8010101318359375,
      "learning_rate": 0.00019146699105113772,
      "loss": 0.3288,
      "step": 9549
    },
    {
      "epoch": 0.04492130539902349,
      "grad_norm": 1.748716115951538,
      "learning_rate": 0.00019146604807302424,
      "loss": 0.1404,
      "step": 9550
    },
    {
      "epoch": 0.044926009200635955,
      "grad_norm": 5.215689659118652,
      "learning_rate": 0.00019146510509491076,
      "loss": 1.5536,
      "step": 9551
    },
    {
      "epoch": 0.044930713002248414,
      "grad_norm": 1.0602961778640747,
      "learning_rate": 0.00019146416211679728,
      "loss": 0.0634,
      "step": 9552
    },
    {
      "epoch": 0.04493541680386088,
      "grad_norm": 2.793332099914551,
      "learning_rate": 0.0001914632191386838,
      "loss": 0.4749,
      "step": 9553
    },
    {
      "epoch": 0.044940120605473345,
      "grad_norm": 2.1746132373809814,
      "learning_rate": 0.00019146227616057034,
      "loss": 0.2881,
      "step": 9554
    },
    {
      "epoch": 0.044944824407085804,
      "grad_norm": 0.9854423999786377,
      "learning_rate": 0.00019146133318245686,
      "loss": 0.063,
      "step": 9555
    },
    {
      "epoch": 0.04494952820869827,
      "grad_norm": 2.4441208839416504,
      "learning_rate": 0.00019146039020434338,
      "loss": 0.2286,
      "step": 9556
    },
    {
      "epoch": 0.044954232010310735,
      "grad_norm": 0.6844735145568848,
      "learning_rate": 0.00019145944722622987,
      "loss": 0.0496,
      "step": 9557
    },
    {
      "epoch": 0.044958935811923194,
      "grad_norm": 2.1245455741882324,
      "learning_rate": 0.0001914585042481164,
      "loss": 0.1558,
      "step": 9558
    },
    {
      "epoch": 0.04496363961353566,
      "grad_norm": 5.408128261566162,
      "learning_rate": 0.00019145756127000293,
      "loss": 0.6413,
      "step": 9559
    },
    {
      "epoch": 0.044968343415148125,
      "grad_norm": 8.034552574157715,
      "learning_rate": 0.00019145661829188945,
      "loss": 0.61,
      "step": 9560
    },
    {
      "epoch": 0.044973047216760584,
      "grad_norm": 3.8501882553100586,
      "learning_rate": 0.00019145567531377597,
      "loss": 0.2417,
      "step": 9561
    },
    {
      "epoch": 0.04497775101837305,
      "grad_norm": 2.5449233055114746,
      "learning_rate": 0.0001914547323356625,
      "loss": 0.2066,
      "step": 9562
    },
    {
      "epoch": 0.044982454819985515,
      "grad_norm": 2.672520875930786,
      "learning_rate": 0.00019145378935754904,
      "loss": 0.242,
      "step": 9563
    },
    {
      "epoch": 0.044987158621597974,
      "grad_norm": 2.7141592502593994,
      "learning_rate": 0.00019145284637943555,
      "loss": 0.3535,
      "step": 9564
    },
    {
      "epoch": 0.04499186242321044,
      "grad_norm": 1.6097767353057861,
      "learning_rate": 0.00019145190340132207,
      "loss": 0.1706,
      "step": 9565
    },
    {
      "epoch": 0.044996566224822905,
      "grad_norm": 2.186739444732666,
      "learning_rate": 0.0001914509604232086,
      "loss": 0.3062,
      "step": 9566
    },
    {
      "epoch": 0.045001270026435364,
      "grad_norm": 4.6665544509887695,
      "learning_rate": 0.0001914500174450951,
      "loss": 0.7442,
      "step": 9567
    },
    {
      "epoch": 0.04500597382804783,
      "grad_norm": 1.6764848232269287,
      "learning_rate": 0.00019144907446698163,
      "loss": 0.1448,
      "step": 9568
    },
    {
      "epoch": 0.04501067762966029,
      "grad_norm": 4.901820659637451,
      "learning_rate": 0.00019144813148886815,
      "loss": 0.8321,
      "step": 9569
    },
    {
      "epoch": 0.045015381431272754,
      "grad_norm": 3.2157773971557617,
      "learning_rate": 0.00019144718851075467,
      "loss": 0.6674,
      "step": 9570
    },
    {
      "epoch": 0.04502008523288522,
      "grad_norm": 4.117443084716797,
      "learning_rate": 0.00019144624553264118,
      "loss": 1.2418,
      "step": 9571
    },
    {
      "epoch": 0.04502478903449768,
      "grad_norm": 2.4020228385925293,
      "learning_rate": 0.00019144530255452773,
      "loss": 0.3538,
      "step": 9572
    },
    {
      "epoch": 0.045029492836110144,
      "grad_norm": 0.8307231068611145,
      "learning_rate": 0.00019144435957641425,
      "loss": 0.086,
      "step": 9573
    },
    {
      "epoch": 0.04503419663772261,
      "grad_norm": 0.8610669374465942,
      "learning_rate": 0.00019144341659830077,
      "loss": 0.0827,
      "step": 9574
    },
    {
      "epoch": 0.04503890043933507,
      "grad_norm": 1.8385374546051025,
      "learning_rate": 0.0001914424736201873,
      "loss": 0.533,
      "step": 9575
    },
    {
      "epoch": 0.045043604240947534,
      "grad_norm": 0.5380725860595703,
      "learning_rate": 0.0001914415306420738,
      "loss": 0.0487,
      "step": 9576
    },
    {
      "epoch": 0.04504830804256,
      "grad_norm": 0.8660648465156555,
      "learning_rate": 0.00019144058766396032,
      "loss": 0.1865,
      "step": 9577
    },
    {
      "epoch": 0.04505301184417246,
      "grad_norm": 1.391093373298645,
      "learning_rate": 0.00019143964468584684,
      "loss": 0.2569,
      "step": 9578
    },
    {
      "epoch": 0.045057715645784924,
      "grad_norm": 0.985610842704773,
      "learning_rate": 0.00019143870170773336,
      "loss": 0.0813,
      "step": 9579
    },
    {
      "epoch": 0.04506241944739739,
      "grad_norm": 1.8607450723648071,
      "learning_rate": 0.00019143775872961988,
      "loss": 0.3689,
      "step": 9580
    },
    {
      "epoch": 0.04506712324900985,
      "grad_norm": 1.3018145561218262,
      "learning_rate": 0.00019143681575150643,
      "loss": 0.2069,
      "step": 9581
    },
    {
      "epoch": 0.045071827050622314,
      "grad_norm": 0.4048192501068115,
      "learning_rate": 0.00019143587277339294,
      "loss": 0.0372,
      "step": 9582
    },
    {
      "epoch": 0.04507653085223478,
      "grad_norm": 1.134060025215149,
      "learning_rate": 0.00019143492979527946,
      "loss": 0.1159,
      "step": 9583
    },
    {
      "epoch": 0.04508123465384724,
      "grad_norm": 2.8159584999084473,
      "learning_rate": 0.00019143398681716598,
      "loss": 0.5464,
      "step": 9584
    },
    {
      "epoch": 0.045085938455459704,
      "grad_norm": 1.4448537826538086,
      "learning_rate": 0.0001914330438390525,
      "loss": 0.2645,
      "step": 9585
    },
    {
      "epoch": 0.04509064225707216,
      "grad_norm": 2.793135643005371,
      "learning_rate": 0.00019143210086093905,
      "loss": 0.6536,
      "step": 9586
    },
    {
      "epoch": 0.04509534605868463,
      "grad_norm": 0.5166053771972656,
      "learning_rate": 0.00019143115788282556,
      "loss": 0.0517,
      "step": 9587
    },
    {
      "epoch": 0.045100049860297094,
      "grad_norm": 2.4873645305633545,
      "learning_rate": 0.00019143021490471206,
      "loss": 0.5601,
      "step": 9588
    },
    {
      "epoch": 0.04510475366190955,
      "grad_norm": 1.0258771181106567,
      "learning_rate": 0.00019142927192659857,
      "loss": 0.3092,
      "step": 9589
    },
    {
      "epoch": 0.04510945746352202,
      "grad_norm": 0.9963788986206055,
      "learning_rate": 0.00019142832894848512,
      "loss": 0.1594,
      "step": 9590
    },
    {
      "epoch": 0.045114161265134484,
      "grad_norm": 1.7658405303955078,
      "learning_rate": 0.00019142738597037164,
      "loss": 0.4333,
      "step": 9591
    },
    {
      "epoch": 0.04511886506674694,
      "grad_norm": 1.579163670539856,
      "learning_rate": 0.00019142644299225816,
      "loss": 0.4391,
      "step": 9592
    },
    {
      "epoch": 0.04512356886835941,
      "grad_norm": 2.9082281589508057,
      "learning_rate": 0.00019142550001414468,
      "loss": 0.6464,
      "step": 9593
    },
    {
      "epoch": 0.045128272669971874,
      "grad_norm": 0.791445255279541,
      "learning_rate": 0.0001914245570360312,
      "loss": 0.1195,
      "step": 9594
    },
    {
      "epoch": 0.04513297647158433,
      "grad_norm": 1.6703838109970093,
      "learning_rate": 0.00019142361405791774,
      "loss": 0.6047,
      "step": 9595
    },
    {
      "epoch": 0.0451376802731968,
      "grad_norm": 4.261801242828369,
      "learning_rate": 0.00019142267107980426,
      "loss": 0.5942,
      "step": 9596
    },
    {
      "epoch": 0.045142384074809264,
      "grad_norm": 1.1312131881713867,
      "learning_rate": 0.00019142172810169078,
      "loss": 0.2015,
      "step": 9597
    },
    {
      "epoch": 0.04514708787642172,
      "grad_norm": 1.5758602619171143,
      "learning_rate": 0.0001914207851235773,
      "loss": 0.3612,
      "step": 9598
    },
    {
      "epoch": 0.04515179167803419,
      "grad_norm": 1.0590506792068481,
      "learning_rate": 0.00019141984214546382,
      "loss": 0.1538,
      "step": 9599
    },
    {
      "epoch": 0.045156495479646654,
      "grad_norm": 1.4721826314926147,
      "learning_rate": 0.00019141889916735033,
      "loss": 0.4587,
      "step": 9600
    },
    {
      "epoch": 0.04516119928125911,
      "grad_norm": 1.7627671957015991,
      "learning_rate": 0.00019141795618923685,
      "loss": 0.2224,
      "step": 9601
    },
    {
      "epoch": 0.04516590308287158,
      "grad_norm": 2.49188494682312,
      "learning_rate": 0.00019141701321112337,
      "loss": 0.2451,
      "step": 9602
    },
    {
      "epoch": 0.04517060688448404,
      "grad_norm": 2.5748414993286133,
      "learning_rate": 0.0001914160702330099,
      "loss": 0.5372,
      "step": 9603
    },
    {
      "epoch": 0.0451753106860965,
      "grad_norm": 0.4529407322406769,
      "learning_rate": 0.00019141512725489644,
      "loss": 0.0708,
      "step": 9604
    },
    {
      "epoch": 0.04518001448770897,
      "grad_norm": 1.4931907653808594,
      "learning_rate": 0.00019141418427678295,
      "loss": 0.3498,
      "step": 9605
    },
    {
      "epoch": 0.04518471828932143,
      "grad_norm": 0.9429553151130676,
      "learning_rate": 0.00019141324129866947,
      "loss": 0.1195,
      "step": 9606
    },
    {
      "epoch": 0.04518942209093389,
      "grad_norm": 1.8816579580307007,
      "learning_rate": 0.000191412298320556,
      "loss": 0.3691,
      "step": 9607
    },
    {
      "epoch": 0.04519412589254636,
      "grad_norm": 2.120579957962036,
      "learning_rate": 0.0001914113553424425,
      "loss": 0.4501,
      "step": 9608
    },
    {
      "epoch": 0.04519882969415882,
      "grad_norm": 1.8677126169204712,
      "learning_rate": 0.00019141041236432903,
      "loss": 0.3776,
      "step": 9609
    },
    {
      "epoch": 0.04520353349577128,
      "grad_norm": 2.5934267044067383,
      "learning_rate": 0.00019140946938621555,
      "loss": 0.6021,
      "step": 9610
    },
    {
      "epoch": 0.04520823729738375,
      "grad_norm": 1.371403455734253,
      "learning_rate": 0.00019140852640810207,
      "loss": 0.1736,
      "step": 9611
    },
    {
      "epoch": 0.04521294109899621,
      "grad_norm": 2.5684099197387695,
      "learning_rate": 0.00019140758342998858,
      "loss": 0.3474,
      "step": 9612
    },
    {
      "epoch": 0.04521764490060867,
      "grad_norm": 1.3323367834091187,
      "learning_rate": 0.00019140664045187513,
      "loss": 0.1944,
      "step": 9613
    },
    {
      "epoch": 0.04522234870222114,
      "grad_norm": 0.9631009101867676,
      "learning_rate": 0.00019140569747376165,
      "loss": 0.1474,
      "step": 9614
    },
    {
      "epoch": 0.0452270525038336,
      "grad_norm": 0.9423693418502808,
      "learning_rate": 0.00019140475449564817,
      "loss": 0.229,
      "step": 9615
    },
    {
      "epoch": 0.04523175630544606,
      "grad_norm": 1.0860456228256226,
      "learning_rate": 0.0001914038115175347,
      "loss": 0.1742,
      "step": 9616
    },
    {
      "epoch": 0.04523646010705853,
      "grad_norm": 1.728806734085083,
      "learning_rate": 0.0001914028685394212,
      "loss": 0.3218,
      "step": 9617
    },
    {
      "epoch": 0.04524116390867099,
      "grad_norm": 2.475644826889038,
      "learning_rate": 0.00019140192556130775,
      "loss": 0.6729,
      "step": 9618
    },
    {
      "epoch": 0.04524586771028345,
      "grad_norm": 1.438589334487915,
      "learning_rate": 0.00019140098258319424,
      "loss": 0.1874,
      "step": 9619
    },
    {
      "epoch": 0.04525057151189591,
      "grad_norm": 1.5881389379501343,
      "learning_rate": 0.00019140003960508076,
      "loss": 0.3363,
      "step": 9620
    },
    {
      "epoch": 0.04525527531350838,
      "grad_norm": 1.0091170072555542,
      "learning_rate": 0.00019139909662696728,
      "loss": 0.1863,
      "step": 9621
    },
    {
      "epoch": 0.04525997911512084,
      "grad_norm": 1.3824095726013184,
      "learning_rate": 0.00019139815364885383,
      "loss": 0.2409,
      "step": 9622
    },
    {
      "epoch": 0.0452646829167333,
      "grad_norm": 2.187727212905884,
      "learning_rate": 0.00019139721067074034,
      "loss": 0.2112,
      "step": 9623
    },
    {
      "epoch": 0.04526938671834577,
      "grad_norm": 3.106304883956909,
      "learning_rate": 0.00019139626769262686,
      "loss": 0.8996,
      "step": 9624
    },
    {
      "epoch": 0.04527409051995823,
      "grad_norm": 1.2818355560302734,
      "learning_rate": 0.00019139532471451338,
      "loss": 0.2831,
      "step": 9625
    },
    {
      "epoch": 0.04527879432157069,
      "grad_norm": 1.315005898475647,
      "learning_rate": 0.0001913943817363999,
      "loss": 0.2631,
      "step": 9626
    },
    {
      "epoch": 0.04528349812318316,
      "grad_norm": 0.4512568712234497,
      "learning_rate": 0.00019139343875828645,
      "loss": 0.0668,
      "step": 9627
    },
    {
      "epoch": 0.04528820192479562,
      "grad_norm": 1.1531280279159546,
      "learning_rate": 0.00019139249578017296,
      "loss": 0.1379,
      "step": 9628
    },
    {
      "epoch": 0.04529290572640808,
      "grad_norm": 0.9210083484649658,
      "learning_rate": 0.00019139155280205948,
      "loss": 0.1036,
      "step": 9629
    },
    {
      "epoch": 0.045297609528020547,
      "grad_norm": 1.2869800329208374,
      "learning_rate": 0.00019139060982394597,
      "loss": 0.1891,
      "step": 9630
    },
    {
      "epoch": 0.04530231332963301,
      "grad_norm": 1.300460696220398,
      "learning_rate": 0.00019138966684583252,
      "loss": 0.1328,
      "step": 9631
    },
    {
      "epoch": 0.04530701713124547,
      "grad_norm": 2.1320786476135254,
      "learning_rate": 0.00019138872386771904,
      "loss": 0.4407,
      "step": 9632
    },
    {
      "epoch": 0.045311720932857937,
      "grad_norm": 1.751738429069519,
      "learning_rate": 0.00019138778088960556,
      "loss": 0.3956,
      "step": 9633
    },
    {
      "epoch": 0.0453164247344704,
      "grad_norm": 1.131924033164978,
      "learning_rate": 0.00019138683791149208,
      "loss": 0.1617,
      "step": 9634
    },
    {
      "epoch": 0.04532112853608286,
      "grad_norm": 0.8012063503265381,
      "learning_rate": 0.0001913858949333786,
      "loss": 0.1075,
      "step": 9635
    },
    {
      "epoch": 0.045325832337695326,
      "grad_norm": 2.8624701499938965,
      "learning_rate": 0.00019138495195526514,
      "loss": 0.7454,
      "step": 9636
    },
    {
      "epoch": 0.045330536139307785,
      "grad_norm": 0.7747941017150879,
      "learning_rate": 0.00019138400897715166,
      "loss": 0.1196,
      "step": 9637
    },
    {
      "epoch": 0.04533523994092025,
      "grad_norm": 0.6088622808456421,
      "learning_rate": 0.00019138306599903818,
      "loss": 0.0829,
      "step": 9638
    },
    {
      "epoch": 0.045339943742532716,
      "grad_norm": 0.9919500946998596,
      "learning_rate": 0.0001913821230209247,
      "loss": 0.1405,
      "step": 9639
    },
    {
      "epoch": 0.045344647544145175,
      "grad_norm": 3.055894613265991,
      "learning_rate": 0.00019138118004281122,
      "loss": 0.3274,
      "step": 9640
    },
    {
      "epoch": 0.04534935134575764,
      "grad_norm": 1.1422098875045776,
      "learning_rate": 0.00019138023706469773,
      "loss": 0.2255,
      "step": 9641
    },
    {
      "epoch": 0.045354055147370106,
      "grad_norm": 3.45367169380188,
      "learning_rate": 0.00019137929408658425,
      "loss": 0.7332,
      "step": 9642
    },
    {
      "epoch": 0.045358758948982565,
      "grad_norm": 0.6512478590011597,
      "learning_rate": 0.00019137835110847077,
      "loss": 0.0669,
      "step": 9643
    },
    {
      "epoch": 0.04536346275059503,
      "grad_norm": 0.3452609181404114,
      "learning_rate": 0.0001913774081303573,
      "loss": 0.0339,
      "step": 9644
    },
    {
      "epoch": 0.045368166552207496,
      "grad_norm": 1.7490631341934204,
      "learning_rate": 0.00019137646515224384,
      "loss": 0.2843,
      "step": 9645
    },
    {
      "epoch": 0.045372870353819955,
      "grad_norm": 6.320898056030273,
      "learning_rate": 0.00019137552217413035,
      "loss": 0.4457,
      "step": 9646
    },
    {
      "epoch": 0.04537757415543242,
      "grad_norm": 0.7174249887466431,
      "learning_rate": 0.00019137457919601687,
      "loss": 0.1389,
      "step": 9647
    },
    {
      "epoch": 0.045382277957044886,
      "grad_norm": 4.3091206550598145,
      "learning_rate": 0.0001913736362179034,
      "loss": 0.3715,
      "step": 9648
    },
    {
      "epoch": 0.045386981758657345,
      "grad_norm": 0.5584757924079895,
      "learning_rate": 0.00019137269323978994,
      "loss": 0.0424,
      "step": 9649
    },
    {
      "epoch": 0.04539168556026981,
      "grad_norm": 1.884352207183838,
      "learning_rate": 0.00019137175026167643,
      "loss": 0.2374,
      "step": 9650
    },
    {
      "epoch": 0.045396389361882276,
      "grad_norm": 7.360207557678223,
      "learning_rate": 0.00019137080728356295,
      "loss": 0.6069,
      "step": 9651
    },
    {
      "epoch": 0.045401093163494735,
      "grad_norm": 8.741899490356445,
      "learning_rate": 0.00019136986430544947,
      "loss": 0.7419,
      "step": 9652
    },
    {
      "epoch": 0.0454057969651072,
      "grad_norm": 0.9447444081306458,
      "learning_rate": 0.00019136892132733598,
      "loss": 0.073,
      "step": 9653
    },
    {
      "epoch": 0.04541050076671966,
      "grad_norm": 2.410097360610962,
      "learning_rate": 0.00019136797834922253,
      "loss": 0.1222,
      "step": 9654
    },
    {
      "epoch": 0.045415204568332125,
      "grad_norm": 3.5352699756622314,
      "learning_rate": 0.00019136703537110905,
      "loss": 0.2441,
      "step": 9655
    },
    {
      "epoch": 0.04541990836994459,
      "grad_norm": 3.1039552688598633,
      "learning_rate": 0.00019136609239299557,
      "loss": 0.4433,
      "step": 9656
    },
    {
      "epoch": 0.04542461217155705,
      "grad_norm": 1.7673531770706177,
      "learning_rate": 0.0001913651494148821,
      "loss": 0.0747,
      "step": 9657
    },
    {
      "epoch": 0.045429315973169515,
      "grad_norm": 2.580288887023926,
      "learning_rate": 0.00019136420643676863,
      "loss": 0.1879,
      "step": 9658
    },
    {
      "epoch": 0.04543401977478198,
      "grad_norm": 4.871906757354736,
      "learning_rate": 0.00019136326345865515,
      "loss": 0.2219,
      "step": 9659
    },
    {
      "epoch": 0.04543872357639444,
      "grad_norm": 3.269132375717163,
      "learning_rate": 0.00019136232048054167,
      "loss": 0.1181,
      "step": 9660
    },
    {
      "epoch": 0.045443427378006905,
      "grad_norm": 0.26377788186073303,
      "learning_rate": 0.00019136137750242816,
      "loss": 0.0138,
      "step": 9661
    },
    {
      "epoch": 0.04544813117961937,
      "grad_norm": 2.387697458267212,
      "learning_rate": 0.00019136043452431468,
      "loss": 0.235,
      "step": 9662
    },
    {
      "epoch": 0.04545283498123183,
      "grad_norm": 1.2144172191619873,
      "learning_rate": 0.00019135949154620123,
      "loss": 0.0489,
      "step": 9663
    },
    {
      "epoch": 0.045457538782844295,
      "grad_norm": 4.489652633666992,
      "learning_rate": 0.00019135854856808774,
      "loss": 0.3959,
      "step": 9664
    },
    {
      "epoch": 0.04546224258445676,
      "grad_norm": 3.880502223968506,
      "learning_rate": 0.00019135760558997426,
      "loss": 0.2233,
      "step": 9665
    },
    {
      "epoch": 0.04546694638606922,
      "grad_norm": 2.866675853729248,
      "learning_rate": 0.00019135666261186078,
      "loss": 0.2863,
      "step": 9666
    },
    {
      "epoch": 0.045471650187681685,
      "grad_norm": 0.10675309598445892,
      "learning_rate": 0.0001913557196337473,
      "loss": 0.0041,
      "step": 9667
    },
    {
      "epoch": 0.04547635398929415,
      "grad_norm": 0.8173344731330872,
      "learning_rate": 0.00019135477665563385,
      "loss": 0.0823,
      "step": 9668
    },
    {
      "epoch": 0.04548105779090661,
      "grad_norm": 2.6456685066223145,
      "learning_rate": 0.00019135383367752036,
      "loss": 0.2301,
      "step": 9669
    },
    {
      "epoch": 0.045485761592519075,
      "grad_norm": 0.3107694983482361,
      "learning_rate": 0.00019135289069940688,
      "loss": 0.0154,
      "step": 9670
    },
    {
      "epoch": 0.045490465394131534,
      "grad_norm": 6.0954270362854,
      "learning_rate": 0.0001913519477212934,
      "loss": 0.8362,
      "step": 9671
    },
    {
      "epoch": 0.045495169195744,
      "grad_norm": 4.081809997558594,
      "learning_rate": 0.00019135100474317992,
      "loss": 0.2976,
      "step": 9672
    },
    {
      "epoch": 0.045499872997356465,
      "grad_norm": 1.6209207773208618,
      "learning_rate": 0.00019135006176506644,
      "loss": 0.1237,
      "step": 9673
    },
    {
      "epoch": 0.045504576798968924,
      "grad_norm": 2.811049699783325,
      "learning_rate": 0.00019134911878695296,
      "loss": 0.2402,
      "step": 9674
    },
    {
      "epoch": 0.04550928060058139,
      "grad_norm": 4.6571502685546875,
      "learning_rate": 0.00019134817580883948,
      "loss": 0.6277,
      "step": 9675
    },
    {
      "epoch": 0.045513984402193855,
      "grad_norm": 6.483584880828857,
      "learning_rate": 0.000191347232830726,
      "loss": 1.1007,
      "step": 9676
    },
    {
      "epoch": 0.045518688203806314,
      "grad_norm": 4.20064640045166,
      "learning_rate": 0.00019134628985261254,
      "loss": 0.5358,
      "step": 9677
    },
    {
      "epoch": 0.04552339200541878,
      "grad_norm": 3.7458620071411133,
      "learning_rate": 0.00019134534687449906,
      "loss": 0.3964,
      "step": 9678
    },
    {
      "epoch": 0.045528095807031245,
      "grad_norm": 3.704404354095459,
      "learning_rate": 0.00019134440389638558,
      "loss": 0.2913,
      "step": 9679
    },
    {
      "epoch": 0.045532799608643704,
      "grad_norm": 3.0979840755462646,
      "learning_rate": 0.0001913434609182721,
      "loss": 0.4509,
      "step": 9680
    },
    {
      "epoch": 0.04553750341025617,
      "grad_norm": 5.234127998352051,
      "learning_rate": 0.00019134251794015862,
      "loss": 0.3943,
      "step": 9681
    },
    {
      "epoch": 0.045542207211868635,
      "grad_norm": 3.2768425941467285,
      "learning_rate": 0.00019134157496204513,
      "loss": 0.3518,
      "step": 9682
    },
    {
      "epoch": 0.045546911013481094,
      "grad_norm": 3.3516435623168945,
      "learning_rate": 0.00019134063198393165,
      "loss": 0.312,
      "step": 9683
    },
    {
      "epoch": 0.04555161481509356,
      "grad_norm": 1.7111320495605469,
      "learning_rate": 0.00019133968900581817,
      "loss": 0.152,
      "step": 9684
    },
    {
      "epoch": 0.045556318616706025,
      "grad_norm": 0.486451119184494,
      "learning_rate": 0.0001913387460277047,
      "loss": 0.0769,
      "step": 9685
    },
    {
      "epoch": 0.045561022418318484,
      "grad_norm": 1.9912974834442139,
      "learning_rate": 0.00019133780304959124,
      "loss": 0.2052,
      "step": 9686
    },
    {
      "epoch": 0.04556572621993095,
      "grad_norm": 1.1889210939407349,
      "learning_rate": 0.00019133686007147775,
      "loss": 0.1615,
      "step": 9687
    },
    {
      "epoch": 0.04557043002154341,
      "grad_norm": 1.6188217401504517,
      "learning_rate": 0.00019133591709336427,
      "loss": 0.1166,
      "step": 9688
    },
    {
      "epoch": 0.045575133823155874,
      "grad_norm": 1.669596791267395,
      "learning_rate": 0.0001913349741152508,
      "loss": 0.1871,
      "step": 9689
    },
    {
      "epoch": 0.04557983762476834,
      "grad_norm": 1.418810248374939,
      "learning_rate": 0.00019133403113713734,
      "loss": 0.1581,
      "step": 9690
    },
    {
      "epoch": 0.0455845414263808,
      "grad_norm": 2.418321132659912,
      "learning_rate": 0.00019133308815902386,
      "loss": 0.2501,
      "step": 9691
    },
    {
      "epoch": 0.045589245227993264,
      "grad_norm": 0.32522115111351013,
      "learning_rate": 0.00019133214518091035,
      "loss": 0.0258,
      "step": 9692
    },
    {
      "epoch": 0.04559394902960573,
      "grad_norm": 0.7950681447982788,
      "learning_rate": 0.00019133120220279687,
      "loss": 0.0885,
      "step": 9693
    },
    {
      "epoch": 0.04559865283121819,
      "grad_norm": 0.447155237197876,
      "learning_rate": 0.00019133025922468338,
      "loss": 0.0694,
      "step": 9694
    },
    {
      "epoch": 0.045603356632830654,
      "grad_norm": 0.09494119882583618,
      "learning_rate": 0.00019132931624656993,
      "loss": 0.0053,
      "step": 9695
    },
    {
      "epoch": 0.04560806043444312,
      "grad_norm": 4.020224571228027,
      "learning_rate": 0.00019132837326845645,
      "loss": 0.7832,
      "step": 9696
    },
    {
      "epoch": 0.04561276423605558,
      "grad_norm": 0.0427396260201931,
      "learning_rate": 0.00019132743029034297,
      "loss": 0.002,
      "step": 9697
    },
    {
      "epoch": 0.045617468037668044,
      "grad_norm": 2.3816850185394287,
      "learning_rate": 0.0001913264873122295,
      "loss": 0.2229,
      "step": 9698
    },
    {
      "epoch": 0.04562217183928051,
      "grad_norm": 1.84521484375,
      "learning_rate": 0.00019132554433411603,
      "loss": 0.2769,
      "step": 9699
    },
    {
      "epoch": 0.04562687564089297,
      "grad_norm": 1.9622753858566284,
      "learning_rate": 0.00019132460135600255,
      "loss": 0.3353,
      "step": 9700
    },
    {
      "epoch": 0.045631579442505434,
      "grad_norm": 1.4034438133239746,
      "learning_rate": 0.00019132365837788907,
      "loss": 0.0642,
      "step": 9701
    },
    {
      "epoch": 0.0456362832441179,
      "grad_norm": 5.640469551086426,
      "learning_rate": 0.0001913227153997756,
      "loss": 0.7535,
      "step": 9702
    },
    {
      "epoch": 0.04564098704573036,
      "grad_norm": 3.048708200454712,
      "learning_rate": 0.00019132177242166208,
      "loss": 0.4733,
      "step": 9703
    },
    {
      "epoch": 0.045645690847342824,
      "grad_norm": 0.33938634395599365,
      "learning_rate": 0.00019132082944354863,
      "loss": 0.0433,
      "step": 9704
    },
    {
      "epoch": 0.04565039464895528,
      "grad_norm": 3.6318306922912598,
      "learning_rate": 0.00019131988646543514,
      "loss": 0.0529,
      "step": 9705
    },
    {
      "epoch": 0.04565509845056775,
      "grad_norm": 1.131166934967041,
      "learning_rate": 0.00019131894348732166,
      "loss": 0.0828,
      "step": 9706
    },
    {
      "epoch": 0.045659802252180214,
      "grad_norm": 1.8875257968902588,
      "learning_rate": 0.00019131800050920818,
      "loss": 0.1464,
      "step": 9707
    },
    {
      "epoch": 0.04566450605379267,
      "grad_norm": 0.038854874670505524,
      "learning_rate": 0.00019131705753109473,
      "loss": 0.0018,
      "step": 9708
    },
    {
      "epoch": 0.04566920985540514,
      "grad_norm": 7.53571081161499,
      "learning_rate": 0.00019131611455298125,
      "loss": 0.528,
      "step": 9709
    },
    {
      "epoch": 0.045673913657017604,
      "grad_norm": 1.7056987285614014,
      "learning_rate": 0.00019131517157486776,
      "loss": 0.0643,
      "step": 9710
    },
    {
      "epoch": 0.04567861745863006,
      "grad_norm": 3.8204023838043213,
      "learning_rate": 0.00019131422859675428,
      "loss": 0.5512,
      "step": 9711
    },
    {
      "epoch": 0.04568332126024253,
      "grad_norm": 3.7254323959350586,
      "learning_rate": 0.0001913132856186408,
      "loss": 0.3567,
      "step": 9712
    },
    {
      "epoch": 0.045688025061854994,
      "grad_norm": 3.5423972606658936,
      "learning_rate": 0.00019131234264052732,
      "loss": 0.2943,
      "step": 9713
    },
    {
      "epoch": 0.04569272886346745,
      "grad_norm": 4.265560150146484,
      "learning_rate": 0.00019131139966241384,
      "loss": 0.7336,
      "step": 9714
    },
    {
      "epoch": 0.04569743266507992,
      "grad_norm": 1.0038042068481445,
      "learning_rate": 0.00019131045668430036,
      "loss": 0.0829,
      "step": 9715
    },
    {
      "epoch": 0.045702136466692383,
      "grad_norm": 3.8406982421875,
      "learning_rate": 0.00019130951370618688,
      "loss": 0.6686,
      "step": 9716
    },
    {
      "epoch": 0.04570684026830484,
      "grad_norm": 0.7264934182167053,
      "learning_rate": 0.0001913085707280734,
      "loss": 0.0354,
      "step": 9717
    },
    {
      "epoch": 0.04571154406991731,
      "grad_norm": 1.7174626588821411,
      "learning_rate": 0.00019130762774995994,
      "loss": 0.1558,
      "step": 9718
    },
    {
      "epoch": 0.045716247871529773,
      "grad_norm": 3.1922264099121094,
      "learning_rate": 0.00019130668477184646,
      "loss": 0.7665,
      "step": 9719
    },
    {
      "epoch": 0.04572095167314223,
      "grad_norm": 1.0201733112335205,
      "learning_rate": 0.00019130574179373298,
      "loss": 0.1416,
      "step": 9720
    },
    {
      "epoch": 0.0457256554747547,
      "grad_norm": 2.1412620544433594,
      "learning_rate": 0.0001913047988156195,
      "loss": 0.3873,
      "step": 9721
    },
    {
      "epoch": 0.045730359276367157,
      "grad_norm": 1.2733415365219116,
      "learning_rate": 0.00019130385583750604,
      "loss": 0.1185,
      "step": 9722
    },
    {
      "epoch": 0.04573506307797962,
      "grad_norm": 3.7697842121124268,
      "learning_rate": 0.00019130291285939253,
      "loss": 0.3074,
      "step": 9723
    },
    {
      "epoch": 0.04573976687959209,
      "grad_norm": 3.000079870223999,
      "learning_rate": 0.00019130196988127905,
      "loss": 0.3108,
      "step": 9724
    },
    {
      "epoch": 0.045744470681204547,
      "grad_norm": 0.29796260595321655,
      "learning_rate": 0.00019130102690316557,
      "loss": 0.0246,
      "step": 9725
    },
    {
      "epoch": 0.04574917448281701,
      "grad_norm": 3.130174160003662,
      "learning_rate": 0.0001913000839250521,
      "loss": 0.4875,
      "step": 9726
    },
    {
      "epoch": 0.04575387828442948,
      "grad_norm": 1.8711917400360107,
      "learning_rate": 0.00019129914094693864,
      "loss": 0.2496,
      "step": 9727
    },
    {
      "epoch": 0.045758582086041936,
      "grad_norm": 0.17978321015834808,
      "learning_rate": 0.00019129819796882515,
      "loss": 0.0122,
      "step": 9728
    },
    {
      "epoch": 0.0457632858876544,
      "grad_norm": 1.8882994651794434,
      "learning_rate": 0.00019129725499071167,
      "loss": 0.1945,
      "step": 9729
    },
    {
      "epoch": 0.04576798968926687,
      "grad_norm": 3.4566829204559326,
      "learning_rate": 0.0001912963120125982,
      "loss": 0.3265,
      "step": 9730
    },
    {
      "epoch": 0.045772693490879326,
      "grad_norm": 2.4925551414489746,
      "learning_rate": 0.00019129536903448474,
      "loss": 0.3899,
      "step": 9731
    },
    {
      "epoch": 0.04577739729249179,
      "grad_norm": 2.839423656463623,
      "learning_rate": 0.00019129442605637126,
      "loss": 0.22,
      "step": 9732
    },
    {
      "epoch": 0.04578210109410426,
      "grad_norm": 0.9657465815544128,
      "learning_rate": 0.00019129348307825777,
      "loss": 0.1043,
      "step": 9733
    },
    {
      "epoch": 0.045786804895716716,
      "grad_norm": 0.8833699226379395,
      "learning_rate": 0.00019129254010014427,
      "loss": 0.0877,
      "step": 9734
    },
    {
      "epoch": 0.04579150869732918,
      "grad_norm": 1.198511004447937,
      "learning_rate": 0.00019129159712203078,
      "loss": 0.205,
      "step": 9735
    },
    {
      "epoch": 0.04579621249894165,
      "grad_norm": 2.0613150596618652,
      "learning_rate": 0.00019129065414391733,
      "loss": 0.2575,
      "step": 9736
    },
    {
      "epoch": 0.045800916300554106,
      "grad_norm": 1.8034782409667969,
      "learning_rate": 0.00019128971116580385,
      "loss": 0.4321,
      "step": 9737
    },
    {
      "epoch": 0.04580562010216657,
      "grad_norm": 1.5712577104568481,
      "learning_rate": 0.00019128876818769037,
      "loss": 0.126,
      "step": 9738
    },
    {
      "epoch": 0.04581032390377903,
      "grad_norm": 1.552826166152954,
      "learning_rate": 0.0001912878252095769,
      "loss": 0.1243,
      "step": 9739
    },
    {
      "epoch": 0.045815027705391496,
      "grad_norm": 3.220912218093872,
      "learning_rate": 0.00019128688223146343,
      "loss": 0.6477,
      "step": 9740
    },
    {
      "epoch": 0.04581973150700396,
      "grad_norm": 1.5833842754364014,
      "learning_rate": 0.00019128593925334995,
      "loss": 0.1322,
      "step": 9741
    },
    {
      "epoch": 0.04582443530861642,
      "grad_norm": 2.139711618423462,
      "learning_rate": 0.00019128499627523647,
      "loss": 0.1651,
      "step": 9742
    },
    {
      "epoch": 0.045829139110228886,
      "grad_norm": 4.473165988922119,
      "learning_rate": 0.000191284053297123,
      "loss": 0.7879,
      "step": 9743
    },
    {
      "epoch": 0.04583384291184135,
      "grad_norm": 1.9864262342453003,
      "learning_rate": 0.0001912831103190095,
      "loss": 0.2118,
      "step": 9744
    },
    {
      "epoch": 0.04583854671345381,
      "grad_norm": 2.7001404762268066,
      "learning_rate": 0.00019128216734089603,
      "loss": 0.383,
      "step": 9745
    },
    {
      "epoch": 0.045843250515066276,
      "grad_norm": 2.178006172180176,
      "learning_rate": 0.00019128122436278254,
      "loss": 0.4972,
      "step": 9746
    },
    {
      "epoch": 0.04584795431667874,
      "grad_norm": 3.7446324825286865,
      "learning_rate": 0.00019128028138466906,
      "loss": 0.5794,
      "step": 9747
    },
    {
      "epoch": 0.0458526581182912,
      "grad_norm": 2.136138677597046,
      "learning_rate": 0.00019127933840655558,
      "loss": 0.4184,
      "step": 9748
    },
    {
      "epoch": 0.045857361919903666,
      "grad_norm": 1.8877416849136353,
      "learning_rate": 0.00019127839542844213,
      "loss": 0.2537,
      "step": 9749
    },
    {
      "epoch": 0.04586206572151613,
      "grad_norm": 2.4551761150360107,
      "learning_rate": 0.00019127745245032865,
      "loss": 0.4671,
      "step": 9750
    },
    {
      "epoch": 0.04586676952312859,
      "grad_norm": 1.4166128635406494,
      "learning_rate": 0.00019127650947221516,
      "loss": 0.107,
      "step": 9751
    },
    {
      "epoch": 0.045871473324741056,
      "grad_norm": 1.2074071168899536,
      "learning_rate": 0.00019127556649410168,
      "loss": 0.072,
      "step": 9752
    },
    {
      "epoch": 0.04587617712635352,
      "grad_norm": 3.841949224472046,
      "learning_rate": 0.0001912746235159882,
      "loss": 0.671,
      "step": 9753
    },
    {
      "epoch": 0.04588088092796598,
      "grad_norm": 1.3363546133041382,
      "learning_rate": 0.00019127368053787472,
      "loss": 0.2099,
      "step": 9754
    },
    {
      "epoch": 0.045885584729578446,
      "grad_norm": 2.343219757080078,
      "learning_rate": 0.00019127273755976124,
      "loss": 0.2465,
      "step": 9755
    },
    {
      "epoch": 0.045890288531190905,
      "grad_norm": 1.1603139638900757,
      "learning_rate": 0.00019127179458164776,
      "loss": 0.1462,
      "step": 9756
    },
    {
      "epoch": 0.04589499233280337,
      "grad_norm": 1.9939075708389282,
      "learning_rate": 0.00019127085160353428,
      "loss": 0.1869,
      "step": 9757
    },
    {
      "epoch": 0.045899696134415836,
      "grad_norm": 8.325623512268066,
      "learning_rate": 0.00019126990862542082,
      "loss": 0.2605,
      "step": 9758
    },
    {
      "epoch": 0.045904399936028295,
      "grad_norm": 2.060922861099243,
      "learning_rate": 0.00019126896564730734,
      "loss": 0.254,
      "step": 9759
    },
    {
      "epoch": 0.04590910373764076,
      "grad_norm": 0.6665160655975342,
      "learning_rate": 0.00019126802266919386,
      "loss": 0.0542,
      "step": 9760
    },
    {
      "epoch": 0.045913807539253226,
      "grad_norm": 1.4433015584945679,
      "learning_rate": 0.00019126707969108038,
      "loss": 0.221,
      "step": 9761
    },
    {
      "epoch": 0.045918511340865685,
      "grad_norm": 0.7783187627792358,
      "learning_rate": 0.0001912661367129669,
      "loss": 0.0483,
      "step": 9762
    },
    {
      "epoch": 0.04592321514247815,
      "grad_norm": 0.7480581998825073,
      "learning_rate": 0.00019126519373485344,
      "loss": 0.0946,
      "step": 9763
    },
    {
      "epoch": 0.045927918944090616,
      "grad_norm": 2.023634910583496,
      "learning_rate": 0.00019126425075673996,
      "loss": 0.3348,
      "step": 9764
    },
    {
      "epoch": 0.045932622745703075,
      "grad_norm": 2.8743886947631836,
      "learning_rate": 0.00019126330777862645,
      "loss": 0.6053,
      "step": 9765
    },
    {
      "epoch": 0.04593732654731554,
      "grad_norm": 2.4517507553100586,
      "learning_rate": 0.00019126236480051297,
      "loss": 0.4631,
      "step": 9766
    },
    {
      "epoch": 0.045942030348928006,
      "grad_norm": 19.20975685119629,
      "learning_rate": 0.0001912614218223995,
      "loss": 0.4943,
      "step": 9767
    },
    {
      "epoch": 0.045946734150540465,
      "grad_norm": 0.9158247709274292,
      "learning_rate": 0.00019126047884428604,
      "loss": 0.1752,
      "step": 9768
    },
    {
      "epoch": 0.04595143795215293,
      "grad_norm": 4.037631988525391,
      "learning_rate": 0.00019125953586617255,
      "loss": 0.6702,
      "step": 9769
    },
    {
      "epoch": 0.045956141753765396,
      "grad_norm": 1.501818299293518,
      "learning_rate": 0.00019125859288805907,
      "loss": 0.1504,
      "step": 9770
    },
    {
      "epoch": 0.045960845555377855,
      "grad_norm": 1.2842379808425903,
      "learning_rate": 0.0001912576499099456,
      "loss": 0.2552,
      "step": 9771
    },
    {
      "epoch": 0.04596554935699032,
      "grad_norm": 2.057586193084717,
      "learning_rate": 0.00019125670693183214,
      "loss": 0.298,
      "step": 9772
    },
    {
      "epoch": 0.04597025315860278,
      "grad_norm": 2.5665605068206787,
      "learning_rate": 0.00019125576395371866,
      "loss": 0.3426,
      "step": 9773
    },
    {
      "epoch": 0.045974956960215245,
      "grad_norm": 1.095008373260498,
      "learning_rate": 0.00019125482097560517,
      "loss": 0.1527,
      "step": 9774
    },
    {
      "epoch": 0.04597966076182771,
      "grad_norm": 1.339692234992981,
      "learning_rate": 0.0001912538779974917,
      "loss": 0.2781,
      "step": 9775
    },
    {
      "epoch": 0.04598436456344017,
      "grad_norm": 2.0115883350372314,
      "learning_rate": 0.0001912529350193782,
      "loss": 0.2675,
      "step": 9776
    },
    {
      "epoch": 0.045989068365052635,
      "grad_norm": 4.409300804138184,
      "learning_rate": 0.00019125199204126473,
      "loss": 0.3995,
      "step": 9777
    },
    {
      "epoch": 0.0459937721666651,
      "grad_norm": 1.2187082767486572,
      "learning_rate": 0.00019125104906315125,
      "loss": 0.12,
      "step": 9778
    },
    {
      "epoch": 0.04599847596827756,
      "grad_norm": 2.717089891433716,
      "learning_rate": 0.00019125010608503777,
      "loss": 0.4181,
      "step": 9779
    },
    {
      "epoch": 0.046003179769890025,
      "grad_norm": 2.226728677749634,
      "learning_rate": 0.0001912491631069243,
      "loss": 0.3025,
      "step": 9780
    },
    {
      "epoch": 0.04600788357150249,
      "grad_norm": 1.8619447946548462,
      "learning_rate": 0.00019124822012881083,
      "loss": 0.3968,
      "step": 9781
    },
    {
      "epoch": 0.04601258737311495,
      "grad_norm": 1.1630258560180664,
      "learning_rate": 0.00019124727715069735,
      "loss": 0.1197,
      "step": 9782
    },
    {
      "epoch": 0.046017291174727415,
      "grad_norm": 2.5707242488861084,
      "learning_rate": 0.00019124633417258387,
      "loss": 0.22,
      "step": 9783
    },
    {
      "epoch": 0.04602199497633988,
      "grad_norm": 0.41500359773635864,
      "learning_rate": 0.0001912453911944704,
      "loss": 0.0626,
      "step": 9784
    },
    {
      "epoch": 0.04602669877795234,
      "grad_norm": 1.210951328277588,
      "learning_rate": 0.0001912444482163569,
      "loss": 0.2358,
      "step": 9785
    },
    {
      "epoch": 0.046031402579564805,
      "grad_norm": 2.2220652103424072,
      "learning_rate": 0.00019124350523824343,
      "loss": 0.3868,
      "step": 9786
    },
    {
      "epoch": 0.04603610638117727,
      "grad_norm": 2.1665663719177246,
      "learning_rate": 0.00019124256226012994,
      "loss": 0.2845,
      "step": 9787
    },
    {
      "epoch": 0.04604081018278973,
      "grad_norm": 1.123393177986145,
      "learning_rate": 0.00019124161928201646,
      "loss": 0.1121,
      "step": 9788
    },
    {
      "epoch": 0.046045513984402195,
      "grad_norm": 1.741400122642517,
      "learning_rate": 0.00019124067630390298,
      "loss": 0.2435,
      "step": 9789
    },
    {
      "epoch": 0.046050217786014654,
      "grad_norm": 1.4144926071166992,
      "learning_rate": 0.00019123973332578953,
      "loss": 0.1893,
      "step": 9790
    },
    {
      "epoch": 0.04605492158762712,
      "grad_norm": 1.544222116470337,
      "learning_rate": 0.00019123879034767605,
      "loss": 0.3825,
      "step": 9791
    },
    {
      "epoch": 0.046059625389239585,
      "grad_norm": 1.4037814140319824,
      "learning_rate": 0.00019123784736956256,
      "loss": 0.3372,
      "step": 9792
    },
    {
      "epoch": 0.046064329190852044,
      "grad_norm": 1.0720534324645996,
      "learning_rate": 0.00019123690439144908,
      "loss": 0.1573,
      "step": 9793
    },
    {
      "epoch": 0.04606903299246451,
      "grad_norm": 2.5320370197296143,
      "learning_rate": 0.0001912359614133356,
      "loss": 0.4621,
      "step": 9794
    },
    {
      "epoch": 0.046073736794076975,
      "grad_norm": 1.2278236150741577,
      "learning_rate": 0.00019123501843522215,
      "loss": 0.3673,
      "step": 9795
    },
    {
      "epoch": 0.046078440595689434,
      "grad_norm": 1.091139793395996,
      "learning_rate": 0.00019123407545710864,
      "loss": 0.1017,
      "step": 9796
    },
    {
      "epoch": 0.0460831443973019,
      "grad_norm": 0.9639604091644287,
      "learning_rate": 0.00019123313247899516,
      "loss": 0.1419,
      "step": 9797
    },
    {
      "epoch": 0.046087848198914365,
      "grad_norm": 1.9499597549438477,
      "learning_rate": 0.00019123218950088168,
      "loss": 0.2612,
      "step": 9798
    },
    {
      "epoch": 0.046092552000526824,
      "grad_norm": 0.7063220143318176,
      "learning_rate": 0.00019123124652276822,
      "loss": 0.0798,
      "step": 9799
    },
    {
      "epoch": 0.04609725580213929,
      "grad_norm": 5.5163116455078125,
      "learning_rate": 0.00019123030354465474,
      "loss": 1.1283,
      "step": 9800
    },
    {
      "epoch": 0.046101959603751755,
      "grad_norm": 1.6464792490005493,
      "learning_rate": 0.00019122936056654126,
      "loss": 0.0823,
      "step": 9801
    },
    {
      "epoch": 0.046106663405364214,
      "grad_norm": 1.7128851413726807,
      "learning_rate": 0.00019122841758842778,
      "loss": 0.0984,
      "step": 9802
    },
    {
      "epoch": 0.04611136720697668,
      "grad_norm": 1.912376046180725,
      "learning_rate": 0.0001912274746103143,
      "loss": 0.2427,
      "step": 9803
    },
    {
      "epoch": 0.046116071008589145,
      "grad_norm": 0.9740297198295593,
      "learning_rate": 0.00019122653163220084,
      "loss": 0.0564,
      "step": 9804
    },
    {
      "epoch": 0.046120774810201604,
      "grad_norm": 4.386468410491943,
      "learning_rate": 0.00019122558865408736,
      "loss": 0.655,
      "step": 9805
    },
    {
      "epoch": 0.04612547861181407,
      "grad_norm": 2.173288345336914,
      "learning_rate": 0.00019122464567597388,
      "loss": 0.2712,
      "step": 9806
    },
    {
      "epoch": 0.04613018241342653,
      "grad_norm": 3.1197566986083984,
      "learning_rate": 0.0001912237026978604,
      "loss": 0.5174,
      "step": 9807
    },
    {
      "epoch": 0.046134886215038994,
      "grad_norm": 3.1944000720977783,
      "learning_rate": 0.00019122275971974692,
      "loss": 0.2365,
      "step": 9808
    },
    {
      "epoch": 0.04613959001665146,
      "grad_norm": 0.48514774441719055,
      "learning_rate": 0.00019122181674163344,
      "loss": 0.0493,
      "step": 9809
    },
    {
      "epoch": 0.04614429381826392,
      "grad_norm": 2.883862257003784,
      "learning_rate": 0.00019122087376351995,
      "loss": 0.2434,
      "step": 9810
    },
    {
      "epoch": 0.046148997619876383,
      "grad_norm": 2.087822675704956,
      "learning_rate": 0.00019121993078540647,
      "loss": 0.3666,
      "step": 9811
    },
    {
      "epoch": 0.04615370142148885,
      "grad_norm": 6.536795616149902,
      "learning_rate": 0.000191218987807293,
      "loss": 0.7904,
      "step": 9812
    },
    {
      "epoch": 0.04615840522310131,
      "grad_norm": 1.481871485710144,
      "learning_rate": 0.00019121804482917954,
      "loss": 0.1227,
      "step": 9813
    },
    {
      "epoch": 0.046163109024713773,
      "grad_norm": 3.321218967437744,
      "learning_rate": 0.00019121710185106606,
      "loss": 0.2947,
      "step": 9814
    },
    {
      "epoch": 0.04616781282632624,
      "grad_norm": 2.3642725944519043,
      "learning_rate": 0.00019121615887295257,
      "loss": 0.2713,
      "step": 9815
    },
    {
      "epoch": 0.0461725166279387,
      "grad_norm": 4.171902656555176,
      "learning_rate": 0.0001912152158948391,
      "loss": 0.4975,
      "step": 9816
    },
    {
      "epoch": 0.04617722042955116,
      "grad_norm": 0.7843979001045227,
      "learning_rate": 0.0001912142729167256,
      "loss": 0.0687,
      "step": 9817
    },
    {
      "epoch": 0.04618192423116363,
      "grad_norm": 2.6215620040893555,
      "learning_rate": 0.00019121332993861213,
      "loss": 0.5947,
      "step": 9818
    },
    {
      "epoch": 0.04618662803277609,
      "grad_norm": 1.000906229019165,
      "learning_rate": 0.00019121238696049865,
      "loss": 0.0806,
      "step": 9819
    },
    {
      "epoch": 0.04619133183438855,
      "grad_norm": 0.5305749773979187,
      "learning_rate": 0.00019121144398238517,
      "loss": 0.0359,
      "step": 9820
    },
    {
      "epoch": 0.04619603563600102,
      "grad_norm": 1.312077522277832,
      "learning_rate": 0.0001912105010042717,
      "loss": 0.1973,
      "step": 9821
    },
    {
      "epoch": 0.04620073943761348,
      "grad_norm": 2.779628276824951,
      "learning_rate": 0.00019120955802615823,
      "loss": 0.2826,
      "step": 9822
    },
    {
      "epoch": 0.04620544323922594,
      "grad_norm": 3.078927993774414,
      "learning_rate": 0.00019120861504804475,
      "loss": 0.793,
      "step": 9823
    },
    {
      "epoch": 0.0462101470408384,
      "grad_norm": 2.6234512329101562,
      "learning_rate": 0.00019120767206993127,
      "loss": 0.2671,
      "step": 9824
    },
    {
      "epoch": 0.04621485084245087,
      "grad_norm": 3.335517168045044,
      "learning_rate": 0.0001912067290918178,
      "loss": 0.731,
      "step": 9825
    },
    {
      "epoch": 0.04621955464406333,
      "grad_norm": 1.4187843799591064,
      "learning_rate": 0.0001912057861137043,
      "loss": 0.1298,
      "step": 9826
    },
    {
      "epoch": 0.04622425844567579,
      "grad_norm": 1.2461506128311157,
      "learning_rate": 0.00019120484313559083,
      "loss": 0.1106,
      "step": 9827
    },
    {
      "epoch": 0.04622896224728826,
      "grad_norm": 2.2578277587890625,
      "learning_rate": 0.00019120390015747734,
      "loss": 0.2665,
      "step": 9828
    },
    {
      "epoch": 0.04623366604890072,
      "grad_norm": 0.3472321629524231,
      "learning_rate": 0.00019120295717936386,
      "loss": 0.0309,
      "step": 9829
    },
    {
      "epoch": 0.04623836985051318,
      "grad_norm": 0.5460019111633301,
      "learning_rate": 0.00019120201420125038,
      "loss": 0.0459,
      "step": 9830
    },
    {
      "epoch": 0.04624307365212565,
      "grad_norm": 1.206462025642395,
      "learning_rate": 0.00019120107122313693,
      "loss": 0.1155,
      "step": 9831
    },
    {
      "epoch": 0.04624777745373811,
      "grad_norm": 3.7286746501922607,
      "learning_rate": 0.00019120012824502345,
      "loss": 0.725,
      "step": 9832
    },
    {
      "epoch": 0.04625248125535057,
      "grad_norm": 1.7785627841949463,
      "learning_rate": 0.00019119918526690996,
      "loss": 0.146,
      "step": 9833
    },
    {
      "epoch": 0.04625718505696304,
      "grad_norm": 0.46005088090896606,
      "learning_rate": 0.00019119824228879648,
      "loss": 0.0506,
      "step": 9834
    },
    {
      "epoch": 0.0462618888585755,
      "grad_norm": 1.9513505697250366,
      "learning_rate": 0.000191197299310683,
      "loss": 0.2109,
      "step": 9835
    },
    {
      "epoch": 0.04626659266018796,
      "grad_norm": 0.7561001777648926,
      "learning_rate": 0.00019119635633256955,
      "loss": 0.0647,
      "step": 9836
    },
    {
      "epoch": 0.04627129646180043,
      "grad_norm": 0.754816472530365,
      "learning_rate": 0.00019119541335445607,
      "loss": 0.0979,
      "step": 9837
    },
    {
      "epoch": 0.04627600026341289,
      "grad_norm": 3.6913392543792725,
      "learning_rate": 0.00019119447037634258,
      "loss": 0.7112,
      "step": 9838
    },
    {
      "epoch": 0.04628070406502535,
      "grad_norm": 3.225792407989502,
      "learning_rate": 0.00019119352739822908,
      "loss": 0.7847,
      "step": 9839
    },
    {
      "epoch": 0.04628540786663782,
      "grad_norm": 2.127814769744873,
      "learning_rate": 0.00019119258442011562,
      "loss": 0.2037,
      "step": 9840
    },
    {
      "epoch": 0.04629011166825028,
      "grad_norm": 2.4616518020629883,
      "learning_rate": 0.00019119164144200214,
      "loss": 0.5256,
      "step": 9841
    },
    {
      "epoch": 0.04629481546986274,
      "grad_norm": 1.4355686902999878,
      "learning_rate": 0.00019119069846388866,
      "loss": 0.0889,
      "step": 9842
    },
    {
      "epoch": 0.04629951927147521,
      "grad_norm": 3.8083698749542236,
      "learning_rate": 0.00019118975548577518,
      "loss": 0.8348,
      "step": 9843
    },
    {
      "epoch": 0.046304223073087666,
      "grad_norm": 6.428911209106445,
      "learning_rate": 0.0001911888125076617,
      "loss": 1.0183,
      "step": 9844
    },
    {
      "epoch": 0.04630892687470013,
      "grad_norm": 3.3734214305877686,
      "learning_rate": 0.00019118786952954824,
      "loss": 0.3458,
      "step": 9845
    },
    {
      "epoch": 0.0463136306763126,
      "grad_norm": 1.4726957082748413,
      "learning_rate": 0.00019118692655143476,
      "loss": 0.2547,
      "step": 9846
    },
    {
      "epoch": 0.046318334477925056,
      "grad_norm": 0.6925724148750305,
      "learning_rate": 0.00019118598357332128,
      "loss": 0.0407,
      "step": 9847
    },
    {
      "epoch": 0.04632303827953752,
      "grad_norm": 2.2169461250305176,
      "learning_rate": 0.0001911850405952078,
      "loss": 0.4286,
      "step": 9848
    },
    {
      "epoch": 0.04632774208114999,
      "grad_norm": 1.2241017818450928,
      "learning_rate": 0.00019118409761709432,
      "loss": 0.1222,
      "step": 9849
    },
    {
      "epoch": 0.046332445882762446,
      "grad_norm": 0.8475289940834045,
      "learning_rate": 0.00019118315463898084,
      "loss": 0.0937,
      "step": 9850
    },
    {
      "epoch": 0.04633714968437491,
      "grad_norm": 1.090938925743103,
      "learning_rate": 0.00019118221166086735,
      "loss": 0.0612,
      "step": 9851
    },
    {
      "epoch": 0.04634185348598738,
      "grad_norm": 3.309091329574585,
      "learning_rate": 0.00019118126868275387,
      "loss": 0.4452,
      "step": 9852
    },
    {
      "epoch": 0.046346557287599836,
      "grad_norm": 3.1393072605133057,
      "learning_rate": 0.0001911803257046404,
      "loss": 0.4227,
      "step": 9853
    },
    {
      "epoch": 0.0463512610892123,
      "grad_norm": 0.403543621301651,
      "learning_rate": 0.00019117938272652694,
      "loss": 0.0233,
      "step": 9854
    },
    {
      "epoch": 0.04635596489082477,
      "grad_norm": 1.283807396888733,
      "learning_rate": 0.00019117843974841346,
      "loss": 0.1641,
      "step": 9855
    },
    {
      "epoch": 0.046360668692437226,
      "grad_norm": 2.618964672088623,
      "learning_rate": 0.00019117749677029997,
      "loss": 0.3772,
      "step": 9856
    },
    {
      "epoch": 0.04636537249404969,
      "grad_norm": 1.3951574563980103,
      "learning_rate": 0.0001911765537921865,
      "loss": 0.2901,
      "step": 9857
    },
    {
      "epoch": 0.04637007629566216,
      "grad_norm": 0.8235853314399719,
      "learning_rate": 0.000191175610814073,
      "loss": 0.1091,
      "step": 9858
    },
    {
      "epoch": 0.046374780097274616,
      "grad_norm": 1.5637130737304688,
      "learning_rate": 0.00019117466783595953,
      "loss": 0.1787,
      "step": 9859
    },
    {
      "epoch": 0.04637948389888708,
      "grad_norm": 1.2627525329589844,
      "learning_rate": 0.00019117372485784605,
      "loss": 0.1067,
      "step": 9860
    },
    {
      "epoch": 0.04638418770049954,
      "grad_norm": 2.587315320968628,
      "learning_rate": 0.00019117278187973257,
      "loss": 0.4163,
      "step": 9861
    },
    {
      "epoch": 0.046388891502112006,
      "grad_norm": 1.3800935745239258,
      "learning_rate": 0.00019117183890161909,
      "loss": 0.1485,
      "step": 9862
    },
    {
      "epoch": 0.04639359530372447,
      "grad_norm": 0.895516574382782,
      "learning_rate": 0.00019117089592350563,
      "loss": 0.0935,
      "step": 9863
    },
    {
      "epoch": 0.04639829910533693,
      "grad_norm": 2.2546913623809814,
      "learning_rate": 0.00019116995294539215,
      "loss": 0.3549,
      "step": 9864
    },
    {
      "epoch": 0.046403002906949396,
      "grad_norm": 0.2169511467218399,
      "learning_rate": 0.00019116900996727867,
      "loss": 0.0165,
      "step": 9865
    },
    {
      "epoch": 0.04640770670856186,
      "grad_norm": 0.25662606954574585,
      "learning_rate": 0.0001911680669891652,
      "loss": 0.0247,
      "step": 9866
    },
    {
      "epoch": 0.04641241051017432,
      "grad_norm": 1.726823329925537,
      "learning_rate": 0.00019116712401105173,
      "loss": 0.1488,
      "step": 9867
    },
    {
      "epoch": 0.046417114311786786,
      "grad_norm": 0.879339337348938,
      "learning_rate": 0.00019116618103293825,
      "loss": 0.0825,
      "step": 9868
    },
    {
      "epoch": 0.04642181811339925,
      "grad_norm": 0.4325934946537018,
      "learning_rate": 0.00019116523805482477,
      "loss": 0.0317,
      "step": 9869
    },
    {
      "epoch": 0.04642652191501171,
      "grad_norm": 3.5519626140594482,
      "learning_rate": 0.00019116429507671126,
      "loss": 0.6045,
      "step": 9870
    },
    {
      "epoch": 0.046431225716624176,
      "grad_norm": 5.140141010284424,
      "learning_rate": 0.00019116335209859778,
      "loss": 1.2803,
      "step": 9871
    },
    {
      "epoch": 0.04643592951823664,
      "grad_norm": 3.7787978649139404,
      "learning_rate": 0.00019116240912048433,
      "loss": 0.7149,
      "step": 9872
    },
    {
      "epoch": 0.0464406333198491,
      "grad_norm": 0.8818976283073425,
      "learning_rate": 0.00019116146614237085,
      "loss": 0.0768,
      "step": 9873
    },
    {
      "epoch": 0.046445337121461566,
      "grad_norm": 2.1287622451782227,
      "learning_rate": 0.00019116052316425736,
      "loss": 0.1889,
      "step": 9874
    },
    {
      "epoch": 0.04645004092307403,
      "grad_norm": 0.9135809540748596,
      "learning_rate": 0.00019115958018614388,
      "loss": 0.0801,
      "step": 9875
    },
    {
      "epoch": 0.04645474472468649,
      "grad_norm": 2.324705123901367,
      "learning_rate": 0.0001911586372080304,
      "loss": 0.2247,
      "step": 9876
    },
    {
      "epoch": 0.046459448526298956,
      "grad_norm": 0.7959268093109131,
      "learning_rate": 0.00019115769422991695,
      "loss": 0.0644,
      "step": 9877
    },
    {
      "epoch": 0.046464152327911415,
      "grad_norm": 1.8023083209991455,
      "learning_rate": 0.00019115675125180347,
      "loss": 0.3594,
      "step": 9878
    },
    {
      "epoch": 0.04646885612952388,
      "grad_norm": 2.8550961017608643,
      "learning_rate": 0.00019115580827368998,
      "loss": 0.4443,
      "step": 9879
    },
    {
      "epoch": 0.046473559931136346,
      "grad_norm": 3.581205129623413,
      "learning_rate": 0.0001911548652955765,
      "loss": 0.3597,
      "step": 9880
    },
    {
      "epoch": 0.046478263732748805,
      "grad_norm": 1.3827886581420898,
      "learning_rate": 0.00019115392231746302,
      "loss": 0.0938,
      "step": 9881
    },
    {
      "epoch": 0.04648296753436127,
      "grad_norm": 3.574012279510498,
      "learning_rate": 0.00019115297933934954,
      "loss": 0.8583,
      "step": 9882
    },
    {
      "epoch": 0.046487671335973736,
      "grad_norm": 0.8171574473381042,
      "learning_rate": 0.00019115203636123606,
      "loss": 0.0952,
      "step": 9883
    },
    {
      "epoch": 0.046492375137586195,
      "grad_norm": 3.5179951190948486,
      "learning_rate": 0.00019115109338312258,
      "loss": 0.6291,
      "step": 9884
    },
    {
      "epoch": 0.04649707893919866,
      "grad_norm": 1.1373523473739624,
      "learning_rate": 0.0001911501504050091,
      "loss": 0.0976,
      "step": 9885
    },
    {
      "epoch": 0.046501782740811126,
      "grad_norm": 1.0734429359436035,
      "learning_rate": 0.00019114920742689564,
      "loss": 0.1464,
      "step": 9886
    },
    {
      "epoch": 0.046506486542423585,
      "grad_norm": 1.3935546875,
      "learning_rate": 0.00019114826444878216,
      "loss": 0.2914,
      "step": 9887
    },
    {
      "epoch": 0.04651119034403605,
      "grad_norm": 0.22488626837730408,
      "learning_rate": 0.00019114732147066868,
      "loss": 0.0161,
      "step": 9888
    },
    {
      "epoch": 0.046515894145648516,
      "grad_norm": 1.7756952047348022,
      "learning_rate": 0.0001911463784925552,
      "loss": 0.3132,
      "step": 9889
    },
    {
      "epoch": 0.046520597947260975,
      "grad_norm": 3.253084659576416,
      "learning_rate": 0.00019114543551444172,
      "loss": 0.6697,
      "step": 9890
    },
    {
      "epoch": 0.04652530174887344,
      "grad_norm": 1.9577767848968506,
      "learning_rate": 0.00019114449253632824,
      "loss": 0.3083,
      "step": 9891
    },
    {
      "epoch": 0.046530005550485906,
      "grad_norm": 1.3319201469421387,
      "learning_rate": 0.00019114354955821475,
      "loss": 0.1909,
      "step": 9892
    },
    {
      "epoch": 0.046534709352098365,
      "grad_norm": 4.474828243255615,
      "learning_rate": 0.00019114260658010127,
      "loss": 0.2479,
      "step": 9893
    },
    {
      "epoch": 0.04653941315371083,
      "grad_norm": 0.5254290699958801,
      "learning_rate": 0.0001911416636019878,
      "loss": 0.0561,
      "step": 9894
    },
    {
      "epoch": 0.04654411695532329,
      "grad_norm": 0.5256781578063965,
      "learning_rate": 0.00019114072062387434,
      "loss": 0.0597,
      "step": 9895
    },
    {
      "epoch": 0.046548820756935755,
      "grad_norm": 4.018682956695557,
      "learning_rate": 0.00019113977764576086,
      "loss": 0.3242,
      "step": 9896
    },
    {
      "epoch": 0.04655352455854822,
      "grad_norm": 0.9709364175796509,
      "learning_rate": 0.00019113883466764737,
      "loss": 0.1307,
      "step": 9897
    },
    {
      "epoch": 0.04655822836016068,
      "grad_norm": 2.289151430130005,
      "learning_rate": 0.0001911378916895339,
      "loss": 0.2968,
      "step": 9898
    },
    {
      "epoch": 0.046562932161773145,
      "grad_norm": 1.468269944190979,
      "learning_rate": 0.00019113694871142044,
      "loss": 0.1527,
      "step": 9899
    },
    {
      "epoch": 0.04656763596338561,
      "grad_norm": 1.7435215711593628,
      "learning_rate": 0.00019113600573330696,
      "loss": 0.4307,
      "step": 9900
    },
    {
      "epoch": 0.04657233976499807,
      "grad_norm": 1.4823848009109497,
      "learning_rate": 0.00019113506275519345,
      "loss": 0.1485,
      "step": 9901
    },
    {
      "epoch": 0.046577043566610535,
      "grad_norm": 1.437352180480957,
      "learning_rate": 0.00019113411977707997,
      "loss": 0.1249,
      "step": 9902
    },
    {
      "epoch": 0.046581747368223,
      "grad_norm": 4.462879180908203,
      "learning_rate": 0.00019113317679896649,
      "loss": 0.1899,
      "step": 9903
    },
    {
      "epoch": 0.04658645116983546,
      "grad_norm": 4.709909915924072,
      "learning_rate": 0.00019113223382085303,
      "loss": 0.9158,
      "step": 9904
    },
    {
      "epoch": 0.046591154971447925,
      "grad_norm": 3.6750640869140625,
      "learning_rate": 0.00019113129084273955,
      "loss": 0.3694,
      "step": 9905
    },
    {
      "epoch": 0.04659585877306039,
      "grad_norm": 1.5735657215118408,
      "learning_rate": 0.00019113034786462607,
      "loss": 0.1869,
      "step": 9906
    },
    {
      "epoch": 0.04660056257467285,
      "grad_norm": 3.1644389629364014,
      "learning_rate": 0.0001911294048865126,
      "loss": 0.5912,
      "step": 9907
    },
    {
      "epoch": 0.046605266376285315,
      "grad_norm": 4.431942462921143,
      "learning_rate": 0.00019112846190839913,
      "loss": 0.7037,
      "step": 9908
    },
    {
      "epoch": 0.04660997017789778,
      "grad_norm": 1.3096600770950317,
      "learning_rate": 0.00019112751893028565,
      "loss": 0.1313,
      "step": 9909
    },
    {
      "epoch": 0.04661467397951024,
      "grad_norm": 0.9200699925422668,
      "learning_rate": 0.00019112657595217217,
      "loss": 0.1141,
      "step": 9910
    },
    {
      "epoch": 0.046619377781122705,
      "grad_norm": 2.882676601409912,
      "learning_rate": 0.0001911256329740587,
      "loss": 0.4819,
      "step": 9911
    },
    {
      "epoch": 0.04662408158273516,
      "grad_norm": 2.104903221130371,
      "learning_rate": 0.00019112468999594518,
      "loss": 0.1975,
      "step": 9912
    },
    {
      "epoch": 0.04662878538434763,
      "grad_norm": 3.381243944168091,
      "learning_rate": 0.00019112374701783173,
      "loss": 0.6732,
      "step": 9913
    },
    {
      "epoch": 0.046633489185960095,
      "grad_norm": 13.564167022705078,
      "learning_rate": 0.00019112280403971825,
      "loss": 0.5417,
      "step": 9914
    },
    {
      "epoch": 0.04663819298757255,
      "grad_norm": 2.2177655696868896,
      "learning_rate": 0.00019112186106160476,
      "loss": 0.4821,
      "step": 9915
    },
    {
      "epoch": 0.04664289678918502,
      "grad_norm": 4.9851155281066895,
      "learning_rate": 0.00019112091808349128,
      "loss": 0.6046,
      "step": 9916
    },
    {
      "epoch": 0.046647600590797485,
      "grad_norm": 3.663175344467163,
      "learning_rate": 0.00019111997510537783,
      "loss": 0.6593,
      "step": 9917
    },
    {
      "epoch": 0.04665230439240994,
      "grad_norm": 0.9143182635307312,
      "learning_rate": 0.00019111903212726435,
      "loss": 0.0944,
      "step": 9918
    },
    {
      "epoch": 0.04665700819402241,
      "grad_norm": 1.1345491409301758,
      "learning_rate": 0.00019111808914915087,
      "loss": 0.1491,
      "step": 9919
    },
    {
      "epoch": 0.046661711995634875,
      "grad_norm": 3.468111991882324,
      "learning_rate": 0.00019111714617103738,
      "loss": 0.7628,
      "step": 9920
    },
    {
      "epoch": 0.04666641579724733,
      "grad_norm": 1.6592992544174194,
      "learning_rate": 0.0001911162031929239,
      "loss": 0.3691,
      "step": 9921
    },
    {
      "epoch": 0.0466711195988598,
      "grad_norm": 0.8018012046813965,
      "learning_rate": 0.00019111526021481042,
      "loss": 0.0907,
      "step": 9922
    },
    {
      "epoch": 0.046675823400472265,
      "grad_norm": 1.6222413778305054,
      "learning_rate": 0.00019111431723669694,
      "loss": 0.2529,
      "step": 9923
    },
    {
      "epoch": 0.04668052720208472,
      "grad_norm": 1.2237989902496338,
      "learning_rate": 0.00019111337425858346,
      "loss": 0.2285,
      "step": 9924
    },
    {
      "epoch": 0.04668523100369719,
      "grad_norm": 1.9456607103347778,
      "learning_rate": 0.00019111243128046998,
      "loss": 0.5663,
      "step": 9925
    },
    {
      "epoch": 0.046689934805309655,
      "grad_norm": 2.6251919269561768,
      "learning_rate": 0.0001911114883023565,
      "loss": 0.2945,
      "step": 9926
    },
    {
      "epoch": 0.04669463860692211,
      "grad_norm": 1.2981438636779785,
      "learning_rate": 0.00019111054532424304,
      "loss": 0.1539,
      "step": 9927
    },
    {
      "epoch": 0.04669934240853458,
      "grad_norm": 1.1096824407577515,
      "learning_rate": 0.00019110960234612956,
      "loss": 0.1733,
      "step": 9928
    },
    {
      "epoch": 0.04670404621014704,
      "grad_norm": 1.8991605043411255,
      "learning_rate": 0.00019110865936801608,
      "loss": 0.2932,
      "step": 9929
    },
    {
      "epoch": 0.0467087500117595,
      "grad_norm": 4.056088447570801,
      "learning_rate": 0.0001911077163899026,
      "loss": 0.6233,
      "step": 9930
    },
    {
      "epoch": 0.04671345381337197,
      "grad_norm": 2.956411838531494,
      "learning_rate": 0.00019110677341178914,
      "loss": 0.5196,
      "step": 9931
    },
    {
      "epoch": 0.04671815761498443,
      "grad_norm": 1.5670500993728638,
      "learning_rate": 0.00019110583043367564,
      "loss": 0.2089,
      "step": 9932
    },
    {
      "epoch": 0.04672286141659689,
      "grad_norm": 1.189844012260437,
      "learning_rate": 0.00019110488745556215,
      "loss": 0.2526,
      "step": 9933
    },
    {
      "epoch": 0.04672756521820936,
      "grad_norm": 1.5093562602996826,
      "learning_rate": 0.00019110394447744867,
      "loss": 0.2561,
      "step": 9934
    },
    {
      "epoch": 0.04673226901982182,
      "grad_norm": 0.5440952777862549,
      "learning_rate": 0.0001911030014993352,
      "loss": 0.043,
      "step": 9935
    },
    {
      "epoch": 0.04673697282143428,
      "grad_norm": 1.122052550315857,
      "learning_rate": 0.00019110205852122174,
      "loss": 0.1919,
      "step": 9936
    },
    {
      "epoch": 0.04674167662304675,
      "grad_norm": 1.0624871253967285,
      "learning_rate": 0.00019110111554310826,
      "loss": 0.1167,
      "step": 9937
    },
    {
      "epoch": 0.04674638042465921,
      "grad_norm": 0.8291951417922974,
      "learning_rate": 0.00019110017256499477,
      "loss": 0.1172,
      "step": 9938
    },
    {
      "epoch": 0.04675108422627167,
      "grad_norm": 0.7450950145721436,
      "learning_rate": 0.0001910992295868813,
      "loss": 0.119,
      "step": 9939
    },
    {
      "epoch": 0.04675578802788414,
      "grad_norm": 4.02091121673584,
      "learning_rate": 0.00019109828660876784,
      "loss": 0.6214,
      "step": 9940
    },
    {
      "epoch": 0.0467604918294966,
      "grad_norm": 0.2086370587348938,
      "learning_rate": 0.00019109734363065436,
      "loss": 0.015,
      "step": 9941
    },
    {
      "epoch": 0.04676519563110906,
      "grad_norm": 1.803863763809204,
      "learning_rate": 0.00019109640065254088,
      "loss": 0.2755,
      "step": 9942
    },
    {
      "epoch": 0.04676989943272153,
      "grad_norm": 3.582873582839966,
      "learning_rate": 0.00019109545767442737,
      "loss": 0.1379,
      "step": 9943
    },
    {
      "epoch": 0.04677460323433399,
      "grad_norm": 2.832994222640991,
      "learning_rate": 0.00019109451469631389,
      "loss": 0.4322,
      "step": 9944
    },
    {
      "epoch": 0.04677930703594645,
      "grad_norm": 0.12703728675842285,
      "learning_rate": 0.00019109357171820043,
      "loss": 0.0076,
      "step": 9945
    },
    {
      "epoch": 0.04678401083755891,
      "grad_norm": 2.634458065032959,
      "learning_rate": 0.00019109262874008695,
      "loss": 0.5348,
      "step": 9946
    },
    {
      "epoch": 0.04678871463917138,
      "grad_norm": 1.4723458290100098,
      "learning_rate": 0.00019109168576197347,
      "loss": 0.1294,
      "step": 9947
    },
    {
      "epoch": 0.04679341844078384,
      "grad_norm": 0.1246684119105339,
      "learning_rate": 0.00019109074278386,
      "loss": 0.0074,
      "step": 9948
    },
    {
      "epoch": 0.0467981222423963,
      "grad_norm": 0.8004997968673706,
      "learning_rate": 0.00019108979980574653,
      "loss": 0.0733,
      "step": 9949
    },
    {
      "epoch": 0.04680282604400877,
      "grad_norm": 1.8337374925613403,
      "learning_rate": 0.00019108885682763305,
      "loss": 0.3522,
      "step": 9950
    },
    {
      "epoch": 0.04680752984562123,
      "grad_norm": 1.4897416830062866,
      "learning_rate": 0.00019108791384951957,
      "loss": 0.1406,
      "step": 9951
    },
    {
      "epoch": 0.04681223364723369,
      "grad_norm": 3.619752883911133,
      "learning_rate": 0.0001910869708714061,
      "loss": 0.4112,
      "step": 9952
    },
    {
      "epoch": 0.04681693744884616,
      "grad_norm": 2.823937177658081,
      "learning_rate": 0.0001910860278932926,
      "loss": 0.2635,
      "step": 9953
    },
    {
      "epoch": 0.04682164125045862,
      "grad_norm": 2.928853988647461,
      "learning_rate": 0.00019108508491517913,
      "loss": 0.2232,
      "step": 9954
    },
    {
      "epoch": 0.04682634505207108,
      "grad_norm": 4.7207722663879395,
      "learning_rate": 0.00019108414193706565,
      "loss": 0.5843,
      "step": 9955
    },
    {
      "epoch": 0.04683104885368355,
      "grad_norm": 2.304172992706299,
      "learning_rate": 0.00019108319895895216,
      "loss": 0.3571,
      "step": 9956
    },
    {
      "epoch": 0.04683575265529601,
      "grad_norm": 0.6662980914115906,
      "learning_rate": 0.00019108225598083868,
      "loss": 0.0887,
      "step": 9957
    },
    {
      "epoch": 0.04684045645690847,
      "grad_norm": 1.7092924118041992,
      "learning_rate": 0.00019108131300272523,
      "loss": 0.1123,
      "step": 9958
    },
    {
      "epoch": 0.04684516025852094,
      "grad_norm": 0.7369084358215332,
      "learning_rate": 0.00019108037002461175,
      "loss": 0.0792,
      "step": 9959
    },
    {
      "epoch": 0.0468498640601334,
      "grad_norm": 2.7425293922424316,
      "learning_rate": 0.00019107942704649827,
      "loss": 0.2219,
      "step": 9960
    },
    {
      "epoch": 0.04685456786174586,
      "grad_norm": 1.803051233291626,
      "learning_rate": 0.00019107848406838478,
      "loss": 0.2012,
      "step": 9961
    },
    {
      "epoch": 0.04685927166335833,
      "grad_norm": 3.0864453315734863,
      "learning_rate": 0.0001910775410902713,
      "loss": 0.219,
      "step": 9962
    },
    {
      "epoch": 0.046863975464970786,
      "grad_norm": 3.232304573059082,
      "learning_rate": 0.00019107659811215782,
      "loss": 0.4066,
      "step": 9963
    },
    {
      "epoch": 0.04686867926658325,
      "grad_norm": 1.771032691001892,
      "learning_rate": 0.00019107565513404434,
      "loss": 0.1691,
      "step": 9964
    },
    {
      "epoch": 0.04687338306819572,
      "grad_norm": 2.818659543991089,
      "learning_rate": 0.00019107471215593086,
      "loss": 0.4752,
      "step": 9965
    },
    {
      "epoch": 0.046878086869808176,
      "grad_norm": 0.1770937591791153,
      "learning_rate": 0.00019107376917781738,
      "loss": 0.0086,
      "step": 9966
    },
    {
      "epoch": 0.04688279067142064,
      "grad_norm": 5.3748040199279785,
      "learning_rate": 0.00019107282619970392,
      "loss": 1.0054,
      "step": 9967
    },
    {
      "epoch": 0.04688749447303311,
      "grad_norm": 4.747886657714844,
      "learning_rate": 0.00019107188322159044,
      "loss": 0.9254,
      "step": 9968
    },
    {
      "epoch": 0.046892198274645566,
      "grad_norm": 3.24125337600708,
      "learning_rate": 0.00019107094024347696,
      "loss": 0.394,
      "step": 9969
    },
    {
      "epoch": 0.04689690207625803,
      "grad_norm": 1.9711353778839111,
      "learning_rate": 0.00019106999726536348,
      "loss": 0.1924,
      "step": 9970
    },
    {
      "epoch": 0.0469016058778705,
      "grad_norm": 0.33542773127555847,
      "learning_rate": 0.00019106905428725,
      "loss": 0.0234,
      "step": 9971
    },
    {
      "epoch": 0.046906309679482956,
      "grad_norm": 2.4290997982025146,
      "learning_rate": 0.00019106811130913654,
      "loss": 0.4351,
      "step": 9972
    },
    {
      "epoch": 0.04691101348109542,
      "grad_norm": 1.9452557563781738,
      "learning_rate": 0.00019106716833102306,
      "loss": 0.2432,
      "step": 9973
    },
    {
      "epoch": 0.04691571728270789,
      "grad_norm": 1.9429388046264648,
      "learning_rate": 0.00019106622535290955,
      "loss": 0.1505,
      "step": 9974
    },
    {
      "epoch": 0.046920421084320346,
      "grad_norm": 2.5220632553100586,
      "learning_rate": 0.00019106528237479607,
      "loss": 0.3626,
      "step": 9975
    },
    {
      "epoch": 0.04692512488593281,
      "grad_norm": 2.2624356746673584,
      "learning_rate": 0.0001910643393966826,
      "loss": 0.6261,
      "step": 9976
    },
    {
      "epoch": 0.04692982868754528,
      "grad_norm": 1.7003811597824097,
      "learning_rate": 0.00019106339641856914,
      "loss": 0.2988,
      "step": 9977
    },
    {
      "epoch": 0.046934532489157736,
      "grad_norm": 1.2648046016693115,
      "learning_rate": 0.00019106245344045566,
      "loss": 0.2074,
      "step": 9978
    },
    {
      "epoch": 0.0469392362907702,
      "grad_norm": 0.686216413974762,
      "learning_rate": 0.00019106151046234217,
      "loss": 0.1076,
      "step": 9979
    },
    {
      "epoch": 0.04694394009238266,
      "grad_norm": 2.53225040435791,
      "learning_rate": 0.0001910605674842287,
      "loss": 0.4695,
      "step": 9980
    },
    {
      "epoch": 0.046948643893995126,
      "grad_norm": 0.6773209571838379,
      "learning_rate": 0.00019105962450611524,
      "loss": 0.0963,
      "step": 9981
    },
    {
      "epoch": 0.04695334769560759,
      "grad_norm": 0.21030794084072113,
      "learning_rate": 0.00019105868152800176,
      "loss": 0.0165,
      "step": 9982
    },
    {
      "epoch": 0.04695805149722005,
      "grad_norm": 1.0242387056350708,
      "learning_rate": 0.00019105773854988828,
      "loss": 0.1006,
      "step": 9983
    },
    {
      "epoch": 0.046962755298832516,
      "grad_norm": 1.5589386224746704,
      "learning_rate": 0.0001910567955717748,
      "loss": 0.3916,
      "step": 9984
    },
    {
      "epoch": 0.04696745910044498,
      "grad_norm": 0.8056951761245728,
      "learning_rate": 0.00019105585259366129,
      "loss": 0.0405,
      "step": 9985
    },
    {
      "epoch": 0.04697216290205744,
      "grad_norm": 0.12609650194644928,
      "learning_rate": 0.00019105490961554783,
      "loss": 0.0086,
      "step": 9986
    },
    {
      "epoch": 0.046976866703669906,
      "grad_norm": 0.3913942277431488,
      "learning_rate": 0.00019105396663743435,
      "loss": 0.0313,
      "step": 9987
    },
    {
      "epoch": 0.04698157050528237,
      "grad_norm": 0.3330841660499573,
      "learning_rate": 0.00019105302365932087,
      "loss": 0.0283,
      "step": 9988
    },
    {
      "epoch": 0.04698627430689483,
      "grad_norm": 1.093619465827942,
      "learning_rate": 0.0001910520806812074,
      "loss": 0.1843,
      "step": 9989
    },
    {
      "epoch": 0.046990978108507296,
      "grad_norm": 0.41650456190109253,
      "learning_rate": 0.00019105113770309393,
      "loss": 0.0535,
      "step": 9990
    },
    {
      "epoch": 0.04699568191011976,
      "grad_norm": 2.029451370239258,
      "learning_rate": 0.00019105019472498045,
      "loss": 0.5156,
      "step": 9991
    },
    {
      "epoch": 0.04700038571173222,
      "grad_norm": 0.43216270208358765,
      "learning_rate": 0.00019104925174686697,
      "loss": 0.0335,
      "step": 9992
    },
    {
      "epoch": 0.047005089513344686,
      "grad_norm": 2.596900463104248,
      "learning_rate": 0.0001910483087687535,
      "loss": 0.5143,
      "step": 9993
    },
    {
      "epoch": 0.04700979331495715,
      "grad_norm": 1.378567099571228,
      "learning_rate": 0.00019104736579064,
      "loss": 0.1702,
      "step": 9994
    },
    {
      "epoch": 0.04701449711656961,
      "grad_norm": 2.5367767810821533,
      "learning_rate": 0.00019104642281252653,
      "loss": 0.3872,
      "step": 9995
    },
    {
      "epoch": 0.047019200918182076,
      "grad_norm": 1.835279107093811,
      "learning_rate": 0.00019104547983441305,
      "loss": 0.2114,
      "step": 9996
    },
    {
      "epoch": 0.047023904719794535,
      "grad_norm": 2.4373764991760254,
      "learning_rate": 0.00019104453685629956,
      "loss": 0.5409,
      "step": 9997
    },
    {
      "epoch": 0.047028608521407,
      "grad_norm": 0.3959552049636841,
      "learning_rate": 0.00019104359387818608,
      "loss": 0.031,
      "step": 9998
    },
    {
      "epoch": 0.047033312323019466,
      "grad_norm": 0.7734575271606445,
      "learning_rate": 0.00019104265090007263,
      "loss": 0.0753,
      "step": 9999
    },
    {
      "epoch": 0.047038016124631925,
      "grad_norm": 1.5847759246826172,
      "learning_rate": 0.00019104170792195915,
      "loss": 0.3638,
      "step": 10000
    },
    {
      "epoch": 0.04704271992624439,
      "grad_norm": 2.3502745628356934,
      "learning_rate": 0.00019104076494384567,
      "loss": 0.2615,
      "step": 10001
    },
    {
      "epoch": 0.047047423727856856,
      "grad_norm": 2.445101261138916,
      "learning_rate": 0.00019103982196573218,
      "loss": 0.4269,
      "step": 10002
    },
    {
      "epoch": 0.047052127529469315,
      "grad_norm": 0.6470316052436829,
      "learning_rate": 0.0001910388789876187,
      "loss": 0.053,
      "step": 10003
    },
    {
      "epoch": 0.04705683133108178,
      "grad_norm": 1.8735893964767456,
      "learning_rate": 0.00019103793600950525,
      "loss": 0.2016,
      "step": 10004
    },
    {
      "epoch": 0.047061535132694246,
      "grad_norm": 5.759008407592773,
      "learning_rate": 0.00019103699303139174,
      "loss": 0.9724,
      "step": 10005
    },
    {
      "epoch": 0.047066238934306705,
      "grad_norm": 1.878934383392334,
      "learning_rate": 0.00019103605005327826,
      "loss": 0.2208,
      "step": 10006
    },
    {
      "epoch": 0.04707094273591917,
      "grad_norm": 3.3531978130340576,
      "learning_rate": 0.00019103510707516478,
      "loss": 0.9283,
      "step": 10007
    },
    {
      "epoch": 0.047075646537531636,
      "grad_norm": 4.409656524658203,
      "learning_rate": 0.00019103416409705132,
      "loss": 0.2993,
      "step": 10008
    },
    {
      "epoch": 0.047080350339144095,
      "grad_norm": 1.0864472389221191,
      "learning_rate": 0.00019103322111893784,
      "loss": 0.0983,
      "step": 10009
    },
    {
      "epoch": 0.04708505414075656,
      "grad_norm": 0.8960611820220947,
      "learning_rate": 0.00019103227814082436,
      "loss": 0.0601,
      "step": 10010
    },
    {
      "epoch": 0.047089757942369026,
      "grad_norm": 0.8780533671379089,
      "learning_rate": 0.00019103133516271088,
      "loss": 0.0707,
      "step": 10011
    },
    {
      "epoch": 0.047094461743981485,
      "grad_norm": 1.742976188659668,
      "learning_rate": 0.0001910303921845974,
      "loss": 0.1932,
      "step": 10012
    },
    {
      "epoch": 0.04709916554559395,
      "grad_norm": 0.7307915091514587,
      "learning_rate": 0.00019102944920648394,
      "loss": 0.0958,
      "step": 10013
    },
    {
      "epoch": 0.04710386934720641,
      "grad_norm": 1.074842095375061,
      "learning_rate": 0.00019102850622837046,
      "loss": 0.1046,
      "step": 10014
    },
    {
      "epoch": 0.047108573148818875,
      "grad_norm": 2.2340543270111084,
      "learning_rate": 0.00019102756325025698,
      "loss": 0.3053,
      "step": 10015
    },
    {
      "epoch": 0.04711327695043134,
      "grad_norm": 1.1673721075057983,
      "learning_rate": 0.00019102662027214347,
      "loss": 0.1071,
      "step": 10016
    },
    {
      "epoch": 0.0471179807520438,
      "grad_norm": 1.3266676664352417,
      "learning_rate": 0.00019102567729403002,
      "loss": 0.1722,
      "step": 10017
    },
    {
      "epoch": 0.047122684553656265,
      "grad_norm": 2.145810604095459,
      "learning_rate": 0.00019102473431591654,
      "loss": 0.2466,
      "step": 10018
    },
    {
      "epoch": 0.04712738835526873,
      "grad_norm": 0.42102527618408203,
      "learning_rate": 0.00019102379133780306,
      "loss": 0.048,
      "step": 10019
    },
    {
      "epoch": 0.04713209215688119,
      "grad_norm": 1.0600838661193848,
      "learning_rate": 0.00019102284835968957,
      "loss": 0.075,
      "step": 10020
    },
    {
      "epoch": 0.047136795958493655,
      "grad_norm": 1.1459251642227173,
      "learning_rate": 0.0001910219053815761,
      "loss": 0.1029,
      "step": 10021
    },
    {
      "epoch": 0.04714149976010612,
      "grad_norm": 2.233642339706421,
      "learning_rate": 0.00019102096240346264,
      "loss": 0.4758,
      "step": 10022
    },
    {
      "epoch": 0.04714620356171858,
      "grad_norm": 2.040470838546753,
      "learning_rate": 0.00019102001942534916,
      "loss": 0.0966,
      "step": 10023
    },
    {
      "epoch": 0.047150907363331045,
      "grad_norm": 2.571136474609375,
      "learning_rate": 0.00019101907644723568,
      "loss": 0.627,
      "step": 10024
    },
    {
      "epoch": 0.04715561116494351,
      "grad_norm": 1.4555318355560303,
      "learning_rate": 0.0001910181334691222,
      "loss": 0.0971,
      "step": 10025
    },
    {
      "epoch": 0.04716031496655597,
      "grad_norm": 1.9338009357452393,
      "learning_rate": 0.0001910171904910087,
      "loss": 0.4001,
      "step": 10026
    },
    {
      "epoch": 0.047165018768168435,
      "grad_norm": 2.627224922180176,
      "learning_rate": 0.00019101624751289523,
      "loss": 0.2617,
      "step": 10027
    },
    {
      "epoch": 0.0471697225697809,
      "grad_norm": 1.9180207252502441,
      "learning_rate": 0.00019101530453478175,
      "loss": 0.2756,
      "step": 10028
    },
    {
      "epoch": 0.04717442637139336,
      "grad_norm": 3.065464973449707,
      "learning_rate": 0.00019101436155666827,
      "loss": 0.4054,
      "step": 10029
    },
    {
      "epoch": 0.047179130173005825,
      "grad_norm": 3.3289434909820557,
      "learning_rate": 0.0001910134185785548,
      "loss": 0.399,
      "step": 10030
    },
    {
      "epoch": 0.04718383397461828,
      "grad_norm": 4.974565029144287,
      "learning_rate": 0.00019101247560044133,
      "loss": 0.5254,
      "step": 10031
    },
    {
      "epoch": 0.04718853777623075,
      "grad_norm": 2.1705241203308105,
      "learning_rate": 0.00019101153262232785,
      "loss": 0.3199,
      "step": 10032
    },
    {
      "epoch": 0.047193241577843215,
      "grad_norm": 2.3658242225646973,
      "learning_rate": 0.00019101058964421437,
      "loss": 0.268,
      "step": 10033
    },
    {
      "epoch": 0.04719794537945567,
      "grad_norm": 0.41128483414649963,
      "learning_rate": 0.0001910096466661009,
      "loss": 0.0269,
      "step": 10034
    },
    {
      "epoch": 0.04720264918106814,
      "grad_norm": 1.505985975265503,
      "learning_rate": 0.0001910087036879874,
      "loss": 0.1585,
      "step": 10035
    },
    {
      "epoch": 0.047207352982680605,
      "grad_norm": 2.0296449661254883,
      "learning_rate": 0.00019100776070987393,
      "loss": 0.3113,
      "step": 10036
    },
    {
      "epoch": 0.04721205678429306,
      "grad_norm": 3.3541946411132812,
      "learning_rate": 0.00019100681773176045,
      "loss": 0.5609,
      "step": 10037
    },
    {
      "epoch": 0.04721676058590553,
      "grad_norm": 1.8285188674926758,
      "learning_rate": 0.00019100587475364696,
      "loss": 0.192,
      "step": 10038
    },
    {
      "epoch": 0.047221464387517995,
      "grad_norm": 4.04030179977417,
      "learning_rate": 0.00019100493177553348,
      "loss": 0.6771,
      "step": 10039
    },
    {
      "epoch": 0.04722616818913045,
      "grad_norm": 2.0734429359436035,
      "learning_rate": 0.00019100398879742003,
      "loss": 0.2946,
      "step": 10040
    },
    {
      "epoch": 0.04723087199074292,
      "grad_norm": 5.098970890045166,
      "learning_rate": 0.00019100304581930655,
      "loss": 1.4272,
      "step": 10041
    },
    {
      "epoch": 0.047235575792355385,
      "grad_norm": 2.5247693061828613,
      "learning_rate": 0.00019100210284119307,
      "loss": 0.4225,
      "step": 10042
    },
    {
      "epoch": 0.04724027959396784,
      "grad_norm": 3.8270773887634277,
      "learning_rate": 0.00019100115986307958,
      "loss": 0.243,
      "step": 10043
    },
    {
      "epoch": 0.04724498339558031,
      "grad_norm": 2.405317544937134,
      "learning_rate": 0.0001910002168849661,
      "loss": 0.4268,
      "step": 10044
    },
    {
      "epoch": 0.047249687197192775,
      "grad_norm": 1.57235848903656,
      "learning_rate": 0.00019099927390685265,
      "loss": 0.3851,
      "step": 10045
    },
    {
      "epoch": 0.04725439099880523,
      "grad_norm": 1.1965831518173218,
      "learning_rate": 0.00019099833092873917,
      "loss": 0.1568,
      "step": 10046
    },
    {
      "epoch": 0.0472590948004177,
      "grad_norm": 1.3281420469284058,
      "learning_rate": 0.00019099738795062566,
      "loss": 0.1326,
      "step": 10047
    },
    {
      "epoch": 0.04726379860203016,
      "grad_norm": 3.4066715240478516,
      "learning_rate": 0.00019099644497251218,
      "loss": 0.6769,
      "step": 10048
    },
    {
      "epoch": 0.04726850240364262,
      "grad_norm": 0.8502542972564697,
      "learning_rate": 0.00019099550199439872,
      "loss": 0.2191,
      "step": 10049
    },
    {
      "epoch": 0.04727320620525509,
      "grad_norm": 1.0355020761489868,
      "learning_rate": 0.00019099455901628524,
      "loss": 0.2618,
      "step": 10050
    },
    {
      "epoch": 0.04727791000686755,
      "grad_norm": 2.554749011993408,
      "learning_rate": 0.00019099361603817176,
      "loss": 0.4375,
      "step": 10051
    },
    {
      "epoch": 0.04728261380848001,
      "grad_norm": 1.1447196006774902,
      "learning_rate": 0.00019099267306005828,
      "loss": 0.1994,
      "step": 10052
    },
    {
      "epoch": 0.04728731761009248,
      "grad_norm": 3.005638837814331,
      "learning_rate": 0.0001909917300819448,
      "loss": 0.7028,
      "step": 10053
    },
    {
      "epoch": 0.04729202141170494,
      "grad_norm": 0.7749421000480652,
      "learning_rate": 0.00019099078710383134,
      "loss": 0.0895,
      "step": 10054
    },
    {
      "epoch": 0.0472967252133174,
      "grad_norm": 1.4242750406265259,
      "learning_rate": 0.00019098984412571786,
      "loss": 0.2211,
      "step": 10055
    },
    {
      "epoch": 0.04730142901492987,
      "grad_norm": 2.082932710647583,
      "learning_rate": 0.00019098890114760438,
      "loss": 0.2923,
      "step": 10056
    },
    {
      "epoch": 0.04730613281654233,
      "grad_norm": 0.7453974485397339,
      "learning_rate": 0.0001909879581694909,
      "loss": 0.0927,
      "step": 10057
    },
    {
      "epoch": 0.04731083661815479,
      "grad_norm": 1.2814244031906128,
      "learning_rate": 0.00019098701519137742,
      "loss": 0.2671,
      "step": 10058
    },
    {
      "epoch": 0.04731554041976726,
      "grad_norm": 1.692621111869812,
      "learning_rate": 0.00019098607221326394,
      "loss": 0.2361,
      "step": 10059
    },
    {
      "epoch": 0.04732024422137972,
      "grad_norm": 2.0502471923828125,
      "learning_rate": 0.00019098512923515046,
      "loss": 0.4663,
      "step": 10060
    },
    {
      "epoch": 0.04732494802299218,
      "grad_norm": 2.365159511566162,
      "learning_rate": 0.00019098418625703697,
      "loss": 0.3234,
      "step": 10061
    },
    {
      "epoch": 0.04732965182460465,
      "grad_norm": 1.5334991216659546,
      "learning_rate": 0.0001909832432789235,
      "loss": 0.3246,
      "step": 10062
    },
    {
      "epoch": 0.04733435562621711,
      "grad_norm": 0.8216545581817627,
      "learning_rate": 0.00019098230030081004,
      "loss": 0.1138,
      "step": 10063
    },
    {
      "epoch": 0.04733905942782957,
      "grad_norm": 3.526841402053833,
      "learning_rate": 0.00019098135732269656,
      "loss": 0.9762,
      "step": 10064
    },
    {
      "epoch": 0.04734376322944203,
      "grad_norm": 1.3082607984542847,
      "learning_rate": 0.00019098041434458308,
      "loss": 0.1636,
      "step": 10065
    },
    {
      "epoch": 0.0473484670310545,
      "grad_norm": 2.2020223140716553,
      "learning_rate": 0.0001909794713664696,
      "loss": 0.1967,
      "step": 10066
    },
    {
      "epoch": 0.04735317083266696,
      "grad_norm": 2.5852138996124268,
      "learning_rate": 0.0001909785283883561,
      "loss": 0.4704,
      "step": 10067
    },
    {
      "epoch": 0.04735787463427942,
      "grad_norm": 0.5186812877655029,
      "learning_rate": 0.00019097758541024263,
      "loss": 0.0771,
      "step": 10068
    },
    {
      "epoch": 0.04736257843589189,
      "grad_norm": 2.481015920639038,
      "learning_rate": 0.00019097664243212915,
      "loss": 0.3613,
      "step": 10069
    },
    {
      "epoch": 0.04736728223750435,
      "grad_norm": 2.0255496501922607,
      "learning_rate": 0.00019097569945401567,
      "loss": 0.3363,
      "step": 10070
    },
    {
      "epoch": 0.04737198603911681,
      "grad_norm": 2.170966386795044,
      "learning_rate": 0.0001909747564759022,
      "loss": 0.2632,
      "step": 10071
    },
    {
      "epoch": 0.04737668984072928,
      "grad_norm": 2.165851354598999,
      "learning_rate": 0.00019097381349778873,
      "loss": 0.3398,
      "step": 10072
    },
    {
      "epoch": 0.04738139364234174,
      "grad_norm": 1.0119086503982544,
      "learning_rate": 0.00019097287051967525,
      "loss": 0.1227,
      "step": 10073
    },
    {
      "epoch": 0.0473860974439542,
      "grad_norm": 0.9792810678482056,
      "learning_rate": 0.00019097192754156177,
      "loss": 0.1274,
      "step": 10074
    },
    {
      "epoch": 0.04739080124556667,
      "grad_norm": 2.395512342453003,
      "learning_rate": 0.0001909709845634483,
      "loss": 0.5333,
      "step": 10075
    },
    {
      "epoch": 0.04739550504717913,
      "grad_norm": 2.000645399093628,
      "learning_rate": 0.00019097004158533484,
      "loss": 0.2466,
      "step": 10076
    },
    {
      "epoch": 0.04740020884879159,
      "grad_norm": 2.1882176399230957,
      "learning_rate": 0.00019096909860722135,
      "loss": 0.2986,
      "step": 10077
    },
    {
      "epoch": 0.04740491265040406,
      "grad_norm": 0.9848790764808655,
      "learning_rate": 0.00019096815562910785,
      "loss": 0.0965,
      "step": 10078
    },
    {
      "epoch": 0.04740961645201652,
      "grad_norm": 0.7031300067901611,
      "learning_rate": 0.00019096721265099436,
      "loss": 0.0856,
      "step": 10079
    },
    {
      "epoch": 0.04741432025362898,
      "grad_norm": 4.232758522033691,
      "learning_rate": 0.00019096626967288088,
      "loss": 0.9271,
      "step": 10080
    },
    {
      "epoch": 0.04741902405524145,
      "grad_norm": 1.159348487854004,
      "learning_rate": 0.00019096532669476743,
      "loss": 0.1219,
      "step": 10081
    },
    {
      "epoch": 0.047423727856853906,
      "grad_norm": 1.5529181957244873,
      "learning_rate": 0.00019096438371665395,
      "loss": 0.3593,
      "step": 10082
    },
    {
      "epoch": 0.04742843165846637,
      "grad_norm": 2.1320791244506836,
      "learning_rate": 0.00019096344073854047,
      "loss": 0.4101,
      "step": 10083
    },
    {
      "epoch": 0.04743313546007884,
      "grad_norm": 1.6992855072021484,
      "learning_rate": 0.00019096249776042698,
      "loss": 0.28,
      "step": 10084
    },
    {
      "epoch": 0.047437839261691296,
      "grad_norm": 0.838322103023529,
      "learning_rate": 0.0001909615547823135,
      "loss": 0.1468,
      "step": 10085
    },
    {
      "epoch": 0.04744254306330376,
      "grad_norm": 2.272731304168701,
      "learning_rate": 0.00019096061180420005,
      "loss": 0.3728,
      "step": 10086
    },
    {
      "epoch": 0.04744724686491623,
      "grad_norm": 1.7293838262557983,
      "learning_rate": 0.00019095966882608657,
      "loss": 0.2216,
      "step": 10087
    },
    {
      "epoch": 0.047451950666528686,
      "grad_norm": 1.4293502569198608,
      "learning_rate": 0.00019095872584797309,
      "loss": 0.162,
      "step": 10088
    },
    {
      "epoch": 0.04745665446814115,
      "grad_norm": 0.5219433307647705,
      "learning_rate": 0.0001909577828698596,
      "loss": 0.0799,
      "step": 10089
    },
    {
      "epoch": 0.04746135826975362,
      "grad_norm": 0.7842340469360352,
      "learning_rate": 0.00019095683989174612,
      "loss": 0.0747,
      "step": 10090
    },
    {
      "epoch": 0.047466062071366076,
      "grad_norm": 1.5033565759658813,
      "learning_rate": 0.00019095589691363264,
      "loss": 0.2849,
      "step": 10091
    },
    {
      "epoch": 0.04747076587297854,
      "grad_norm": 1.1521474123001099,
      "learning_rate": 0.00019095495393551916,
      "loss": 0.0911,
      "step": 10092
    },
    {
      "epoch": 0.04747546967459101,
      "grad_norm": 7.087916374206543,
      "learning_rate": 0.00019095401095740568,
      "loss": 1.4509,
      "step": 10093
    },
    {
      "epoch": 0.047480173476203466,
      "grad_norm": 2.9630544185638428,
      "learning_rate": 0.0001909530679792922,
      "loss": 0.3996,
      "step": 10094
    },
    {
      "epoch": 0.04748487727781593,
      "grad_norm": 1.6336780786514282,
      "learning_rate": 0.00019095212500117874,
      "loss": 0.2593,
      "step": 10095
    },
    {
      "epoch": 0.0474895810794284,
      "grad_norm": 2.2320337295532227,
      "learning_rate": 0.00019095118202306526,
      "loss": 0.1773,
      "step": 10096
    },
    {
      "epoch": 0.047494284881040856,
      "grad_norm": 1.7691818475723267,
      "learning_rate": 0.00019095023904495178,
      "loss": 0.314,
      "step": 10097
    },
    {
      "epoch": 0.04749898868265332,
      "grad_norm": 1.255373239517212,
      "learning_rate": 0.0001909492960668383,
      "loss": 0.2126,
      "step": 10098
    },
    {
      "epoch": 0.04750369248426578,
      "grad_norm": 0.5136979818344116,
      "learning_rate": 0.00019094835308872482,
      "loss": 0.0422,
      "step": 10099
    },
    {
      "epoch": 0.047508396285878246,
      "grad_norm": 1.4207175970077515,
      "learning_rate": 0.00019094741011061134,
      "loss": 0.1832,
      "step": 10100
    },
    {
      "epoch": 0.04751310008749071,
      "grad_norm": 0.25443899631500244,
      "learning_rate": 0.00019094646713249786,
      "loss": 0.0172,
      "step": 10101
    },
    {
      "epoch": 0.04751780388910317,
      "grad_norm": 5.667386531829834,
      "learning_rate": 0.00019094552415438437,
      "loss": 0.9751,
      "step": 10102
    },
    {
      "epoch": 0.047522507690715636,
      "grad_norm": 3.481656551361084,
      "learning_rate": 0.0001909445811762709,
      "loss": 0.3976,
      "step": 10103
    },
    {
      "epoch": 0.0475272114923281,
      "grad_norm": 3.5558016300201416,
      "learning_rate": 0.00019094363819815744,
      "loss": 0.4086,
      "step": 10104
    },
    {
      "epoch": 0.04753191529394056,
      "grad_norm": 1.945162057876587,
      "learning_rate": 0.00019094269522004396,
      "loss": 0.2072,
      "step": 10105
    },
    {
      "epoch": 0.047536619095553026,
      "grad_norm": 0.39239445328712463,
      "learning_rate": 0.00019094175224193048,
      "loss": 0.0326,
      "step": 10106
    },
    {
      "epoch": 0.04754132289716549,
      "grad_norm": 3.3046836853027344,
      "learning_rate": 0.000190940809263817,
      "loss": 0.4196,
      "step": 10107
    },
    {
      "epoch": 0.04754602669877795,
      "grad_norm": 2.6877267360687256,
      "learning_rate": 0.00019093986628570354,
      "loss": 0.2773,
      "step": 10108
    },
    {
      "epoch": 0.047550730500390416,
      "grad_norm": 1.0104540586471558,
      "learning_rate": 0.00019093892330759003,
      "loss": 0.1219,
      "step": 10109
    },
    {
      "epoch": 0.04755543430200288,
      "grad_norm": 0.1306336373090744,
      "learning_rate": 0.00019093798032947655,
      "loss": 0.0082,
      "step": 10110
    },
    {
      "epoch": 0.04756013810361534,
      "grad_norm": 4.99395751953125,
      "learning_rate": 0.00019093703735136307,
      "loss": 0.9525,
      "step": 10111
    },
    {
      "epoch": 0.047564841905227806,
      "grad_norm": 2.754014492034912,
      "learning_rate": 0.0001909360943732496,
      "loss": 0.595,
      "step": 10112
    },
    {
      "epoch": 0.04756954570684027,
      "grad_norm": 1.5048831701278687,
      "learning_rate": 0.00019093515139513613,
      "loss": 0.1697,
      "step": 10113
    },
    {
      "epoch": 0.04757424950845273,
      "grad_norm": 1.8972357511520386,
      "learning_rate": 0.00019093420841702265,
      "loss": 0.2598,
      "step": 10114
    },
    {
      "epoch": 0.047578953310065196,
      "grad_norm": 2.5035412311553955,
      "learning_rate": 0.00019093326543890917,
      "loss": 0.3274,
      "step": 10115
    },
    {
      "epoch": 0.047583657111677655,
      "grad_norm": 1.9810185432434082,
      "learning_rate": 0.0001909323224607957,
      "loss": 0.1756,
      "step": 10116
    },
    {
      "epoch": 0.04758836091329012,
      "grad_norm": 5.542299270629883,
      "learning_rate": 0.00019093137948268224,
      "loss": 0.8354,
      "step": 10117
    },
    {
      "epoch": 0.047593064714902586,
      "grad_norm": 1.4788333177566528,
      "learning_rate": 0.00019093043650456875,
      "loss": 0.1833,
      "step": 10118
    },
    {
      "epoch": 0.047597768516515045,
      "grad_norm": 1.631767988204956,
      "learning_rate": 0.00019092949352645527,
      "loss": 0.2287,
      "step": 10119
    },
    {
      "epoch": 0.04760247231812751,
      "grad_norm": 2.3288798332214355,
      "learning_rate": 0.0001909285505483418,
      "loss": 0.3419,
      "step": 10120
    },
    {
      "epoch": 0.047607176119739976,
      "grad_norm": 0.7621592283248901,
      "learning_rate": 0.00019092760757022828,
      "loss": 0.0697,
      "step": 10121
    },
    {
      "epoch": 0.047611879921352435,
      "grad_norm": 3.243170738220215,
      "learning_rate": 0.00019092666459211483,
      "loss": 0.6823,
      "step": 10122
    },
    {
      "epoch": 0.0476165837229649,
      "grad_norm": 5.414485931396484,
      "learning_rate": 0.00019092572161400135,
      "loss": 0.6216,
      "step": 10123
    },
    {
      "epoch": 0.047621287524577366,
      "grad_norm": 2.0674691200256348,
      "learning_rate": 0.00019092477863588787,
      "loss": 0.2566,
      "step": 10124
    },
    {
      "epoch": 0.047625991326189825,
      "grad_norm": 2.5517306327819824,
      "learning_rate": 0.00019092383565777438,
      "loss": 0.2434,
      "step": 10125
    },
    {
      "epoch": 0.04763069512780229,
      "grad_norm": 1.490821123123169,
      "learning_rate": 0.00019092289267966093,
      "loss": 0.1838,
      "step": 10126
    },
    {
      "epoch": 0.047635398929414756,
      "grad_norm": 1.8267589807510376,
      "learning_rate": 0.00019092194970154745,
      "loss": 0.3844,
      "step": 10127
    },
    {
      "epoch": 0.047640102731027215,
      "grad_norm": 0.2510024309158325,
      "learning_rate": 0.00019092100672343397,
      "loss": 0.0199,
      "step": 10128
    },
    {
      "epoch": 0.04764480653263968,
      "grad_norm": 1.8852120637893677,
      "learning_rate": 0.00019092006374532049,
      "loss": 0.2069,
      "step": 10129
    },
    {
      "epoch": 0.047649510334252146,
      "grad_norm": 2.322920322418213,
      "learning_rate": 0.000190919120767207,
      "loss": 0.3627,
      "step": 10130
    },
    {
      "epoch": 0.047654214135864605,
      "grad_norm": 0.8839154839515686,
      "learning_rate": 0.00019091817778909352,
      "loss": 0.2158,
      "step": 10131
    },
    {
      "epoch": 0.04765891793747707,
      "grad_norm": 2.0610251426696777,
      "learning_rate": 0.00019091723481098004,
      "loss": 0.3446,
      "step": 10132
    },
    {
      "epoch": 0.04766362173908953,
      "grad_norm": 0.705863356590271,
      "learning_rate": 0.00019091629183286656,
      "loss": 0.0713,
      "step": 10133
    },
    {
      "epoch": 0.047668325540701995,
      "grad_norm": 0.9942565560340881,
      "learning_rate": 0.00019091534885475308,
      "loss": 0.1445,
      "step": 10134
    },
    {
      "epoch": 0.04767302934231446,
      "grad_norm": 0.8437761664390564,
      "learning_rate": 0.0001909144058766396,
      "loss": 0.0724,
      "step": 10135
    },
    {
      "epoch": 0.04767773314392692,
      "grad_norm": 4.118560314178467,
      "learning_rate": 0.00019091346289852614,
      "loss": 0.9547,
      "step": 10136
    },
    {
      "epoch": 0.047682436945539385,
      "grad_norm": 1.8147801160812378,
      "learning_rate": 0.00019091251992041266,
      "loss": 0.3826,
      "step": 10137
    },
    {
      "epoch": 0.04768714074715185,
      "grad_norm": 1.0674253702163696,
      "learning_rate": 0.00019091157694229918,
      "loss": 0.0961,
      "step": 10138
    },
    {
      "epoch": 0.04769184454876431,
      "grad_norm": 1.1282955408096313,
      "learning_rate": 0.0001909106339641857,
      "loss": 0.2045,
      "step": 10139
    },
    {
      "epoch": 0.047696548350376775,
      "grad_norm": 0.8495429158210754,
      "learning_rate": 0.00019090969098607222,
      "loss": 0.0871,
      "step": 10140
    },
    {
      "epoch": 0.04770125215198924,
      "grad_norm": 0.5746497511863708,
      "learning_rate": 0.00019090874800795874,
      "loss": 0.0524,
      "step": 10141
    },
    {
      "epoch": 0.0477059559536017,
      "grad_norm": 1.6179873943328857,
      "learning_rate": 0.00019090780502984526,
      "loss": 0.1883,
      "step": 10142
    },
    {
      "epoch": 0.047710659755214165,
      "grad_norm": 3.2187623977661133,
      "learning_rate": 0.00019090686205173177,
      "loss": 0.5494,
      "step": 10143
    },
    {
      "epoch": 0.04771536355682663,
      "grad_norm": 0.7311460971832275,
      "learning_rate": 0.0001909059190736183,
      "loss": 0.0765,
      "step": 10144
    },
    {
      "epoch": 0.04772006735843909,
      "grad_norm": 1.5678801536560059,
      "learning_rate": 0.00019090497609550484,
      "loss": 0.1794,
      "step": 10145
    },
    {
      "epoch": 0.047724771160051555,
      "grad_norm": 3.5708632469177246,
      "learning_rate": 0.00019090403311739136,
      "loss": 0.8562,
      "step": 10146
    },
    {
      "epoch": 0.04772947496166402,
      "grad_norm": 3.7521719932556152,
      "learning_rate": 0.00019090309013927788,
      "loss": 0.6911,
      "step": 10147
    },
    {
      "epoch": 0.04773417876327648,
      "grad_norm": 3.241342544555664,
      "learning_rate": 0.0001909021471611644,
      "loss": 0.5486,
      "step": 10148
    },
    {
      "epoch": 0.047738882564888944,
      "grad_norm": 0.6935761570930481,
      "learning_rate": 0.00019090120418305094,
      "loss": 0.0614,
      "step": 10149
    },
    {
      "epoch": 0.0477435863665014,
      "grad_norm": 1.8253356218338013,
      "learning_rate": 0.00019090026120493746,
      "loss": 0.2288,
      "step": 10150
    },
    {
      "epoch": 0.04774829016811387,
      "grad_norm": 0.46965864300727844,
      "learning_rate": 0.00019089931822682398,
      "loss": 0.0321,
      "step": 10151
    },
    {
      "epoch": 0.047752993969726334,
      "grad_norm": 1.5033979415893555,
      "learning_rate": 0.00019089837524871047,
      "loss": 0.1852,
      "step": 10152
    },
    {
      "epoch": 0.04775769777133879,
      "grad_norm": 1.277079463005066,
      "learning_rate": 0.000190897432270597,
      "loss": 0.1356,
      "step": 10153
    },
    {
      "epoch": 0.04776240157295126,
      "grad_norm": 0.6574967503547668,
      "learning_rate": 0.00019089648929248353,
      "loss": 0.0598,
      "step": 10154
    },
    {
      "epoch": 0.047767105374563724,
      "grad_norm": 2.9565107822418213,
      "learning_rate": 0.00019089554631437005,
      "loss": 0.4931,
      "step": 10155
    },
    {
      "epoch": 0.04777180917617618,
      "grad_norm": 2.576568126678467,
      "learning_rate": 0.00019089460333625657,
      "loss": 0.4962,
      "step": 10156
    },
    {
      "epoch": 0.04777651297778865,
      "grad_norm": 2.525493860244751,
      "learning_rate": 0.0001908936603581431,
      "loss": 0.4609,
      "step": 10157
    },
    {
      "epoch": 0.047781216779401114,
      "grad_norm": 0.44959813356399536,
      "learning_rate": 0.00019089271738002964,
      "loss": 0.0624,
      "step": 10158
    },
    {
      "epoch": 0.04778592058101357,
      "grad_norm": 1.0316303968429565,
      "learning_rate": 0.00019089177440191615,
      "loss": 0.1063,
      "step": 10159
    },
    {
      "epoch": 0.04779062438262604,
      "grad_norm": 3.0687367916107178,
      "learning_rate": 0.00019089083142380267,
      "loss": 0.2485,
      "step": 10160
    },
    {
      "epoch": 0.047795328184238504,
      "grad_norm": 2.703320026397705,
      "learning_rate": 0.0001908898884456892,
      "loss": 0.4168,
      "step": 10161
    },
    {
      "epoch": 0.04780003198585096,
      "grad_norm": 2.925649642944336,
      "learning_rate": 0.0001908889454675757,
      "loss": 0.4904,
      "step": 10162
    },
    {
      "epoch": 0.04780473578746343,
      "grad_norm": 1.6739695072174072,
      "learning_rate": 0.00019088800248946223,
      "loss": 0.1504,
      "step": 10163
    },
    {
      "epoch": 0.047809439589075894,
      "grad_norm": 1.778721809387207,
      "learning_rate": 0.00019088705951134875,
      "loss": 0.258,
      "step": 10164
    },
    {
      "epoch": 0.04781414339068835,
      "grad_norm": 2.7644927501678467,
      "learning_rate": 0.00019088611653323527,
      "loss": 0.5098,
      "step": 10165
    },
    {
      "epoch": 0.04781884719230082,
      "grad_norm": 3.3850162029266357,
      "learning_rate": 0.00019088517355512178,
      "loss": 0.5316,
      "step": 10166
    },
    {
      "epoch": 0.04782355099391328,
      "grad_norm": 3.5482661724090576,
      "learning_rate": 0.00019088423057700833,
      "loss": 0.5906,
      "step": 10167
    },
    {
      "epoch": 0.04782825479552574,
      "grad_norm": 2.556345224380493,
      "learning_rate": 0.00019088328759889485,
      "loss": 0.4641,
      "step": 10168
    },
    {
      "epoch": 0.04783295859713821,
      "grad_norm": 3.461698055267334,
      "learning_rate": 0.00019088234462078137,
      "loss": 0.72,
      "step": 10169
    },
    {
      "epoch": 0.04783766239875067,
      "grad_norm": 2.1253435611724854,
      "learning_rate": 0.00019088140164266789,
      "loss": 0.2302,
      "step": 10170
    },
    {
      "epoch": 0.04784236620036313,
      "grad_norm": 1.3526521921157837,
      "learning_rate": 0.0001908804586645544,
      "loss": 0.192,
      "step": 10171
    },
    {
      "epoch": 0.0478470700019756,
      "grad_norm": 2.584883689880371,
      "learning_rate": 0.00019087951568644092,
      "loss": 0.4162,
      "step": 10172
    },
    {
      "epoch": 0.04785177380358806,
      "grad_norm": 2.0457141399383545,
      "learning_rate": 0.00019087857270832744,
      "loss": 0.3899,
      "step": 10173
    },
    {
      "epoch": 0.04785647760520052,
      "grad_norm": 3.609072685241699,
      "learning_rate": 0.00019087762973021396,
      "loss": 0.5089,
      "step": 10174
    },
    {
      "epoch": 0.04786118140681299,
      "grad_norm": 1.4813594818115234,
      "learning_rate": 0.00019087668675210048,
      "loss": 0.1486,
      "step": 10175
    },
    {
      "epoch": 0.04786588520842545,
      "grad_norm": 1.1663904190063477,
      "learning_rate": 0.00019087574377398703,
      "loss": 0.2321,
      "step": 10176
    },
    {
      "epoch": 0.04787058901003791,
      "grad_norm": 1.2958701848983765,
      "learning_rate": 0.00019087480079587354,
      "loss": 0.1771,
      "step": 10177
    },
    {
      "epoch": 0.04787529281165038,
      "grad_norm": 1.9806768894195557,
      "learning_rate": 0.00019087385781776006,
      "loss": 0.3785,
      "step": 10178
    },
    {
      "epoch": 0.04787999661326284,
      "grad_norm": 3.700483560562134,
      "learning_rate": 0.00019087291483964658,
      "loss": 0.5207,
      "step": 10179
    },
    {
      "epoch": 0.0478847004148753,
      "grad_norm": 1.7018468379974365,
      "learning_rate": 0.0001908719718615331,
      "loss": 0.3732,
      "step": 10180
    },
    {
      "epoch": 0.04788940421648777,
      "grad_norm": 1.7361769676208496,
      "learning_rate": 0.00019087102888341965,
      "loss": 0.221,
      "step": 10181
    },
    {
      "epoch": 0.04789410801810023,
      "grad_norm": 1.2085940837860107,
      "learning_rate": 0.00019087008590530616,
      "loss": 0.2143,
      "step": 10182
    },
    {
      "epoch": 0.04789881181971269,
      "grad_norm": 0.7043557167053223,
      "learning_rate": 0.00019086914292719266,
      "loss": 0.0505,
      "step": 10183
    },
    {
      "epoch": 0.04790351562132515,
      "grad_norm": 0.9590184688568115,
      "learning_rate": 0.00019086819994907917,
      "loss": 0.1352,
      "step": 10184
    },
    {
      "epoch": 0.04790821942293762,
      "grad_norm": 1.7401561737060547,
      "learning_rate": 0.0001908672569709657,
      "loss": 0.2247,
      "step": 10185
    },
    {
      "epoch": 0.04791292322455008,
      "grad_norm": 0.844158411026001,
      "learning_rate": 0.00019086631399285224,
      "loss": 0.0855,
      "step": 10186
    },
    {
      "epoch": 0.04791762702616254,
      "grad_norm": 1.8446894884109497,
      "learning_rate": 0.00019086537101473876,
      "loss": 0.2587,
      "step": 10187
    },
    {
      "epoch": 0.04792233082777501,
      "grad_norm": 1.712379813194275,
      "learning_rate": 0.00019086442803662528,
      "loss": 0.3202,
      "step": 10188
    },
    {
      "epoch": 0.04792703462938747,
      "grad_norm": 3.0920522212982178,
      "learning_rate": 0.0001908634850585118,
      "loss": 0.6377,
      "step": 10189
    },
    {
      "epoch": 0.04793173843099993,
      "grad_norm": 1.369945764541626,
      "learning_rate": 0.00019086254208039834,
      "loss": 0.1417,
      "step": 10190
    },
    {
      "epoch": 0.0479364422326124,
      "grad_norm": 1.3909116983413696,
      "learning_rate": 0.00019086159910228486,
      "loss": 0.1592,
      "step": 10191
    },
    {
      "epoch": 0.04794114603422486,
      "grad_norm": 0.4702630043029785,
      "learning_rate": 0.00019086065612417138,
      "loss": 0.0762,
      "step": 10192
    },
    {
      "epoch": 0.04794584983583732,
      "grad_norm": 0.7962366342544556,
      "learning_rate": 0.0001908597131460579,
      "loss": 0.0687,
      "step": 10193
    },
    {
      "epoch": 0.04795055363744979,
      "grad_norm": 1.6680047512054443,
      "learning_rate": 0.0001908587701679444,
      "loss": 0.2095,
      "step": 10194
    },
    {
      "epoch": 0.04795525743906225,
      "grad_norm": 1.4254034757614136,
      "learning_rate": 0.00019085782718983093,
      "loss": 0.1841,
      "step": 10195
    },
    {
      "epoch": 0.04795996124067471,
      "grad_norm": 3.0312159061431885,
      "learning_rate": 0.00019085688421171745,
      "loss": 0.2446,
      "step": 10196
    },
    {
      "epoch": 0.04796466504228718,
      "grad_norm": 2.835357427597046,
      "learning_rate": 0.00019085594123360397,
      "loss": 0.4855,
      "step": 10197
    },
    {
      "epoch": 0.04796936884389964,
      "grad_norm": 2.8242084980010986,
      "learning_rate": 0.0001908549982554905,
      "loss": 0.4414,
      "step": 10198
    },
    {
      "epoch": 0.0479740726455121,
      "grad_norm": 1.8656423091888428,
      "learning_rate": 0.00019085405527737704,
      "loss": 0.1854,
      "step": 10199
    },
    {
      "epoch": 0.04797877644712457,
      "grad_norm": 3.2584800720214844,
      "learning_rate": 0.00019085311229926355,
      "loss": 1.0761,
      "step": 10200
    },
    {
      "epoch": 0.047983480248737026,
      "grad_norm": 1.420386791229248,
      "learning_rate": 0.00019085216932115007,
      "loss": 0.2297,
      "step": 10201
    },
    {
      "epoch": 0.04798818405034949,
      "grad_norm": 3.009658098220825,
      "learning_rate": 0.0001908512263430366,
      "loss": 0.3737,
      "step": 10202
    },
    {
      "epoch": 0.04799288785196196,
      "grad_norm": 2.7214720249176025,
      "learning_rate": 0.0001908502833649231,
      "loss": 0.4515,
      "step": 10203
    },
    {
      "epoch": 0.047997591653574416,
      "grad_norm": 2.02240252494812,
      "learning_rate": 0.00019084934038680963,
      "loss": 0.2061,
      "step": 10204
    },
    {
      "epoch": 0.04800229545518688,
      "grad_norm": 0.26705384254455566,
      "learning_rate": 0.00019084839740869615,
      "loss": 0.0282,
      "step": 10205
    },
    {
      "epoch": 0.04800699925679935,
      "grad_norm": 2.1388309001922607,
      "learning_rate": 0.00019084745443058267,
      "loss": 0.2642,
      "step": 10206
    },
    {
      "epoch": 0.048011703058411806,
      "grad_norm": 2.9872498512268066,
      "learning_rate": 0.00019084651145246918,
      "loss": 0.3985,
      "step": 10207
    },
    {
      "epoch": 0.04801640686002427,
      "grad_norm": 0.8192524313926697,
      "learning_rate": 0.00019084556847435573,
      "loss": 0.0633,
      "step": 10208
    },
    {
      "epoch": 0.04802111066163674,
      "grad_norm": 4.250947952270508,
      "learning_rate": 0.00019084462549624225,
      "loss": 0.5187,
      "step": 10209
    },
    {
      "epoch": 0.048025814463249196,
      "grad_norm": 1.1864089965820312,
      "learning_rate": 0.00019084368251812877,
      "loss": 0.097,
      "step": 10210
    },
    {
      "epoch": 0.04803051826486166,
      "grad_norm": 3.6116578578948975,
      "learning_rate": 0.00019084273954001529,
      "loss": 0.5091,
      "step": 10211
    },
    {
      "epoch": 0.04803522206647413,
      "grad_norm": 2.1921980381011963,
      "learning_rate": 0.0001908417965619018,
      "loss": 0.2307,
      "step": 10212
    },
    {
      "epoch": 0.048039925868086586,
      "grad_norm": 2.972720146179199,
      "learning_rate": 0.00019084085358378835,
      "loss": 0.26,
      "step": 10213
    },
    {
      "epoch": 0.04804462966969905,
      "grad_norm": 2.099268913269043,
      "learning_rate": 0.00019083991060567484,
      "loss": 0.2315,
      "step": 10214
    },
    {
      "epoch": 0.04804933347131152,
      "grad_norm": 1.223397970199585,
      "learning_rate": 0.00019083896762756136,
      "loss": 0.1125,
      "step": 10215
    },
    {
      "epoch": 0.048054037272923976,
      "grad_norm": 1.5832633972167969,
      "learning_rate": 0.00019083802464944788,
      "loss": 0.1468,
      "step": 10216
    },
    {
      "epoch": 0.04805874107453644,
      "grad_norm": 5.18780517578125,
      "learning_rate": 0.00019083708167133443,
      "loss": 0.644,
      "step": 10217
    },
    {
      "epoch": 0.0480634448761489,
      "grad_norm": 2.7673399448394775,
      "learning_rate": 0.00019083613869322094,
      "loss": 0.2648,
      "step": 10218
    },
    {
      "epoch": 0.048068148677761366,
      "grad_norm": 1.840183138847351,
      "learning_rate": 0.00019083519571510746,
      "loss": 0.2118,
      "step": 10219
    },
    {
      "epoch": 0.04807285247937383,
      "grad_norm": 1.9390448331832886,
      "learning_rate": 0.00019083425273699398,
      "loss": 0.2509,
      "step": 10220
    },
    {
      "epoch": 0.04807755628098629,
      "grad_norm": 1.3392817974090576,
      "learning_rate": 0.0001908333097588805,
      "loss": 0.0966,
      "step": 10221
    },
    {
      "epoch": 0.048082260082598756,
      "grad_norm": 2.0015478134155273,
      "learning_rate": 0.00019083236678076705,
      "loss": 0.2263,
      "step": 10222
    },
    {
      "epoch": 0.04808696388421122,
      "grad_norm": 2.4821114540100098,
      "learning_rate": 0.00019083142380265356,
      "loss": 0.5829,
      "step": 10223
    },
    {
      "epoch": 0.04809166768582368,
      "grad_norm": 6.9127984046936035,
      "learning_rate": 0.00019083048082454008,
      "loss": 0.2581,
      "step": 10224
    },
    {
      "epoch": 0.048096371487436146,
      "grad_norm": 0.1039975956082344,
      "learning_rate": 0.00019082953784642657,
      "loss": 0.0056,
      "step": 10225
    },
    {
      "epoch": 0.04810107528904861,
      "grad_norm": 1.1411892175674438,
      "learning_rate": 0.00019082859486831312,
      "loss": 0.1711,
      "step": 10226
    },
    {
      "epoch": 0.04810577909066107,
      "grad_norm": 2.3107283115386963,
      "learning_rate": 0.00019082765189019964,
      "loss": 0.2661,
      "step": 10227
    },
    {
      "epoch": 0.048110482892273536,
      "grad_norm": 1.0442967414855957,
      "learning_rate": 0.00019082670891208616,
      "loss": 0.1093,
      "step": 10228
    },
    {
      "epoch": 0.048115186693886,
      "grad_norm": 0.7698086500167847,
      "learning_rate": 0.00019082576593397268,
      "loss": 0.0576,
      "step": 10229
    },
    {
      "epoch": 0.04811989049549846,
      "grad_norm": 1.458296298980713,
      "learning_rate": 0.0001908248229558592,
      "loss": 0.1689,
      "step": 10230
    },
    {
      "epoch": 0.048124594297110926,
      "grad_norm": 3.871807336807251,
      "learning_rate": 0.00019082387997774574,
      "loss": 0.7884,
      "step": 10231
    },
    {
      "epoch": 0.04812929809872339,
      "grad_norm": 1.9211328029632568,
      "learning_rate": 0.00019082293699963226,
      "loss": 0.2105,
      "step": 10232
    },
    {
      "epoch": 0.04813400190033585,
      "grad_norm": 0.19955134391784668,
      "learning_rate": 0.00019082199402151878,
      "loss": 0.0153,
      "step": 10233
    },
    {
      "epoch": 0.048138705701948316,
      "grad_norm": 1.624739170074463,
      "learning_rate": 0.0001908210510434053,
      "loss": 0.1533,
      "step": 10234
    },
    {
      "epoch": 0.048143409503560775,
      "grad_norm": 1.8441569805145264,
      "learning_rate": 0.00019082010806529181,
      "loss": 0.1882,
      "step": 10235
    },
    {
      "epoch": 0.04814811330517324,
      "grad_norm": 1.2580138444900513,
      "learning_rate": 0.00019081916508717833,
      "loss": 0.1025,
      "step": 10236
    },
    {
      "epoch": 0.048152817106785706,
      "grad_norm": 4.3378448486328125,
      "learning_rate": 0.00019081822210906485,
      "loss": 0.5396,
      "step": 10237
    },
    {
      "epoch": 0.048157520908398165,
      "grad_norm": 2.9144675731658936,
      "learning_rate": 0.00019081727913095137,
      "loss": 0.3169,
      "step": 10238
    },
    {
      "epoch": 0.04816222471001063,
      "grad_norm": 2.2159366607666016,
      "learning_rate": 0.0001908163361528379,
      "loss": 0.4638,
      "step": 10239
    },
    {
      "epoch": 0.048166928511623096,
      "grad_norm": 1.0681874752044678,
      "learning_rate": 0.00019081539317472444,
      "loss": 0.111,
      "step": 10240
    },
    {
      "epoch": 0.048171632313235555,
      "grad_norm": 1.208685278892517,
      "learning_rate": 0.00019081445019661095,
      "loss": 0.09,
      "step": 10241
    },
    {
      "epoch": 0.04817633611484802,
      "grad_norm": 3.3303616046905518,
      "learning_rate": 0.00019081350721849747,
      "loss": 0.7224,
      "step": 10242
    },
    {
      "epoch": 0.048181039916460486,
      "grad_norm": 2.709789991378784,
      "learning_rate": 0.000190812564240384,
      "loss": 0.3829,
      "step": 10243
    },
    {
      "epoch": 0.048185743718072944,
      "grad_norm": 0.640359103679657,
      "learning_rate": 0.0001908116212622705,
      "loss": 0.0495,
      "step": 10244
    },
    {
      "epoch": 0.04819044751968541,
      "grad_norm": 0.4454604387283325,
      "learning_rate": 0.00019081067828415703,
      "loss": 0.0203,
      "step": 10245
    },
    {
      "epoch": 0.048195151321297876,
      "grad_norm": 0.49105703830718994,
      "learning_rate": 0.00019080973530604355,
      "loss": 0.0453,
      "step": 10246
    },
    {
      "epoch": 0.048199855122910334,
      "grad_norm": 2.449289321899414,
      "learning_rate": 0.00019080879232793007,
      "loss": 0.3752,
      "step": 10247
    },
    {
      "epoch": 0.0482045589245228,
      "grad_norm": 0.7022053003311157,
      "learning_rate": 0.00019080784934981658,
      "loss": 0.0273,
      "step": 10248
    },
    {
      "epoch": 0.048209262726135266,
      "grad_norm": 0.9893139004707336,
      "learning_rate": 0.00019080690637170313,
      "loss": 0.1166,
      "step": 10249
    },
    {
      "epoch": 0.048213966527747724,
      "grad_norm": 0.7149044275283813,
      "learning_rate": 0.00019080596339358965,
      "loss": 0.0444,
      "step": 10250
    },
    {
      "epoch": 0.04821867032936019,
      "grad_norm": 4.052587985992432,
      "learning_rate": 0.00019080502041547617,
      "loss": 0.2847,
      "step": 10251
    },
    {
      "epoch": 0.04822337413097265,
      "grad_norm": 2.4624836444854736,
      "learning_rate": 0.00019080407743736269,
      "loss": 0.3175,
      "step": 10252
    },
    {
      "epoch": 0.048228077932585114,
      "grad_norm": 2.4982035160064697,
      "learning_rate": 0.0001908031344592492,
      "loss": 0.222,
      "step": 10253
    },
    {
      "epoch": 0.04823278173419758,
      "grad_norm": 2.788228750228882,
      "learning_rate": 0.00019080219148113575,
      "loss": 0.3336,
      "step": 10254
    },
    {
      "epoch": 0.04823748553581004,
      "grad_norm": 1.3163727521896362,
      "learning_rate": 0.00019080124850302227,
      "loss": 0.1526,
      "step": 10255
    },
    {
      "epoch": 0.048242189337422504,
      "grad_norm": 3.6989834308624268,
      "learning_rate": 0.00019080030552490876,
      "loss": 0.4404,
      "step": 10256
    },
    {
      "epoch": 0.04824689313903497,
      "grad_norm": 3.57888126373291,
      "learning_rate": 0.00019079936254679528,
      "loss": 0.4332,
      "step": 10257
    },
    {
      "epoch": 0.04825159694064743,
      "grad_norm": 3.2205519676208496,
      "learning_rate": 0.00019079841956868183,
      "loss": 0.4231,
      "step": 10258
    },
    {
      "epoch": 0.048256300742259894,
      "grad_norm": 4.661947727203369,
      "learning_rate": 0.00019079747659056834,
      "loss": 0.6548,
      "step": 10259
    },
    {
      "epoch": 0.04826100454387236,
      "grad_norm": 0.4245440363883972,
      "learning_rate": 0.00019079653361245486,
      "loss": 0.0551,
      "step": 10260
    },
    {
      "epoch": 0.04826570834548482,
      "grad_norm": 1.1540184020996094,
      "learning_rate": 0.00019079559063434138,
      "loss": 0.0897,
      "step": 10261
    },
    {
      "epoch": 0.048270412147097284,
      "grad_norm": 3.853137493133545,
      "learning_rate": 0.0001907946476562279,
      "loss": 0.6404,
      "step": 10262
    },
    {
      "epoch": 0.04827511594870975,
      "grad_norm": 0.4444599151611328,
      "learning_rate": 0.00019079370467811445,
      "loss": 0.0204,
      "step": 10263
    },
    {
      "epoch": 0.04827981975032221,
      "grad_norm": 2.7780566215515137,
      "learning_rate": 0.00019079276170000096,
      "loss": 0.4719,
      "step": 10264
    },
    {
      "epoch": 0.048284523551934674,
      "grad_norm": 2.233945369720459,
      "learning_rate": 0.00019079181872188748,
      "loss": 0.2929,
      "step": 10265
    },
    {
      "epoch": 0.04828922735354714,
      "grad_norm": 2.684931993484497,
      "learning_rate": 0.000190790875743774,
      "loss": 0.3163,
      "step": 10266
    },
    {
      "epoch": 0.0482939311551596,
      "grad_norm": 2.3703863620758057,
      "learning_rate": 0.00019078993276566052,
      "loss": 0.2418,
      "step": 10267
    },
    {
      "epoch": 0.048298634956772064,
      "grad_norm": 5.240431785583496,
      "learning_rate": 0.00019078898978754704,
      "loss": 0.5434,
      "step": 10268
    },
    {
      "epoch": 0.04830333875838452,
      "grad_norm": 3.1388907432556152,
      "learning_rate": 0.00019078804680943356,
      "loss": 0.5372,
      "step": 10269
    },
    {
      "epoch": 0.04830804255999699,
      "grad_norm": 0.5026587843894958,
      "learning_rate": 0.00019078710383132008,
      "loss": 0.0361,
      "step": 10270
    },
    {
      "epoch": 0.048312746361609454,
      "grad_norm": 7.253565311431885,
      "learning_rate": 0.0001907861608532066,
      "loss": 0.921,
      "step": 10271
    },
    {
      "epoch": 0.04831745016322191,
      "grad_norm": 1.7848527431488037,
      "learning_rate": 0.00019078521787509314,
      "loss": 0.1449,
      "step": 10272
    },
    {
      "epoch": 0.04832215396483438,
      "grad_norm": 2.4309046268463135,
      "learning_rate": 0.00019078427489697966,
      "loss": 0.3083,
      "step": 10273
    },
    {
      "epoch": 0.048326857766446844,
      "grad_norm": 0.45160412788391113,
      "learning_rate": 0.00019078333191886618,
      "loss": 0.0353,
      "step": 10274
    },
    {
      "epoch": 0.0483315615680593,
      "grad_norm": 3.1672801971435547,
      "learning_rate": 0.0001907823889407527,
      "loss": 0.362,
      "step": 10275
    },
    {
      "epoch": 0.04833626536967177,
      "grad_norm": 3.5583770275115967,
      "learning_rate": 0.00019078144596263921,
      "loss": 0.7269,
      "step": 10276
    },
    {
      "epoch": 0.048340969171284234,
      "grad_norm": 1.9537991285324097,
      "learning_rate": 0.00019078050298452573,
      "loss": 0.1466,
      "step": 10277
    },
    {
      "epoch": 0.04834567297289669,
      "grad_norm": 2.0404837131500244,
      "learning_rate": 0.00019077956000641225,
      "loss": 0.1928,
      "step": 10278
    },
    {
      "epoch": 0.04835037677450916,
      "grad_norm": 3.095651626586914,
      "learning_rate": 0.00019077861702829877,
      "loss": 0.395,
      "step": 10279
    },
    {
      "epoch": 0.048355080576121624,
      "grad_norm": 1.9104270935058594,
      "learning_rate": 0.0001907776740501853,
      "loss": 0.2882,
      "step": 10280
    },
    {
      "epoch": 0.04835978437773408,
      "grad_norm": 4.774659156799316,
      "learning_rate": 0.00019077673107207184,
      "loss": 0.8736,
      "step": 10281
    },
    {
      "epoch": 0.04836448817934655,
      "grad_norm": 0.957385241985321,
      "learning_rate": 0.00019077578809395835,
      "loss": 0.0682,
      "step": 10282
    },
    {
      "epoch": 0.048369191980959014,
      "grad_norm": 2.309443235397339,
      "learning_rate": 0.00019077484511584487,
      "loss": 0.2463,
      "step": 10283
    },
    {
      "epoch": 0.04837389578257147,
      "grad_norm": 5.379131317138672,
      "learning_rate": 0.0001907739021377314,
      "loss": 0.9184,
      "step": 10284
    },
    {
      "epoch": 0.04837859958418394,
      "grad_norm": 2.743025064468384,
      "learning_rate": 0.0001907729591596179,
      "loss": 0.6125,
      "step": 10285
    },
    {
      "epoch": 0.0483833033857964,
      "grad_norm": 2.6609158515930176,
      "learning_rate": 0.00019077201618150446,
      "loss": 0.3815,
      "step": 10286
    },
    {
      "epoch": 0.04838800718740886,
      "grad_norm": 1.7629847526550293,
      "learning_rate": 0.00019077107320339095,
      "loss": 0.3963,
      "step": 10287
    },
    {
      "epoch": 0.04839271098902133,
      "grad_norm": 3.1586825847625732,
      "learning_rate": 0.00019077013022527747,
      "loss": 0.1592,
      "step": 10288
    },
    {
      "epoch": 0.04839741479063379,
      "grad_norm": 0.98725426197052,
      "learning_rate": 0.00019076918724716398,
      "loss": 0.106,
      "step": 10289
    },
    {
      "epoch": 0.04840211859224625,
      "grad_norm": 0.7974294424057007,
      "learning_rate": 0.00019076824426905053,
      "loss": 0.0725,
      "step": 10290
    },
    {
      "epoch": 0.04840682239385872,
      "grad_norm": 2.892733573913574,
      "learning_rate": 0.00019076730129093705,
      "loss": 0.1937,
      "step": 10291
    },
    {
      "epoch": 0.04841152619547118,
      "grad_norm": 2.08327054977417,
      "learning_rate": 0.00019076635831282357,
      "loss": 0.4306,
      "step": 10292
    },
    {
      "epoch": 0.04841622999708364,
      "grad_norm": 4.078409194946289,
      "learning_rate": 0.00019076541533471009,
      "loss": 0.9348,
      "step": 10293
    },
    {
      "epoch": 0.04842093379869611,
      "grad_norm": 0.7579838633537292,
      "learning_rate": 0.0001907644723565966,
      "loss": 0.0979,
      "step": 10294
    },
    {
      "epoch": 0.04842563760030857,
      "grad_norm": 0.34519073367118835,
      "learning_rate": 0.00019076352937848315,
      "loss": 0.0469,
      "step": 10295
    },
    {
      "epoch": 0.04843034140192103,
      "grad_norm": 2.7204291820526123,
      "learning_rate": 0.00019076258640036967,
      "loss": 0.2825,
      "step": 10296
    },
    {
      "epoch": 0.0484350452035335,
      "grad_norm": 1.6771750450134277,
      "learning_rate": 0.0001907616434222562,
      "loss": 0.48,
      "step": 10297
    },
    {
      "epoch": 0.04843974900514596,
      "grad_norm": 0.8138934373855591,
      "learning_rate": 0.00019076070044414268,
      "loss": 0.2133,
      "step": 10298
    },
    {
      "epoch": 0.04844445280675842,
      "grad_norm": 2.206245183944702,
      "learning_rate": 0.00019075975746602923,
      "loss": 0.4202,
      "step": 10299
    },
    {
      "epoch": 0.04844915660837089,
      "grad_norm": 0.9814165830612183,
      "learning_rate": 0.00019075881448791574,
      "loss": 0.1956,
      "step": 10300
    },
    {
      "epoch": 0.04845386040998335,
      "grad_norm": 3.715651273727417,
      "learning_rate": 0.00019075787150980226,
      "loss": 0.7537,
      "step": 10301
    },
    {
      "epoch": 0.04845856421159581,
      "grad_norm": 1.85274338722229,
      "learning_rate": 0.00019075692853168878,
      "loss": 0.3446,
      "step": 10302
    },
    {
      "epoch": 0.04846326801320827,
      "grad_norm": 0.6366668343544006,
      "learning_rate": 0.0001907559855535753,
      "loss": 0.0846,
      "step": 10303
    },
    {
      "epoch": 0.04846797181482074,
      "grad_norm": 1.5425819158554077,
      "learning_rate": 0.00019075504257546185,
      "loss": 0.2206,
      "step": 10304
    },
    {
      "epoch": 0.0484726756164332,
      "grad_norm": 0.7355653047561646,
      "learning_rate": 0.00019075409959734836,
      "loss": 0.0565,
      "step": 10305
    },
    {
      "epoch": 0.04847737941804566,
      "grad_norm": 3.0789802074432373,
      "learning_rate": 0.00019075315661923488,
      "loss": 0.553,
      "step": 10306
    },
    {
      "epoch": 0.04848208321965813,
      "grad_norm": 3.9363443851470947,
      "learning_rate": 0.0001907522136411214,
      "loss": 0.413,
      "step": 10307
    },
    {
      "epoch": 0.04848678702127059,
      "grad_norm": 0.934881865978241,
      "learning_rate": 0.00019075127066300792,
      "loss": 0.0985,
      "step": 10308
    },
    {
      "epoch": 0.04849149082288305,
      "grad_norm": 2.3881006240844727,
      "learning_rate": 0.00019075032768489444,
      "loss": 0.3916,
      "step": 10309
    },
    {
      "epoch": 0.04849619462449552,
      "grad_norm": 3.699730396270752,
      "learning_rate": 0.00019074938470678096,
      "loss": 0.3817,
      "step": 10310
    },
    {
      "epoch": 0.04850089842610798,
      "grad_norm": 0.9631365537643433,
      "learning_rate": 0.00019074844172866748,
      "loss": 0.0815,
      "step": 10311
    },
    {
      "epoch": 0.04850560222772044,
      "grad_norm": 0.731005072593689,
      "learning_rate": 0.000190747498750554,
      "loss": 0.1489,
      "step": 10312
    },
    {
      "epoch": 0.04851030602933291,
      "grad_norm": 1.0688941478729248,
      "learning_rate": 0.00019074655577244054,
      "loss": 0.1546,
      "step": 10313
    },
    {
      "epoch": 0.04851500983094537,
      "grad_norm": 0.7527912855148315,
      "learning_rate": 0.00019074561279432706,
      "loss": 0.0844,
      "step": 10314
    },
    {
      "epoch": 0.04851971363255783,
      "grad_norm": 1.1647270917892456,
      "learning_rate": 0.00019074466981621358,
      "loss": 0.1691,
      "step": 10315
    },
    {
      "epoch": 0.0485244174341703,
      "grad_norm": 1.3919726610183716,
      "learning_rate": 0.0001907437268381001,
      "loss": 0.1674,
      "step": 10316
    },
    {
      "epoch": 0.04852912123578276,
      "grad_norm": 1.5094141960144043,
      "learning_rate": 0.00019074278385998664,
      "loss": 0.4382,
      "step": 10317
    },
    {
      "epoch": 0.04853382503739522,
      "grad_norm": 1.7455024719238281,
      "learning_rate": 0.00019074184088187313,
      "loss": 0.2477,
      "step": 10318
    },
    {
      "epoch": 0.04853852883900769,
      "grad_norm": 1.7876527309417725,
      "learning_rate": 0.00019074089790375965,
      "loss": 0.1829,
      "step": 10319
    },
    {
      "epoch": 0.048543232640620146,
      "grad_norm": 2.5468451976776123,
      "learning_rate": 0.00019073995492564617,
      "loss": 0.3568,
      "step": 10320
    },
    {
      "epoch": 0.04854793644223261,
      "grad_norm": 1.7954798936843872,
      "learning_rate": 0.0001907390119475327,
      "loss": 0.3127,
      "step": 10321
    },
    {
      "epoch": 0.04855264024384508,
      "grad_norm": 2.8881139755249023,
      "learning_rate": 0.00019073806896941924,
      "loss": 0.3162,
      "step": 10322
    },
    {
      "epoch": 0.048557344045457536,
      "grad_norm": 2.3956124782562256,
      "learning_rate": 0.00019073712599130575,
      "loss": 0.4535,
      "step": 10323
    },
    {
      "epoch": 0.04856204784707,
      "grad_norm": 1.0310068130493164,
      "learning_rate": 0.00019073618301319227,
      "loss": 0.0948,
      "step": 10324
    },
    {
      "epoch": 0.04856675164868247,
      "grad_norm": 0.43953824043273926,
      "learning_rate": 0.0001907352400350788,
      "loss": 0.0353,
      "step": 10325
    },
    {
      "epoch": 0.048571455450294926,
      "grad_norm": 0.987102746963501,
      "learning_rate": 0.00019073429705696534,
      "loss": 0.1363,
      "step": 10326
    },
    {
      "epoch": 0.04857615925190739,
      "grad_norm": 1.9486088752746582,
      "learning_rate": 0.00019073335407885186,
      "loss": 0.1511,
      "step": 10327
    },
    {
      "epoch": 0.04858086305351986,
      "grad_norm": 3.108818769454956,
      "learning_rate": 0.00019073241110073837,
      "loss": 0.4675,
      "step": 10328
    },
    {
      "epoch": 0.048585566855132316,
      "grad_norm": 1.14418363571167,
      "learning_rate": 0.00019073146812262487,
      "loss": 0.1038,
      "step": 10329
    },
    {
      "epoch": 0.04859027065674478,
      "grad_norm": 1.0235956907272339,
      "learning_rate": 0.00019073052514451138,
      "loss": 0.0655,
      "step": 10330
    },
    {
      "epoch": 0.04859497445835725,
      "grad_norm": 0.880593478679657,
      "learning_rate": 0.00019072958216639793,
      "loss": 0.0709,
      "step": 10331
    },
    {
      "epoch": 0.048599678259969706,
      "grad_norm": 2.761315107345581,
      "learning_rate": 0.00019072863918828445,
      "loss": 0.2468,
      "step": 10332
    },
    {
      "epoch": 0.04860438206158217,
      "grad_norm": 2.795156955718994,
      "learning_rate": 0.00019072769621017097,
      "loss": 0.4791,
      "step": 10333
    },
    {
      "epoch": 0.04860908586319464,
      "grad_norm": 0.10042225569486618,
      "learning_rate": 0.00019072675323205749,
      "loss": 0.0062,
      "step": 10334
    },
    {
      "epoch": 0.048613789664807096,
      "grad_norm": 3.454357147216797,
      "learning_rate": 0.00019072581025394403,
      "loss": 0.5858,
      "step": 10335
    },
    {
      "epoch": 0.04861849346641956,
      "grad_norm": 2.591157913208008,
      "learning_rate": 0.00019072486727583055,
      "loss": 0.2089,
      "step": 10336
    },
    {
      "epoch": 0.04862319726803202,
      "grad_norm": 0.8657382130622864,
      "learning_rate": 0.00019072392429771707,
      "loss": 0.0881,
      "step": 10337
    },
    {
      "epoch": 0.048627901069644486,
      "grad_norm": 0.7759364247322083,
      "learning_rate": 0.0001907229813196036,
      "loss": 0.0922,
      "step": 10338
    },
    {
      "epoch": 0.04863260487125695,
      "grad_norm": 0.37540748715400696,
      "learning_rate": 0.0001907220383414901,
      "loss": 0.0261,
      "step": 10339
    },
    {
      "epoch": 0.04863730867286941,
      "grad_norm": 0.08760980516672134,
      "learning_rate": 0.00019072109536337663,
      "loss": 0.0056,
      "step": 10340
    },
    {
      "epoch": 0.048642012474481876,
      "grad_norm": 0.3151325583457947,
      "learning_rate": 0.00019072015238526314,
      "loss": 0.0229,
      "step": 10341
    },
    {
      "epoch": 0.04864671627609434,
      "grad_norm": 0.2954939901828766,
      "learning_rate": 0.00019071920940714966,
      "loss": 0.0213,
      "step": 10342
    },
    {
      "epoch": 0.0486514200777068,
      "grad_norm": 2.4589855670928955,
      "learning_rate": 0.00019071826642903618,
      "loss": 0.2572,
      "step": 10343
    },
    {
      "epoch": 0.048656123879319266,
      "grad_norm": 4.13266134262085,
      "learning_rate": 0.0001907173234509227,
      "loss": 0.6076,
      "step": 10344
    },
    {
      "epoch": 0.04866082768093173,
      "grad_norm": 4.208945274353027,
      "learning_rate": 0.00019071638047280925,
      "loss": 0.7028,
      "step": 10345
    },
    {
      "epoch": 0.04866553148254419,
      "grad_norm": 2.81967830657959,
      "learning_rate": 0.00019071543749469576,
      "loss": 0.5939,
      "step": 10346
    },
    {
      "epoch": 0.048670235284156656,
      "grad_norm": 1.6615806818008423,
      "learning_rate": 0.00019071449451658228,
      "loss": 0.3036,
      "step": 10347
    },
    {
      "epoch": 0.04867493908576912,
      "grad_norm": 2.91361141204834,
      "learning_rate": 0.0001907135515384688,
      "loss": 0.2535,
      "step": 10348
    },
    {
      "epoch": 0.04867964288738158,
      "grad_norm": 4.198248386383057,
      "learning_rate": 0.00019071260856035532,
      "loss": 0.8054,
      "step": 10349
    },
    {
      "epoch": 0.048684346688994046,
      "grad_norm": 0.6654871106147766,
      "learning_rate": 0.00019071166558224184,
      "loss": 0.0576,
      "step": 10350
    },
    {
      "epoch": 0.04868905049060651,
      "grad_norm": 0.5576784610748291,
      "learning_rate": 0.00019071072260412836,
      "loss": 0.0437,
      "step": 10351
    },
    {
      "epoch": 0.04869375429221897,
      "grad_norm": 3.68591570854187,
      "learning_rate": 0.00019070977962601488,
      "loss": 0.5222,
      "step": 10352
    },
    {
      "epoch": 0.048698458093831436,
      "grad_norm": 4.7948102951049805,
      "learning_rate": 0.0001907088366479014,
      "loss": 0.6271,
      "step": 10353
    },
    {
      "epoch": 0.048703161895443894,
      "grad_norm": 0.7521795034408569,
      "learning_rate": 0.00019070789366978794,
      "loss": 0.0768,
      "step": 10354
    },
    {
      "epoch": 0.04870786569705636,
      "grad_norm": 1.9983012676239014,
      "learning_rate": 0.00019070695069167446,
      "loss": 0.1809,
      "step": 10355
    },
    {
      "epoch": 0.048712569498668826,
      "grad_norm": 7.003011703491211,
      "learning_rate": 0.00019070600771356098,
      "loss": 0.7016,
      "step": 10356
    },
    {
      "epoch": 0.048717273300281284,
      "grad_norm": 2.0210793018341064,
      "learning_rate": 0.0001907050647354475,
      "loss": 0.1226,
      "step": 10357
    },
    {
      "epoch": 0.04872197710189375,
      "grad_norm": 2.0947022438049316,
      "learning_rate": 0.00019070412175733404,
      "loss": 0.2136,
      "step": 10358
    },
    {
      "epoch": 0.048726680903506216,
      "grad_norm": 3.77880597114563,
      "learning_rate": 0.00019070317877922056,
      "loss": 0.4811,
      "step": 10359
    },
    {
      "epoch": 0.048731384705118674,
      "grad_norm": 1.5379648208618164,
      "learning_rate": 0.00019070223580110705,
      "loss": 0.112,
      "step": 10360
    },
    {
      "epoch": 0.04873608850673114,
      "grad_norm": 2.356889486312866,
      "learning_rate": 0.00019070129282299357,
      "loss": 0.4432,
      "step": 10361
    },
    {
      "epoch": 0.048740792308343606,
      "grad_norm": 0.6123073101043701,
      "learning_rate": 0.0001907003498448801,
      "loss": 0.0637,
      "step": 10362
    },
    {
      "epoch": 0.048745496109956064,
      "grad_norm": 5.018322944641113,
      "learning_rate": 0.00019069940686676664,
      "loss": 0.8517,
      "step": 10363
    },
    {
      "epoch": 0.04875019991156853,
      "grad_norm": 0.9867376089096069,
      "learning_rate": 0.00019069846388865315,
      "loss": 0.0867,
      "step": 10364
    },
    {
      "epoch": 0.048754903713180996,
      "grad_norm": 4.445859432220459,
      "learning_rate": 0.00019069752091053967,
      "loss": 0.5405,
      "step": 10365
    },
    {
      "epoch": 0.048759607514793454,
      "grad_norm": 1.3930294513702393,
      "learning_rate": 0.0001906965779324262,
      "loss": 0.1211,
      "step": 10366
    },
    {
      "epoch": 0.04876431131640592,
      "grad_norm": 3.1823952198028564,
      "learning_rate": 0.00019069563495431274,
      "loss": 0.4379,
      "step": 10367
    },
    {
      "epoch": 0.048769015118018386,
      "grad_norm": 1.7238290309906006,
      "learning_rate": 0.00019069469197619926,
      "loss": 0.2528,
      "step": 10368
    },
    {
      "epoch": 0.048773718919630844,
      "grad_norm": 3.740009069442749,
      "learning_rate": 0.00019069374899808577,
      "loss": 0.4949,
      "step": 10369
    },
    {
      "epoch": 0.04877842272124331,
      "grad_norm": 2.1961143016815186,
      "learning_rate": 0.0001906928060199723,
      "loss": 0.4313,
      "step": 10370
    },
    {
      "epoch": 0.04878312652285577,
      "grad_norm": 2.6006557941436768,
      "learning_rate": 0.0001906918630418588,
      "loss": 0.4508,
      "step": 10371
    },
    {
      "epoch": 0.048787830324468234,
      "grad_norm": 0.39243239164352417,
      "learning_rate": 0.00019069092006374533,
      "loss": 0.0401,
      "step": 10372
    },
    {
      "epoch": 0.0487925341260807,
      "grad_norm": 1.5042859315872192,
      "learning_rate": 0.00019068997708563185,
      "loss": 0.165,
      "step": 10373
    },
    {
      "epoch": 0.04879723792769316,
      "grad_norm": 1.296584129333496,
      "learning_rate": 0.00019068903410751837,
      "loss": 0.2212,
      "step": 10374
    },
    {
      "epoch": 0.048801941729305624,
      "grad_norm": 4.315062522888184,
      "learning_rate": 0.00019068809112940489,
      "loss": 0.6721,
      "step": 10375
    },
    {
      "epoch": 0.04880664553091809,
      "grad_norm": 1.7326000928878784,
      "learning_rate": 0.00019068714815129143,
      "loss": 0.1318,
      "step": 10376
    },
    {
      "epoch": 0.04881134933253055,
      "grad_norm": 2.109229326248169,
      "learning_rate": 0.00019068620517317795,
      "loss": 0.2432,
      "step": 10377
    },
    {
      "epoch": 0.048816053134143014,
      "grad_norm": 2.565180540084839,
      "learning_rate": 0.00019068526219506447,
      "loss": 0.234,
      "step": 10378
    },
    {
      "epoch": 0.04882075693575548,
      "grad_norm": 1.411707878112793,
      "learning_rate": 0.000190684319216951,
      "loss": 0.1601,
      "step": 10379
    },
    {
      "epoch": 0.04882546073736794,
      "grad_norm": 1.197697639465332,
      "learning_rate": 0.0001906833762388375,
      "loss": 0.1369,
      "step": 10380
    },
    {
      "epoch": 0.048830164538980404,
      "grad_norm": 0.4665117859840393,
      "learning_rate": 0.00019068243326072402,
      "loss": 0.0486,
      "step": 10381
    },
    {
      "epoch": 0.04883486834059287,
      "grad_norm": 0.46361130475997925,
      "learning_rate": 0.00019068149028261054,
      "loss": 0.0365,
      "step": 10382
    },
    {
      "epoch": 0.04883957214220533,
      "grad_norm": 0.8603757619857788,
      "learning_rate": 0.00019068054730449706,
      "loss": 0.0742,
      "step": 10383
    },
    {
      "epoch": 0.048844275943817794,
      "grad_norm": 1.3358052968978882,
      "learning_rate": 0.00019067960432638358,
      "loss": 0.1042,
      "step": 10384
    },
    {
      "epoch": 0.04884897974543026,
      "grad_norm": 1.0010939836502075,
      "learning_rate": 0.00019067866134827013,
      "loss": 0.1591,
      "step": 10385
    },
    {
      "epoch": 0.04885368354704272,
      "grad_norm": 2.065066337585449,
      "learning_rate": 0.00019067771837015665,
      "loss": 0.2991,
      "step": 10386
    },
    {
      "epoch": 0.048858387348655184,
      "grad_norm": 2.8789336681365967,
      "learning_rate": 0.00019067677539204316,
      "loss": 0.3286,
      "step": 10387
    },
    {
      "epoch": 0.04886309115026764,
      "grad_norm": 4.220363616943359,
      "learning_rate": 0.00019067583241392968,
      "loss": 0.2572,
      "step": 10388
    },
    {
      "epoch": 0.04886779495188011,
      "grad_norm": 2.7998552322387695,
      "learning_rate": 0.0001906748894358162,
      "loss": 0.469,
      "step": 10389
    },
    {
      "epoch": 0.048872498753492574,
      "grad_norm": 2.142003297805786,
      "learning_rate": 0.00019067394645770275,
      "loss": 0.2948,
      "step": 10390
    },
    {
      "epoch": 0.04887720255510503,
      "grad_norm": 1.9804930686950684,
      "learning_rate": 0.00019067300347958924,
      "loss": 0.4955,
      "step": 10391
    },
    {
      "epoch": 0.0488819063567175,
      "grad_norm": 0.49726423621177673,
      "learning_rate": 0.00019067206050147576,
      "loss": 0.0389,
      "step": 10392
    },
    {
      "epoch": 0.048886610158329964,
      "grad_norm": 1.8773096799850464,
      "learning_rate": 0.00019067111752336228,
      "loss": 0.4277,
      "step": 10393
    },
    {
      "epoch": 0.04889131395994242,
      "grad_norm": 0.3235030174255371,
      "learning_rate": 0.0001906701745452488,
      "loss": 0.0242,
      "step": 10394
    },
    {
      "epoch": 0.04889601776155489,
      "grad_norm": 1.5217361450195312,
      "learning_rate": 0.00019066923156713534,
      "loss": 0.147,
      "step": 10395
    },
    {
      "epoch": 0.048900721563167354,
      "grad_norm": 3.2483978271484375,
      "learning_rate": 0.00019066828858902186,
      "loss": 0.28,
      "step": 10396
    },
    {
      "epoch": 0.04890542536477981,
      "grad_norm": 3.1277830600738525,
      "learning_rate": 0.00019066734561090838,
      "loss": 0.699,
      "step": 10397
    },
    {
      "epoch": 0.04891012916639228,
      "grad_norm": 0.8699893355369568,
      "learning_rate": 0.0001906664026327949,
      "loss": 0.0661,
      "step": 10398
    },
    {
      "epoch": 0.048914832968004744,
      "grad_norm": 0.6270025372505188,
      "learning_rate": 0.00019066545965468144,
      "loss": 0.0589,
      "step": 10399
    },
    {
      "epoch": 0.0489195367696172,
      "grad_norm": 1.404621958732605,
      "learning_rate": 0.00019066451667656796,
      "loss": 0.1277,
      "step": 10400
    },
    {
      "epoch": 0.04892424057122967,
      "grad_norm": 5.175647258758545,
      "learning_rate": 0.00019066357369845448,
      "loss": 0.2475,
      "step": 10401
    },
    {
      "epoch": 0.048928944372842134,
      "grad_norm": 3.044804573059082,
      "learning_rate": 0.000190662630720341,
      "loss": 0.2244,
      "step": 10402
    },
    {
      "epoch": 0.04893364817445459,
      "grad_norm": 4.839712619781494,
      "learning_rate": 0.0001906616877422275,
      "loss": 0.3306,
      "step": 10403
    },
    {
      "epoch": 0.04893835197606706,
      "grad_norm": 2.3567984104156494,
      "learning_rate": 0.00019066074476411404,
      "loss": 0.1742,
      "step": 10404
    },
    {
      "epoch": 0.04894305577767952,
      "grad_norm": 4.6775312423706055,
      "learning_rate": 0.00019065980178600055,
      "loss": 0.1313,
      "step": 10405
    },
    {
      "epoch": 0.04894775957929198,
      "grad_norm": 4.245616912841797,
      "learning_rate": 0.00019065885880788707,
      "loss": 0.8744,
      "step": 10406
    },
    {
      "epoch": 0.04895246338090445,
      "grad_norm": 6.846833229064941,
      "learning_rate": 0.0001906579158297736,
      "loss": 0.7172,
      "step": 10407
    },
    {
      "epoch": 0.04895716718251691,
      "grad_norm": 3.6447343826293945,
      "learning_rate": 0.00019065697285166014,
      "loss": 0.4432,
      "step": 10408
    },
    {
      "epoch": 0.04896187098412937,
      "grad_norm": 2.4717748165130615,
      "learning_rate": 0.00019065602987354666,
      "loss": 0.4039,
      "step": 10409
    },
    {
      "epoch": 0.04896657478574184,
      "grad_norm": 2.5333092212677,
      "learning_rate": 0.00019065508689543317,
      "loss": 0.384,
      "step": 10410
    },
    {
      "epoch": 0.0489712785873543,
      "grad_norm": 7.1858320236206055,
      "learning_rate": 0.0001906541439173197,
      "loss": 0.4669,
      "step": 10411
    },
    {
      "epoch": 0.04897598238896676,
      "grad_norm": 1.8499152660369873,
      "learning_rate": 0.0001906532009392062,
      "loss": 0.2938,
      "step": 10412
    },
    {
      "epoch": 0.04898068619057923,
      "grad_norm": 1.6354840993881226,
      "learning_rate": 0.00019065225796109273,
      "loss": 0.1849,
      "step": 10413
    },
    {
      "epoch": 0.04898538999219169,
      "grad_norm": 2.847060203552246,
      "learning_rate": 0.00019065131498297925,
      "loss": 0.4317,
      "step": 10414
    },
    {
      "epoch": 0.04899009379380415,
      "grad_norm": 2.558044910430908,
      "learning_rate": 0.00019065037200486577,
      "loss": 0.5597,
      "step": 10415
    },
    {
      "epoch": 0.04899479759541662,
      "grad_norm": 2.377795457839966,
      "learning_rate": 0.00019064942902675229,
      "loss": 0.4105,
      "step": 10416
    },
    {
      "epoch": 0.04899950139702908,
      "grad_norm": 3.5423078536987305,
      "learning_rate": 0.00019064848604863883,
      "loss": 0.574,
      "step": 10417
    },
    {
      "epoch": 0.04900420519864154,
      "grad_norm": 0.96797114610672,
      "learning_rate": 0.00019064754307052535,
      "loss": 0.1197,
      "step": 10418
    },
    {
      "epoch": 0.04900890900025401,
      "grad_norm": 0.7296010851860046,
      "learning_rate": 0.00019064660009241187,
      "loss": 0.0699,
      "step": 10419
    },
    {
      "epoch": 0.04901361280186647,
      "grad_norm": 2.1923904418945312,
      "learning_rate": 0.0001906456571142984,
      "loss": 0.1877,
      "step": 10420
    },
    {
      "epoch": 0.04901831660347893,
      "grad_norm": 3.9943699836730957,
      "learning_rate": 0.0001906447141361849,
      "loss": 0.5179,
      "step": 10421
    },
    {
      "epoch": 0.04902302040509139,
      "grad_norm": 2.4032394886016846,
      "learning_rate": 0.00019064377115807142,
      "loss": 0.3773,
      "step": 10422
    },
    {
      "epoch": 0.04902772420670386,
      "grad_norm": 1.2627452611923218,
      "learning_rate": 0.00019064282817995794,
      "loss": 0.0897,
      "step": 10423
    },
    {
      "epoch": 0.04903242800831632,
      "grad_norm": 1.5872997045516968,
      "learning_rate": 0.00019064188520184446,
      "loss": 0.1941,
      "step": 10424
    },
    {
      "epoch": 0.04903713180992878,
      "grad_norm": 1.2706137895584106,
      "learning_rate": 0.00019064094222373098,
      "loss": 0.2266,
      "step": 10425
    },
    {
      "epoch": 0.04904183561154125,
      "grad_norm": 0.9017341136932373,
      "learning_rate": 0.00019063999924561753,
      "loss": 0.1205,
      "step": 10426
    },
    {
      "epoch": 0.04904653941315371,
      "grad_norm": 2.1561317443847656,
      "learning_rate": 0.00019063905626750405,
      "loss": 0.3941,
      "step": 10427
    },
    {
      "epoch": 0.04905124321476617,
      "grad_norm": 1.4925438165664673,
      "learning_rate": 0.00019063811328939056,
      "loss": 0.1525,
      "step": 10428
    },
    {
      "epoch": 0.04905594701637864,
      "grad_norm": 0.8312084674835205,
      "learning_rate": 0.00019063717031127708,
      "loss": 0.1074,
      "step": 10429
    },
    {
      "epoch": 0.0490606508179911,
      "grad_norm": 0.7286939024925232,
      "learning_rate": 0.0001906362273331636,
      "loss": 0.0784,
      "step": 10430
    },
    {
      "epoch": 0.04906535461960356,
      "grad_norm": 1.0184093713760376,
      "learning_rate": 0.00019063528435505015,
      "loss": 0.1229,
      "step": 10431
    },
    {
      "epoch": 0.04907005842121603,
      "grad_norm": 0.43423449993133545,
      "learning_rate": 0.00019063434137693667,
      "loss": 0.0643,
      "step": 10432
    },
    {
      "epoch": 0.04907476222282849,
      "grad_norm": 1.3327288627624512,
      "learning_rate": 0.00019063339839882318,
      "loss": 0.3644,
      "step": 10433
    },
    {
      "epoch": 0.04907946602444095,
      "grad_norm": 1.7551325559616089,
      "learning_rate": 0.00019063245542070968,
      "loss": 0.0849,
      "step": 10434
    },
    {
      "epoch": 0.04908416982605342,
      "grad_norm": 0.6724776029586792,
      "learning_rate": 0.00019063151244259622,
      "loss": 0.092,
      "step": 10435
    },
    {
      "epoch": 0.04908887362766588,
      "grad_norm": 0.24574345350265503,
      "learning_rate": 0.00019063056946448274,
      "loss": 0.0198,
      "step": 10436
    },
    {
      "epoch": 0.04909357742927834,
      "grad_norm": 1.3178577423095703,
      "learning_rate": 0.00019062962648636926,
      "loss": 0.121,
      "step": 10437
    },
    {
      "epoch": 0.04909828123089081,
      "grad_norm": 1.8467880487442017,
      "learning_rate": 0.00019062868350825578,
      "loss": 0.2022,
      "step": 10438
    },
    {
      "epoch": 0.049102985032503266,
      "grad_norm": 5.793025493621826,
      "learning_rate": 0.0001906277405301423,
      "loss": 0.3443,
      "step": 10439
    },
    {
      "epoch": 0.04910768883411573,
      "grad_norm": 0.7737938761711121,
      "learning_rate": 0.00019062679755202884,
      "loss": 0.0572,
      "step": 10440
    },
    {
      "epoch": 0.0491123926357282,
      "grad_norm": 0.775274932384491,
      "learning_rate": 0.00019062585457391536,
      "loss": 0.0783,
      "step": 10441
    },
    {
      "epoch": 0.049117096437340656,
      "grad_norm": 7.8845906257629395,
      "learning_rate": 0.00019062491159580188,
      "loss": 0.2916,
      "step": 10442
    },
    {
      "epoch": 0.04912180023895312,
      "grad_norm": 1.2300560474395752,
      "learning_rate": 0.0001906239686176884,
      "loss": 0.127,
      "step": 10443
    },
    {
      "epoch": 0.04912650404056559,
      "grad_norm": 2.5902023315429688,
      "learning_rate": 0.00019062302563957492,
      "loss": 0.1209,
      "step": 10444
    },
    {
      "epoch": 0.049131207842178046,
      "grad_norm": 4.117790222167969,
      "learning_rate": 0.00019062208266146144,
      "loss": 0.7247,
      "step": 10445
    },
    {
      "epoch": 0.04913591164379051,
      "grad_norm": 1.8945960998535156,
      "learning_rate": 0.00019062113968334795,
      "loss": 0.5414,
      "step": 10446
    },
    {
      "epoch": 0.04914061544540298,
      "grad_norm": 2.1898109912872314,
      "learning_rate": 0.00019062019670523447,
      "loss": 0.4736,
      "step": 10447
    },
    {
      "epoch": 0.049145319247015436,
      "grad_norm": 7.94690465927124,
      "learning_rate": 0.000190619253727121,
      "loss": 0.9819,
      "step": 10448
    },
    {
      "epoch": 0.0491500230486279,
      "grad_norm": 1.9062113761901855,
      "learning_rate": 0.00019061831074900754,
      "loss": 0.2152,
      "step": 10449
    },
    {
      "epoch": 0.04915472685024037,
      "grad_norm": 0.44780808687210083,
      "learning_rate": 0.00019061736777089406,
      "loss": 0.032,
      "step": 10450
    },
    {
      "epoch": 0.049159430651852826,
      "grad_norm": 7.0383524894714355,
      "learning_rate": 0.00019061642479278057,
      "loss": 0.5294,
      "step": 10451
    },
    {
      "epoch": 0.04916413445346529,
      "grad_norm": 5.356876373291016,
      "learning_rate": 0.0001906154818146671,
      "loss": 0.7716,
      "step": 10452
    },
    {
      "epoch": 0.04916883825507776,
      "grad_norm": 0.6968196034431458,
      "learning_rate": 0.0001906145388365536,
      "loss": 0.0361,
      "step": 10453
    },
    {
      "epoch": 0.049173542056690216,
      "grad_norm": 3.108574628829956,
      "learning_rate": 0.00019061359585844013,
      "loss": 0.6696,
      "step": 10454
    },
    {
      "epoch": 0.04917824585830268,
      "grad_norm": 2.077449321746826,
      "learning_rate": 0.00019061265288032665,
      "loss": 0.1261,
      "step": 10455
    },
    {
      "epoch": 0.04918294965991514,
      "grad_norm": 1.6957653760910034,
      "learning_rate": 0.00019061170990221317,
      "loss": 0.1193,
      "step": 10456
    },
    {
      "epoch": 0.049187653461527606,
      "grad_norm": 0.5321872234344482,
      "learning_rate": 0.00019061076692409969,
      "loss": 0.0404,
      "step": 10457
    },
    {
      "epoch": 0.04919235726314007,
      "grad_norm": 6.3559370040893555,
      "learning_rate": 0.00019060982394598623,
      "loss": 0.8714,
      "step": 10458
    },
    {
      "epoch": 0.04919706106475253,
      "grad_norm": 1.9765174388885498,
      "learning_rate": 0.00019060888096787275,
      "loss": 0.2063,
      "step": 10459
    },
    {
      "epoch": 0.049201764866364996,
      "grad_norm": 4.443111896514893,
      "learning_rate": 0.00019060793798975927,
      "loss": 0.2031,
      "step": 10460
    },
    {
      "epoch": 0.04920646866797746,
      "grad_norm": 5.866309642791748,
      "learning_rate": 0.0001906069950116458,
      "loss": 1.157,
      "step": 10461
    },
    {
      "epoch": 0.04921117246958992,
      "grad_norm": 1.2181308269500732,
      "learning_rate": 0.0001906060520335323,
      "loss": 0.1181,
      "step": 10462
    },
    {
      "epoch": 0.049215876271202386,
      "grad_norm": 1.6343140602111816,
      "learning_rate": 0.00019060510905541885,
      "loss": 0.2176,
      "step": 10463
    },
    {
      "epoch": 0.04922058007281485,
      "grad_norm": 1.8321430683135986,
      "learning_rate": 0.00019060416607730537,
      "loss": 0.3115,
      "step": 10464
    },
    {
      "epoch": 0.04922528387442731,
      "grad_norm": 2.664372444152832,
      "learning_rate": 0.00019060322309919186,
      "loss": 0.1852,
      "step": 10465
    },
    {
      "epoch": 0.049229987676039776,
      "grad_norm": 0.011955348774790764,
      "learning_rate": 0.00019060228012107838,
      "loss": 0.0006,
      "step": 10466
    },
    {
      "epoch": 0.04923469147765224,
      "grad_norm": 4.101175308227539,
      "learning_rate": 0.00019060133714296493,
      "loss": 0.5439,
      "step": 10467
    },
    {
      "epoch": 0.0492393952792647,
      "grad_norm": 1.6526591777801514,
      "learning_rate": 0.00019060039416485145,
      "loss": 0.151,
      "step": 10468
    },
    {
      "epoch": 0.049244099080877166,
      "grad_norm": 1.639039158821106,
      "learning_rate": 0.00019059945118673796,
      "loss": 0.1757,
      "step": 10469
    },
    {
      "epoch": 0.04924880288248963,
      "grad_norm": 2.0769946575164795,
      "learning_rate": 0.00019059850820862448,
      "loss": 0.1311,
      "step": 10470
    },
    {
      "epoch": 0.04925350668410209,
      "grad_norm": 2.3479268550872803,
      "learning_rate": 0.000190597565230511,
      "loss": 0.2785,
      "step": 10471
    },
    {
      "epoch": 0.049258210485714556,
      "grad_norm": 2.707737922668457,
      "learning_rate": 0.00019059662225239755,
      "loss": 0.2698,
      "step": 10472
    },
    {
      "epoch": 0.049262914287327014,
      "grad_norm": 0.40060558915138245,
      "learning_rate": 0.00019059567927428407,
      "loss": 0.0324,
      "step": 10473
    },
    {
      "epoch": 0.04926761808893948,
      "grad_norm": 1.3920776844024658,
      "learning_rate": 0.00019059473629617058,
      "loss": 0.1287,
      "step": 10474
    },
    {
      "epoch": 0.049272321890551946,
      "grad_norm": 1.0636591911315918,
      "learning_rate": 0.0001905937933180571,
      "loss": 0.0865,
      "step": 10475
    },
    {
      "epoch": 0.049277025692164404,
      "grad_norm": 2.035576820373535,
      "learning_rate": 0.00019059285033994362,
      "loss": 0.2371,
      "step": 10476
    },
    {
      "epoch": 0.04928172949377687,
      "grad_norm": 1.0650033950805664,
      "learning_rate": 0.00019059190736183014,
      "loss": 0.0667,
      "step": 10477
    },
    {
      "epoch": 0.049286433295389336,
      "grad_norm": 1.2996107339859009,
      "learning_rate": 0.00019059096438371666,
      "loss": 0.0949,
      "step": 10478
    },
    {
      "epoch": 0.049291137097001794,
      "grad_norm": 3.956393003463745,
      "learning_rate": 0.00019059002140560318,
      "loss": 0.8215,
      "step": 10479
    },
    {
      "epoch": 0.04929584089861426,
      "grad_norm": 1.1848441362380981,
      "learning_rate": 0.0001905890784274897,
      "loss": 0.0865,
      "step": 10480
    },
    {
      "epoch": 0.049300544700226726,
      "grad_norm": 0.28754714131355286,
      "learning_rate": 0.00019058813544937624,
      "loss": 0.0239,
      "step": 10481
    },
    {
      "epoch": 0.049305248501839184,
      "grad_norm": 2.031156063079834,
      "learning_rate": 0.00019058719247126276,
      "loss": 0.1639,
      "step": 10482
    },
    {
      "epoch": 0.04930995230345165,
      "grad_norm": 6.742217540740967,
      "learning_rate": 0.00019058624949314928,
      "loss": 0.551,
      "step": 10483
    },
    {
      "epoch": 0.049314656105064116,
      "grad_norm": 2.699917793273926,
      "learning_rate": 0.0001905853065150358,
      "loss": 0.2676,
      "step": 10484
    },
    {
      "epoch": 0.049319359906676574,
      "grad_norm": 4.962220668792725,
      "learning_rate": 0.00019058436353692232,
      "loss": 0.6212,
      "step": 10485
    },
    {
      "epoch": 0.04932406370828904,
      "grad_norm": 4.4601922035217285,
      "learning_rate": 0.00019058342055880884,
      "loss": 0.244,
      "step": 10486
    },
    {
      "epoch": 0.049328767509901505,
      "grad_norm": 0.6345421671867371,
      "learning_rate": 0.00019058247758069535,
      "loss": 0.0228,
      "step": 10487
    },
    {
      "epoch": 0.049333471311513964,
      "grad_norm": 0.12130489945411682,
      "learning_rate": 0.00019058153460258187,
      "loss": 0.0077,
      "step": 10488
    },
    {
      "epoch": 0.04933817511312643,
      "grad_norm": 3.924344301223755,
      "learning_rate": 0.0001905805916244684,
      "loss": 0.4726,
      "step": 10489
    },
    {
      "epoch": 0.04934287891473889,
      "grad_norm": 16.691356658935547,
      "learning_rate": 0.00019057964864635494,
      "loss": 0.2972,
      "step": 10490
    },
    {
      "epoch": 0.049347582716351354,
      "grad_norm": 6.077662944793701,
      "learning_rate": 0.00019057870566824146,
      "loss": 0.2607,
      "step": 10491
    },
    {
      "epoch": 0.04935228651796382,
      "grad_norm": 0.3440605103969574,
      "learning_rate": 0.00019057776269012797,
      "loss": 0.0224,
      "step": 10492
    },
    {
      "epoch": 0.04935699031957628,
      "grad_norm": 1.4029910564422607,
      "learning_rate": 0.0001905768197120145,
      "loss": 0.1369,
      "step": 10493
    },
    {
      "epoch": 0.049361694121188744,
      "grad_norm": 0.6120491623878479,
      "learning_rate": 0.000190575876733901,
      "loss": 0.0359,
      "step": 10494
    },
    {
      "epoch": 0.04936639792280121,
      "grad_norm": 5.298816204071045,
      "learning_rate": 0.00019057493375578756,
      "loss": 0.4907,
      "step": 10495
    },
    {
      "epoch": 0.04937110172441367,
      "grad_norm": 1.50824773311615,
      "learning_rate": 0.00019057399077767405,
      "loss": 0.2277,
      "step": 10496
    },
    {
      "epoch": 0.049375805526026134,
      "grad_norm": 7.381725788116455,
      "learning_rate": 0.00019057304779956057,
      "loss": 1.2148,
      "step": 10497
    },
    {
      "epoch": 0.0493805093276386,
      "grad_norm": 0.34161806106567383,
      "learning_rate": 0.00019057210482144709,
      "loss": 0.0227,
      "step": 10498
    },
    {
      "epoch": 0.04938521312925106,
      "grad_norm": 3.059257745742798,
      "learning_rate": 0.00019057116184333363,
      "loss": 0.8199,
      "step": 10499
    },
    {
      "epoch": 0.049389916930863524,
      "grad_norm": 6.116018295288086,
      "learning_rate": 0.00019057021886522015,
      "loss": 0.2423,
      "step": 10500
    },
    {
      "epoch": 0.04939462073247599,
      "grad_norm": 0.8785305619239807,
      "learning_rate": 0.00019056927588710667,
      "loss": 0.0346,
      "step": 10501
    },
    {
      "epoch": 0.04939932453408845,
      "grad_norm": 4.121591567993164,
      "learning_rate": 0.0001905683329089932,
      "loss": 0.4819,
      "step": 10502
    },
    {
      "epoch": 0.049404028335700914,
      "grad_norm": 2.3522448539733887,
      "learning_rate": 0.0001905673899308797,
      "loss": 0.1843,
      "step": 10503
    },
    {
      "epoch": 0.04940873213731338,
      "grad_norm": 4.5588250160217285,
      "learning_rate": 0.00019056644695276625,
      "loss": 0.5943,
      "step": 10504
    },
    {
      "epoch": 0.04941343593892584,
      "grad_norm": 1.2384779453277588,
      "learning_rate": 0.00019056550397465277,
      "loss": 0.0711,
      "step": 10505
    },
    {
      "epoch": 0.049418139740538304,
      "grad_norm": 3.8967807292938232,
      "learning_rate": 0.0001905645609965393,
      "loss": 0.3356,
      "step": 10506
    },
    {
      "epoch": 0.04942284354215076,
      "grad_norm": 5.427297115325928,
      "learning_rate": 0.00019056361801842578,
      "loss": 0.569,
      "step": 10507
    },
    {
      "epoch": 0.04942754734376323,
      "grad_norm": 2.68760347366333,
      "learning_rate": 0.00019056267504031233,
      "loss": 0.2867,
      "step": 10508
    },
    {
      "epoch": 0.049432251145375694,
      "grad_norm": 0.2393808662891388,
      "learning_rate": 0.00019056173206219885,
      "loss": 0.0128,
      "step": 10509
    },
    {
      "epoch": 0.04943695494698815,
      "grad_norm": 3.726445436477661,
      "learning_rate": 0.00019056078908408536,
      "loss": 0.5006,
      "step": 10510
    },
    {
      "epoch": 0.04944165874860062,
      "grad_norm": 2.1345903873443604,
      "learning_rate": 0.00019055984610597188,
      "loss": 0.2508,
      "step": 10511
    },
    {
      "epoch": 0.049446362550213084,
      "grad_norm": 2.043386697769165,
      "learning_rate": 0.0001905589031278584,
      "loss": 0.1487,
      "step": 10512
    },
    {
      "epoch": 0.04945106635182554,
      "grad_norm": 8.933459281921387,
      "learning_rate": 0.00019055796014974495,
      "loss": 0.1495,
      "step": 10513
    },
    {
      "epoch": 0.04945577015343801,
      "grad_norm": 2.402161121368408,
      "learning_rate": 0.00019055701717163147,
      "loss": 0.1966,
      "step": 10514
    },
    {
      "epoch": 0.049460473955050474,
      "grad_norm": 2.1977450847625732,
      "learning_rate": 0.00019055607419351798,
      "loss": 0.2925,
      "step": 10515
    },
    {
      "epoch": 0.04946517775666293,
      "grad_norm": 5.556447982788086,
      "learning_rate": 0.0001905551312154045,
      "loss": 0.4003,
      "step": 10516
    },
    {
      "epoch": 0.0494698815582754,
      "grad_norm": 1.8703030347824097,
      "learning_rate": 0.00019055418823729102,
      "loss": 0.184,
      "step": 10517
    },
    {
      "epoch": 0.049474585359887864,
      "grad_norm": 2.9737839698791504,
      "learning_rate": 0.00019055324525917754,
      "loss": 0.505,
      "step": 10518
    },
    {
      "epoch": 0.04947928916150032,
      "grad_norm": 4.720761775970459,
      "learning_rate": 0.00019055230228106406,
      "loss": 0.8618,
      "step": 10519
    },
    {
      "epoch": 0.04948399296311279,
      "grad_norm": 1.891679048538208,
      "learning_rate": 0.00019055135930295058,
      "loss": 0.1213,
      "step": 10520
    },
    {
      "epoch": 0.049488696764725254,
      "grad_norm": 2.264075756072998,
      "learning_rate": 0.0001905504163248371,
      "loss": 0.2215,
      "step": 10521
    },
    {
      "epoch": 0.04949340056633771,
      "grad_norm": 0.9340769052505493,
      "learning_rate": 0.00019054947334672364,
      "loss": 0.1131,
      "step": 10522
    },
    {
      "epoch": 0.04949810436795018,
      "grad_norm": 2.8346259593963623,
      "learning_rate": 0.00019054853036861016,
      "loss": 0.5704,
      "step": 10523
    },
    {
      "epoch": 0.04950280816956264,
      "grad_norm": 1.8315091133117676,
      "learning_rate": 0.00019054758739049668,
      "loss": 0.2344,
      "step": 10524
    },
    {
      "epoch": 0.0495075119711751,
      "grad_norm": 0.5164399743080139,
      "learning_rate": 0.0001905466444123832,
      "loss": 0.0578,
      "step": 10525
    },
    {
      "epoch": 0.04951221577278757,
      "grad_norm": 2.6271491050720215,
      "learning_rate": 0.00019054570143426974,
      "loss": 0.4352,
      "step": 10526
    },
    {
      "epoch": 0.04951691957440003,
      "grad_norm": 2.3630831241607666,
      "learning_rate": 0.00019054475845615624,
      "loss": 0.4285,
      "step": 10527
    },
    {
      "epoch": 0.04952162337601249,
      "grad_norm": 1.4204106330871582,
      "learning_rate": 0.00019054381547804275,
      "loss": 0.0977,
      "step": 10528
    },
    {
      "epoch": 0.04952632717762496,
      "grad_norm": 0.8302678465843201,
      "learning_rate": 0.00019054287249992927,
      "loss": 0.0688,
      "step": 10529
    },
    {
      "epoch": 0.04953103097923742,
      "grad_norm": 2.3172802925109863,
      "learning_rate": 0.0001905419295218158,
      "loss": 0.3426,
      "step": 10530
    },
    {
      "epoch": 0.04953573478084988,
      "grad_norm": 1.6912249326705933,
      "learning_rate": 0.00019054098654370234,
      "loss": 0.3824,
      "step": 10531
    },
    {
      "epoch": 0.04954043858246235,
      "grad_norm": 1.4118379354476929,
      "learning_rate": 0.00019054004356558886,
      "loss": 0.2423,
      "step": 10532
    },
    {
      "epoch": 0.04954514238407481,
      "grad_norm": 1.3096082210540771,
      "learning_rate": 0.00019053910058747537,
      "loss": 0.1227,
      "step": 10533
    },
    {
      "epoch": 0.04954984618568727,
      "grad_norm": 0.3718787729740143,
      "learning_rate": 0.0001905381576093619,
      "loss": 0.033,
      "step": 10534
    },
    {
      "epoch": 0.04955454998729974,
      "grad_norm": 1.9702529907226562,
      "learning_rate": 0.00019053721463124844,
      "loss": 0.2547,
      "step": 10535
    },
    {
      "epoch": 0.0495592537889122,
      "grad_norm": 0.5881653428077698,
      "learning_rate": 0.00019053627165313496,
      "loss": 0.0863,
      "step": 10536
    },
    {
      "epoch": 0.04956395759052466,
      "grad_norm": 1.7003352642059326,
      "learning_rate": 0.00019053532867502148,
      "loss": 0.193,
      "step": 10537
    },
    {
      "epoch": 0.04956866139213713,
      "grad_norm": 2.4867637157440186,
      "learning_rate": 0.00019053438569690797,
      "loss": 0.5298,
      "step": 10538
    },
    {
      "epoch": 0.04957336519374959,
      "grad_norm": 0.5212709903717041,
      "learning_rate": 0.00019053344271879449,
      "loss": 0.0578,
      "step": 10539
    },
    {
      "epoch": 0.04957806899536205,
      "grad_norm": 1.6386120319366455,
      "learning_rate": 0.00019053249974068103,
      "loss": 0.1738,
      "step": 10540
    },
    {
      "epoch": 0.04958277279697451,
      "grad_norm": 1.5031397342681885,
      "learning_rate": 0.00019053155676256755,
      "loss": 0.1467,
      "step": 10541
    },
    {
      "epoch": 0.04958747659858698,
      "grad_norm": 1.4369829893112183,
      "learning_rate": 0.00019053061378445407,
      "loss": 0.289,
      "step": 10542
    },
    {
      "epoch": 0.04959218040019944,
      "grad_norm": 6.042906284332275,
      "learning_rate": 0.0001905296708063406,
      "loss": 0.9727,
      "step": 10543
    },
    {
      "epoch": 0.0495968842018119,
      "grad_norm": 4.956854343414307,
      "learning_rate": 0.00019052872782822713,
      "loss": 0.6064,
      "step": 10544
    },
    {
      "epoch": 0.04960158800342437,
      "grad_norm": 7.737869739532471,
      "learning_rate": 0.00019052778485011365,
      "loss": 0.9511,
      "step": 10545
    },
    {
      "epoch": 0.04960629180503683,
      "grad_norm": 0.7356151938438416,
      "learning_rate": 0.00019052684187200017,
      "loss": 0.0868,
      "step": 10546
    },
    {
      "epoch": 0.04961099560664929,
      "grad_norm": 1.371640920639038,
      "learning_rate": 0.0001905258988938867,
      "loss": 0.267,
      "step": 10547
    },
    {
      "epoch": 0.04961569940826176,
      "grad_norm": 0.4953595995903015,
      "learning_rate": 0.0001905249559157732,
      "loss": 0.0424,
      "step": 10548
    },
    {
      "epoch": 0.04962040320987422,
      "grad_norm": 1.5579559803009033,
      "learning_rate": 0.00019052401293765973,
      "loss": 0.1795,
      "step": 10549
    },
    {
      "epoch": 0.04962510701148668,
      "grad_norm": 1.2621862888336182,
      "learning_rate": 0.00019052306995954625,
      "loss": 0.1299,
      "step": 10550
    },
    {
      "epoch": 0.04962981081309915,
      "grad_norm": 3.117809772491455,
      "learning_rate": 0.00019052212698143276,
      "loss": 0.218,
      "step": 10551
    },
    {
      "epoch": 0.04963451461471161,
      "grad_norm": 2.087791681289673,
      "learning_rate": 0.00019052118400331928,
      "loss": 0.3426,
      "step": 10552
    },
    {
      "epoch": 0.04963921841632407,
      "grad_norm": 3.503871202468872,
      "learning_rate": 0.0001905202410252058,
      "loss": 0.3055,
      "step": 10553
    },
    {
      "epoch": 0.04964392221793654,
      "grad_norm": 2.2879958152770996,
      "learning_rate": 0.00019051929804709235,
      "loss": 0.2416,
      "step": 10554
    },
    {
      "epoch": 0.049648626019549,
      "grad_norm": 27.57937240600586,
      "learning_rate": 0.00019051835506897887,
      "loss": 0.7778,
      "step": 10555
    },
    {
      "epoch": 0.04965332982116146,
      "grad_norm": 3.5408689975738525,
      "learning_rate": 0.00019051741209086538,
      "loss": 1.0111,
      "step": 10556
    },
    {
      "epoch": 0.04965803362277393,
      "grad_norm": 2.0728538036346436,
      "learning_rate": 0.0001905164691127519,
      "loss": 0.1665,
      "step": 10557
    },
    {
      "epoch": 0.04966273742438639,
      "grad_norm": 1.463875651359558,
      "learning_rate": 0.00019051552613463842,
      "loss": 0.1115,
      "step": 10558
    },
    {
      "epoch": 0.04966744122599885,
      "grad_norm": 0.7561824917793274,
      "learning_rate": 0.00019051458315652494,
      "loss": 0.0645,
      "step": 10559
    },
    {
      "epoch": 0.04967214502761132,
      "grad_norm": 2.0655860900878906,
      "learning_rate": 0.00019051364017841146,
      "loss": 0.198,
      "step": 10560
    },
    {
      "epoch": 0.049676848829223776,
      "grad_norm": 2.0514140129089355,
      "learning_rate": 0.00019051269720029798,
      "loss": 0.2247,
      "step": 10561
    },
    {
      "epoch": 0.04968155263083624,
      "grad_norm": 3.894796371459961,
      "learning_rate": 0.0001905117542221845,
      "loss": 0.5708,
      "step": 10562
    },
    {
      "epoch": 0.04968625643244871,
      "grad_norm": 2.5463037490844727,
      "learning_rate": 0.00019051081124407104,
      "loss": 0.3471,
      "step": 10563
    },
    {
      "epoch": 0.049690960234061166,
      "grad_norm": 2.839181423187256,
      "learning_rate": 0.00019050986826595756,
      "loss": 0.4059,
      "step": 10564
    },
    {
      "epoch": 0.04969566403567363,
      "grad_norm": 2.271904230117798,
      "learning_rate": 0.00019050892528784408,
      "loss": 0.2738,
      "step": 10565
    },
    {
      "epoch": 0.0497003678372861,
      "grad_norm": 2.8941919803619385,
      "learning_rate": 0.0001905079823097306,
      "loss": 0.5567,
      "step": 10566
    },
    {
      "epoch": 0.049705071638898556,
      "grad_norm": 3.5150535106658936,
      "learning_rate": 0.00019050703933161714,
      "loss": 0.3517,
      "step": 10567
    },
    {
      "epoch": 0.04970977544051102,
      "grad_norm": 1.4320261478424072,
      "learning_rate": 0.00019050609635350366,
      "loss": 0.2083,
      "step": 10568
    },
    {
      "epoch": 0.04971447924212349,
      "grad_norm": 0.5567181706428528,
      "learning_rate": 0.00019050515337539015,
      "loss": 0.0622,
      "step": 10569
    },
    {
      "epoch": 0.049719183043735946,
      "grad_norm": 1.129338026046753,
      "learning_rate": 0.00019050421039727667,
      "loss": 0.1044,
      "step": 10570
    },
    {
      "epoch": 0.04972388684534841,
      "grad_norm": 2.582658529281616,
      "learning_rate": 0.0001905032674191632,
      "loss": 0.2582,
      "step": 10571
    },
    {
      "epoch": 0.04972859064696088,
      "grad_norm": 1.4031513929367065,
      "learning_rate": 0.00019050232444104974,
      "loss": 0.1966,
      "step": 10572
    },
    {
      "epoch": 0.049733294448573336,
      "grad_norm": 2.6589484214782715,
      "learning_rate": 0.00019050138146293626,
      "loss": 0.2645,
      "step": 10573
    },
    {
      "epoch": 0.0497379982501858,
      "grad_norm": 5.2066969871521,
      "learning_rate": 0.00019050043848482277,
      "loss": 1.0672,
      "step": 10574
    },
    {
      "epoch": 0.04974270205179827,
      "grad_norm": 0.5680248141288757,
      "learning_rate": 0.0001904994955067093,
      "loss": 0.0352,
      "step": 10575
    },
    {
      "epoch": 0.049747405853410726,
      "grad_norm": 1.8862248659133911,
      "learning_rate": 0.00019049855252859584,
      "loss": 0.3188,
      "step": 10576
    },
    {
      "epoch": 0.04975210965502319,
      "grad_norm": 1.2678309679031372,
      "learning_rate": 0.00019049760955048236,
      "loss": 0.1369,
      "step": 10577
    },
    {
      "epoch": 0.04975681345663565,
      "grad_norm": 3.5214924812316895,
      "learning_rate": 0.00019049666657236888,
      "loss": 0.6055,
      "step": 10578
    },
    {
      "epoch": 0.049761517258248116,
      "grad_norm": 1.2151273488998413,
      "learning_rate": 0.0001904957235942554,
      "loss": 0.1344,
      "step": 10579
    },
    {
      "epoch": 0.04976622105986058,
      "grad_norm": 3.3202667236328125,
      "learning_rate": 0.00019049478061614189,
      "loss": 0.6209,
      "step": 10580
    },
    {
      "epoch": 0.04977092486147304,
      "grad_norm": 0.5693233609199524,
      "learning_rate": 0.00019049383763802843,
      "loss": 0.0493,
      "step": 10581
    },
    {
      "epoch": 0.049775628663085505,
      "grad_norm": 1.5415935516357422,
      "learning_rate": 0.00019049289465991495,
      "loss": 0.246,
      "step": 10582
    },
    {
      "epoch": 0.04978033246469797,
      "grad_norm": 0.23264619708061218,
      "learning_rate": 0.00019049195168180147,
      "loss": 0.0178,
      "step": 10583
    },
    {
      "epoch": 0.04978503626631043,
      "grad_norm": 1.4686435461044312,
      "learning_rate": 0.000190491008703688,
      "loss": 0.1254,
      "step": 10584
    },
    {
      "epoch": 0.049789740067922895,
      "grad_norm": 1.6782783269882202,
      "learning_rate": 0.00019049006572557453,
      "loss": 0.4059,
      "step": 10585
    },
    {
      "epoch": 0.04979444386953536,
      "grad_norm": 0.6425259113311768,
      "learning_rate": 0.00019048912274746105,
      "loss": 0.0458,
      "step": 10586
    },
    {
      "epoch": 0.04979914767114782,
      "grad_norm": 0.7263931035995483,
      "learning_rate": 0.00019048817976934757,
      "loss": 0.0544,
      "step": 10587
    },
    {
      "epoch": 0.049803851472760285,
      "grad_norm": 2.560643434524536,
      "learning_rate": 0.0001904872367912341,
      "loss": 0.3303,
      "step": 10588
    },
    {
      "epoch": 0.04980855527437275,
      "grad_norm": 0.6738398671150208,
      "learning_rate": 0.0001904862938131206,
      "loss": 0.0605,
      "step": 10589
    },
    {
      "epoch": 0.04981325907598521,
      "grad_norm": 1.481256365776062,
      "learning_rate": 0.00019048535083500713,
      "loss": 0.2192,
      "step": 10590
    },
    {
      "epoch": 0.049817962877597675,
      "grad_norm": 0.622597873210907,
      "learning_rate": 0.00019048440785689365,
      "loss": 0.0879,
      "step": 10591
    },
    {
      "epoch": 0.04982266667921014,
      "grad_norm": 0.9773200750350952,
      "learning_rate": 0.00019048346487878016,
      "loss": 0.0414,
      "step": 10592
    },
    {
      "epoch": 0.0498273704808226,
      "grad_norm": 1.7869224548339844,
      "learning_rate": 0.00019048252190066668,
      "loss": 0.1696,
      "step": 10593
    },
    {
      "epoch": 0.049832074282435065,
      "grad_norm": 3.2161459922790527,
      "learning_rate": 0.00019048157892255323,
      "loss": 0.5462,
      "step": 10594
    },
    {
      "epoch": 0.049836778084047524,
      "grad_norm": 1.3653076887130737,
      "learning_rate": 0.00019048063594443975,
      "loss": 0.206,
      "step": 10595
    },
    {
      "epoch": 0.04984148188565999,
      "grad_norm": 1.8991996049880981,
      "learning_rate": 0.00019047969296632627,
      "loss": 0.461,
      "step": 10596
    },
    {
      "epoch": 0.049846185687272455,
      "grad_norm": 4.411873817443848,
      "learning_rate": 0.00019047874998821278,
      "loss": 0.4861,
      "step": 10597
    },
    {
      "epoch": 0.049850889488884914,
      "grad_norm": 2.8195607662200928,
      "learning_rate": 0.0001904778070100993,
      "loss": 0.5909,
      "step": 10598
    },
    {
      "epoch": 0.04985559329049738,
      "grad_norm": 1.6912566423416138,
      "learning_rate": 0.00019047686403198585,
      "loss": 0.2515,
      "step": 10599
    },
    {
      "epoch": 0.049860297092109845,
      "grad_norm": 0.2367672175168991,
      "learning_rate": 0.00019047592105387234,
      "loss": 0.0157,
      "step": 10600
    },
    {
      "epoch": 0.049865000893722304,
      "grad_norm": 1.504342794418335,
      "learning_rate": 0.00019047497807575886,
      "loss": 0.2038,
      "step": 10601
    },
    {
      "epoch": 0.04986970469533477,
      "grad_norm": 1.6547895669937134,
      "learning_rate": 0.00019047403509764538,
      "loss": 0.1789,
      "step": 10602
    },
    {
      "epoch": 0.049874408496947235,
      "grad_norm": 1.8036530017852783,
      "learning_rate": 0.0001904730921195319,
      "loss": 0.2418,
      "step": 10603
    },
    {
      "epoch": 0.049879112298559694,
      "grad_norm": 1.9334661960601807,
      "learning_rate": 0.00019047214914141844,
      "loss": 0.1965,
      "step": 10604
    },
    {
      "epoch": 0.04988381610017216,
      "grad_norm": 3.695164680480957,
      "learning_rate": 0.00019047120616330496,
      "loss": 0.5546,
      "step": 10605
    },
    {
      "epoch": 0.049888519901784625,
      "grad_norm": 2.6116156578063965,
      "learning_rate": 0.00019047026318519148,
      "loss": 0.234,
      "step": 10606
    },
    {
      "epoch": 0.049893223703397084,
      "grad_norm": 2.013239622116089,
      "learning_rate": 0.000190469320207078,
      "loss": 0.1611,
      "step": 10607
    },
    {
      "epoch": 0.04989792750500955,
      "grad_norm": 0.9710667133331299,
      "learning_rate": 0.00019046837722896454,
      "loss": 0.0869,
      "step": 10608
    },
    {
      "epoch": 0.049902631306622015,
      "grad_norm": 2.402247667312622,
      "learning_rate": 0.00019046743425085106,
      "loss": 0.2461,
      "step": 10609
    },
    {
      "epoch": 0.049907335108234474,
      "grad_norm": 1.0104318857192993,
      "learning_rate": 0.00019046649127273758,
      "loss": 0.0864,
      "step": 10610
    },
    {
      "epoch": 0.04991203890984694,
      "grad_norm": 3.2370691299438477,
      "learning_rate": 0.00019046554829462407,
      "loss": 0.3787,
      "step": 10611
    },
    {
      "epoch": 0.0499167427114594,
      "grad_norm": 1.0682117938995361,
      "learning_rate": 0.0001904646053165106,
      "loss": 0.1187,
      "step": 10612
    },
    {
      "epoch": 0.049921446513071864,
      "grad_norm": 1.7607578039169312,
      "learning_rate": 0.00019046366233839714,
      "loss": 0.1844,
      "step": 10613
    },
    {
      "epoch": 0.04992615031468433,
      "grad_norm": 2.101884603500366,
      "learning_rate": 0.00019046271936028366,
      "loss": 0.2966,
      "step": 10614
    },
    {
      "epoch": 0.04993085411629679,
      "grad_norm": 0.4333602488040924,
      "learning_rate": 0.00019046177638217017,
      "loss": 0.0318,
      "step": 10615
    },
    {
      "epoch": 0.049935557917909254,
      "grad_norm": 2.3809125423431396,
      "learning_rate": 0.0001904608334040567,
      "loss": 0.2243,
      "step": 10616
    },
    {
      "epoch": 0.04994026171952172,
      "grad_norm": 4.059053897857666,
      "learning_rate": 0.00019045989042594324,
      "loss": 0.3793,
      "step": 10617
    },
    {
      "epoch": 0.04994496552113418,
      "grad_norm": 3.3722057342529297,
      "learning_rate": 0.00019045894744782976,
      "loss": 0.3256,
      "step": 10618
    },
    {
      "epoch": 0.049949669322746644,
      "grad_norm": 3.2406556606292725,
      "learning_rate": 0.00019045800446971628,
      "loss": 0.3702,
      "step": 10619
    },
    {
      "epoch": 0.04995437312435911,
      "grad_norm": 2.792614221572876,
      "learning_rate": 0.0001904570614916028,
      "loss": 0.2778,
      "step": 10620
    },
    {
      "epoch": 0.04995907692597157,
      "grad_norm": 1.6636216640472412,
      "learning_rate": 0.0001904561185134893,
      "loss": 0.168,
      "step": 10621
    },
    {
      "epoch": 0.049963780727584034,
      "grad_norm": 0.611850917339325,
      "learning_rate": 0.00019045517553537583,
      "loss": 0.0608,
      "step": 10622
    },
    {
      "epoch": 0.0499684845291965,
      "grad_norm": 2.4390506744384766,
      "learning_rate": 0.00019045423255726235,
      "loss": 0.367,
      "step": 10623
    },
    {
      "epoch": 0.04997318833080896,
      "grad_norm": 1.1227363348007202,
      "learning_rate": 0.00019045328957914887,
      "loss": 0.0974,
      "step": 10624
    },
    {
      "epoch": 0.049977892132421424,
      "grad_norm": 1.0879930257797241,
      "learning_rate": 0.0001904523466010354,
      "loss": 0.0686,
      "step": 10625
    },
    {
      "epoch": 0.04998259593403389,
      "grad_norm": 0.4228808581829071,
      "learning_rate": 0.00019045140362292193,
      "loss": 0.0267,
      "step": 10626
    },
    {
      "epoch": 0.04998729973564635,
      "grad_norm": 1.7951263189315796,
      "learning_rate": 0.00019045046064480845,
      "loss": 0.0987,
      "step": 10627
    },
    {
      "epoch": 0.049992003537258814,
      "grad_norm": 0.9365957379341125,
      "learning_rate": 0.00019044951766669497,
      "loss": 0.0845,
      "step": 10628
    },
    {
      "epoch": 0.04999670733887127,
      "grad_norm": 0.07025481015443802,
      "learning_rate": 0.0001904485746885815,
      "loss": 0.0036,
      "step": 10629
    },
    {
      "epoch": 0.05000141114048374,
      "grad_norm": 4.63820219039917,
      "learning_rate": 0.000190447631710468,
      "loss": 1.0086,
      "step": 10630
    },
    {
      "epoch": 0.050006114942096204,
      "grad_norm": 3.901841878890991,
      "learning_rate": 0.00019044668873235453,
      "loss": 0.1953,
      "step": 10631
    },
    {
      "epoch": 0.05001081874370866,
      "grad_norm": 2.870485782623291,
      "learning_rate": 0.00019044574575424105,
      "loss": 0.1859,
      "step": 10632
    },
    {
      "epoch": 0.05001552254532113,
      "grad_norm": 0.4024885296821594,
      "learning_rate": 0.00019044480277612756,
      "loss": 0.0535,
      "step": 10633
    },
    {
      "epoch": 0.050020226346933594,
      "grad_norm": 5.044755935668945,
      "learning_rate": 0.00019044385979801408,
      "loss": 0.9413,
      "step": 10634
    },
    {
      "epoch": 0.05002493014854605,
      "grad_norm": 3.6611835956573486,
      "learning_rate": 0.00019044291681990063,
      "loss": 0.4358,
      "step": 10635
    },
    {
      "epoch": 0.05002963395015852,
      "grad_norm": 3.1434686183929443,
      "learning_rate": 0.00019044197384178715,
      "loss": 0.3868,
      "step": 10636
    },
    {
      "epoch": 0.050034337751770984,
      "grad_norm": 2.7267937660217285,
      "learning_rate": 0.00019044103086367367,
      "loss": 0.3029,
      "step": 10637
    },
    {
      "epoch": 0.05003904155338344,
      "grad_norm": 3.7756919860839844,
      "learning_rate": 0.00019044008788556018,
      "loss": 0.5748,
      "step": 10638
    },
    {
      "epoch": 0.05004374535499591,
      "grad_norm": 2.179959297180176,
      "learning_rate": 0.0001904391449074467,
      "loss": 0.1598,
      "step": 10639
    },
    {
      "epoch": 0.050048449156608374,
      "grad_norm": 5.697215557098389,
      "learning_rate": 0.00019043820192933325,
      "loss": 1.2792,
      "step": 10640
    },
    {
      "epoch": 0.05005315295822083,
      "grad_norm": 1.0606966018676758,
      "learning_rate": 0.00019043725895121977,
      "loss": 0.0906,
      "step": 10641
    },
    {
      "epoch": 0.0500578567598333,
      "grad_norm": 2.2464897632598877,
      "learning_rate": 0.00019043631597310626,
      "loss": 0.3127,
      "step": 10642
    },
    {
      "epoch": 0.050062560561445764,
      "grad_norm": 0.1636817306280136,
      "learning_rate": 0.00019043537299499278,
      "loss": 0.0092,
      "step": 10643
    },
    {
      "epoch": 0.05006726436305822,
      "grad_norm": 0.4592594504356384,
      "learning_rate": 0.00019043443001687932,
      "loss": 0.0342,
      "step": 10644
    },
    {
      "epoch": 0.05007196816467069,
      "grad_norm": 3.8074607849121094,
      "learning_rate": 0.00019043348703876584,
      "loss": 0.4113,
      "step": 10645
    },
    {
      "epoch": 0.05007667196628315,
      "grad_norm": 2.399667501449585,
      "learning_rate": 0.00019043254406065236,
      "loss": 0.6965,
      "step": 10646
    },
    {
      "epoch": 0.05008137576789561,
      "grad_norm": 0.7046265006065369,
      "learning_rate": 0.00019043160108253888,
      "loss": 0.0552,
      "step": 10647
    },
    {
      "epoch": 0.05008607956950808,
      "grad_norm": 1.5210908651351929,
      "learning_rate": 0.0001904306581044254,
      "loss": 0.1341,
      "step": 10648
    },
    {
      "epoch": 0.05009078337112054,
      "grad_norm": 1.7498962879180908,
      "learning_rate": 0.00019042971512631194,
      "loss": 0.1338,
      "step": 10649
    },
    {
      "epoch": 0.050095487172733,
      "grad_norm": 1.0104011297225952,
      "learning_rate": 0.00019042877214819846,
      "loss": 0.0702,
      "step": 10650
    },
    {
      "epoch": 0.05010019097434547,
      "grad_norm": 2.8774991035461426,
      "learning_rate": 0.00019042782917008498,
      "loss": 0.1537,
      "step": 10651
    },
    {
      "epoch": 0.05010489477595793,
      "grad_norm": 4.431482791900635,
      "learning_rate": 0.0001904268861919715,
      "loss": 0.2718,
      "step": 10652
    },
    {
      "epoch": 0.05010959857757039,
      "grad_norm": 1.511034607887268,
      "learning_rate": 0.00019042594321385802,
      "loss": 0.2051,
      "step": 10653
    },
    {
      "epoch": 0.05011430237918286,
      "grad_norm": 2.1589627265930176,
      "learning_rate": 0.00019042500023574454,
      "loss": 0.3012,
      "step": 10654
    },
    {
      "epoch": 0.05011900618079532,
      "grad_norm": 2.3958523273468018,
      "learning_rate": 0.00019042405725763106,
      "loss": 0.2621,
      "step": 10655
    },
    {
      "epoch": 0.05012370998240778,
      "grad_norm": 1.8652602434158325,
      "learning_rate": 0.00019042311427951757,
      "loss": 0.1215,
      "step": 10656
    },
    {
      "epoch": 0.05012841378402025,
      "grad_norm": 0.417408287525177,
      "learning_rate": 0.0001904221713014041,
      "loss": 0.035,
      "step": 10657
    },
    {
      "epoch": 0.05013311758563271,
      "grad_norm": 3.839935064315796,
      "learning_rate": 0.00019042122832329064,
      "loss": 0.5085,
      "step": 10658
    },
    {
      "epoch": 0.05013782138724517,
      "grad_norm": 2.665074110031128,
      "learning_rate": 0.00019042028534517716,
      "loss": 0.3555,
      "step": 10659
    },
    {
      "epoch": 0.05014252518885764,
      "grad_norm": 5.3269147872924805,
      "learning_rate": 0.00019041934236706368,
      "loss": 1.5871,
      "step": 10660
    },
    {
      "epoch": 0.0501472289904701,
      "grad_norm": 2.1496381759643555,
      "learning_rate": 0.0001904183993889502,
      "loss": 0.2447,
      "step": 10661
    },
    {
      "epoch": 0.05015193279208256,
      "grad_norm": 0.21509577333927155,
      "learning_rate": 0.0001904174564108367,
      "loss": 0.019,
      "step": 10662
    },
    {
      "epoch": 0.05015663659369502,
      "grad_norm": 0.47588881850242615,
      "learning_rate": 0.00019041651343272323,
      "loss": 0.0605,
      "step": 10663
    },
    {
      "epoch": 0.05016134039530749,
      "grad_norm": 3.2287862300872803,
      "learning_rate": 0.00019041557045460975,
      "loss": 0.2073,
      "step": 10664
    },
    {
      "epoch": 0.05016604419691995,
      "grad_norm": 1.3857029676437378,
      "learning_rate": 0.00019041462747649627,
      "loss": 0.1099,
      "step": 10665
    },
    {
      "epoch": 0.05017074799853241,
      "grad_norm": 0.7821828722953796,
      "learning_rate": 0.0001904136844983828,
      "loss": 0.1112,
      "step": 10666
    },
    {
      "epoch": 0.05017545180014488,
      "grad_norm": 1.9994114637374878,
      "learning_rate": 0.00019041274152026933,
      "loss": 0.2162,
      "step": 10667
    },
    {
      "epoch": 0.05018015560175734,
      "grad_norm": 2.447751045227051,
      "learning_rate": 0.00019041179854215585,
      "loss": 0.3503,
      "step": 10668
    },
    {
      "epoch": 0.0501848594033698,
      "grad_norm": 1.522843599319458,
      "learning_rate": 0.00019041085556404237,
      "loss": 0.1137,
      "step": 10669
    },
    {
      "epoch": 0.05018956320498227,
      "grad_norm": 1.5253033638000488,
      "learning_rate": 0.0001904099125859289,
      "loss": 0.2169,
      "step": 10670
    },
    {
      "epoch": 0.05019426700659473,
      "grad_norm": 3.007568359375,
      "learning_rate": 0.0001904089696078154,
      "loss": 0.2658,
      "step": 10671
    },
    {
      "epoch": 0.05019897080820719,
      "grad_norm": 1.1228796243667603,
      "learning_rate": 0.00019040802662970195,
      "loss": 0.0896,
      "step": 10672
    },
    {
      "epoch": 0.05020367460981966,
      "grad_norm": 1.4112039804458618,
      "learning_rate": 0.00019040708365158845,
      "loss": 0.1241,
      "step": 10673
    },
    {
      "epoch": 0.05020837841143212,
      "grad_norm": 2.090073823928833,
      "learning_rate": 0.00019040614067347496,
      "loss": 0.2257,
      "step": 10674
    },
    {
      "epoch": 0.05021308221304458,
      "grad_norm": 1.4443106651306152,
      "learning_rate": 0.00019040519769536148,
      "loss": 0.0504,
      "step": 10675
    },
    {
      "epoch": 0.05021778601465705,
      "grad_norm": 9.436419486999512,
      "learning_rate": 0.00019040425471724803,
      "loss": 0.6307,
      "step": 10676
    },
    {
      "epoch": 0.05022248981626951,
      "grad_norm": 2.2263498306274414,
      "learning_rate": 0.00019040331173913455,
      "loss": 0.2408,
      "step": 10677
    },
    {
      "epoch": 0.05022719361788197,
      "grad_norm": 2.444934368133545,
      "learning_rate": 0.00019040236876102107,
      "loss": 0.3134,
      "step": 10678
    },
    {
      "epoch": 0.05023189741949444,
      "grad_norm": 2.1377243995666504,
      "learning_rate": 0.00019040142578290758,
      "loss": 0.2048,
      "step": 10679
    },
    {
      "epoch": 0.050236601221106895,
      "grad_norm": 2.1692724227905273,
      "learning_rate": 0.0001904004828047941,
      "loss": 0.4821,
      "step": 10680
    },
    {
      "epoch": 0.05024130502271936,
      "grad_norm": 3.150031328201294,
      "learning_rate": 0.00019039953982668065,
      "loss": 0.2536,
      "step": 10681
    },
    {
      "epoch": 0.05024600882433183,
      "grad_norm": 0.7226293683052063,
      "learning_rate": 0.00019039859684856717,
      "loss": 0.0599,
      "step": 10682
    },
    {
      "epoch": 0.050250712625944285,
      "grad_norm": 4.10179328918457,
      "learning_rate": 0.00019039765387045369,
      "loss": 0.3833,
      "step": 10683
    },
    {
      "epoch": 0.05025541642755675,
      "grad_norm": 3.3565309047698975,
      "learning_rate": 0.0001903967108923402,
      "loss": 0.3662,
      "step": 10684
    },
    {
      "epoch": 0.05026012022916922,
      "grad_norm": 0.8625348806381226,
      "learning_rate": 0.00019039576791422672,
      "loss": 0.1086,
      "step": 10685
    },
    {
      "epoch": 0.050264824030781675,
      "grad_norm": 0.3911982774734497,
      "learning_rate": 0.00019039482493611324,
      "loss": 0.0451,
      "step": 10686
    },
    {
      "epoch": 0.05026952783239414,
      "grad_norm": 3.9751288890838623,
      "learning_rate": 0.00019039388195799976,
      "loss": 0.6487,
      "step": 10687
    },
    {
      "epoch": 0.05027423163400661,
      "grad_norm": 0.797145664691925,
      "learning_rate": 0.00019039293897988628,
      "loss": 0.101,
      "step": 10688
    },
    {
      "epoch": 0.050278935435619065,
      "grad_norm": 2.190810441970825,
      "learning_rate": 0.0001903919960017728,
      "loss": 0.3983,
      "step": 10689
    },
    {
      "epoch": 0.05028363923723153,
      "grad_norm": 1.629220962524414,
      "learning_rate": 0.00019039105302365934,
      "loss": 0.1382,
      "step": 10690
    },
    {
      "epoch": 0.050288343038844,
      "grad_norm": 0.21338911354541779,
      "learning_rate": 0.00019039011004554586,
      "loss": 0.0136,
      "step": 10691
    },
    {
      "epoch": 0.050293046840456455,
      "grad_norm": 1.3056519031524658,
      "learning_rate": 0.00019038916706743238,
      "loss": 0.1357,
      "step": 10692
    },
    {
      "epoch": 0.05029775064206892,
      "grad_norm": 1.188448190689087,
      "learning_rate": 0.0001903882240893189,
      "loss": 0.1236,
      "step": 10693
    },
    {
      "epoch": 0.05030245444368139,
      "grad_norm": 4.199909687042236,
      "learning_rate": 0.00019038728111120542,
      "loss": 0.4194,
      "step": 10694
    },
    {
      "epoch": 0.050307158245293845,
      "grad_norm": 4.3269853591918945,
      "learning_rate": 0.00019038633813309194,
      "loss": 0.8859,
      "step": 10695
    },
    {
      "epoch": 0.05031186204690631,
      "grad_norm": 2.507600784301758,
      "learning_rate": 0.00019038539515497846,
      "loss": 0.3186,
      "step": 10696
    },
    {
      "epoch": 0.05031656584851877,
      "grad_norm": 0.9079542756080627,
      "learning_rate": 0.00019038445217686497,
      "loss": 0.0774,
      "step": 10697
    },
    {
      "epoch": 0.050321269650131235,
      "grad_norm": 0.5280552506446838,
      "learning_rate": 0.0001903835091987515,
      "loss": 0.0481,
      "step": 10698
    },
    {
      "epoch": 0.0503259734517437,
      "grad_norm": 1.7513211965560913,
      "learning_rate": 0.00019038256622063804,
      "loss": 0.1678,
      "step": 10699
    },
    {
      "epoch": 0.05033067725335616,
      "grad_norm": 4.709589004516602,
      "learning_rate": 0.00019038162324252456,
      "loss": 1.0514,
      "step": 10700
    },
    {
      "epoch": 0.050335381054968625,
      "grad_norm": 2.2507247924804688,
      "learning_rate": 0.00019038068026441108,
      "loss": 0.2897,
      "step": 10701
    },
    {
      "epoch": 0.05034008485658109,
      "grad_norm": 5.732150554656982,
      "learning_rate": 0.0001903797372862976,
      "loss": 0.3274,
      "step": 10702
    },
    {
      "epoch": 0.05034478865819355,
      "grad_norm": 6.77663516998291,
      "learning_rate": 0.0001903787943081841,
      "loss": 0.4983,
      "step": 10703
    },
    {
      "epoch": 0.050349492459806015,
      "grad_norm": 0.13174648582935333,
      "learning_rate": 0.00019037785133007063,
      "loss": 0.0072,
      "step": 10704
    },
    {
      "epoch": 0.05035419626141848,
      "grad_norm": 1.0059624910354614,
      "learning_rate": 0.00019037690835195715,
      "loss": 0.0833,
      "step": 10705
    },
    {
      "epoch": 0.05035890006303094,
      "grad_norm": 4.9331159591674805,
      "learning_rate": 0.00019037596537384367,
      "loss": 0.6341,
      "step": 10706
    },
    {
      "epoch": 0.050363603864643405,
      "grad_norm": 4.810269832611084,
      "learning_rate": 0.0001903750223957302,
      "loss": 0.9342,
      "step": 10707
    },
    {
      "epoch": 0.05036830766625587,
      "grad_norm": 2.4793596267700195,
      "learning_rate": 0.00019037407941761673,
      "loss": 0.2291,
      "step": 10708
    },
    {
      "epoch": 0.05037301146786833,
      "grad_norm": 2.4829208850860596,
      "learning_rate": 0.00019037313643950325,
      "loss": 0.2651,
      "step": 10709
    },
    {
      "epoch": 0.050377715269480795,
      "grad_norm": 2.399104595184326,
      "learning_rate": 0.00019037219346138977,
      "loss": 0.2075,
      "step": 10710
    },
    {
      "epoch": 0.05038241907109326,
      "grad_norm": 5.91179084777832,
      "learning_rate": 0.0001903712504832763,
      "loss": 0.4955,
      "step": 10711
    },
    {
      "epoch": 0.05038712287270572,
      "grad_norm": 2.596999406814575,
      "learning_rate": 0.0001903703075051628,
      "loss": 0.2875,
      "step": 10712
    },
    {
      "epoch": 0.050391826674318185,
      "grad_norm": 1.012807846069336,
      "learning_rate": 0.00019036936452704935,
      "loss": 0.0851,
      "step": 10713
    },
    {
      "epoch": 0.050396530475930644,
      "grad_norm": 4.060642242431641,
      "learning_rate": 0.00019036842154893587,
      "loss": 0.574,
      "step": 10714
    },
    {
      "epoch": 0.05040123427754311,
      "grad_norm": 1.1859605312347412,
      "learning_rate": 0.0001903674785708224,
      "loss": 0.1282,
      "step": 10715
    },
    {
      "epoch": 0.050405938079155575,
      "grad_norm": 2.053785562515259,
      "learning_rate": 0.00019036653559270888,
      "loss": 0.211,
      "step": 10716
    },
    {
      "epoch": 0.050410641880768034,
      "grad_norm": 2.725404739379883,
      "learning_rate": 0.00019036559261459543,
      "loss": 0.447,
      "step": 10717
    },
    {
      "epoch": 0.0504153456823805,
      "grad_norm": 1.5201772451400757,
      "learning_rate": 0.00019036464963648195,
      "loss": 0.1417,
      "step": 10718
    },
    {
      "epoch": 0.050420049483992965,
      "grad_norm": 0.480474591255188,
      "learning_rate": 0.00019036370665836847,
      "loss": 0.0549,
      "step": 10719
    },
    {
      "epoch": 0.050424753285605424,
      "grad_norm": 2.0939905643463135,
      "learning_rate": 0.00019036276368025498,
      "loss": 0.2412,
      "step": 10720
    },
    {
      "epoch": 0.05042945708721789,
      "grad_norm": 3.6018879413604736,
      "learning_rate": 0.0001903618207021415,
      "loss": 0.4513,
      "step": 10721
    },
    {
      "epoch": 0.050434160888830355,
      "grad_norm": 2.1442270278930664,
      "learning_rate": 0.00019036087772402805,
      "loss": 0.3771,
      "step": 10722
    },
    {
      "epoch": 0.050438864690442814,
      "grad_norm": 3.123704671859741,
      "learning_rate": 0.00019035993474591457,
      "loss": 0.3854,
      "step": 10723
    },
    {
      "epoch": 0.05044356849205528,
      "grad_norm": 1.9848295450210571,
      "learning_rate": 0.00019035899176780109,
      "loss": 0.2483,
      "step": 10724
    },
    {
      "epoch": 0.050448272293667745,
      "grad_norm": 2.932501792907715,
      "learning_rate": 0.0001903580487896876,
      "loss": 0.3852,
      "step": 10725
    },
    {
      "epoch": 0.050452976095280204,
      "grad_norm": 2.2985622882843018,
      "learning_rate": 0.00019035710581157412,
      "loss": 0.289,
      "step": 10726
    },
    {
      "epoch": 0.05045767989689267,
      "grad_norm": 1.7418559789657593,
      "learning_rate": 0.00019035616283346064,
      "loss": 0.1548,
      "step": 10727
    },
    {
      "epoch": 0.050462383698505135,
      "grad_norm": 0.6000676155090332,
      "learning_rate": 0.00019035521985534716,
      "loss": 0.0963,
      "step": 10728
    },
    {
      "epoch": 0.050467087500117594,
      "grad_norm": 0.18449728190898895,
      "learning_rate": 0.00019035427687723368,
      "loss": 0.02,
      "step": 10729
    },
    {
      "epoch": 0.05047179130173006,
      "grad_norm": 3.1381661891937256,
      "learning_rate": 0.0001903533338991202,
      "loss": 0.3814,
      "step": 10730
    },
    {
      "epoch": 0.05047649510334252,
      "grad_norm": 0.12072925269603729,
      "learning_rate": 0.00019035239092100674,
      "loss": 0.0074,
      "step": 10731
    },
    {
      "epoch": 0.050481198904954984,
      "grad_norm": 2.564246892929077,
      "learning_rate": 0.00019035144794289326,
      "loss": 0.315,
      "step": 10732
    },
    {
      "epoch": 0.05048590270656745,
      "grad_norm": 1.8430434465408325,
      "learning_rate": 0.00019035050496477978,
      "loss": 0.3036,
      "step": 10733
    },
    {
      "epoch": 0.05049060650817991,
      "grad_norm": 2.3454880714416504,
      "learning_rate": 0.0001903495619866663,
      "loss": 0.1234,
      "step": 10734
    },
    {
      "epoch": 0.050495310309792374,
      "grad_norm": 2.0129499435424805,
      "learning_rate": 0.00019034861900855282,
      "loss": 0.2766,
      "step": 10735
    },
    {
      "epoch": 0.05050001411140484,
      "grad_norm": 2.7721104621887207,
      "learning_rate": 0.00019034767603043934,
      "loss": 0.6085,
      "step": 10736
    },
    {
      "epoch": 0.0505047179130173,
      "grad_norm": 1.970563530921936,
      "learning_rate": 0.00019034673305232586,
      "loss": 0.3116,
      "step": 10737
    },
    {
      "epoch": 0.050509421714629764,
      "grad_norm": 6.134072303771973,
      "learning_rate": 0.00019034579007421237,
      "loss": 0.6254,
      "step": 10738
    },
    {
      "epoch": 0.05051412551624223,
      "grad_norm": 0.40900132060050964,
      "learning_rate": 0.0001903448470960989,
      "loss": 0.0457,
      "step": 10739
    },
    {
      "epoch": 0.05051882931785469,
      "grad_norm": 2.6157262325286865,
      "learning_rate": 0.00019034390411798544,
      "loss": 0.4659,
      "step": 10740
    },
    {
      "epoch": 0.050523533119467154,
      "grad_norm": 0.16764435172080994,
      "learning_rate": 0.00019034296113987196,
      "loss": 0.0113,
      "step": 10741
    },
    {
      "epoch": 0.05052823692107962,
      "grad_norm": 0.95595782995224,
      "learning_rate": 0.00019034201816175848,
      "loss": 0.0734,
      "step": 10742
    },
    {
      "epoch": 0.05053294072269208,
      "grad_norm": 0.4134156405925751,
      "learning_rate": 0.000190341075183645,
      "loss": 0.0281,
      "step": 10743
    },
    {
      "epoch": 0.050537644524304544,
      "grad_norm": 1.4361777305603027,
      "learning_rate": 0.00019034013220553154,
      "loss": 0.3223,
      "step": 10744
    },
    {
      "epoch": 0.05054234832591701,
      "grad_norm": 0.4830542802810669,
      "learning_rate": 0.00019033918922741806,
      "loss": 0.0435,
      "step": 10745
    },
    {
      "epoch": 0.05054705212752947,
      "grad_norm": 0.31794843077659607,
      "learning_rate": 0.00019033824624930458,
      "loss": 0.0225,
      "step": 10746
    },
    {
      "epoch": 0.050551755929141934,
      "grad_norm": 1.1039849519729614,
      "learning_rate": 0.00019033730327119107,
      "loss": 0.1602,
      "step": 10747
    },
    {
      "epoch": 0.05055645973075439,
      "grad_norm": 0.44639888405799866,
      "learning_rate": 0.0001903363602930776,
      "loss": 0.0283,
      "step": 10748
    },
    {
      "epoch": 0.05056116353236686,
      "grad_norm": 7.233236789703369,
      "learning_rate": 0.00019033541731496413,
      "loss": 1.0678,
      "step": 10749
    },
    {
      "epoch": 0.050565867333979324,
      "grad_norm": 1.520830750465393,
      "learning_rate": 0.00019033447433685065,
      "loss": 0.2272,
      "step": 10750
    },
    {
      "epoch": 0.05057057113559178,
      "grad_norm": 9.839240074157715,
      "learning_rate": 0.00019033353135873717,
      "loss": 0.7084,
      "step": 10751
    },
    {
      "epoch": 0.05057527493720425,
      "grad_norm": 3.6734275817871094,
      "learning_rate": 0.0001903325883806237,
      "loss": 0.4933,
      "step": 10752
    },
    {
      "epoch": 0.050579978738816714,
      "grad_norm": 9.042610168457031,
      "learning_rate": 0.0001903316454025102,
      "loss": 0.844,
      "step": 10753
    },
    {
      "epoch": 0.05058468254042917,
      "grad_norm": 6.041717052459717,
      "learning_rate": 0.00019033070242439675,
      "loss": 0.8508,
      "step": 10754
    },
    {
      "epoch": 0.05058938634204164,
      "grad_norm": 3.8094542026519775,
      "learning_rate": 0.00019032975944628327,
      "loss": 0.4551,
      "step": 10755
    },
    {
      "epoch": 0.050594090143654104,
      "grad_norm": 2.0659875869750977,
      "learning_rate": 0.0001903288164681698,
      "loss": 0.1948,
      "step": 10756
    },
    {
      "epoch": 0.05059879394526656,
      "grad_norm": 0.6608893871307373,
      "learning_rate": 0.0001903278734900563,
      "loss": 0.0359,
      "step": 10757
    },
    {
      "epoch": 0.05060349774687903,
      "grad_norm": 2.8074426651000977,
      "learning_rate": 0.00019032693051194283,
      "loss": 0.4115,
      "step": 10758
    },
    {
      "epoch": 0.050608201548491494,
      "grad_norm": 3.979947805404663,
      "learning_rate": 0.00019032598753382935,
      "loss": 0.5531,
      "step": 10759
    },
    {
      "epoch": 0.05061290535010395,
      "grad_norm": 0.2904839813709259,
      "learning_rate": 0.00019032504455571587,
      "loss": 0.019,
      "step": 10760
    },
    {
      "epoch": 0.05061760915171642,
      "grad_norm": 3.042889356613159,
      "learning_rate": 0.00019032410157760238,
      "loss": 0.3378,
      "step": 10761
    },
    {
      "epoch": 0.050622312953328884,
      "grad_norm": 2.30342960357666,
      "learning_rate": 0.0001903231585994889,
      "loss": 0.4019,
      "step": 10762
    },
    {
      "epoch": 0.05062701675494134,
      "grad_norm": 3.337946891784668,
      "learning_rate": 0.00019032221562137545,
      "loss": 0.2328,
      "step": 10763
    },
    {
      "epoch": 0.05063172055655381,
      "grad_norm": 4.813737869262695,
      "learning_rate": 0.00019032127264326197,
      "loss": 1.2097,
      "step": 10764
    },
    {
      "epoch": 0.05063642435816627,
      "grad_norm": 2.7446508407592773,
      "learning_rate": 0.00019032032966514849,
      "loss": 0.4413,
      "step": 10765
    },
    {
      "epoch": 0.05064112815977873,
      "grad_norm": 3.816422700881958,
      "learning_rate": 0.000190319386687035,
      "loss": 0.7291,
      "step": 10766
    },
    {
      "epoch": 0.0506458319613912,
      "grad_norm": 1.2082592248916626,
      "learning_rate": 0.00019031844370892152,
      "loss": 0.1167,
      "step": 10767
    },
    {
      "epoch": 0.05065053576300366,
      "grad_norm": 3.5691545009613037,
      "learning_rate": 0.00019031750073080804,
      "loss": 0.5416,
      "step": 10768
    },
    {
      "epoch": 0.05065523956461612,
      "grad_norm": 2.0675389766693115,
      "learning_rate": 0.00019031655775269456,
      "loss": 0.2576,
      "step": 10769
    },
    {
      "epoch": 0.05065994336622859,
      "grad_norm": 3.533179998397827,
      "learning_rate": 0.00019031561477458108,
      "loss": 0.3947,
      "step": 10770
    },
    {
      "epoch": 0.05066464716784105,
      "grad_norm": 0.7141983509063721,
      "learning_rate": 0.0001903146717964676,
      "loss": 0.1095,
      "step": 10771
    },
    {
      "epoch": 0.05066935096945351,
      "grad_norm": 1.2205240726470947,
      "learning_rate": 0.00019031372881835414,
      "loss": 0.1317,
      "step": 10772
    },
    {
      "epoch": 0.05067405477106598,
      "grad_norm": 3.0198934078216553,
      "learning_rate": 0.00019031278584024066,
      "loss": 0.4398,
      "step": 10773
    },
    {
      "epoch": 0.05067875857267844,
      "grad_norm": 1.4563966989517212,
      "learning_rate": 0.00019031184286212718,
      "loss": 0.2531,
      "step": 10774
    },
    {
      "epoch": 0.0506834623742909,
      "grad_norm": 0.8262953758239746,
      "learning_rate": 0.0001903108998840137,
      "loss": 0.1632,
      "step": 10775
    },
    {
      "epoch": 0.05068816617590337,
      "grad_norm": 0.9944761991500854,
      "learning_rate": 0.00019030995690590024,
      "loss": 0.0603,
      "step": 10776
    },
    {
      "epoch": 0.05069286997751583,
      "grad_norm": 3.4085628986358643,
      "learning_rate": 0.00019030901392778676,
      "loss": 0.3287,
      "step": 10777
    },
    {
      "epoch": 0.05069757377912829,
      "grad_norm": 1.083621859550476,
      "learning_rate": 0.00019030807094967326,
      "loss": 0.1113,
      "step": 10778
    },
    {
      "epoch": 0.05070227758074076,
      "grad_norm": 2.218501567840576,
      "learning_rate": 0.00019030712797155977,
      "loss": 0.3165,
      "step": 10779
    },
    {
      "epoch": 0.05070698138235322,
      "grad_norm": 1.1534253358840942,
      "learning_rate": 0.0001903061849934463,
      "loss": 0.2007,
      "step": 10780
    },
    {
      "epoch": 0.05071168518396568,
      "grad_norm": 0.7385878562927246,
      "learning_rate": 0.00019030524201533284,
      "loss": 0.0907,
      "step": 10781
    },
    {
      "epoch": 0.05071638898557814,
      "grad_norm": 2.528212308883667,
      "learning_rate": 0.00019030429903721936,
      "loss": 0.5116,
      "step": 10782
    },
    {
      "epoch": 0.05072109278719061,
      "grad_norm": 0.8575196862220764,
      "learning_rate": 0.00019030335605910588,
      "loss": 0.1602,
      "step": 10783
    },
    {
      "epoch": 0.05072579658880307,
      "grad_norm": 1.5446339845657349,
      "learning_rate": 0.0001903024130809924,
      "loss": 0.1886,
      "step": 10784
    },
    {
      "epoch": 0.05073050039041553,
      "grad_norm": 1.2681200504302979,
      "learning_rate": 0.00019030147010287894,
      "loss": 0.1131,
      "step": 10785
    },
    {
      "epoch": 0.050735204192028,
      "grad_norm": 1.5620445013046265,
      "learning_rate": 0.00019030052712476546,
      "loss": 0.2254,
      "step": 10786
    },
    {
      "epoch": 0.05073990799364046,
      "grad_norm": 2.756075382232666,
      "learning_rate": 0.00019029958414665198,
      "loss": 0.3038,
      "step": 10787
    },
    {
      "epoch": 0.05074461179525292,
      "grad_norm": 2.9232242107391357,
      "learning_rate": 0.0001902986411685385,
      "loss": 0.4833,
      "step": 10788
    },
    {
      "epoch": 0.05074931559686539,
      "grad_norm": 4.04111909866333,
      "learning_rate": 0.000190297698190425,
      "loss": 0.9644,
      "step": 10789
    },
    {
      "epoch": 0.05075401939847785,
      "grad_norm": 0.7972691655158997,
      "learning_rate": 0.00019029675521231153,
      "loss": 0.0484,
      "step": 10790
    },
    {
      "epoch": 0.05075872320009031,
      "grad_norm": 0.6182138919830322,
      "learning_rate": 0.00019029581223419805,
      "loss": 0.067,
      "step": 10791
    },
    {
      "epoch": 0.05076342700170278,
      "grad_norm": 2.2310032844543457,
      "learning_rate": 0.00019029486925608457,
      "loss": 0.4688,
      "step": 10792
    },
    {
      "epoch": 0.05076813080331524,
      "grad_norm": 1.7093180418014526,
      "learning_rate": 0.0001902939262779711,
      "loss": 0.3152,
      "step": 10793
    },
    {
      "epoch": 0.0507728346049277,
      "grad_norm": 0.9680871963500977,
      "learning_rate": 0.00019029298329985763,
      "loss": 0.0772,
      "step": 10794
    },
    {
      "epoch": 0.05077753840654017,
      "grad_norm": 0.9346885681152344,
      "learning_rate": 0.00019029204032174415,
      "loss": 0.1498,
      "step": 10795
    },
    {
      "epoch": 0.05078224220815263,
      "grad_norm": 2.32948899269104,
      "learning_rate": 0.00019029109734363067,
      "loss": 0.2824,
      "step": 10796
    },
    {
      "epoch": 0.05078694600976509,
      "grad_norm": 2.23899507522583,
      "learning_rate": 0.0001902901543655172,
      "loss": 0.2333,
      "step": 10797
    },
    {
      "epoch": 0.05079164981137756,
      "grad_norm": 0.37538036704063416,
      "learning_rate": 0.0001902892113874037,
      "loss": 0.0262,
      "step": 10798
    },
    {
      "epoch": 0.050796353612990015,
      "grad_norm": 1.1438359022140503,
      "learning_rate": 0.00019028826840929023,
      "loss": 0.1446,
      "step": 10799
    },
    {
      "epoch": 0.05080105741460248,
      "grad_norm": 1.9053524732589722,
      "learning_rate": 0.00019028732543117675,
      "loss": 0.0508,
      "step": 10800
    },
    {
      "epoch": 0.05080576121621495,
      "grad_norm": 1.2317883968353271,
      "learning_rate": 0.00019028638245306327,
      "loss": 0.0751,
      "step": 10801
    },
    {
      "epoch": 0.050810465017827405,
      "grad_norm": 3.9063937664031982,
      "learning_rate": 0.00019028543947494978,
      "loss": 0.5803,
      "step": 10802
    },
    {
      "epoch": 0.05081516881943987,
      "grad_norm": 4.776498794555664,
      "learning_rate": 0.00019028449649683633,
      "loss": 0.6504,
      "step": 10803
    },
    {
      "epoch": 0.05081987262105234,
      "grad_norm": 1.8001606464385986,
      "learning_rate": 0.00019028355351872285,
      "loss": 0.1556,
      "step": 10804
    },
    {
      "epoch": 0.050824576422664795,
      "grad_norm": 2.9455177783966064,
      "learning_rate": 0.00019028261054060937,
      "loss": 0.2139,
      "step": 10805
    },
    {
      "epoch": 0.05082928022427726,
      "grad_norm": 3.0888803005218506,
      "learning_rate": 0.00019028166756249589,
      "loss": 0.2616,
      "step": 10806
    },
    {
      "epoch": 0.05083398402588973,
      "grad_norm": 3.5228590965270996,
      "learning_rate": 0.0001902807245843824,
      "loss": 0.4265,
      "step": 10807
    },
    {
      "epoch": 0.050838687827502185,
      "grad_norm": 0.9365473985671997,
      "learning_rate": 0.00019027978160626892,
      "loss": 0.1254,
      "step": 10808
    },
    {
      "epoch": 0.05084339162911465,
      "grad_norm": 3.374483346939087,
      "learning_rate": 0.00019027883862815544,
      "loss": 0.0711,
      "step": 10809
    },
    {
      "epoch": 0.05084809543072712,
      "grad_norm": 3.124725341796875,
      "learning_rate": 0.00019027789565004196,
      "loss": 0.3918,
      "step": 10810
    },
    {
      "epoch": 0.050852799232339575,
      "grad_norm": 1.3076688051223755,
      "learning_rate": 0.00019027695267192848,
      "loss": 0.1176,
      "step": 10811
    },
    {
      "epoch": 0.05085750303395204,
      "grad_norm": 3.5140974521636963,
      "learning_rate": 0.000190276009693815,
      "loss": 0.2844,
      "step": 10812
    },
    {
      "epoch": 0.05086220683556451,
      "grad_norm": 1.764976143836975,
      "learning_rate": 0.00019027506671570154,
      "loss": 0.1166,
      "step": 10813
    },
    {
      "epoch": 0.050866910637176965,
      "grad_norm": 4.611593246459961,
      "learning_rate": 0.00019027412373758806,
      "loss": 0.6789,
      "step": 10814
    },
    {
      "epoch": 0.05087161443878943,
      "grad_norm": 2.3228001594543457,
      "learning_rate": 0.00019027318075947458,
      "loss": 0.3086,
      "step": 10815
    },
    {
      "epoch": 0.05087631824040189,
      "grad_norm": 2.2267141342163086,
      "learning_rate": 0.0001902722377813611,
      "loss": 0.1833,
      "step": 10816
    },
    {
      "epoch": 0.050881022042014355,
      "grad_norm": 2.7589235305786133,
      "learning_rate": 0.00019027129480324764,
      "loss": 0.2429,
      "step": 10817
    },
    {
      "epoch": 0.05088572584362682,
      "grad_norm": 3.535665273666382,
      "learning_rate": 0.00019027035182513416,
      "loss": 0.6705,
      "step": 10818
    },
    {
      "epoch": 0.05089042964523928,
      "grad_norm": 2.946927547454834,
      "learning_rate": 0.00019026940884702068,
      "loss": 0.2014,
      "step": 10819
    },
    {
      "epoch": 0.050895133446851745,
      "grad_norm": 1.2658692598342896,
      "learning_rate": 0.00019026846586890717,
      "loss": 0.1073,
      "step": 10820
    },
    {
      "epoch": 0.05089983724846421,
      "grad_norm": 6.242615222930908,
      "learning_rate": 0.0001902675228907937,
      "loss": 0.6512,
      "step": 10821
    },
    {
      "epoch": 0.05090454105007667,
      "grad_norm": 0.6154091358184814,
      "learning_rate": 0.00019026657991268024,
      "loss": 0.0668,
      "step": 10822
    },
    {
      "epoch": 0.050909244851689135,
      "grad_norm": 3.5392274856567383,
      "learning_rate": 0.00019026563693456676,
      "loss": 0.4757,
      "step": 10823
    },
    {
      "epoch": 0.0509139486533016,
      "grad_norm": 2.673550605773926,
      "learning_rate": 0.00019026469395645328,
      "loss": 0.3115,
      "step": 10824
    },
    {
      "epoch": 0.05091865245491406,
      "grad_norm": 3.7822530269622803,
      "learning_rate": 0.0001902637509783398,
      "loss": 0.4799,
      "step": 10825
    },
    {
      "epoch": 0.050923356256526525,
      "grad_norm": 1.7959845066070557,
      "learning_rate": 0.00019026280800022634,
      "loss": 0.1746,
      "step": 10826
    },
    {
      "epoch": 0.05092806005813899,
      "grad_norm": 0.8927867412567139,
      "learning_rate": 0.00019026186502211286,
      "loss": 0.0476,
      "step": 10827
    },
    {
      "epoch": 0.05093276385975145,
      "grad_norm": 2.090456962585449,
      "learning_rate": 0.00019026092204399938,
      "loss": 0.1774,
      "step": 10828
    },
    {
      "epoch": 0.050937467661363915,
      "grad_norm": 3.593506097793579,
      "learning_rate": 0.0001902599790658859,
      "loss": 0.4345,
      "step": 10829
    },
    {
      "epoch": 0.05094217146297638,
      "grad_norm": 1.8260166645050049,
      "learning_rate": 0.00019025903608777241,
      "loss": 0.1211,
      "step": 10830
    },
    {
      "epoch": 0.05094687526458884,
      "grad_norm": 3.8951311111450195,
      "learning_rate": 0.00019025809310965893,
      "loss": 0.2965,
      "step": 10831
    },
    {
      "epoch": 0.050951579066201305,
      "grad_norm": 2.9762485027313232,
      "learning_rate": 0.00019025715013154545,
      "loss": 0.3305,
      "step": 10832
    },
    {
      "epoch": 0.050956282867813764,
      "grad_norm": 2.626943349838257,
      "learning_rate": 0.00019025620715343197,
      "loss": 0.3636,
      "step": 10833
    },
    {
      "epoch": 0.05096098666942623,
      "grad_norm": 1.6913272142410278,
      "learning_rate": 0.0001902552641753185,
      "loss": 0.2397,
      "step": 10834
    },
    {
      "epoch": 0.050965690471038695,
      "grad_norm": 3.792346477508545,
      "learning_rate": 0.00019025432119720503,
      "loss": 0.5989,
      "step": 10835
    },
    {
      "epoch": 0.050970394272651154,
      "grad_norm": 2.779083490371704,
      "learning_rate": 0.00019025337821909155,
      "loss": 0.3827,
      "step": 10836
    },
    {
      "epoch": 0.05097509807426362,
      "grad_norm": 5.8017802238464355,
      "learning_rate": 0.00019025243524097807,
      "loss": 0.468,
      "step": 10837
    },
    {
      "epoch": 0.050979801875876085,
      "grad_norm": 1.0867011547088623,
      "learning_rate": 0.0001902514922628646,
      "loss": 0.0687,
      "step": 10838
    },
    {
      "epoch": 0.050984505677488544,
      "grad_norm": 5.680191516876221,
      "learning_rate": 0.0001902505492847511,
      "loss": 0.2532,
      "step": 10839
    },
    {
      "epoch": 0.05098920947910101,
      "grad_norm": 6.036440372467041,
      "learning_rate": 0.00019024960630663763,
      "loss": 0.4725,
      "step": 10840
    },
    {
      "epoch": 0.050993913280713475,
      "grad_norm": 0.609875500202179,
      "learning_rate": 0.00019024866332852415,
      "loss": 0.0383,
      "step": 10841
    },
    {
      "epoch": 0.050998617082325934,
      "grad_norm": 2.708069086074829,
      "learning_rate": 0.00019024772035041067,
      "loss": 0.4472,
      "step": 10842
    },
    {
      "epoch": 0.0510033208839384,
      "grad_norm": 2.146390914916992,
      "learning_rate": 0.00019024677737229718,
      "loss": 0.1872,
      "step": 10843
    },
    {
      "epoch": 0.051008024685550865,
      "grad_norm": 3.5965163707733154,
      "learning_rate": 0.00019024583439418373,
      "loss": 0.1974,
      "step": 10844
    },
    {
      "epoch": 0.051012728487163324,
      "grad_norm": 5.310114860534668,
      "learning_rate": 0.00019024489141607025,
      "loss": 0.1754,
      "step": 10845
    },
    {
      "epoch": 0.05101743228877579,
      "grad_norm": 4.844920635223389,
      "learning_rate": 0.00019024394843795677,
      "loss": 0.5806,
      "step": 10846
    },
    {
      "epoch": 0.051022136090388255,
      "grad_norm": 0.5239397883415222,
      "learning_rate": 0.00019024300545984329,
      "loss": 0.0321,
      "step": 10847
    },
    {
      "epoch": 0.051026839892000714,
      "grad_norm": 0.569327175617218,
      "learning_rate": 0.0001902420624817298,
      "loss": 0.0483,
      "step": 10848
    },
    {
      "epoch": 0.05103154369361318,
      "grad_norm": 5.113495349884033,
      "learning_rate": 0.00019024111950361635,
      "loss": 0.3851,
      "step": 10849
    },
    {
      "epoch": 0.05103624749522564,
      "grad_norm": 1.7659637928009033,
      "learning_rate": 0.00019024017652550287,
      "loss": 0.147,
      "step": 10850
    },
    {
      "epoch": 0.051040951296838104,
      "grad_norm": 0.7637147307395935,
      "learning_rate": 0.00019023923354738936,
      "loss": 0.0224,
      "step": 10851
    },
    {
      "epoch": 0.05104565509845057,
      "grad_norm": 0.1668798327445984,
      "learning_rate": 0.00019023829056927588,
      "loss": 0.0089,
      "step": 10852
    },
    {
      "epoch": 0.05105035890006303,
      "grad_norm": 1.042984962463379,
      "learning_rate": 0.00019023734759116242,
      "loss": 0.0587,
      "step": 10853
    },
    {
      "epoch": 0.051055062701675494,
      "grad_norm": 1.215019702911377,
      "learning_rate": 0.00019023640461304894,
      "loss": 0.0636,
      "step": 10854
    },
    {
      "epoch": 0.05105976650328796,
      "grad_norm": 3.9460062980651855,
      "learning_rate": 0.00019023546163493546,
      "loss": 0.3101,
      "step": 10855
    },
    {
      "epoch": 0.05106447030490042,
      "grad_norm": 0.6974589824676514,
      "learning_rate": 0.00019023451865682198,
      "loss": 0.0412,
      "step": 10856
    },
    {
      "epoch": 0.051069174106512884,
      "grad_norm": 2.6989011764526367,
      "learning_rate": 0.0001902335756787085,
      "loss": 0.2101,
      "step": 10857
    },
    {
      "epoch": 0.05107387790812535,
      "grad_norm": 2.82686710357666,
      "learning_rate": 0.00019023263270059504,
      "loss": 0.3808,
      "step": 10858
    },
    {
      "epoch": 0.05107858170973781,
      "grad_norm": 3.1901419162750244,
      "learning_rate": 0.00019023168972248156,
      "loss": 0.4685,
      "step": 10859
    },
    {
      "epoch": 0.051083285511350274,
      "grad_norm": 4.464807510375977,
      "learning_rate": 0.00019023074674436808,
      "loss": 0.3139,
      "step": 10860
    },
    {
      "epoch": 0.05108798931296274,
      "grad_norm": 2.6924946308135986,
      "learning_rate": 0.0001902298037662546,
      "loss": 0.3391,
      "step": 10861
    },
    {
      "epoch": 0.0510926931145752,
      "grad_norm": 1.6427251100540161,
      "learning_rate": 0.0001902288607881411,
      "loss": 0.125,
      "step": 10862
    },
    {
      "epoch": 0.051097396916187664,
      "grad_norm": 4.290607452392578,
      "learning_rate": 0.00019022791781002764,
      "loss": 0.7725,
      "step": 10863
    },
    {
      "epoch": 0.05110210071780013,
      "grad_norm": 5.516851425170898,
      "learning_rate": 0.00019022697483191416,
      "loss": 0.412,
      "step": 10864
    },
    {
      "epoch": 0.05110680451941259,
      "grad_norm": 0.9077186584472656,
      "learning_rate": 0.00019022603185380068,
      "loss": 0.0307,
      "step": 10865
    },
    {
      "epoch": 0.051111508321025054,
      "grad_norm": 1.4105207920074463,
      "learning_rate": 0.0001902250888756872,
      "loss": 0.0637,
      "step": 10866
    },
    {
      "epoch": 0.05111621212263751,
      "grad_norm": 4.7583417892456055,
      "learning_rate": 0.00019022414589757374,
      "loss": 0.8083,
      "step": 10867
    },
    {
      "epoch": 0.05112091592424998,
      "grad_norm": 3.8646509647369385,
      "learning_rate": 0.00019022320291946026,
      "loss": 0.5512,
      "step": 10868
    },
    {
      "epoch": 0.051125619725862444,
      "grad_norm": 2.37917423248291,
      "learning_rate": 0.00019022225994134678,
      "loss": 0.1749,
      "step": 10869
    },
    {
      "epoch": 0.0511303235274749,
      "grad_norm": 0.470740407705307,
      "learning_rate": 0.0001902213169632333,
      "loss": 0.0183,
      "step": 10870
    },
    {
      "epoch": 0.05113502732908737,
      "grad_norm": 5.218606948852539,
      "learning_rate": 0.00019022037398511981,
      "loss": 0.731,
      "step": 10871
    },
    {
      "epoch": 0.051139731130699834,
      "grad_norm": 5.532425880432129,
      "learning_rate": 0.00019021943100700633,
      "loss": 0.4118,
      "step": 10872
    },
    {
      "epoch": 0.05114443493231229,
      "grad_norm": 4.125880718231201,
      "learning_rate": 0.00019021848802889285,
      "loss": 0.1892,
      "step": 10873
    },
    {
      "epoch": 0.05114913873392476,
      "grad_norm": 0.9451213479042053,
      "learning_rate": 0.00019021754505077937,
      "loss": 0.0641,
      "step": 10874
    },
    {
      "epoch": 0.051153842535537224,
      "grad_norm": 5.961754322052002,
      "learning_rate": 0.0001902166020726659,
      "loss": 0.7976,
      "step": 10875
    },
    {
      "epoch": 0.05115854633714968,
      "grad_norm": 0.8867175579071045,
      "learning_rate": 0.00019021565909455243,
      "loss": 0.0699,
      "step": 10876
    },
    {
      "epoch": 0.05116325013876215,
      "grad_norm": 2.1901445388793945,
      "learning_rate": 0.00019021471611643895,
      "loss": 0.1281,
      "step": 10877
    },
    {
      "epoch": 0.051167953940374614,
      "grad_norm": 5.907530784606934,
      "learning_rate": 0.00019021377313832547,
      "loss": 0.9037,
      "step": 10878
    },
    {
      "epoch": 0.05117265774198707,
      "grad_norm": 0.46406519412994385,
      "learning_rate": 0.000190212830160212,
      "loss": 0.0243,
      "step": 10879
    },
    {
      "epoch": 0.05117736154359954,
      "grad_norm": 2.0170319080352783,
      "learning_rate": 0.0001902118871820985,
      "loss": 0.2093,
      "step": 10880
    },
    {
      "epoch": 0.051182065345212004,
      "grad_norm": 3.682374954223633,
      "learning_rate": 0.00019021094420398506,
      "loss": 0.4658,
      "step": 10881
    },
    {
      "epoch": 0.05118676914682446,
      "grad_norm": 0.6904790997505188,
      "learning_rate": 0.00019021000122587155,
      "loss": 0.1018,
      "step": 10882
    },
    {
      "epoch": 0.05119147294843693,
      "grad_norm": 1.42356276512146,
      "learning_rate": 0.00019020905824775807,
      "loss": 0.1324,
      "step": 10883
    },
    {
      "epoch": 0.05119617675004939,
      "grad_norm": 0.2635509669780731,
      "learning_rate": 0.00019020811526964458,
      "loss": 0.0153,
      "step": 10884
    },
    {
      "epoch": 0.05120088055166185,
      "grad_norm": 6.092578411102295,
      "learning_rate": 0.00019020717229153113,
      "loss": 0.8281,
      "step": 10885
    },
    {
      "epoch": 0.05120558435327432,
      "grad_norm": 3.5834338665008545,
      "learning_rate": 0.00019020622931341765,
      "loss": 0.599,
      "step": 10886
    },
    {
      "epoch": 0.05121028815488678,
      "grad_norm": 3.0110647678375244,
      "learning_rate": 0.00019020528633530417,
      "loss": 0.329,
      "step": 10887
    },
    {
      "epoch": 0.05121499195649924,
      "grad_norm": 1.4462929964065552,
      "learning_rate": 0.00019020434335719069,
      "loss": 0.1915,
      "step": 10888
    },
    {
      "epoch": 0.05121969575811171,
      "grad_norm": 1.7161844968795776,
      "learning_rate": 0.0001902034003790772,
      "loss": 0.1379,
      "step": 10889
    },
    {
      "epoch": 0.05122439955972417,
      "grad_norm": 2.3054370880126953,
      "learning_rate": 0.00019020245740096375,
      "loss": 0.1516,
      "step": 10890
    },
    {
      "epoch": 0.05122910336133663,
      "grad_norm": 1.429623007774353,
      "learning_rate": 0.00019020151442285027,
      "loss": 0.0733,
      "step": 10891
    },
    {
      "epoch": 0.0512338071629491,
      "grad_norm": 5.4409074783325195,
      "learning_rate": 0.0001902005714447368,
      "loss": 0.5732,
      "step": 10892
    },
    {
      "epoch": 0.05123851096456156,
      "grad_norm": 3.3092563152313232,
      "learning_rate": 0.00019019962846662328,
      "loss": 0.7499,
      "step": 10893
    },
    {
      "epoch": 0.05124321476617402,
      "grad_norm": 1.8975069522857666,
      "learning_rate": 0.00019019868548850982,
      "loss": 0.2916,
      "step": 10894
    },
    {
      "epoch": 0.05124791856778649,
      "grad_norm": 2.3332529067993164,
      "learning_rate": 0.00019019774251039634,
      "loss": 0.1407,
      "step": 10895
    },
    {
      "epoch": 0.05125262236939895,
      "grad_norm": 3.2734694480895996,
      "learning_rate": 0.00019019679953228286,
      "loss": 0.1863,
      "step": 10896
    },
    {
      "epoch": 0.05125732617101141,
      "grad_norm": 1.814695119857788,
      "learning_rate": 0.00019019585655416938,
      "loss": 0.174,
      "step": 10897
    },
    {
      "epoch": 0.05126202997262388,
      "grad_norm": 3.379995822906494,
      "learning_rate": 0.0001901949135760559,
      "loss": 0.4785,
      "step": 10898
    },
    {
      "epoch": 0.05126673377423634,
      "grad_norm": 2.162442922592163,
      "learning_rate": 0.00019019397059794244,
      "loss": 0.2783,
      "step": 10899
    },
    {
      "epoch": 0.0512714375758488,
      "grad_norm": 1.424213171005249,
      "learning_rate": 0.00019019302761982896,
      "loss": 0.1204,
      "step": 10900
    },
    {
      "epoch": 0.05127614137746126,
      "grad_norm": 7.348347187042236,
      "learning_rate": 0.00019019208464171548,
      "loss": 0.7395,
      "step": 10901
    },
    {
      "epoch": 0.05128084517907373,
      "grad_norm": 4.5567307472229,
      "learning_rate": 0.000190191141663602,
      "loss": 0.5176,
      "step": 10902
    },
    {
      "epoch": 0.05128554898068619,
      "grad_norm": 10.673945426940918,
      "learning_rate": 0.00019019019868548852,
      "loss": 0.4463,
      "step": 10903
    },
    {
      "epoch": 0.05129025278229865,
      "grad_norm": 2.1673290729522705,
      "learning_rate": 0.00019018925570737504,
      "loss": 0.1277,
      "step": 10904
    },
    {
      "epoch": 0.05129495658391112,
      "grad_norm": 2.060001850128174,
      "learning_rate": 0.00019018831272926156,
      "loss": 0.1434,
      "step": 10905
    },
    {
      "epoch": 0.05129966038552358,
      "grad_norm": 3.5672619342803955,
      "learning_rate": 0.00019018736975114808,
      "loss": 0.319,
      "step": 10906
    },
    {
      "epoch": 0.05130436418713604,
      "grad_norm": 8.94815731048584,
      "learning_rate": 0.0001901864267730346,
      "loss": 0.5762,
      "step": 10907
    },
    {
      "epoch": 0.05130906798874851,
      "grad_norm": 5.423834323883057,
      "learning_rate": 0.00019018548379492114,
      "loss": 0.1573,
      "step": 10908
    },
    {
      "epoch": 0.05131377179036097,
      "grad_norm": 7.386592388153076,
      "learning_rate": 0.00019018454081680766,
      "loss": 0.2831,
      "step": 10909
    },
    {
      "epoch": 0.05131847559197343,
      "grad_norm": 16.21215057373047,
      "learning_rate": 0.00019018359783869418,
      "loss": 0.4625,
      "step": 10910
    },
    {
      "epoch": 0.0513231793935859,
      "grad_norm": 10.193909645080566,
      "learning_rate": 0.0001901826548605807,
      "loss": 0.3568,
      "step": 10911
    },
    {
      "epoch": 0.05132788319519836,
      "grad_norm": 8.49482536315918,
      "learning_rate": 0.00019018171188246721,
      "loss": 0.3256,
      "step": 10912
    },
    {
      "epoch": 0.05133258699681082,
      "grad_norm": 16.77203369140625,
      "learning_rate": 0.00019018076890435373,
      "loss": 0.4624,
      "step": 10913
    },
    {
      "epoch": 0.051337290798423287,
      "grad_norm": 3.0691680908203125,
      "learning_rate": 0.00019017982592624025,
      "loss": 0.4087,
      "step": 10914
    },
    {
      "epoch": 0.05134199460003575,
      "grad_norm": 2.2967734336853027,
      "learning_rate": 0.00019017888294812677,
      "loss": 0.2334,
      "step": 10915
    },
    {
      "epoch": 0.05134669840164821,
      "grad_norm": 11.760533332824707,
      "learning_rate": 0.0001901779399700133,
      "loss": 0.2665,
      "step": 10916
    },
    {
      "epoch": 0.051351402203260677,
      "grad_norm": 30.034149169921875,
      "learning_rate": 0.00019017699699189983,
      "loss": 0.5013,
      "step": 10917
    },
    {
      "epoch": 0.051356106004873135,
      "grad_norm": 6.237149238586426,
      "learning_rate": 0.00019017605401378635,
      "loss": 0.2747,
      "step": 10918
    },
    {
      "epoch": 0.0513608098064856,
      "grad_norm": 19.00081443786621,
      "learning_rate": 0.00019017511103567287,
      "loss": 0.2023,
      "step": 10919
    },
    {
      "epoch": 0.051365513608098066,
      "grad_norm": 4.491031169891357,
      "learning_rate": 0.0001901741680575594,
      "loss": 0.1121,
      "step": 10920
    },
    {
      "epoch": 0.051370217409710525,
      "grad_norm": 2.5574607849121094,
      "learning_rate": 0.0001901732250794459,
      "loss": 0.1464,
      "step": 10921
    },
    {
      "epoch": 0.05137492121132299,
      "grad_norm": 6.895098686218262,
      "learning_rate": 0.00019017228210133246,
      "loss": 0.4311,
      "step": 10922
    },
    {
      "epoch": 0.051379625012935456,
      "grad_norm": 3.6163387298583984,
      "learning_rate": 0.00019017133912321897,
      "loss": 0.2405,
      "step": 10923
    },
    {
      "epoch": 0.051384328814547915,
      "grad_norm": 5.138922691345215,
      "learning_rate": 0.00019017039614510547,
      "loss": 0.18,
      "step": 10924
    },
    {
      "epoch": 0.05138903261616038,
      "grad_norm": 1.4961742162704468,
      "learning_rate": 0.00019016945316699198,
      "loss": 0.0892,
      "step": 10925
    },
    {
      "epoch": 0.051393736417772846,
      "grad_norm": 3.1497602462768555,
      "learning_rate": 0.00019016851018887853,
      "loss": 0.1574,
      "step": 10926
    },
    {
      "epoch": 0.051398440219385305,
      "grad_norm": 2.119234323501587,
      "learning_rate": 0.00019016756721076505,
      "loss": 0.2213,
      "step": 10927
    },
    {
      "epoch": 0.05140314402099777,
      "grad_norm": 4.277467250823975,
      "learning_rate": 0.00019016662423265157,
      "loss": 0.2629,
      "step": 10928
    },
    {
      "epoch": 0.051407847822610236,
      "grad_norm": 5.463613510131836,
      "learning_rate": 0.00019016568125453809,
      "loss": 1.0477,
      "step": 10929
    },
    {
      "epoch": 0.051412551624222695,
      "grad_norm": 0.797350287437439,
      "learning_rate": 0.0001901647382764246,
      "loss": 0.063,
      "step": 10930
    },
    {
      "epoch": 0.05141725542583516,
      "grad_norm": 5.1688361167907715,
      "learning_rate": 0.00019016379529831115,
      "loss": 0.5615,
      "step": 10931
    },
    {
      "epoch": 0.051421959227447626,
      "grad_norm": 1.288062572479248,
      "learning_rate": 0.00019016285232019767,
      "loss": 0.0946,
      "step": 10932
    },
    {
      "epoch": 0.051426663029060085,
      "grad_norm": 0.4333306550979614,
      "learning_rate": 0.0001901619093420842,
      "loss": 0.0262,
      "step": 10933
    },
    {
      "epoch": 0.05143136683067255,
      "grad_norm": 4.089451313018799,
      "learning_rate": 0.0001901609663639707,
      "loss": 0.3377,
      "step": 10934
    },
    {
      "epoch": 0.05143607063228501,
      "grad_norm": 0.7629790306091309,
      "learning_rate": 0.00019016002338585722,
      "loss": 0.0465,
      "step": 10935
    },
    {
      "epoch": 0.051440774433897475,
      "grad_norm": 5.809053897857666,
      "learning_rate": 0.00019015908040774374,
      "loss": 0.7325,
      "step": 10936
    },
    {
      "epoch": 0.05144547823550994,
      "grad_norm": 2.335330009460449,
      "learning_rate": 0.00019015813742963026,
      "loss": 0.264,
      "step": 10937
    },
    {
      "epoch": 0.0514501820371224,
      "grad_norm": 2.2246527671813965,
      "learning_rate": 0.00019015719445151678,
      "loss": 0.3322,
      "step": 10938
    },
    {
      "epoch": 0.051454885838734865,
      "grad_norm": 0.5327059030532837,
      "learning_rate": 0.0001901562514734033,
      "loss": 0.0404,
      "step": 10939
    },
    {
      "epoch": 0.05145958964034733,
      "grad_norm": 1.59390127658844,
      "learning_rate": 0.00019015530849528984,
      "loss": 0.1428,
      "step": 10940
    },
    {
      "epoch": 0.05146429344195979,
      "grad_norm": 3.1371419429779053,
      "learning_rate": 0.00019015436551717636,
      "loss": 0.4614,
      "step": 10941
    },
    {
      "epoch": 0.051468997243572255,
      "grad_norm": 2.4274847507476807,
      "learning_rate": 0.00019015342253906288,
      "loss": 0.1922,
      "step": 10942
    },
    {
      "epoch": 0.05147370104518472,
      "grad_norm": 3.6306700706481934,
      "learning_rate": 0.0001901524795609494,
      "loss": 0.3354,
      "step": 10943
    },
    {
      "epoch": 0.05147840484679718,
      "grad_norm": 3.3439035415649414,
      "learning_rate": 0.00019015153658283592,
      "loss": 0.3931,
      "step": 10944
    },
    {
      "epoch": 0.051483108648409645,
      "grad_norm": 3.422462224960327,
      "learning_rate": 0.00019015059360472244,
      "loss": 0.672,
      "step": 10945
    },
    {
      "epoch": 0.05148781245002211,
      "grad_norm": 1.5211836099624634,
      "learning_rate": 0.00019014965062660896,
      "loss": 0.2922,
      "step": 10946
    },
    {
      "epoch": 0.05149251625163457,
      "grad_norm": 3.394850015640259,
      "learning_rate": 0.00019014870764849548,
      "loss": 0.4051,
      "step": 10947
    },
    {
      "epoch": 0.051497220053247035,
      "grad_norm": 1.1303859949111938,
      "learning_rate": 0.000190147764670382,
      "loss": 0.082,
      "step": 10948
    },
    {
      "epoch": 0.0515019238548595,
      "grad_norm": 6.66648006439209,
      "learning_rate": 0.00019014682169226854,
      "loss": 0.6475,
      "step": 10949
    },
    {
      "epoch": 0.05150662765647196,
      "grad_norm": 2.8027851581573486,
      "learning_rate": 0.00019014587871415506,
      "loss": 0.2066,
      "step": 10950
    },
    {
      "epoch": 0.051511331458084425,
      "grad_norm": 1.1014434099197388,
      "learning_rate": 0.00019014493573604158,
      "loss": 0.1048,
      "step": 10951
    },
    {
      "epoch": 0.051516035259696884,
      "grad_norm": 3.4359097480773926,
      "learning_rate": 0.0001901439927579281,
      "loss": 0.7325,
      "step": 10952
    },
    {
      "epoch": 0.05152073906130935,
      "grad_norm": 0.5377479195594788,
      "learning_rate": 0.00019014304977981464,
      "loss": 0.0455,
      "step": 10953
    },
    {
      "epoch": 0.051525442862921815,
      "grad_norm": 2.0201826095581055,
      "learning_rate": 0.00019014210680170116,
      "loss": 0.1345,
      "step": 10954
    },
    {
      "epoch": 0.051530146664534274,
      "grad_norm": 2.684185743331909,
      "learning_rate": 0.00019014116382358765,
      "loss": 0.2584,
      "step": 10955
    },
    {
      "epoch": 0.05153485046614674,
      "grad_norm": 1.7560784816741943,
      "learning_rate": 0.00019014022084547417,
      "loss": 0.1134,
      "step": 10956
    },
    {
      "epoch": 0.051539554267759205,
      "grad_norm": 3.5937392711639404,
      "learning_rate": 0.0001901392778673607,
      "loss": 0.6383,
      "step": 10957
    },
    {
      "epoch": 0.051544258069371664,
      "grad_norm": 2.729738473892212,
      "learning_rate": 0.00019013833488924723,
      "loss": 0.271,
      "step": 10958
    },
    {
      "epoch": 0.05154896187098413,
      "grad_norm": 1.5899291038513184,
      "learning_rate": 0.00019013739191113375,
      "loss": 0.2655,
      "step": 10959
    },
    {
      "epoch": 0.051553665672596595,
      "grad_norm": 0.8826619982719421,
      "learning_rate": 0.00019013644893302027,
      "loss": 0.0649,
      "step": 10960
    },
    {
      "epoch": 0.051558369474209054,
      "grad_norm": 3.022794723510742,
      "learning_rate": 0.0001901355059549068,
      "loss": 0.3866,
      "step": 10961
    },
    {
      "epoch": 0.05156307327582152,
      "grad_norm": 0.8661173582077026,
      "learning_rate": 0.0001901345629767933,
      "loss": 0.0802,
      "step": 10962
    },
    {
      "epoch": 0.051567777077433985,
      "grad_norm": 1.266213297843933,
      "learning_rate": 0.00019013361999867985,
      "loss": 0.1354,
      "step": 10963
    },
    {
      "epoch": 0.051572480879046444,
      "grad_norm": 0.3491860628128052,
      "learning_rate": 0.00019013267702056637,
      "loss": 0.0339,
      "step": 10964
    },
    {
      "epoch": 0.05157718468065891,
      "grad_norm": 1.112863302230835,
      "learning_rate": 0.0001901317340424529,
      "loss": 0.1183,
      "step": 10965
    },
    {
      "epoch": 0.051581888482271375,
      "grad_norm": 0.9694629907608032,
      "learning_rate": 0.0001901307910643394,
      "loss": 0.0822,
      "step": 10966
    },
    {
      "epoch": 0.051586592283883834,
      "grad_norm": 1.661826729774475,
      "learning_rate": 0.00019012984808622593,
      "loss": 0.1549,
      "step": 10967
    },
    {
      "epoch": 0.0515912960854963,
      "grad_norm": 2.396489143371582,
      "learning_rate": 0.00019012890510811245,
      "loss": 0.2077,
      "step": 10968
    },
    {
      "epoch": 0.05159599988710876,
      "grad_norm": 2.2136831283569336,
      "learning_rate": 0.00019012796212999897,
      "loss": 0.1829,
      "step": 10969
    },
    {
      "epoch": 0.051600703688721224,
      "grad_norm": 2.427626132965088,
      "learning_rate": 0.00019012701915188549,
      "loss": 0.4621,
      "step": 10970
    },
    {
      "epoch": 0.05160540749033369,
      "grad_norm": 2.2011353969573975,
      "learning_rate": 0.000190126076173772,
      "loss": 0.3885,
      "step": 10971
    },
    {
      "epoch": 0.05161011129194615,
      "grad_norm": 4.236046314239502,
      "learning_rate": 0.00019012513319565855,
      "loss": 0.3666,
      "step": 10972
    },
    {
      "epoch": 0.051614815093558614,
      "grad_norm": 2.0449204444885254,
      "learning_rate": 0.00019012419021754507,
      "loss": 0.1797,
      "step": 10973
    },
    {
      "epoch": 0.05161951889517108,
      "grad_norm": 2.43151593208313,
      "learning_rate": 0.0001901232472394316,
      "loss": 0.1623,
      "step": 10974
    },
    {
      "epoch": 0.05162422269678354,
      "grad_norm": 2.422900915145874,
      "learning_rate": 0.0001901223042613181,
      "loss": 0.2419,
      "step": 10975
    },
    {
      "epoch": 0.051628926498396004,
      "grad_norm": 2.9725215435028076,
      "learning_rate": 0.00019012136128320462,
      "loss": 0.3482,
      "step": 10976
    },
    {
      "epoch": 0.05163363030000847,
      "grad_norm": 0.436755895614624,
      "learning_rate": 0.00019012041830509114,
      "loss": 0.0411,
      "step": 10977
    },
    {
      "epoch": 0.05163833410162093,
      "grad_norm": 2.845102071762085,
      "learning_rate": 0.00019011947532697766,
      "loss": 0.2445,
      "step": 10978
    },
    {
      "epoch": 0.051643037903233394,
      "grad_norm": 1.8129664659500122,
      "learning_rate": 0.00019011853234886418,
      "loss": 0.158,
      "step": 10979
    },
    {
      "epoch": 0.05164774170484586,
      "grad_norm": 2.6577680110931396,
      "learning_rate": 0.0001901175893707507,
      "loss": 0.5391,
      "step": 10980
    },
    {
      "epoch": 0.05165244550645832,
      "grad_norm": 0.643649160861969,
      "learning_rate": 0.00019011664639263724,
      "loss": 0.046,
      "step": 10981
    },
    {
      "epoch": 0.051657149308070784,
      "grad_norm": 3.785982608795166,
      "learning_rate": 0.00019011570341452376,
      "loss": 0.4654,
      "step": 10982
    },
    {
      "epoch": 0.05166185310968325,
      "grad_norm": 3.6449530124664307,
      "learning_rate": 0.00019011476043641028,
      "loss": 0.7729,
      "step": 10983
    },
    {
      "epoch": 0.05166655691129571,
      "grad_norm": 2.998685359954834,
      "learning_rate": 0.0001901138174582968,
      "loss": 0.333,
      "step": 10984
    },
    {
      "epoch": 0.051671260712908174,
      "grad_norm": 3.498365879058838,
      "learning_rate": 0.00019011287448018335,
      "loss": 0.472,
      "step": 10985
    },
    {
      "epoch": 0.05167596451452063,
      "grad_norm": 1.6784623861312866,
      "learning_rate": 0.00019011193150206984,
      "loss": 0.1149,
      "step": 10986
    },
    {
      "epoch": 0.0516806683161331,
      "grad_norm": 0.1419268697500229,
      "learning_rate": 0.00019011098852395636,
      "loss": 0.0087,
      "step": 10987
    },
    {
      "epoch": 0.051685372117745564,
      "grad_norm": 2.149912118911743,
      "learning_rate": 0.00019011004554584288,
      "loss": 0.2687,
      "step": 10988
    },
    {
      "epoch": 0.05169007591935802,
      "grad_norm": 2.8551766872406006,
      "learning_rate": 0.0001901091025677294,
      "loss": 0.4611,
      "step": 10989
    },
    {
      "epoch": 0.05169477972097049,
      "grad_norm": 1.4651919603347778,
      "learning_rate": 0.00019010815958961594,
      "loss": 0.1257,
      "step": 10990
    },
    {
      "epoch": 0.051699483522582954,
      "grad_norm": 3.1325817108154297,
      "learning_rate": 0.00019010721661150246,
      "loss": 0.5145,
      "step": 10991
    },
    {
      "epoch": 0.05170418732419541,
      "grad_norm": 3.8000714778900146,
      "learning_rate": 0.00019010627363338898,
      "loss": 0.693,
      "step": 10992
    },
    {
      "epoch": 0.05170889112580788,
      "grad_norm": 3.4657576084136963,
      "learning_rate": 0.0001901053306552755,
      "loss": 0.5951,
      "step": 10993
    },
    {
      "epoch": 0.051713594927420344,
      "grad_norm": 2.650359869003296,
      "learning_rate": 0.00019010438767716204,
      "loss": 0.3841,
      "step": 10994
    },
    {
      "epoch": 0.0517182987290328,
      "grad_norm": 2.050048589706421,
      "learning_rate": 0.00019010344469904856,
      "loss": 0.3411,
      "step": 10995
    },
    {
      "epoch": 0.05172300253064527,
      "grad_norm": 0.26583003997802734,
      "learning_rate": 0.00019010250172093508,
      "loss": 0.0187,
      "step": 10996
    },
    {
      "epoch": 0.051727706332257734,
      "grad_norm": 0.4534895122051239,
      "learning_rate": 0.0001901015587428216,
      "loss": 0.0332,
      "step": 10997
    },
    {
      "epoch": 0.05173241013387019,
      "grad_norm": 2.0239291191101074,
      "learning_rate": 0.0001901006157647081,
      "loss": 0.2458,
      "step": 10998
    },
    {
      "epoch": 0.05173711393548266,
      "grad_norm": 2.257266044616699,
      "learning_rate": 0.00019009967278659463,
      "loss": 0.2206,
      "step": 10999
    },
    {
      "epoch": 0.051741817737095123,
      "grad_norm": 3.1383628845214844,
      "learning_rate": 0.00019009872980848115,
      "loss": 0.6697,
      "step": 11000
    },
    {
      "epoch": 0.05174652153870758,
      "grad_norm": 1.7532958984375,
      "learning_rate": 0.00019009778683036767,
      "loss": 0.1267,
      "step": 11001
    },
    {
      "epoch": 0.05175122534032005,
      "grad_norm": 2.138442277908325,
      "learning_rate": 0.0001900968438522542,
      "loss": 0.2083,
      "step": 11002
    },
    {
      "epoch": 0.05175592914193251,
      "grad_norm": 4.1897172927856445,
      "learning_rate": 0.00019009590087414074,
      "loss": 0.5577,
      "step": 11003
    },
    {
      "epoch": 0.05176063294354497,
      "grad_norm": 2.297334909439087,
      "learning_rate": 0.00019009495789602725,
      "loss": 0.258,
      "step": 11004
    },
    {
      "epoch": 0.05176533674515744,
      "grad_norm": 4.137601375579834,
      "learning_rate": 0.00019009401491791377,
      "loss": 0.6911,
      "step": 11005
    },
    {
      "epoch": 0.051770040546769897,
      "grad_norm": 0.5614426732063293,
      "learning_rate": 0.0001900930719398003,
      "loss": 0.0629,
      "step": 11006
    },
    {
      "epoch": 0.05177474434838236,
      "grad_norm": 2.3674819469451904,
      "learning_rate": 0.0001900921289616868,
      "loss": 0.2849,
      "step": 11007
    },
    {
      "epoch": 0.05177944814999483,
      "grad_norm": 0.8307853937149048,
      "learning_rate": 0.00019009118598357333,
      "loss": 0.0882,
      "step": 11008
    },
    {
      "epoch": 0.051784151951607287,
      "grad_norm": 1.134679913520813,
      "learning_rate": 0.00019009024300545985,
      "loss": 0.1532,
      "step": 11009
    },
    {
      "epoch": 0.05178885575321975,
      "grad_norm": 1.5757914781570435,
      "learning_rate": 0.00019008930002734637,
      "loss": 0.1121,
      "step": 11010
    },
    {
      "epoch": 0.05179355955483222,
      "grad_norm": 1.903930902481079,
      "learning_rate": 0.00019008835704923289,
      "loss": 0.2692,
      "step": 11011
    },
    {
      "epoch": 0.051798263356444676,
      "grad_norm": 2.0401339530944824,
      "learning_rate": 0.0001900874140711194,
      "loss": 0.4388,
      "step": 11012
    },
    {
      "epoch": 0.05180296715805714,
      "grad_norm": 0.2736993730068207,
      "learning_rate": 0.00019008647109300595,
      "loss": 0.0186,
      "step": 11013
    },
    {
      "epoch": 0.05180767095966961,
      "grad_norm": 1.343443751335144,
      "learning_rate": 0.00019008552811489247,
      "loss": 0.117,
      "step": 11014
    },
    {
      "epoch": 0.051812374761282066,
      "grad_norm": 1.4611948728561401,
      "learning_rate": 0.000190084585136779,
      "loss": 0.1702,
      "step": 11015
    },
    {
      "epoch": 0.05181707856289453,
      "grad_norm": 3.400768280029297,
      "learning_rate": 0.0001900836421586655,
      "loss": 0.5516,
      "step": 11016
    },
    {
      "epoch": 0.051821782364507,
      "grad_norm": 2.1966962814331055,
      "learning_rate": 0.00019008269918055202,
      "loss": 0.2373,
      "step": 11017
    },
    {
      "epoch": 0.051826486166119456,
      "grad_norm": 2.9453377723693848,
      "learning_rate": 0.00019008175620243854,
      "loss": 0.6484,
      "step": 11018
    },
    {
      "epoch": 0.05183118996773192,
      "grad_norm": 2.90714955329895,
      "learning_rate": 0.00019008081322432506,
      "loss": 0.323,
      "step": 11019
    },
    {
      "epoch": 0.05183589376934438,
      "grad_norm": 2.6150944232940674,
      "learning_rate": 0.00019007987024621158,
      "loss": 0.273,
      "step": 11020
    },
    {
      "epoch": 0.051840597570956846,
      "grad_norm": 1.9260749816894531,
      "learning_rate": 0.0001900789272680981,
      "loss": 0.2146,
      "step": 11021
    },
    {
      "epoch": 0.05184530137256931,
      "grad_norm": 1.9825109243392944,
      "learning_rate": 0.00019007798428998464,
      "loss": 0.2643,
      "step": 11022
    },
    {
      "epoch": 0.05185000517418177,
      "grad_norm": 2.3736932277679443,
      "learning_rate": 0.00019007704131187116,
      "loss": 0.303,
      "step": 11023
    },
    {
      "epoch": 0.051854708975794236,
      "grad_norm": 1.7161791324615479,
      "learning_rate": 0.00019007609833375768,
      "loss": 0.2482,
      "step": 11024
    },
    {
      "epoch": 0.0518594127774067,
      "grad_norm": 1.2494760751724243,
      "learning_rate": 0.0001900751553556442,
      "loss": 0.1383,
      "step": 11025
    },
    {
      "epoch": 0.05186411657901916,
      "grad_norm": 3.619813919067383,
      "learning_rate": 0.00019007421237753075,
      "loss": 0.5482,
      "step": 11026
    },
    {
      "epoch": 0.051868820380631626,
      "grad_norm": 0.41159626841545105,
      "learning_rate": 0.00019007326939941727,
      "loss": 0.0497,
      "step": 11027
    },
    {
      "epoch": 0.05187352418224409,
      "grad_norm": 1.250971794128418,
      "learning_rate": 0.00019007232642130378,
      "loss": 0.1799,
      "step": 11028
    },
    {
      "epoch": 0.05187822798385655,
      "grad_norm": 1.223811149597168,
      "learning_rate": 0.00019007138344319028,
      "loss": 0.1216,
      "step": 11029
    },
    {
      "epoch": 0.051882931785469016,
      "grad_norm": 0.34530436992645264,
      "learning_rate": 0.0001900704404650768,
      "loss": 0.0112,
      "step": 11030
    },
    {
      "epoch": 0.05188763558708148,
      "grad_norm": 2.918731689453125,
      "learning_rate": 0.00019006949748696334,
      "loss": 0.3209,
      "step": 11031
    },
    {
      "epoch": 0.05189233938869394,
      "grad_norm": 2.252068042755127,
      "learning_rate": 0.00019006855450884986,
      "loss": 0.3669,
      "step": 11032
    },
    {
      "epoch": 0.051897043190306406,
      "grad_norm": 1.0840502977371216,
      "learning_rate": 0.00019006761153073638,
      "loss": 0.0889,
      "step": 11033
    },
    {
      "epoch": 0.05190174699191887,
      "grad_norm": 1.8368077278137207,
      "learning_rate": 0.0001900666685526229,
      "loss": 0.2167,
      "step": 11034
    },
    {
      "epoch": 0.05190645079353133,
      "grad_norm": 1.936286449432373,
      "learning_rate": 0.00019006572557450944,
      "loss": 0.21,
      "step": 11035
    },
    {
      "epoch": 0.051911154595143796,
      "grad_norm": 0.9263085126876831,
      "learning_rate": 0.00019006478259639596,
      "loss": 0.0676,
      "step": 11036
    },
    {
      "epoch": 0.051915858396756255,
      "grad_norm": 1.8503761291503906,
      "learning_rate": 0.00019006383961828248,
      "loss": 0.3846,
      "step": 11037
    },
    {
      "epoch": 0.05192056219836872,
      "grad_norm": 2.272580862045288,
      "learning_rate": 0.000190062896640169,
      "loss": 0.4056,
      "step": 11038
    },
    {
      "epoch": 0.051925265999981186,
      "grad_norm": 2.5119528770446777,
      "learning_rate": 0.00019006195366205552,
      "loss": 0.733,
      "step": 11039
    },
    {
      "epoch": 0.051929969801593645,
      "grad_norm": 0.5477415323257446,
      "learning_rate": 0.00019006101068394203,
      "loss": 0.0536,
      "step": 11040
    },
    {
      "epoch": 0.05193467360320611,
      "grad_norm": 1.5349258184432983,
      "learning_rate": 0.00019006006770582855,
      "loss": 0.2178,
      "step": 11041
    },
    {
      "epoch": 0.051939377404818576,
      "grad_norm": 1.8701844215393066,
      "learning_rate": 0.00019005912472771507,
      "loss": 0.2842,
      "step": 11042
    },
    {
      "epoch": 0.051944081206431035,
      "grad_norm": 3.6064815521240234,
      "learning_rate": 0.0001900581817496016,
      "loss": 0.2878,
      "step": 11043
    },
    {
      "epoch": 0.0519487850080435,
      "grad_norm": 0.22093172371387482,
      "learning_rate": 0.00019005723877148814,
      "loss": 0.0176,
      "step": 11044
    },
    {
      "epoch": 0.051953488809655966,
      "grad_norm": 1.4116005897521973,
      "learning_rate": 0.00019005629579337465,
      "loss": 0.1383,
      "step": 11045
    },
    {
      "epoch": 0.051958192611268425,
      "grad_norm": 4.924196243286133,
      "learning_rate": 0.00019005535281526117,
      "loss": 0.6665,
      "step": 11046
    },
    {
      "epoch": 0.05196289641288089,
      "grad_norm": 2.2474822998046875,
      "learning_rate": 0.0001900544098371477,
      "loss": 0.4846,
      "step": 11047
    },
    {
      "epoch": 0.051967600214493356,
      "grad_norm": 1.4589959383010864,
      "learning_rate": 0.0001900534668590342,
      "loss": 0.2731,
      "step": 11048
    },
    {
      "epoch": 0.051972304016105815,
      "grad_norm": 2.5523693561553955,
      "learning_rate": 0.00019005252388092073,
      "loss": 0.6689,
      "step": 11049
    },
    {
      "epoch": 0.05197700781771828,
      "grad_norm": 1.5613898038864136,
      "learning_rate": 0.00019005158090280725,
      "loss": 0.1682,
      "step": 11050
    },
    {
      "epoch": 0.051981711619330746,
      "grad_norm": 0.7380350828170776,
      "learning_rate": 0.00019005063792469377,
      "loss": 0.0627,
      "step": 11051
    },
    {
      "epoch": 0.051986415420943205,
      "grad_norm": 0.3231314718723297,
      "learning_rate": 0.00019004969494658029,
      "loss": 0.0125,
      "step": 11052
    },
    {
      "epoch": 0.05199111922255567,
      "grad_norm": 1.9696178436279297,
      "learning_rate": 0.00019004875196846683,
      "loss": 0.0867,
      "step": 11053
    },
    {
      "epoch": 0.05199582302416813,
      "grad_norm": 1.986286997795105,
      "learning_rate": 0.00019004780899035335,
      "loss": 0.1924,
      "step": 11054
    },
    {
      "epoch": 0.052000526825780595,
      "grad_norm": 1.3947477340698242,
      "learning_rate": 0.00019004686601223987,
      "loss": 0.1487,
      "step": 11055
    },
    {
      "epoch": 0.05200523062739306,
      "grad_norm": 5.665118217468262,
      "learning_rate": 0.0001900459230341264,
      "loss": 0.3274,
      "step": 11056
    },
    {
      "epoch": 0.05200993442900552,
      "grad_norm": 1.4623607397079468,
      "learning_rate": 0.0001900449800560129,
      "loss": 0.1413,
      "step": 11057
    },
    {
      "epoch": 0.052014638230617985,
      "grad_norm": 3.1016833782196045,
      "learning_rate": 0.00019004403707789945,
      "loss": 0.3795,
      "step": 11058
    },
    {
      "epoch": 0.05201934203223045,
      "grad_norm": 1.3036638498306274,
      "learning_rate": 0.00019004309409978594,
      "loss": 0.1522,
      "step": 11059
    },
    {
      "epoch": 0.05202404583384291,
      "grad_norm": 1.10479736328125,
      "learning_rate": 0.00019004215112167246,
      "loss": 0.1335,
      "step": 11060
    },
    {
      "epoch": 0.052028749635455375,
      "grad_norm": 2.844092607498169,
      "learning_rate": 0.00019004120814355898,
      "loss": 0.4109,
      "step": 11061
    },
    {
      "epoch": 0.05203345343706784,
      "grad_norm": 0.34100183844566345,
      "learning_rate": 0.00019004026516544553,
      "loss": 0.0308,
      "step": 11062
    },
    {
      "epoch": 0.0520381572386803,
      "grad_norm": 3.551525354385376,
      "learning_rate": 0.00019003932218733204,
      "loss": 0.2739,
      "step": 11063
    },
    {
      "epoch": 0.052042861040292765,
      "grad_norm": 1.9758890867233276,
      "learning_rate": 0.00019003837920921856,
      "loss": 0.3624,
      "step": 11064
    },
    {
      "epoch": 0.05204756484190523,
      "grad_norm": 1.0016382932662964,
      "learning_rate": 0.00019003743623110508,
      "loss": 0.1091,
      "step": 11065
    },
    {
      "epoch": 0.05205226864351769,
      "grad_norm": 2.5191891193389893,
      "learning_rate": 0.0001900364932529916,
      "loss": 0.3092,
      "step": 11066
    },
    {
      "epoch": 0.052056972445130155,
      "grad_norm": 3.701449155807495,
      "learning_rate": 0.00019003555027487815,
      "loss": 0.7196,
      "step": 11067
    },
    {
      "epoch": 0.05206167624674262,
      "grad_norm": 1.406813383102417,
      "learning_rate": 0.00019003460729676467,
      "loss": 0.1227,
      "step": 11068
    },
    {
      "epoch": 0.05206638004835508,
      "grad_norm": 2.531587600708008,
      "learning_rate": 0.00019003366431865118,
      "loss": 0.5032,
      "step": 11069
    },
    {
      "epoch": 0.052071083849967545,
      "grad_norm": 1.0878509283065796,
      "learning_rate": 0.0001900327213405377,
      "loss": 0.0654,
      "step": 11070
    },
    {
      "epoch": 0.052075787651580004,
      "grad_norm": 3.932840585708618,
      "learning_rate": 0.0001900317783624242,
      "loss": 0.2523,
      "step": 11071
    },
    {
      "epoch": 0.05208049145319247,
      "grad_norm": 2.7302181720733643,
      "learning_rate": 0.00019003083538431074,
      "loss": 0.2836,
      "step": 11072
    },
    {
      "epoch": 0.052085195254804935,
      "grad_norm": 0.2603735029697418,
      "learning_rate": 0.00019002989240619726,
      "loss": 0.0252,
      "step": 11073
    },
    {
      "epoch": 0.052089899056417394,
      "grad_norm": 1.3242275714874268,
      "learning_rate": 0.00019002894942808378,
      "loss": 0.1153,
      "step": 11074
    },
    {
      "epoch": 0.05209460285802986,
      "grad_norm": 4.3252081871032715,
      "learning_rate": 0.0001900280064499703,
      "loss": 0.6396,
      "step": 11075
    },
    {
      "epoch": 0.052099306659642325,
      "grad_norm": 1.7399364709854126,
      "learning_rate": 0.00019002706347185684,
      "loss": 0.1611,
      "step": 11076
    },
    {
      "epoch": 0.052104010461254784,
      "grad_norm": 2.8131775856018066,
      "learning_rate": 0.00019002612049374336,
      "loss": 0.3659,
      "step": 11077
    },
    {
      "epoch": 0.05210871426286725,
      "grad_norm": 1.9579784870147705,
      "learning_rate": 0.00019002517751562988,
      "loss": 0.3205,
      "step": 11078
    },
    {
      "epoch": 0.052113418064479715,
      "grad_norm": 0.9392823576927185,
      "learning_rate": 0.0001900242345375164,
      "loss": 0.0441,
      "step": 11079
    },
    {
      "epoch": 0.052118121866092174,
      "grad_norm": 1.7574195861816406,
      "learning_rate": 0.00019002329155940292,
      "loss": 0.2297,
      "step": 11080
    },
    {
      "epoch": 0.05212282566770464,
      "grad_norm": 2.987518787384033,
      "learning_rate": 0.00019002234858128943,
      "loss": 0.5035,
      "step": 11081
    },
    {
      "epoch": 0.052127529469317105,
      "grad_norm": 0.4463447332382202,
      "learning_rate": 0.00019002140560317595,
      "loss": 0.0378,
      "step": 11082
    },
    {
      "epoch": 0.052132233270929564,
      "grad_norm": 2.990255355834961,
      "learning_rate": 0.00019002046262506247,
      "loss": 0.3046,
      "step": 11083
    },
    {
      "epoch": 0.05213693707254203,
      "grad_norm": 4.497349262237549,
      "learning_rate": 0.000190019519646949,
      "loss": 0.804,
      "step": 11084
    },
    {
      "epoch": 0.052141640874154495,
      "grad_norm": 2.2062184810638428,
      "learning_rate": 0.00019001857666883554,
      "loss": 0.2597,
      "step": 11085
    },
    {
      "epoch": 0.052146344675766954,
      "grad_norm": 2.217270612716675,
      "learning_rate": 0.00019001763369072205,
      "loss": 0.2283,
      "step": 11086
    },
    {
      "epoch": 0.05215104847737942,
      "grad_norm": 0.8653646111488342,
      "learning_rate": 0.00019001669071260857,
      "loss": 0.0736,
      "step": 11087
    },
    {
      "epoch": 0.05215575227899188,
      "grad_norm": 1.6937980651855469,
      "learning_rate": 0.0001900157477344951,
      "loss": 0.1234,
      "step": 11088
    },
    {
      "epoch": 0.052160456080604344,
      "grad_norm": 3.6255595684051514,
      "learning_rate": 0.0001900148047563816,
      "loss": 0.5396,
      "step": 11089
    },
    {
      "epoch": 0.05216515988221681,
      "grad_norm": 1.65769624710083,
      "learning_rate": 0.00019001386177826813,
      "loss": 0.0966,
      "step": 11090
    },
    {
      "epoch": 0.05216986368382927,
      "grad_norm": 2.2064461708068848,
      "learning_rate": 0.00019001291880015465,
      "loss": 0.3336,
      "step": 11091
    },
    {
      "epoch": 0.052174567485441734,
      "grad_norm": 3.099323034286499,
      "learning_rate": 0.00019001197582204117,
      "loss": 0.5888,
      "step": 11092
    },
    {
      "epoch": 0.0521792712870542,
      "grad_norm": 2.579270124435425,
      "learning_rate": 0.00019001103284392769,
      "loss": 0.4666,
      "step": 11093
    },
    {
      "epoch": 0.05218397508866666,
      "grad_norm": 0.8116515874862671,
      "learning_rate": 0.00019001008986581423,
      "loss": 0.1133,
      "step": 11094
    },
    {
      "epoch": 0.052188678890279123,
      "grad_norm": 1.4292629957199097,
      "learning_rate": 0.00019000914688770075,
      "loss": 0.1228,
      "step": 11095
    },
    {
      "epoch": 0.05219338269189159,
      "grad_norm": 0.715370774269104,
      "learning_rate": 0.00019000820390958727,
      "loss": 0.061,
      "step": 11096
    },
    {
      "epoch": 0.05219808649350405,
      "grad_norm": 3.71022891998291,
      "learning_rate": 0.0001900072609314738,
      "loss": 0.38,
      "step": 11097
    },
    {
      "epoch": 0.052202790295116513,
      "grad_norm": 0.8055135607719421,
      "learning_rate": 0.0001900063179533603,
      "loss": 0.0542,
      "step": 11098
    },
    {
      "epoch": 0.05220749409672898,
      "grad_norm": 2.4304051399230957,
      "learning_rate": 0.00019000537497524685,
      "loss": 0.2833,
      "step": 11099
    },
    {
      "epoch": 0.05221219789834144,
      "grad_norm": 2.92529559135437,
      "learning_rate": 0.00019000443199713337,
      "loss": 0.4789,
      "step": 11100
    },
    {
      "epoch": 0.0522169016999539,
      "grad_norm": 3.2858874797821045,
      "learning_rate": 0.0001900034890190199,
      "loss": 0.1267,
      "step": 11101
    },
    {
      "epoch": 0.05222160550156637,
      "grad_norm": 3.6729443073272705,
      "learning_rate": 0.00019000254604090638,
      "loss": 0.6028,
      "step": 11102
    },
    {
      "epoch": 0.05222630930317883,
      "grad_norm": 2.5145294666290283,
      "learning_rate": 0.00019000160306279293,
      "loss": 0.1885,
      "step": 11103
    },
    {
      "epoch": 0.05223101310479129,
      "grad_norm": 1.0477339029312134,
      "learning_rate": 0.00019000066008467944,
      "loss": 0.0758,
      "step": 11104
    },
    {
      "epoch": 0.05223571690640375,
      "grad_norm": 0.6464416980743408,
      "learning_rate": 0.00018999971710656596,
      "loss": 0.0255,
      "step": 11105
    },
    {
      "epoch": 0.05224042070801622,
      "grad_norm": 1.7589777708053589,
      "learning_rate": 0.00018999877412845248,
      "loss": 0.1,
      "step": 11106
    },
    {
      "epoch": 0.05224512450962868,
      "grad_norm": 4.413743495941162,
      "learning_rate": 0.000189997831150339,
      "loss": 0.2934,
      "step": 11107
    },
    {
      "epoch": 0.05224982831124114,
      "grad_norm": 0.5541917681694031,
      "learning_rate": 0.00018999688817222555,
      "loss": 0.0495,
      "step": 11108
    },
    {
      "epoch": 0.05225453211285361,
      "grad_norm": 2.431572675704956,
      "learning_rate": 0.00018999594519411207,
      "loss": 0.1943,
      "step": 11109
    },
    {
      "epoch": 0.05225923591446607,
      "grad_norm": 4.197354316711426,
      "learning_rate": 0.00018999500221599858,
      "loss": 0.8855,
      "step": 11110
    },
    {
      "epoch": 0.05226393971607853,
      "grad_norm": 0.10126475244760513,
      "learning_rate": 0.0001899940592378851,
      "loss": 0.0057,
      "step": 11111
    },
    {
      "epoch": 0.052268643517691,
      "grad_norm": 2.5200836658477783,
      "learning_rate": 0.00018999311625977162,
      "loss": 0.1923,
      "step": 11112
    },
    {
      "epoch": 0.05227334731930346,
      "grad_norm": 2.7021806240081787,
      "learning_rate": 0.00018999217328165814,
      "loss": 0.1076,
      "step": 11113
    },
    {
      "epoch": 0.05227805112091592,
      "grad_norm": 2.9505879878997803,
      "learning_rate": 0.00018999123030354466,
      "loss": 0.1904,
      "step": 11114
    },
    {
      "epoch": 0.05228275492252839,
      "grad_norm": 3.8683955669403076,
      "learning_rate": 0.00018999028732543118,
      "loss": 0.3822,
      "step": 11115
    },
    {
      "epoch": 0.05228745872414085,
      "grad_norm": 1.4911465644836426,
      "learning_rate": 0.0001899893443473177,
      "loss": 0.1114,
      "step": 11116
    },
    {
      "epoch": 0.05229216252575331,
      "grad_norm": 3.144834041595459,
      "learning_rate": 0.00018998840136920424,
      "loss": 0.2493,
      "step": 11117
    },
    {
      "epoch": 0.05229686632736578,
      "grad_norm": 1.0737428665161133,
      "learning_rate": 0.00018998745839109076,
      "loss": 0.0414,
      "step": 11118
    },
    {
      "epoch": 0.05230157012897824,
      "grad_norm": 0.1606682389974594,
      "learning_rate": 0.00018998651541297728,
      "loss": 0.0097,
      "step": 11119
    },
    {
      "epoch": 0.0523062739305907,
      "grad_norm": 0.535260021686554,
      "learning_rate": 0.0001899855724348638,
      "loss": 0.0611,
      "step": 11120
    },
    {
      "epoch": 0.05231097773220317,
      "grad_norm": 0.9710077047348022,
      "learning_rate": 0.00018998462945675032,
      "loss": 0.0446,
      "step": 11121
    },
    {
      "epoch": 0.052315681533815626,
      "grad_norm": 0.5043680667877197,
      "learning_rate": 0.00018998368647863683,
      "loss": 0.0257,
      "step": 11122
    },
    {
      "epoch": 0.05232038533542809,
      "grad_norm": 2.291208267211914,
      "learning_rate": 0.00018998274350052335,
      "loss": 0.1731,
      "step": 11123
    },
    {
      "epoch": 0.05232508913704056,
      "grad_norm": 4.498997211456299,
      "learning_rate": 0.00018998180052240987,
      "loss": 0.6803,
      "step": 11124
    },
    {
      "epoch": 0.052329792938653016,
      "grad_norm": 0.040628548711538315,
      "learning_rate": 0.0001899808575442964,
      "loss": 0.0019,
      "step": 11125
    },
    {
      "epoch": 0.05233449674026548,
      "grad_norm": 12.689960479736328,
      "learning_rate": 0.00018997991456618294,
      "loss": 1.1912,
      "step": 11126
    },
    {
      "epoch": 0.05233920054187795,
      "grad_norm": 2.641982316970825,
      "learning_rate": 0.00018997897158806945,
      "loss": 0.268,
      "step": 11127
    },
    {
      "epoch": 0.052343904343490406,
      "grad_norm": 2.85896635055542,
      "learning_rate": 0.00018997802860995597,
      "loss": 0.5846,
      "step": 11128
    },
    {
      "epoch": 0.05234860814510287,
      "grad_norm": 3.88913893699646,
      "learning_rate": 0.0001899770856318425,
      "loss": 0.7268,
      "step": 11129
    },
    {
      "epoch": 0.05235331194671534,
      "grad_norm": 3.194193124771118,
      "learning_rate": 0.000189976142653729,
      "loss": 0.715,
      "step": 11130
    },
    {
      "epoch": 0.052358015748327796,
      "grad_norm": 0.49716687202453613,
      "learning_rate": 0.00018997519967561556,
      "loss": 0.017,
      "step": 11131
    },
    {
      "epoch": 0.05236271954994026,
      "grad_norm": 1.6637630462646484,
      "learning_rate": 0.00018997425669750208,
      "loss": 0.1406,
      "step": 11132
    },
    {
      "epoch": 0.05236742335155273,
      "grad_norm": 0.32943516969680786,
      "learning_rate": 0.00018997331371938857,
      "loss": 0.0255,
      "step": 11133
    },
    {
      "epoch": 0.052372127153165186,
      "grad_norm": 2.781660795211792,
      "learning_rate": 0.00018997237074127509,
      "loss": 0.2514,
      "step": 11134
    },
    {
      "epoch": 0.05237683095477765,
      "grad_norm": 2.7213969230651855,
      "learning_rate": 0.00018997142776316163,
      "loss": 0.7306,
      "step": 11135
    },
    {
      "epoch": 0.05238153475639012,
      "grad_norm": 1.9200797080993652,
      "learning_rate": 0.00018997048478504815,
      "loss": 0.3476,
      "step": 11136
    },
    {
      "epoch": 0.052386238558002576,
      "grad_norm": 1.9515976905822754,
      "learning_rate": 0.00018996954180693467,
      "loss": 0.2227,
      "step": 11137
    },
    {
      "epoch": 0.05239094235961504,
      "grad_norm": 0.6851751804351807,
      "learning_rate": 0.0001899685988288212,
      "loss": 0.0875,
      "step": 11138
    },
    {
      "epoch": 0.0523956461612275,
      "grad_norm": 2.5540709495544434,
      "learning_rate": 0.0001899676558507077,
      "loss": 0.5465,
      "step": 11139
    },
    {
      "epoch": 0.052400349962839966,
      "grad_norm": 3.414304733276367,
      "learning_rate": 0.00018996671287259425,
      "loss": 0.3197,
      "step": 11140
    },
    {
      "epoch": 0.05240505376445243,
      "grad_norm": 0.6781253814697266,
      "learning_rate": 0.00018996576989448077,
      "loss": 0.0974,
      "step": 11141
    },
    {
      "epoch": 0.05240975756606489,
      "grad_norm": 1.7534810304641724,
      "learning_rate": 0.0001899648269163673,
      "loss": 0.1545,
      "step": 11142
    },
    {
      "epoch": 0.052414461367677356,
      "grad_norm": 0.7889833450317383,
      "learning_rate": 0.0001899638839382538,
      "loss": 0.0707,
      "step": 11143
    },
    {
      "epoch": 0.05241916516928982,
      "grad_norm": 2.133751392364502,
      "learning_rate": 0.00018996294096014033,
      "loss": 0.3764,
      "step": 11144
    },
    {
      "epoch": 0.05242386897090228,
      "grad_norm": 1.9348348379135132,
      "learning_rate": 0.00018996199798202684,
      "loss": 0.2125,
      "step": 11145
    },
    {
      "epoch": 0.052428572772514746,
      "grad_norm": 1.110252857208252,
      "learning_rate": 0.00018996105500391336,
      "loss": 0.1192,
      "step": 11146
    },
    {
      "epoch": 0.05243327657412721,
      "grad_norm": 1.2101842164993286,
      "learning_rate": 0.00018996011202579988,
      "loss": 0.1396,
      "step": 11147
    },
    {
      "epoch": 0.05243798037573967,
      "grad_norm": 2.377950668334961,
      "learning_rate": 0.0001899591690476864,
      "loss": 0.4778,
      "step": 11148
    },
    {
      "epoch": 0.052442684177352136,
      "grad_norm": 2.0783169269561768,
      "learning_rate": 0.00018995822606957295,
      "loss": 0.3098,
      "step": 11149
    },
    {
      "epoch": 0.0524473879789646,
      "grad_norm": 3.123185396194458,
      "learning_rate": 0.00018995728309145946,
      "loss": 0.6739,
      "step": 11150
    },
    {
      "epoch": 0.05245209178057706,
      "grad_norm": 17.4063663482666,
      "learning_rate": 0.00018995634011334598,
      "loss": 0.8308,
      "step": 11151
    },
    {
      "epoch": 0.052456795582189526,
      "grad_norm": 2.758925437927246,
      "learning_rate": 0.0001899553971352325,
      "loss": 0.2659,
      "step": 11152
    },
    {
      "epoch": 0.05246149938380199,
      "grad_norm": 0.24121852219104767,
      "learning_rate": 0.00018995445415711902,
      "loss": 0.0207,
      "step": 11153
    },
    {
      "epoch": 0.05246620318541445,
      "grad_norm": 1.739131212234497,
      "learning_rate": 0.00018995351117900554,
      "loss": 0.077,
      "step": 11154
    },
    {
      "epoch": 0.052470906987026916,
      "grad_norm": 4.752703666687012,
      "learning_rate": 0.00018995256820089206,
      "loss": 0.4522,
      "step": 11155
    },
    {
      "epoch": 0.052475610788639375,
      "grad_norm": 3.584534168243408,
      "learning_rate": 0.00018995162522277858,
      "loss": 0.3245,
      "step": 11156
    },
    {
      "epoch": 0.05248031459025184,
      "grad_norm": 0.33758091926574707,
      "learning_rate": 0.0001899506822446651,
      "loss": 0.0158,
      "step": 11157
    },
    {
      "epoch": 0.052485018391864306,
      "grad_norm": 1.6561439037322998,
      "learning_rate": 0.00018994973926655164,
      "loss": 0.1144,
      "step": 11158
    },
    {
      "epoch": 0.052489722193476765,
      "grad_norm": 0.7506991028785706,
      "learning_rate": 0.00018994879628843816,
      "loss": 0.0286,
      "step": 11159
    },
    {
      "epoch": 0.05249442599508923,
      "grad_norm": 2.2251346111297607,
      "learning_rate": 0.00018994785331032468,
      "loss": 0.2676,
      "step": 11160
    },
    {
      "epoch": 0.052499129796701696,
      "grad_norm": 0.6259548664093018,
      "learning_rate": 0.0001899469103322112,
      "loss": 0.078,
      "step": 11161
    },
    {
      "epoch": 0.052503833598314155,
      "grad_norm": 5.833580493927002,
      "learning_rate": 0.00018994596735409774,
      "loss": 0.8711,
      "step": 11162
    },
    {
      "epoch": 0.05250853739992662,
      "grad_norm": 0.7586231231689453,
      "learning_rate": 0.00018994502437598426,
      "loss": 0.0807,
      "step": 11163
    },
    {
      "epoch": 0.052513241201539086,
      "grad_norm": 3.613525390625,
      "learning_rate": 0.00018994408139787075,
      "loss": 0.437,
      "step": 11164
    },
    {
      "epoch": 0.052517945003151545,
      "grad_norm": 4.412552356719971,
      "learning_rate": 0.00018994313841975727,
      "loss": 0.6452,
      "step": 11165
    },
    {
      "epoch": 0.05252264880476401,
      "grad_norm": 0.3045692443847656,
      "learning_rate": 0.0001899421954416438,
      "loss": 0.0366,
      "step": 11166
    },
    {
      "epoch": 0.052527352606376476,
      "grad_norm": 3.684244394302368,
      "learning_rate": 0.00018994125246353034,
      "loss": 0.7382,
      "step": 11167
    },
    {
      "epoch": 0.052532056407988935,
      "grad_norm": 4.292344570159912,
      "learning_rate": 0.00018994030948541685,
      "loss": 0.6834,
      "step": 11168
    },
    {
      "epoch": 0.0525367602096014,
      "grad_norm": 0.7776791453361511,
      "learning_rate": 0.00018993936650730337,
      "loss": 0.0808,
      "step": 11169
    },
    {
      "epoch": 0.052541464011213866,
      "grad_norm": 3.2894465923309326,
      "learning_rate": 0.0001899384235291899,
      "loss": 0.4779,
      "step": 11170
    },
    {
      "epoch": 0.052546167812826325,
      "grad_norm": 4.310189723968506,
      "learning_rate": 0.0001899374805510764,
      "loss": 0.7961,
      "step": 11171
    },
    {
      "epoch": 0.05255087161443879,
      "grad_norm": 0.9689311981201172,
      "learning_rate": 0.00018993653757296296,
      "loss": 0.0628,
      "step": 11172
    },
    {
      "epoch": 0.05255557541605125,
      "grad_norm": 1.5579594373703003,
      "learning_rate": 0.00018993559459484948,
      "loss": 0.1759,
      "step": 11173
    },
    {
      "epoch": 0.052560279217663715,
      "grad_norm": 3.1418590545654297,
      "learning_rate": 0.000189934651616736,
      "loss": 0.5222,
      "step": 11174
    },
    {
      "epoch": 0.05256498301927618,
      "grad_norm": 1.9506062269210815,
      "learning_rate": 0.00018993370863862249,
      "loss": 0.2657,
      "step": 11175
    },
    {
      "epoch": 0.05256968682088864,
      "grad_norm": 1.9069870710372925,
      "learning_rate": 0.00018993276566050903,
      "loss": 0.2553,
      "step": 11176
    },
    {
      "epoch": 0.052574390622501105,
      "grad_norm": 1.3189260959625244,
      "learning_rate": 0.00018993182268239555,
      "loss": 0.1906,
      "step": 11177
    },
    {
      "epoch": 0.05257909442411357,
      "grad_norm": 2.025869369506836,
      "learning_rate": 0.00018993087970428207,
      "loss": 0.1955,
      "step": 11178
    },
    {
      "epoch": 0.05258379822572603,
      "grad_norm": 1.9957581758499146,
      "learning_rate": 0.0001899299367261686,
      "loss": 0.1646,
      "step": 11179
    },
    {
      "epoch": 0.052588502027338495,
      "grad_norm": 2.422396421432495,
      "learning_rate": 0.0001899289937480551,
      "loss": 0.2748,
      "step": 11180
    },
    {
      "epoch": 0.05259320582895096,
      "grad_norm": 2.385218858718872,
      "learning_rate": 0.00018992805076994165,
      "loss": 0.2317,
      "step": 11181
    },
    {
      "epoch": 0.05259790963056342,
      "grad_norm": 3.1881792545318604,
      "learning_rate": 0.00018992710779182817,
      "loss": 0.422,
      "step": 11182
    },
    {
      "epoch": 0.052602613432175885,
      "grad_norm": 0.3419420123100281,
      "learning_rate": 0.0001899261648137147,
      "loss": 0.0204,
      "step": 11183
    },
    {
      "epoch": 0.05260731723378835,
      "grad_norm": 1.0928571224212646,
      "learning_rate": 0.0001899252218356012,
      "loss": 0.0938,
      "step": 11184
    },
    {
      "epoch": 0.05261202103540081,
      "grad_norm": 2.8940072059631348,
      "learning_rate": 0.00018992427885748773,
      "loss": 0.5786,
      "step": 11185
    },
    {
      "epoch": 0.052616724837013275,
      "grad_norm": 2.126316785812378,
      "learning_rate": 0.00018992333587937424,
      "loss": 0.1799,
      "step": 11186
    },
    {
      "epoch": 0.05262142863862574,
      "grad_norm": 2.573225259780884,
      "learning_rate": 0.00018992239290126076,
      "loss": 0.4575,
      "step": 11187
    },
    {
      "epoch": 0.0526261324402382,
      "grad_norm": 1.5622013807296753,
      "learning_rate": 0.00018992144992314728,
      "loss": 0.2884,
      "step": 11188
    },
    {
      "epoch": 0.052630836241850665,
      "grad_norm": 1.6528339385986328,
      "learning_rate": 0.0001899205069450338,
      "loss": 0.2825,
      "step": 11189
    },
    {
      "epoch": 0.052635540043463123,
      "grad_norm": 2.4508614540100098,
      "learning_rate": 0.00018991956396692035,
      "loss": 0.2558,
      "step": 11190
    },
    {
      "epoch": 0.05264024384507559,
      "grad_norm": 2.556305408477783,
      "learning_rate": 0.00018991862098880686,
      "loss": 0.4801,
      "step": 11191
    },
    {
      "epoch": 0.052644947646688055,
      "grad_norm": 0.8549970388412476,
      "learning_rate": 0.00018991767801069338,
      "loss": 0.0918,
      "step": 11192
    },
    {
      "epoch": 0.05264965144830051,
      "grad_norm": 1.7620877027511597,
      "learning_rate": 0.0001899167350325799,
      "loss": 0.372,
      "step": 11193
    },
    {
      "epoch": 0.05265435524991298,
      "grad_norm": 3.97198224067688,
      "learning_rate": 0.00018991579205446645,
      "loss": 0.405,
      "step": 11194
    },
    {
      "epoch": 0.052659059051525445,
      "grad_norm": 2.7865097522735596,
      "learning_rate": 0.00018991484907635294,
      "loss": 0.2706,
      "step": 11195
    },
    {
      "epoch": 0.0526637628531379,
      "grad_norm": 1.8042621612548828,
      "learning_rate": 0.00018991390609823946,
      "loss": 0.298,
      "step": 11196
    },
    {
      "epoch": 0.05266846665475037,
      "grad_norm": 1.9665852785110474,
      "learning_rate": 0.00018991296312012598,
      "loss": 0.1859,
      "step": 11197
    },
    {
      "epoch": 0.052673170456362835,
      "grad_norm": 2.5908565521240234,
      "learning_rate": 0.0001899120201420125,
      "loss": 0.6367,
      "step": 11198
    },
    {
      "epoch": 0.05267787425797529,
      "grad_norm": 2.0494229793548584,
      "learning_rate": 0.00018991107716389904,
      "loss": 0.2023,
      "step": 11199
    },
    {
      "epoch": 0.05268257805958776,
      "grad_norm": 3.9892518520355225,
      "learning_rate": 0.00018991013418578556,
      "loss": 0.5316,
      "step": 11200
    },
    {
      "epoch": 0.052687281861200225,
      "grad_norm": 3.82035493850708,
      "learning_rate": 0.00018990919120767208,
      "loss": 0.3546,
      "step": 11201
    },
    {
      "epoch": 0.05269198566281268,
      "grad_norm": 2.7627201080322266,
      "learning_rate": 0.0001899082482295586,
      "loss": 0.387,
      "step": 11202
    },
    {
      "epoch": 0.05269668946442515,
      "grad_norm": 3.1376187801361084,
      "learning_rate": 0.00018990730525144514,
      "loss": 0.2405,
      "step": 11203
    },
    {
      "epoch": 0.052701393266037615,
      "grad_norm": 1.8607040643692017,
      "learning_rate": 0.00018990636227333166,
      "loss": 0.1074,
      "step": 11204
    },
    {
      "epoch": 0.05270609706765007,
      "grad_norm": 1.5310416221618652,
      "learning_rate": 0.00018990541929521818,
      "loss": 0.0797,
      "step": 11205
    },
    {
      "epoch": 0.05271080086926254,
      "grad_norm": 2.8828635215759277,
      "learning_rate": 0.00018990447631710467,
      "loss": 0.2415,
      "step": 11206
    },
    {
      "epoch": 0.052715504670875,
      "grad_norm": 1.7659273147583008,
      "learning_rate": 0.0001899035333389912,
      "loss": 0.0805,
      "step": 11207
    },
    {
      "epoch": 0.05272020847248746,
      "grad_norm": 3.044348955154419,
      "learning_rate": 0.00018990259036087774,
      "loss": 0.4549,
      "step": 11208
    },
    {
      "epoch": 0.05272491227409993,
      "grad_norm": 1.7051085233688354,
      "learning_rate": 0.00018990164738276425,
      "loss": 0.1709,
      "step": 11209
    },
    {
      "epoch": 0.05272961607571239,
      "grad_norm": 3.0705184936523438,
      "learning_rate": 0.00018990070440465077,
      "loss": 0.2263,
      "step": 11210
    },
    {
      "epoch": 0.05273431987732485,
      "grad_norm": 2.2046072483062744,
      "learning_rate": 0.0001898997614265373,
      "loss": 0.2151,
      "step": 11211
    },
    {
      "epoch": 0.05273902367893732,
      "grad_norm": 3.0324084758758545,
      "learning_rate": 0.00018989881844842384,
      "loss": 0.3913,
      "step": 11212
    },
    {
      "epoch": 0.05274372748054978,
      "grad_norm": 3.7617759704589844,
      "learning_rate": 0.00018989787547031036,
      "loss": 0.7986,
      "step": 11213
    },
    {
      "epoch": 0.05274843128216224,
      "grad_norm": 3.7764008045196533,
      "learning_rate": 0.00018989693249219688,
      "loss": 0.458,
      "step": 11214
    },
    {
      "epoch": 0.05275313508377471,
      "grad_norm": 1.298173189163208,
      "learning_rate": 0.0001898959895140834,
      "loss": 0.1261,
      "step": 11215
    },
    {
      "epoch": 0.05275783888538717,
      "grad_norm": 3.192039728164673,
      "learning_rate": 0.0001898950465359699,
      "loss": 0.4392,
      "step": 11216
    },
    {
      "epoch": 0.05276254268699963,
      "grad_norm": 1.8503361940383911,
      "learning_rate": 0.00018989410355785643,
      "loss": 0.1969,
      "step": 11217
    },
    {
      "epoch": 0.0527672464886121,
      "grad_norm": 1.8828569650650024,
      "learning_rate": 0.00018989316057974295,
      "loss": 0.2299,
      "step": 11218
    },
    {
      "epoch": 0.05277195029022456,
      "grad_norm": 4.414391994476318,
      "learning_rate": 0.00018989221760162947,
      "loss": 0.3683,
      "step": 11219
    },
    {
      "epoch": 0.05277665409183702,
      "grad_norm": 3.8579890727996826,
      "learning_rate": 0.000189891274623516,
      "loss": 0.3874,
      "step": 11220
    },
    {
      "epoch": 0.05278135789344949,
      "grad_norm": 2.0271196365356445,
      "learning_rate": 0.0001898903316454025,
      "loss": 0.0987,
      "step": 11221
    },
    {
      "epoch": 0.05278606169506195,
      "grad_norm": 4.276870250701904,
      "learning_rate": 0.00018988938866728905,
      "loss": 0.1481,
      "step": 11222
    },
    {
      "epoch": 0.05279076549667441,
      "grad_norm": 0.7863626480102539,
      "learning_rate": 0.00018988844568917557,
      "loss": 0.0454,
      "step": 11223
    },
    {
      "epoch": 0.05279546929828687,
      "grad_norm": 2.4485957622528076,
      "learning_rate": 0.0001898875027110621,
      "loss": 0.1563,
      "step": 11224
    },
    {
      "epoch": 0.05280017309989934,
      "grad_norm": 2.6441190242767334,
      "learning_rate": 0.0001898865597329486,
      "loss": 0.1328,
      "step": 11225
    },
    {
      "epoch": 0.0528048769015118,
      "grad_norm": 3.044689416885376,
      "learning_rate": 0.00018988561675483513,
      "loss": 0.0947,
      "step": 11226
    },
    {
      "epoch": 0.05280958070312426,
      "grad_norm": 1.533017873764038,
      "learning_rate": 0.00018988467377672164,
      "loss": 0.0936,
      "step": 11227
    },
    {
      "epoch": 0.05281428450473673,
      "grad_norm": 14.743061065673828,
      "learning_rate": 0.00018988373079860816,
      "loss": 0.6399,
      "step": 11228
    },
    {
      "epoch": 0.05281898830634919,
      "grad_norm": 15.616438865661621,
      "learning_rate": 0.00018988278782049468,
      "loss": 0.4929,
      "step": 11229
    },
    {
      "epoch": 0.05282369210796165,
      "grad_norm": 0.3250987231731415,
      "learning_rate": 0.0001898818448423812,
      "loss": 0.0127,
      "step": 11230
    },
    {
      "epoch": 0.05282839590957412,
      "grad_norm": 2.760237693786621,
      "learning_rate": 0.00018988090186426775,
      "loss": 0.2695,
      "step": 11231
    },
    {
      "epoch": 0.05283309971118658,
      "grad_norm": 2.355215311050415,
      "learning_rate": 0.00018987995888615426,
      "loss": 0.215,
      "step": 11232
    },
    {
      "epoch": 0.05283780351279904,
      "grad_norm": 1.6274102926254272,
      "learning_rate": 0.00018987901590804078,
      "loss": 0.0687,
      "step": 11233
    },
    {
      "epoch": 0.05284250731441151,
      "grad_norm": 1.1853773593902588,
      "learning_rate": 0.0001898780729299273,
      "loss": 0.0753,
      "step": 11234
    },
    {
      "epoch": 0.05284721111602397,
      "grad_norm": 7.841775894165039,
      "learning_rate": 0.00018987712995181385,
      "loss": 1.0139,
      "step": 11235
    },
    {
      "epoch": 0.05285191491763643,
      "grad_norm": 3.482701539993286,
      "learning_rate": 0.00018987618697370037,
      "loss": 0.2768,
      "step": 11236
    },
    {
      "epoch": 0.0528566187192489,
      "grad_norm": 1.0804868936538696,
      "learning_rate": 0.00018987524399558686,
      "loss": 0.0497,
      "step": 11237
    },
    {
      "epoch": 0.05286132252086136,
      "grad_norm": 2.2713816165924072,
      "learning_rate": 0.00018987430101747338,
      "loss": 0.084,
      "step": 11238
    },
    {
      "epoch": 0.05286602632247382,
      "grad_norm": 0.5298280119895935,
      "learning_rate": 0.0001898733580393599,
      "loss": 0.0303,
      "step": 11239
    },
    {
      "epoch": 0.05287073012408629,
      "grad_norm": 2.905207395553589,
      "learning_rate": 0.00018987241506124644,
      "loss": 0.1465,
      "step": 11240
    },
    {
      "epoch": 0.052875433925698746,
      "grad_norm": 5.34696102142334,
      "learning_rate": 0.00018987147208313296,
      "loss": 0.5147,
      "step": 11241
    },
    {
      "epoch": 0.05288013772731121,
      "grad_norm": 1.8087587356567383,
      "learning_rate": 0.00018987052910501948,
      "loss": 0.1165,
      "step": 11242
    },
    {
      "epoch": 0.05288484152892368,
      "grad_norm": 4.3579277992248535,
      "learning_rate": 0.000189869586126906,
      "loss": 0.3691,
      "step": 11243
    },
    {
      "epoch": 0.052889545330536136,
      "grad_norm": 3.8747997283935547,
      "learning_rate": 0.00018986864314879254,
      "loss": 0.2773,
      "step": 11244
    },
    {
      "epoch": 0.0528942491321486,
      "grad_norm": 8.332502365112305,
      "learning_rate": 0.00018986770017067906,
      "loss": 1.3497,
      "step": 11245
    },
    {
      "epoch": 0.05289895293376107,
      "grad_norm": 6.452767848968506,
      "learning_rate": 0.00018986675719256558,
      "loss": 0.9121,
      "step": 11246
    },
    {
      "epoch": 0.052903656735373526,
      "grad_norm": 4.353296279907227,
      "learning_rate": 0.0001898658142144521,
      "loss": 0.3891,
      "step": 11247
    },
    {
      "epoch": 0.05290836053698599,
      "grad_norm": 0.5974558591842651,
      "learning_rate": 0.00018986487123633862,
      "loss": 0.0492,
      "step": 11248
    },
    {
      "epoch": 0.05291306433859846,
      "grad_norm": 0.2658787667751312,
      "learning_rate": 0.00018986392825822514,
      "loss": 0.0112,
      "step": 11249
    },
    {
      "epoch": 0.052917768140210916,
      "grad_norm": 2.026810884475708,
      "learning_rate": 0.00018986298528011165,
      "loss": 0.0826,
      "step": 11250
    },
    {
      "epoch": 0.05292247194182338,
      "grad_norm": 4.119420051574707,
      "learning_rate": 0.00018986204230199817,
      "loss": 0.2306,
      "step": 11251
    },
    {
      "epoch": 0.05292717574343585,
      "grad_norm": 7.496903419494629,
      "learning_rate": 0.0001898610993238847,
      "loss": 0.5746,
      "step": 11252
    },
    {
      "epoch": 0.052931879545048306,
      "grad_norm": 3.5638859272003174,
      "learning_rate": 0.00018986015634577124,
      "loss": 0.2712,
      "step": 11253
    },
    {
      "epoch": 0.05293658334666077,
      "grad_norm": 5.085310459136963,
      "learning_rate": 0.00018985921336765776,
      "loss": 0.5975,
      "step": 11254
    },
    {
      "epoch": 0.05294128714827324,
      "grad_norm": 2.978394031524658,
      "learning_rate": 0.00018985827038954428,
      "loss": 0.1289,
      "step": 11255
    },
    {
      "epoch": 0.052945990949885696,
      "grad_norm": 7.54640007019043,
      "learning_rate": 0.0001898573274114308,
      "loss": 0.453,
      "step": 11256
    },
    {
      "epoch": 0.05295069475149816,
      "grad_norm": 2.340028762817383,
      "learning_rate": 0.0001898563844333173,
      "loss": 0.1448,
      "step": 11257
    },
    {
      "epoch": 0.05295539855311062,
      "grad_norm": 0.4874008595943451,
      "learning_rate": 0.00018985544145520383,
      "loss": 0.0598,
      "step": 11258
    },
    {
      "epoch": 0.052960102354723086,
      "grad_norm": 4.239224433898926,
      "learning_rate": 0.00018985449847709035,
      "loss": 0.4253,
      "step": 11259
    },
    {
      "epoch": 0.05296480615633555,
      "grad_norm": 2.713428497314453,
      "learning_rate": 0.00018985355549897687,
      "loss": 0.2508,
      "step": 11260
    },
    {
      "epoch": 0.05296950995794801,
      "grad_norm": 1.9771661758422852,
      "learning_rate": 0.0001898526125208634,
      "loss": 0.19,
      "step": 11261
    },
    {
      "epoch": 0.052974213759560476,
      "grad_norm": 2.171915054321289,
      "learning_rate": 0.00018985166954274993,
      "loss": 0.1865,
      "step": 11262
    },
    {
      "epoch": 0.05297891756117294,
      "grad_norm": 0.5542830228805542,
      "learning_rate": 0.00018985072656463645,
      "loss": 0.0253,
      "step": 11263
    },
    {
      "epoch": 0.0529836213627854,
      "grad_norm": 3.056234359741211,
      "learning_rate": 0.00018984978358652297,
      "loss": 0.5464,
      "step": 11264
    },
    {
      "epoch": 0.052988325164397866,
      "grad_norm": 0.35690465569496155,
      "learning_rate": 0.0001898488406084095,
      "loss": 0.0264,
      "step": 11265
    },
    {
      "epoch": 0.05299302896601033,
      "grad_norm": 2.48203182220459,
      "learning_rate": 0.000189847897630296,
      "loss": 0.5662,
      "step": 11266
    },
    {
      "epoch": 0.05299773276762279,
      "grad_norm": 2.7388346195220947,
      "learning_rate": 0.00018984695465218255,
      "loss": 0.4728,
      "step": 11267
    },
    {
      "epoch": 0.053002436569235256,
      "grad_norm": 7.853936672210693,
      "learning_rate": 0.00018984601167406904,
      "loss": 0.2781,
      "step": 11268
    },
    {
      "epoch": 0.05300714037084772,
      "grad_norm": 3.9393210411071777,
      "learning_rate": 0.00018984506869595556,
      "loss": 0.614,
      "step": 11269
    },
    {
      "epoch": 0.05301184417246018,
      "grad_norm": 2.547987699508667,
      "learning_rate": 0.00018984412571784208,
      "loss": 0.2477,
      "step": 11270
    },
    {
      "epoch": 0.053016547974072646,
      "grad_norm": 2.279655933380127,
      "learning_rate": 0.0001898431827397286,
      "loss": 0.2772,
      "step": 11271
    },
    {
      "epoch": 0.05302125177568511,
      "grad_norm": 1.9566876888275146,
      "learning_rate": 0.00018984223976161515,
      "loss": 0.2186,
      "step": 11272
    },
    {
      "epoch": 0.05302595557729757,
      "grad_norm": 2.3170013427734375,
      "learning_rate": 0.00018984129678350166,
      "loss": 0.2172,
      "step": 11273
    },
    {
      "epoch": 0.053030659378910036,
      "grad_norm": 2.8274524211883545,
      "learning_rate": 0.00018984035380538818,
      "loss": 0.5732,
      "step": 11274
    },
    {
      "epoch": 0.053035363180522495,
      "grad_norm": 0.06137442961335182,
      "learning_rate": 0.0001898394108272747,
      "loss": 0.0036,
      "step": 11275
    },
    {
      "epoch": 0.05304006698213496,
      "grad_norm": 2.633760690689087,
      "learning_rate": 0.00018983846784916125,
      "loss": 0.393,
      "step": 11276
    },
    {
      "epoch": 0.053044770783747426,
      "grad_norm": 1.2579835653305054,
      "learning_rate": 0.00018983752487104777,
      "loss": 0.1458,
      "step": 11277
    },
    {
      "epoch": 0.053049474585359885,
      "grad_norm": 0.36020877957344055,
      "learning_rate": 0.00018983658189293429,
      "loss": 0.0448,
      "step": 11278
    },
    {
      "epoch": 0.05305417838697235,
      "grad_norm": 2.5341405868530273,
      "learning_rate": 0.0001898356389148208,
      "loss": 0.3775,
      "step": 11279
    },
    {
      "epoch": 0.053058882188584816,
      "grad_norm": 2.6589245796203613,
      "learning_rate": 0.0001898346959367073,
      "loss": 0.3774,
      "step": 11280
    },
    {
      "epoch": 0.053063585990197275,
      "grad_norm": 2.542842149734497,
      "learning_rate": 0.00018983375295859384,
      "loss": 0.4208,
      "step": 11281
    },
    {
      "epoch": 0.05306828979180974,
      "grad_norm": 1.0004878044128418,
      "learning_rate": 0.00018983280998048036,
      "loss": 0.1006,
      "step": 11282
    },
    {
      "epoch": 0.053072993593422206,
      "grad_norm": 1.6735080480575562,
      "learning_rate": 0.00018983186700236688,
      "loss": 0.1711,
      "step": 11283
    },
    {
      "epoch": 0.053077697395034665,
      "grad_norm": 6.654757022857666,
      "learning_rate": 0.0001898309240242534,
      "loss": 0.3718,
      "step": 11284
    },
    {
      "epoch": 0.05308240119664713,
      "grad_norm": 1.0176628828048706,
      "learning_rate": 0.00018982998104613994,
      "loss": 0.1669,
      "step": 11285
    },
    {
      "epoch": 0.053087104998259596,
      "grad_norm": 0.9203254580497742,
      "learning_rate": 0.00018982903806802646,
      "loss": 0.1428,
      "step": 11286
    },
    {
      "epoch": 0.053091808799872055,
      "grad_norm": 2.265411138534546,
      "learning_rate": 0.00018982809508991298,
      "loss": 0.5393,
      "step": 11287
    },
    {
      "epoch": 0.05309651260148452,
      "grad_norm": 1.881600260734558,
      "learning_rate": 0.0001898271521117995,
      "loss": 0.1649,
      "step": 11288
    },
    {
      "epoch": 0.053101216403096986,
      "grad_norm": 2.0405993461608887,
      "learning_rate": 0.00018982620913368602,
      "loss": 0.4853,
      "step": 11289
    },
    {
      "epoch": 0.053105920204709445,
      "grad_norm": 3.2532877922058105,
      "learning_rate": 0.00018982526615557254,
      "loss": 0.6965,
      "step": 11290
    },
    {
      "epoch": 0.05311062400632191,
      "grad_norm": 1.1115771532058716,
      "learning_rate": 0.00018982432317745905,
      "loss": 0.0944,
      "step": 11291
    },
    {
      "epoch": 0.053115327807934376,
      "grad_norm": 0.9445090889930725,
      "learning_rate": 0.00018982338019934557,
      "loss": 0.1309,
      "step": 11292
    },
    {
      "epoch": 0.053120031609546835,
      "grad_norm": 1.4535845518112183,
      "learning_rate": 0.0001898224372212321,
      "loss": 0.208,
      "step": 11293
    },
    {
      "epoch": 0.0531247354111593,
      "grad_norm": 1.0065838098526,
      "learning_rate": 0.00018982149424311864,
      "loss": 0.0937,
      "step": 11294
    },
    {
      "epoch": 0.05312943921277176,
      "grad_norm": 0.8944713473320007,
      "learning_rate": 0.00018982055126500516,
      "loss": 0.1223,
      "step": 11295
    },
    {
      "epoch": 0.053134143014384225,
      "grad_norm": 2.3836824893951416,
      "learning_rate": 0.00018981960828689168,
      "loss": 0.3518,
      "step": 11296
    },
    {
      "epoch": 0.05313884681599669,
      "grad_norm": 4.698430061340332,
      "learning_rate": 0.0001898186653087782,
      "loss": 1.2377,
      "step": 11297
    },
    {
      "epoch": 0.05314355061760915,
      "grad_norm": 0.7899144291877747,
      "learning_rate": 0.0001898177223306647,
      "loss": 0.0601,
      "step": 11298
    },
    {
      "epoch": 0.053148254419221615,
      "grad_norm": 1.2661328315734863,
      "learning_rate": 0.00018981677935255123,
      "loss": 0.198,
      "step": 11299
    },
    {
      "epoch": 0.05315295822083408,
      "grad_norm": 0.7533360123634338,
      "learning_rate": 0.00018981583637443775,
      "loss": 0.0878,
      "step": 11300
    },
    {
      "epoch": 0.05315766202244654,
      "grad_norm": 1.2053239345550537,
      "learning_rate": 0.00018981489339632427,
      "loss": 0.1194,
      "step": 11301
    },
    {
      "epoch": 0.053162365824059005,
      "grad_norm": 3.415482997894287,
      "learning_rate": 0.0001898139504182108,
      "loss": 0.4071,
      "step": 11302
    },
    {
      "epoch": 0.05316706962567147,
      "grad_norm": 1.4988943338394165,
      "learning_rate": 0.00018981300744009733,
      "loss": 0.0263,
      "step": 11303
    },
    {
      "epoch": 0.05317177342728393,
      "grad_norm": 1.1407556533813477,
      "learning_rate": 0.00018981206446198385,
      "loss": 0.058,
      "step": 11304
    },
    {
      "epoch": 0.053176477228896395,
      "grad_norm": 1.1753530502319336,
      "learning_rate": 0.00018981112148387037,
      "loss": 0.0845,
      "step": 11305
    },
    {
      "epoch": 0.05318118103050886,
      "grad_norm": 2.8244435787200928,
      "learning_rate": 0.0001898101785057569,
      "loss": 0.4534,
      "step": 11306
    },
    {
      "epoch": 0.05318588483212132,
      "grad_norm": 3.434605836868286,
      "learning_rate": 0.0001898092355276434,
      "loss": 0.2838,
      "step": 11307
    },
    {
      "epoch": 0.053190588633733785,
      "grad_norm": 1.4603517055511475,
      "learning_rate": 0.00018980829254952995,
      "loss": 0.0925,
      "step": 11308
    },
    {
      "epoch": 0.05319529243534625,
      "grad_norm": 4.576892375946045,
      "learning_rate": 0.00018980734957141647,
      "loss": 0.7792,
      "step": 11309
    },
    {
      "epoch": 0.05319999623695871,
      "grad_norm": 1.6376696825027466,
      "learning_rate": 0.00018980640659330296,
      "loss": 0.1943,
      "step": 11310
    },
    {
      "epoch": 0.053204700038571175,
      "grad_norm": 2.320239305496216,
      "learning_rate": 0.00018980546361518948,
      "loss": 0.2271,
      "step": 11311
    },
    {
      "epoch": 0.05320940384018363,
      "grad_norm": 6.036534309387207,
      "learning_rate": 0.00018980452063707603,
      "loss": 0.6856,
      "step": 11312
    },
    {
      "epoch": 0.0532141076417961,
      "grad_norm": 1.5187456607818604,
      "learning_rate": 0.00018980357765896255,
      "loss": 0.1463,
      "step": 11313
    },
    {
      "epoch": 0.053218811443408565,
      "grad_norm": 4.271427631378174,
      "learning_rate": 0.00018980263468084906,
      "loss": 0.9877,
      "step": 11314
    },
    {
      "epoch": 0.05322351524502102,
      "grad_norm": 2.523989200592041,
      "learning_rate": 0.00018980169170273558,
      "loss": 0.1906,
      "step": 11315
    },
    {
      "epoch": 0.05322821904663349,
      "grad_norm": 4.98156213760376,
      "learning_rate": 0.0001898007487246221,
      "loss": 0.9998,
      "step": 11316
    },
    {
      "epoch": 0.053232922848245955,
      "grad_norm": 0.5896773934364319,
      "learning_rate": 0.00018979980574650865,
      "loss": 0.052,
      "step": 11317
    },
    {
      "epoch": 0.05323762664985841,
      "grad_norm": 0.2659260928630829,
      "learning_rate": 0.00018979886276839517,
      "loss": 0.0217,
      "step": 11318
    },
    {
      "epoch": 0.05324233045147088,
      "grad_norm": 1.585430383682251,
      "learning_rate": 0.00018979791979028169,
      "loss": 0.2227,
      "step": 11319
    },
    {
      "epoch": 0.053247034253083345,
      "grad_norm": 3.3062543869018555,
      "learning_rate": 0.0001897969768121682,
      "loss": 0.5836,
      "step": 11320
    },
    {
      "epoch": 0.0532517380546958,
      "grad_norm": 0.9760976433753967,
      "learning_rate": 0.00018979603383405472,
      "loss": 0.1375,
      "step": 11321
    },
    {
      "epoch": 0.05325644185630827,
      "grad_norm": 9.359551429748535,
      "learning_rate": 0.00018979509085594124,
      "loss": 0.7698,
      "step": 11322
    },
    {
      "epoch": 0.053261145657920735,
      "grad_norm": 2.3758904933929443,
      "learning_rate": 0.00018979414787782776,
      "loss": 0.4363,
      "step": 11323
    },
    {
      "epoch": 0.05326584945953319,
      "grad_norm": 1.179463505744934,
      "learning_rate": 0.00018979320489971428,
      "loss": 0.1316,
      "step": 11324
    },
    {
      "epoch": 0.05327055326114566,
      "grad_norm": 1.4993798732757568,
      "learning_rate": 0.0001897922619216008,
      "loss": 0.1861,
      "step": 11325
    },
    {
      "epoch": 0.053275257062758125,
      "grad_norm": 0.9435189962387085,
      "learning_rate": 0.00018979131894348734,
      "loss": 0.1346,
      "step": 11326
    },
    {
      "epoch": 0.05327996086437058,
      "grad_norm": 3.7691242694854736,
      "learning_rate": 0.00018979037596537386,
      "loss": 0.3522,
      "step": 11327
    },
    {
      "epoch": 0.05328466466598305,
      "grad_norm": 1.630387544631958,
      "learning_rate": 0.00018978943298726038,
      "loss": 0.2484,
      "step": 11328
    },
    {
      "epoch": 0.05328936846759551,
      "grad_norm": 4.577469348907471,
      "learning_rate": 0.0001897884900091469,
      "loss": 0.8541,
      "step": 11329
    },
    {
      "epoch": 0.05329407226920797,
      "grad_norm": 3.6795053482055664,
      "learning_rate": 0.00018978754703103342,
      "loss": 0.5216,
      "step": 11330
    },
    {
      "epoch": 0.05329877607082044,
      "grad_norm": 0.93960040807724,
      "learning_rate": 0.00018978660405291994,
      "loss": 0.1151,
      "step": 11331
    },
    {
      "epoch": 0.0533034798724329,
      "grad_norm": 0.5263534188270569,
      "learning_rate": 0.00018978566107480645,
      "loss": 0.0617,
      "step": 11332
    },
    {
      "epoch": 0.05330818367404536,
      "grad_norm": 1.466442584991455,
      "learning_rate": 0.00018978471809669297,
      "loss": 0.1196,
      "step": 11333
    },
    {
      "epoch": 0.05331288747565783,
      "grad_norm": 1.3672624826431274,
      "learning_rate": 0.0001897837751185795,
      "loss": 0.2388,
      "step": 11334
    },
    {
      "epoch": 0.05331759127727029,
      "grad_norm": 0.6625303030014038,
      "learning_rate": 0.00018978283214046604,
      "loss": 0.1087,
      "step": 11335
    },
    {
      "epoch": 0.05332229507888275,
      "grad_norm": 0.4498996138572693,
      "learning_rate": 0.00018978188916235256,
      "loss": 0.0321,
      "step": 11336
    },
    {
      "epoch": 0.05332699888049522,
      "grad_norm": 3.6828315258026123,
      "learning_rate": 0.00018978094618423908,
      "loss": 0.5165,
      "step": 11337
    },
    {
      "epoch": 0.05333170268210768,
      "grad_norm": 4.636465549468994,
      "learning_rate": 0.0001897800032061256,
      "loss": 0.8804,
      "step": 11338
    },
    {
      "epoch": 0.05333640648372014,
      "grad_norm": 0.5329011082649231,
      "learning_rate": 0.0001897790602280121,
      "loss": 0.0432,
      "step": 11339
    },
    {
      "epoch": 0.05334111028533261,
      "grad_norm": 1.6964603662490845,
      "learning_rate": 0.00018977811724989866,
      "loss": 0.1915,
      "step": 11340
    },
    {
      "epoch": 0.05334581408694507,
      "grad_norm": 1.2824276685714722,
      "learning_rate": 0.00018977717427178515,
      "loss": 0.1175,
      "step": 11341
    },
    {
      "epoch": 0.05335051788855753,
      "grad_norm": 0.1985948234796524,
      "learning_rate": 0.00018977623129367167,
      "loss": 0.013,
      "step": 11342
    },
    {
      "epoch": 0.05335522169017,
      "grad_norm": 0.577021598815918,
      "learning_rate": 0.0001897752883155582,
      "loss": 0.0624,
      "step": 11343
    },
    {
      "epoch": 0.05335992549178246,
      "grad_norm": 1.858668327331543,
      "learning_rate": 0.00018977434533744473,
      "loss": 0.22,
      "step": 11344
    },
    {
      "epoch": 0.05336462929339492,
      "grad_norm": 0.5260825753211975,
      "learning_rate": 0.00018977340235933125,
      "loss": 0.0502,
      "step": 11345
    },
    {
      "epoch": 0.05336933309500738,
      "grad_norm": 1.5841294527053833,
      "learning_rate": 0.00018977245938121777,
      "loss": 0.3119,
      "step": 11346
    },
    {
      "epoch": 0.05337403689661985,
      "grad_norm": 1.7578703165054321,
      "learning_rate": 0.0001897715164031043,
      "loss": 0.2789,
      "step": 11347
    },
    {
      "epoch": 0.05337874069823231,
      "grad_norm": 1.5489624738693237,
      "learning_rate": 0.0001897705734249908,
      "loss": 0.3324,
      "step": 11348
    },
    {
      "epoch": 0.05338344449984477,
      "grad_norm": 1.5101242065429688,
      "learning_rate": 0.00018976963044687735,
      "loss": 0.2162,
      "step": 11349
    },
    {
      "epoch": 0.05338814830145724,
      "grad_norm": 3.162872076034546,
      "learning_rate": 0.00018976868746876387,
      "loss": 0.5856,
      "step": 11350
    },
    {
      "epoch": 0.0533928521030697,
      "grad_norm": 0.29503369331359863,
      "learning_rate": 0.0001897677444906504,
      "loss": 0.0113,
      "step": 11351
    },
    {
      "epoch": 0.05339755590468216,
      "grad_norm": 1.84490966796875,
      "learning_rate": 0.0001897668015125369,
      "loss": 0.1818,
      "step": 11352
    },
    {
      "epoch": 0.05340225970629463,
      "grad_norm": 2.934601306915283,
      "learning_rate": 0.00018976585853442343,
      "loss": 0.1438,
      "step": 11353
    },
    {
      "epoch": 0.05340696350790709,
      "grad_norm": 2.3432281017303467,
      "learning_rate": 0.00018976491555630995,
      "loss": 0.1385,
      "step": 11354
    },
    {
      "epoch": 0.05341166730951955,
      "grad_norm": 3.0488011837005615,
      "learning_rate": 0.00018976397257819646,
      "loss": 0.5959,
      "step": 11355
    },
    {
      "epoch": 0.05341637111113202,
      "grad_norm": 1.5047929286956787,
      "learning_rate": 0.00018976302960008298,
      "loss": 0.1282,
      "step": 11356
    },
    {
      "epoch": 0.05342107491274448,
      "grad_norm": 11.13905143737793,
      "learning_rate": 0.0001897620866219695,
      "loss": 0.3739,
      "step": 11357
    },
    {
      "epoch": 0.05342577871435694,
      "grad_norm": 5.052691459655762,
      "learning_rate": 0.00018976114364385605,
      "loss": 0.6609,
      "step": 11358
    },
    {
      "epoch": 0.05343048251596941,
      "grad_norm": 1.4523096084594727,
      "learning_rate": 0.00018976020066574257,
      "loss": 0.1766,
      "step": 11359
    },
    {
      "epoch": 0.05343518631758187,
      "grad_norm": 3.620882511138916,
      "learning_rate": 0.00018975925768762909,
      "loss": 0.5195,
      "step": 11360
    },
    {
      "epoch": 0.05343989011919433,
      "grad_norm": 4.565093040466309,
      "learning_rate": 0.0001897583147095156,
      "loss": 0.2935,
      "step": 11361
    },
    {
      "epoch": 0.0534445939208068,
      "grad_norm": 2.7266008853912354,
      "learning_rate": 0.00018975737173140212,
      "loss": 0.3005,
      "step": 11362
    },
    {
      "epoch": 0.053449297722419256,
      "grad_norm": 2.3705801963806152,
      "learning_rate": 0.00018975642875328864,
      "loss": 0.3267,
      "step": 11363
    },
    {
      "epoch": 0.05345400152403172,
      "grad_norm": 1.5338640213012695,
      "learning_rate": 0.00018975548577517516,
      "loss": 0.2776,
      "step": 11364
    },
    {
      "epoch": 0.05345870532564419,
      "grad_norm": 4.018645286560059,
      "learning_rate": 0.00018975454279706168,
      "loss": 0.7089,
      "step": 11365
    },
    {
      "epoch": 0.053463409127256646,
      "grad_norm": 0.699396014213562,
      "learning_rate": 0.0001897535998189482,
      "loss": 0.0746,
      "step": 11366
    },
    {
      "epoch": 0.05346811292886911,
      "grad_norm": 1.9102455377578735,
      "learning_rate": 0.00018975265684083474,
      "loss": 0.2583,
      "step": 11367
    },
    {
      "epoch": 0.05347281673048158,
      "grad_norm": 4.2207350730896,
      "learning_rate": 0.00018975171386272126,
      "loss": 0.7523,
      "step": 11368
    },
    {
      "epoch": 0.053477520532094036,
      "grad_norm": 2.7639544010162354,
      "learning_rate": 0.00018975077088460778,
      "loss": 0.605,
      "step": 11369
    },
    {
      "epoch": 0.0534822243337065,
      "grad_norm": 1.9460902214050293,
      "learning_rate": 0.0001897498279064943,
      "loss": 0.3366,
      "step": 11370
    },
    {
      "epoch": 0.05348692813531897,
      "grad_norm": 2.683896064758301,
      "learning_rate": 0.00018974888492838084,
      "loss": 0.4996,
      "step": 11371
    },
    {
      "epoch": 0.053491631936931426,
      "grad_norm": 1.839570164680481,
      "learning_rate": 0.00018974794195026734,
      "loss": 0.3066,
      "step": 11372
    },
    {
      "epoch": 0.05349633573854389,
      "grad_norm": 0.7343195676803589,
      "learning_rate": 0.00018974699897215385,
      "loss": 0.1048,
      "step": 11373
    },
    {
      "epoch": 0.05350103954015636,
      "grad_norm": 1.6465084552764893,
      "learning_rate": 0.00018974605599404037,
      "loss": 0.318,
      "step": 11374
    },
    {
      "epoch": 0.053505743341768816,
      "grad_norm": 2.947295904159546,
      "learning_rate": 0.0001897451130159269,
      "loss": 0.4264,
      "step": 11375
    },
    {
      "epoch": 0.05351044714338128,
      "grad_norm": 1.6605491638183594,
      "learning_rate": 0.00018974417003781344,
      "loss": 0.1795,
      "step": 11376
    },
    {
      "epoch": 0.05351515094499375,
      "grad_norm": 2.3642005920410156,
      "learning_rate": 0.00018974322705969996,
      "loss": 0.4932,
      "step": 11377
    },
    {
      "epoch": 0.053519854746606206,
      "grad_norm": 1.7490020990371704,
      "learning_rate": 0.00018974228408158647,
      "loss": 0.2796,
      "step": 11378
    },
    {
      "epoch": 0.05352455854821867,
      "grad_norm": 1.4695732593536377,
      "learning_rate": 0.000189741341103473,
      "loss": 0.1468,
      "step": 11379
    },
    {
      "epoch": 0.05352926234983113,
      "grad_norm": 0.9993171095848083,
      "learning_rate": 0.0001897403981253595,
      "loss": 0.1611,
      "step": 11380
    },
    {
      "epoch": 0.053533966151443596,
      "grad_norm": 1.278041958808899,
      "learning_rate": 0.00018973945514724606,
      "loss": 0.281,
      "step": 11381
    },
    {
      "epoch": 0.05353866995305606,
      "grad_norm": 1.9003514051437378,
      "learning_rate": 0.00018973851216913258,
      "loss": 0.4877,
      "step": 11382
    },
    {
      "epoch": 0.05354337375466852,
      "grad_norm": 0.9195967316627502,
      "learning_rate": 0.0001897375691910191,
      "loss": 0.0927,
      "step": 11383
    },
    {
      "epoch": 0.053548077556280986,
      "grad_norm": 0.8949160575866699,
      "learning_rate": 0.0001897366262129056,
      "loss": 0.0722,
      "step": 11384
    },
    {
      "epoch": 0.05355278135789345,
      "grad_norm": 1.2396376132965088,
      "learning_rate": 0.00018973568323479213,
      "loss": 0.2137,
      "step": 11385
    },
    {
      "epoch": 0.05355748515950591,
      "grad_norm": 1.482138991355896,
      "learning_rate": 0.00018973474025667865,
      "loss": 0.2841,
      "step": 11386
    },
    {
      "epoch": 0.053562188961118376,
      "grad_norm": 3.0999815464019775,
      "learning_rate": 0.00018973379727856517,
      "loss": 0.747,
      "step": 11387
    },
    {
      "epoch": 0.05356689276273084,
      "grad_norm": 0.6530661582946777,
      "learning_rate": 0.0001897328543004517,
      "loss": 0.0674,
      "step": 11388
    },
    {
      "epoch": 0.0535715965643433,
      "grad_norm": 0.6004030704498291,
      "learning_rate": 0.0001897319113223382,
      "loss": 0.0875,
      "step": 11389
    },
    {
      "epoch": 0.053576300365955766,
      "grad_norm": 1.5766186714172363,
      "learning_rate": 0.00018973096834422475,
      "loss": 0.3012,
      "step": 11390
    },
    {
      "epoch": 0.05358100416756823,
      "grad_norm": 6.008934497833252,
      "learning_rate": 0.00018973002536611127,
      "loss": 0.4348,
      "step": 11391
    },
    {
      "epoch": 0.05358570796918069,
      "grad_norm": 1.7658840417861938,
      "learning_rate": 0.0001897290823879978,
      "loss": 0.2773,
      "step": 11392
    },
    {
      "epoch": 0.053590411770793156,
      "grad_norm": 2.5877652168273926,
      "learning_rate": 0.0001897281394098843,
      "loss": 0.2976,
      "step": 11393
    },
    {
      "epoch": 0.05359511557240562,
      "grad_norm": 1.1641877889633179,
      "learning_rate": 0.00018972719643177083,
      "loss": 0.1205,
      "step": 11394
    },
    {
      "epoch": 0.05359981937401808,
      "grad_norm": 1.4323275089263916,
      "learning_rate": 0.00018972625345365735,
      "loss": 0.1748,
      "step": 11395
    },
    {
      "epoch": 0.053604523175630546,
      "grad_norm": 1.9426167011260986,
      "learning_rate": 0.00018972531047554386,
      "loss": 0.3524,
      "step": 11396
    },
    {
      "epoch": 0.053609226977243005,
      "grad_norm": 1.750556230545044,
      "learning_rate": 0.00018972436749743038,
      "loss": 0.3743,
      "step": 11397
    },
    {
      "epoch": 0.05361393077885547,
      "grad_norm": 3.0862982273101807,
      "learning_rate": 0.0001897234245193169,
      "loss": 0.4621,
      "step": 11398
    },
    {
      "epoch": 0.053618634580467936,
      "grad_norm": 3.183573007583618,
      "learning_rate": 0.00018972248154120345,
      "loss": 0.2685,
      "step": 11399
    },
    {
      "epoch": 0.053623338382080395,
      "grad_norm": 3.568774700164795,
      "learning_rate": 0.00018972153856308997,
      "loss": 1.0455,
      "step": 11400
    },
    {
      "epoch": 0.05362804218369286,
      "grad_norm": 3.3487796783447266,
      "learning_rate": 0.00018972059558497649,
      "loss": 0.4523,
      "step": 11401
    },
    {
      "epoch": 0.053632745985305326,
      "grad_norm": 3.0491228103637695,
      "learning_rate": 0.000189719652606863,
      "loss": 0.153,
      "step": 11402
    },
    {
      "epoch": 0.053637449786917785,
      "grad_norm": 3.873990058898926,
      "learning_rate": 0.00018971870962874952,
      "loss": 0.3216,
      "step": 11403
    },
    {
      "epoch": 0.05364215358853025,
      "grad_norm": 1.8204501867294312,
      "learning_rate": 0.00018971776665063604,
      "loss": 0.2548,
      "step": 11404
    },
    {
      "epoch": 0.053646857390142716,
      "grad_norm": 2.0942726135253906,
      "learning_rate": 0.00018971682367252256,
      "loss": 0.27,
      "step": 11405
    },
    {
      "epoch": 0.053651561191755175,
      "grad_norm": 7.650214195251465,
      "learning_rate": 0.00018971588069440908,
      "loss": 1.0172,
      "step": 11406
    },
    {
      "epoch": 0.05365626499336764,
      "grad_norm": 1.1381816864013672,
      "learning_rate": 0.0001897149377162956,
      "loss": 0.1291,
      "step": 11407
    },
    {
      "epoch": 0.053660968794980106,
      "grad_norm": 1.0303229093551636,
      "learning_rate": 0.00018971399473818214,
      "loss": 0.1252,
      "step": 11408
    },
    {
      "epoch": 0.053665672596592565,
      "grad_norm": 2.4405856132507324,
      "learning_rate": 0.00018971305176006866,
      "loss": 0.3518,
      "step": 11409
    },
    {
      "epoch": 0.05367037639820503,
      "grad_norm": 3.173877477645874,
      "learning_rate": 0.00018971210878195518,
      "loss": 0.4558,
      "step": 11410
    },
    {
      "epoch": 0.053675080199817496,
      "grad_norm": 0.5828625559806824,
      "learning_rate": 0.0001897111658038417,
      "loss": 0.0473,
      "step": 11411
    },
    {
      "epoch": 0.053679784001429955,
      "grad_norm": 1.5361509323120117,
      "learning_rate": 0.00018971022282572824,
      "loss": 0.2067,
      "step": 11412
    },
    {
      "epoch": 0.05368448780304242,
      "grad_norm": 1.3764052391052246,
      "learning_rate": 0.00018970927984761476,
      "loss": 0.1994,
      "step": 11413
    },
    {
      "epoch": 0.05368919160465488,
      "grad_norm": 1.368850588798523,
      "learning_rate": 0.00018970833686950128,
      "loss": 0.1583,
      "step": 11414
    },
    {
      "epoch": 0.053693895406267345,
      "grad_norm": 3.552978515625,
      "learning_rate": 0.00018970739389138777,
      "loss": 0.3997,
      "step": 11415
    },
    {
      "epoch": 0.05369859920787981,
      "grad_norm": 3.033881664276123,
      "learning_rate": 0.0001897064509132743,
      "loss": 0.3899,
      "step": 11416
    },
    {
      "epoch": 0.05370330300949227,
      "grad_norm": 1.056774377822876,
      "learning_rate": 0.00018970550793516084,
      "loss": 0.1145,
      "step": 11417
    },
    {
      "epoch": 0.053708006811104735,
      "grad_norm": 1.2669150829315186,
      "learning_rate": 0.00018970456495704736,
      "loss": 0.1837,
      "step": 11418
    },
    {
      "epoch": 0.0537127106127172,
      "grad_norm": 1.3898870944976807,
      "learning_rate": 0.00018970362197893387,
      "loss": 0.1289,
      "step": 11419
    },
    {
      "epoch": 0.05371741441432966,
      "grad_norm": 1.7164087295532227,
      "learning_rate": 0.0001897026790008204,
      "loss": 0.3017,
      "step": 11420
    },
    {
      "epoch": 0.053722118215942125,
      "grad_norm": 0.9012064337730408,
      "learning_rate": 0.00018970173602270694,
      "loss": 0.086,
      "step": 11421
    },
    {
      "epoch": 0.05372682201755459,
      "grad_norm": 1.0225297212600708,
      "learning_rate": 0.00018970079304459346,
      "loss": 0.1235,
      "step": 11422
    },
    {
      "epoch": 0.05373152581916705,
      "grad_norm": 0.5431203842163086,
      "learning_rate": 0.00018969985006647998,
      "loss": 0.0698,
      "step": 11423
    },
    {
      "epoch": 0.053736229620779515,
      "grad_norm": 0.36497193574905396,
      "learning_rate": 0.0001896989070883665,
      "loss": 0.0255,
      "step": 11424
    },
    {
      "epoch": 0.05374093342239198,
      "grad_norm": 4.385657787322998,
      "learning_rate": 0.00018969796411025301,
      "loss": 0.6812,
      "step": 11425
    },
    {
      "epoch": 0.05374563722400444,
      "grad_norm": 3.5350253582000732,
      "learning_rate": 0.00018969702113213953,
      "loss": 0.4664,
      "step": 11426
    },
    {
      "epoch": 0.053750341025616905,
      "grad_norm": 2.6985814571380615,
      "learning_rate": 0.00018969607815402605,
      "loss": 0.5049,
      "step": 11427
    },
    {
      "epoch": 0.05375504482722937,
      "grad_norm": 1.2736423015594482,
      "learning_rate": 0.00018969513517591257,
      "loss": 0.2197,
      "step": 11428
    },
    {
      "epoch": 0.05375974862884183,
      "grad_norm": 2.2442314624786377,
      "learning_rate": 0.0001896941921977991,
      "loss": 0.1964,
      "step": 11429
    },
    {
      "epoch": 0.053764452430454295,
      "grad_norm": 3.339454174041748,
      "learning_rate": 0.0001896932492196856,
      "loss": 0.5122,
      "step": 11430
    },
    {
      "epoch": 0.05376915623206675,
      "grad_norm": 1.1535841226577759,
      "learning_rate": 0.00018969230624157215,
      "loss": 0.1023,
      "step": 11431
    },
    {
      "epoch": 0.05377386003367922,
      "grad_norm": 2.760284185409546,
      "learning_rate": 0.00018969136326345867,
      "loss": 0.2474,
      "step": 11432
    },
    {
      "epoch": 0.053778563835291684,
      "grad_norm": 2.187472105026245,
      "learning_rate": 0.0001896904202853452,
      "loss": 0.249,
      "step": 11433
    },
    {
      "epoch": 0.05378326763690414,
      "grad_norm": 0.6652581691741943,
      "learning_rate": 0.0001896894773072317,
      "loss": 0.0589,
      "step": 11434
    },
    {
      "epoch": 0.05378797143851661,
      "grad_norm": 2.011652708053589,
      "learning_rate": 0.00018968853432911823,
      "loss": 0.2809,
      "step": 11435
    },
    {
      "epoch": 0.053792675240129074,
      "grad_norm": 2.0266857147216797,
      "learning_rate": 0.00018968759135100475,
      "loss": 0.2171,
      "step": 11436
    },
    {
      "epoch": 0.05379737904174153,
      "grad_norm": 2.3272175788879395,
      "learning_rate": 0.00018968664837289126,
      "loss": 0.3601,
      "step": 11437
    },
    {
      "epoch": 0.053802082843354,
      "grad_norm": 1.1562373638153076,
      "learning_rate": 0.00018968570539477778,
      "loss": 0.0991,
      "step": 11438
    },
    {
      "epoch": 0.053806786644966464,
      "grad_norm": 3.4371485710144043,
      "learning_rate": 0.0001896847624166643,
      "loss": 1.133,
      "step": 11439
    },
    {
      "epoch": 0.05381149044657892,
      "grad_norm": 3.1412696838378906,
      "learning_rate": 0.00018968381943855085,
      "loss": 0.7368,
      "step": 11440
    },
    {
      "epoch": 0.05381619424819139,
      "grad_norm": 0.5731648206710815,
      "learning_rate": 0.00018968287646043737,
      "loss": 0.0603,
      "step": 11441
    },
    {
      "epoch": 0.053820898049803854,
      "grad_norm": 1.5579735040664673,
      "learning_rate": 0.00018968193348232389,
      "loss": 0.1936,
      "step": 11442
    },
    {
      "epoch": 0.05382560185141631,
      "grad_norm": 4.831932067871094,
      "learning_rate": 0.0001896809905042104,
      "loss": 1.5232,
      "step": 11443
    },
    {
      "epoch": 0.05383030565302878,
      "grad_norm": 1.6470049619674683,
      "learning_rate": 0.00018968004752609695,
      "loss": 0.1901,
      "step": 11444
    },
    {
      "epoch": 0.053835009454641244,
      "grad_norm": 3.6362972259521484,
      "learning_rate": 0.00018967910454798347,
      "loss": 1.1444,
      "step": 11445
    },
    {
      "epoch": 0.0538397132562537,
      "grad_norm": 0.6319689750671387,
      "learning_rate": 0.00018967816156986996,
      "loss": 0.084,
      "step": 11446
    },
    {
      "epoch": 0.05384441705786617,
      "grad_norm": 2.659027338027954,
      "learning_rate": 0.00018967721859175648,
      "loss": 0.6082,
      "step": 11447
    },
    {
      "epoch": 0.05384912085947863,
      "grad_norm": 6.517429828643799,
      "learning_rate": 0.000189676275613643,
      "loss": 0.582,
      "step": 11448
    },
    {
      "epoch": 0.05385382466109109,
      "grad_norm": 1.178556203842163,
      "learning_rate": 0.00018967533263552954,
      "loss": 0.1118,
      "step": 11449
    },
    {
      "epoch": 0.05385852846270356,
      "grad_norm": 0.6982410550117493,
      "learning_rate": 0.00018967438965741606,
      "loss": 0.1185,
      "step": 11450
    },
    {
      "epoch": 0.05386323226431602,
      "grad_norm": 7.5657057762146,
      "learning_rate": 0.00018967344667930258,
      "loss": 0.5323,
      "step": 11451
    },
    {
      "epoch": 0.05386793606592848,
      "grad_norm": 2.8686652183532715,
      "learning_rate": 0.0001896725037011891,
      "loss": 0.2243,
      "step": 11452
    },
    {
      "epoch": 0.05387263986754095,
      "grad_norm": 0.41414743661880493,
      "learning_rate": 0.00018967156072307564,
      "loss": 0.0689,
      "step": 11453
    },
    {
      "epoch": 0.05387734366915341,
      "grad_norm": 3.818605661392212,
      "learning_rate": 0.00018967061774496216,
      "loss": 0.3245,
      "step": 11454
    },
    {
      "epoch": 0.05388204747076587,
      "grad_norm": 1.012717604637146,
      "learning_rate": 0.00018966967476684868,
      "loss": 0.1121,
      "step": 11455
    },
    {
      "epoch": 0.05388675127237834,
      "grad_norm": 3.1466729640960693,
      "learning_rate": 0.0001896687317887352,
      "loss": 0.3511,
      "step": 11456
    },
    {
      "epoch": 0.0538914550739908,
      "grad_norm": 1.4935981035232544,
      "learning_rate": 0.0001896677888106217,
      "loss": 0.2117,
      "step": 11457
    },
    {
      "epoch": 0.05389615887560326,
      "grad_norm": 0.500777542591095,
      "learning_rate": 0.00018966684583250824,
      "loss": 0.0365,
      "step": 11458
    },
    {
      "epoch": 0.05390086267721573,
      "grad_norm": 0.7666087746620178,
      "learning_rate": 0.00018966590285439476,
      "loss": 0.0747,
      "step": 11459
    },
    {
      "epoch": 0.05390556647882819,
      "grad_norm": 2.2512388229370117,
      "learning_rate": 0.00018966495987628127,
      "loss": 0.2512,
      "step": 11460
    },
    {
      "epoch": 0.05391027028044065,
      "grad_norm": 2.471043825149536,
      "learning_rate": 0.0001896640168981678,
      "loss": 0.4439,
      "step": 11461
    },
    {
      "epoch": 0.05391497408205312,
      "grad_norm": 1.8303065299987793,
      "learning_rate": 0.00018966307392005434,
      "loss": 0.1905,
      "step": 11462
    },
    {
      "epoch": 0.05391967788366558,
      "grad_norm": 2.627420663833618,
      "learning_rate": 0.00018966213094194086,
      "loss": 0.3105,
      "step": 11463
    },
    {
      "epoch": 0.05392438168527804,
      "grad_norm": 1.9878700971603394,
      "learning_rate": 0.00018966118796382738,
      "loss": 0.1066,
      "step": 11464
    },
    {
      "epoch": 0.0539290854868905,
      "grad_norm": 1.8534966707229614,
      "learning_rate": 0.0001896602449857139,
      "loss": 0.211,
      "step": 11465
    },
    {
      "epoch": 0.05393378928850297,
      "grad_norm": 2.0389788150787354,
      "learning_rate": 0.00018965930200760041,
      "loss": 0.2789,
      "step": 11466
    },
    {
      "epoch": 0.05393849309011543,
      "grad_norm": 0.5033938884735107,
      "learning_rate": 0.00018965835902948693,
      "loss": 0.0529,
      "step": 11467
    },
    {
      "epoch": 0.05394319689172789,
      "grad_norm": 3.5058541297912598,
      "learning_rate": 0.00018965741605137345,
      "loss": 0.5956,
      "step": 11468
    },
    {
      "epoch": 0.05394790069334036,
      "grad_norm": 0.6636449694633484,
      "learning_rate": 0.00018965647307325997,
      "loss": 0.0557,
      "step": 11469
    },
    {
      "epoch": 0.05395260449495282,
      "grad_norm": 2.491323709487915,
      "learning_rate": 0.0001896555300951465,
      "loss": 0.2341,
      "step": 11470
    },
    {
      "epoch": 0.05395730829656528,
      "grad_norm": 2.1201424598693848,
      "learning_rate": 0.00018965458711703303,
      "loss": 0.4909,
      "step": 11471
    },
    {
      "epoch": 0.05396201209817775,
      "grad_norm": 1.1629338264465332,
      "learning_rate": 0.00018965364413891955,
      "loss": 0.1254,
      "step": 11472
    },
    {
      "epoch": 0.05396671589979021,
      "grad_norm": 0.6676263213157654,
      "learning_rate": 0.00018965270116080607,
      "loss": 0.1089,
      "step": 11473
    },
    {
      "epoch": 0.05397141970140267,
      "grad_norm": 2.5067336559295654,
      "learning_rate": 0.0001896517581826926,
      "loss": 0.328,
      "step": 11474
    },
    {
      "epoch": 0.05397612350301514,
      "grad_norm": 0.6538952589035034,
      "learning_rate": 0.0001896508152045791,
      "loss": 0.071,
      "step": 11475
    },
    {
      "epoch": 0.0539808273046276,
      "grad_norm": 1.2474983930587769,
      "learning_rate": 0.00018964987222646565,
      "loss": 0.1036,
      "step": 11476
    },
    {
      "epoch": 0.05398553110624006,
      "grad_norm": 0.6171140074729919,
      "learning_rate": 0.00018964892924835215,
      "loss": 0.0502,
      "step": 11477
    },
    {
      "epoch": 0.05399023490785253,
      "grad_norm": 0.8352941870689392,
      "learning_rate": 0.00018964798627023866,
      "loss": 0.08,
      "step": 11478
    },
    {
      "epoch": 0.05399493870946499,
      "grad_norm": 2.2753243446350098,
      "learning_rate": 0.00018964704329212518,
      "loss": 0.2354,
      "step": 11479
    },
    {
      "epoch": 0.05399964251107745,
      "grad_norm": 2.24483060836792,
      "learning_rate": 0.0001896461003140117,
      "loss": 0.2889,
      "step": 11480
    },
    {
      "epoch": 0.05400434631268992,
      "grad_norm": 3.042526960372925,
      "learning_rate": 0.00018964515733589825,
      "loss": 0.7044,
      "step": 11481
    },
    {
      "epoch": 0.054009050114302376,
      "grad_norm": 0.2483222335577011,
      "learning_rate": 0.00018964421435778477,
      "loss": 0.0155,
      "step": 11482
    },
    {
      "epoch": 0.05401375391591484,
      "grad_norm": 3.53145694732666,
      "learning_rate": 0.00018964327137967129,
      "loss": 0.3199,
      "step": 11483
    },
    {
      "epoch": 0.05401845771752731,
      "grad_norm": 4.0943169593811035,
      "learning_rate": 0.0001896423284015578,
      "loss": 0.8987,
      "step": 11484
    },
    {
      "epoch": 0.054023161519139766,
      "grad_norm": 1.67429780960083,
      "learning_rate": 0.00018964138542344435,
      "loss": 0.2086,
      "step": 11485
    },
    {
      "epoch": 0.05402786532075223,
      "grad_norm": 2.8972842693328857,
      "learning_rate": 0.00018964044244533087,
      "loss": 0.6071,
      "step": 11486
    },
    {
      "epoch": 0.0540325691223647,
      "grad_norm": 3.0322160720825195,
      "learning_rate": 0.0001896394994672174,
      "loss": 0.3291,
      "step": 11487
    },
    {
      "epoch": 0.054037272923977156,
      "grad_norm": 1.9915019273757935,
      "learning_rate": 0.00018963855648910388,
      "loss": 0.4099,
      "step": 11488
    },
    {
      "epoch": 0.05404197672558962,
      "grad_norm": 3.3257079124450684,
      "learning_rate": 0.0001896376135109904,
      "loss": 0.4372,
      "step": 11489
    },
    {
      "epoch": 0.05404668052720209,
      "grad_norm": 2.253572702407837,
      "learning_rate": 0.00018963667053287694,
      "loss": 0.3966,
      "step": 11490
    },
    {
      "epoch": 0.054051384328814546,
      "grad_norm": 5.303562164306641,
      "learning_rate": 0.00018963572755476346,
      "loss": 0.9423,
      "step": 11491
    },
    {
      "epoch": 0.05405608813042701,
      "grad_norm": 2.253488540649414,
      "learning_rate": 0.00018963478457664998,
      "loss": 0.4232,
      "step": 11492
    },
    {
      "epoch": 0.05406079193203948,
      "grad_norm": 2.476624011993408,
      "learning_rate": 0.0001896338415985365,
      "loss": 0.3963,
      "step": 11493
    },
    {
      "epoch": 0.054065495733651936,
      "grad_norm": 1.6734377145767212,
      "learning_rate": 0.00018963289862042304,
      "loss": 0.2226,
      "step": 11494
    },
    {
      "epoch": 0.0540701995352644,
      "grad_norm": 1.00397527217865,
      "learning_rate": 0.00018963195564230956,
      "loss": 0.1097,
      "step": 11495
    },
    {
      "epoch": 0.05407490333687687,
      "grad_norm": 1.8876590728759766,
      "learning_rate": 0.00018963101266419608,
      "loss": 0.3971,
      "step": 11496
    },
    {
      "epoch": 0.054079607138489326,
      "grad_norm": 1.3501019477844238,
      "learning_rate": 0.0001896300696860826,
      "loss": 0.2342,
      "step": 11497
    },
    {
      "epoch": 0.05408431094010179,
      "grad_norm": 0.928714394569397,
      "learning_rate": 0.00018962912670796912,
      "loss": 0.1341,
      "step": 11498
    },
    {
      "epoch": 0.05408901474171425,
      "grad_norm": 0.44383832812309265,
      "learning_rate": 0.00018962818372985564,
      "loss": 0.0391,
      "step": 11499
    },
    {
      "epoch": 0.054093718543326716,
      "grad_norm": 3.2483949661254883,
      "learning_rate": 0.00018962724075174216,
      "loss": 0.4956,
      "step": 11500
    },
    {
      "epoch": 0.05409842234493918,
      "grad_norm": 1.7053353786468506,
      "learning_rate": 0.00018962629777362867,
      "loss": 0.082,
      "step": 11501
    },
    {
      "epoch": 0.05410312614655164,
      "grad_norm": 2.5097126960754395,
      "learning_rate": 0.0001896253547955152,
      "loss": 0.2185,
      "step": 11502
    },
    {
      "epoch": 0.054107829948164106,
      "grad_norm": 1.0779156684875488,
      "learning_rate": 0.00018962441181740174,
      "loss": 0.1294,
      "step": 11503
    },
    {
      "epoch": 0.05411253374977657,
      "grad_norm": 4.150228500366211,
      "learning_rate": 0.00018962346883928826,
      "loss": 0.5841,
      "step": 11504
    },
    {
      "epoch": 0.05411723755138903,
      "grad_norm": 2.300283670425415,
      "learning_rate": 0.00018962252586117478,
      "loss": 0.2905,
      "step": 11505
    },
    {
      "epoch": 0.054121941353001496,
      "grad_norm": 2.7380785942077637,
      "learning_rate": 0.0001896215828830613,
      "loss": 0.1554,
      "step": 11506
    },
    {
      "epoch": 0.05412664515461396,
      "grad_norm": 1.5705183744430542,
      "learning_rate": 0.00018962063990494781,
      "loss": 0.1179,
      "step": 11507
    },
    {
      "epoch": 0.05413134895622642,
      "grad_norm": 2.3335671424865723,
      "learning_rate": 0.00018961969692683433,
      "loss": 0.1617,
      "step": 11508
    },
    {
      "epoch": 0.054136052757838886,
      "grad_norm": 3.260481357574463,
      "learning_rate": 0.00018961875394872085,
      "loss": 0.3206,
      "step": 11509
    },
    {
      "epoch": 0.05414075655945135,
      "grad_norm": 2.5778088569641113,
      "learning_rate": 0.00018961781097060737,
      "loss": 0.24,
      "step": 11510
    },
    {
      "epoch": 0.05414546036106381,
      "grad_norm": 5.665921211242676,
      "learning_rate": 0.0001896168679924939,
      "loss": 0.5631,
      "step": 11511
    },
    {
      "epoch": 0.054150164162676276,
      "grad_norm": 3.038496255874634,
      "learning_rate": 0.00018961592501438043,
      "loss": 0.465,
      "step": 11512
    },
    {
      "epoch": 0.05415486796428874,
      "grad_norm": 3.0562455654144287,
      "learning_rate": 0.00018961498203626695,
      "loss": 0.2669,
      "step": 11513
    },
    {
      "epoch": 0.0541595717659012,
      "grad_norm": 0.5385913848876953,
      "learning_rate": 0.00018961403905815347,
      "loss": 0.0512,
      "step": 11514
    },
    {
      "epoch": 0.054164275567513666,
      "grad_norm": 1.132754921913147,
      "learning_rate": 0.00018961309608004,
      "loss": 0.0869,
      "step": 11515
    },
    {
      "epoch": 0.054168979369126125,
      "grad_norm": 3.1461546421051025,
      "learning_rate": 0.0001896121531019265,
      "loss": 0.3634,
      "step": 11516
    },
    {
      "epoch": 0.05417368317073859,
      "grad_norm": 3.249526023864746,
      "learning_rate": 0.00018961121012381305,
      "loss": 0.5892,
      "step": 11517
    },
    {
      "epoch": 0.054178386972351056,
      "grad_norm": 3.549377202987671,
      "learning_rate": 0.00018961026714569957,
      "loss": 0.3982,
      "step": 11518
    },
    {
      "epoch": 0.054183090773963515,
      "grad_norm": 3.5874412059783936,
      "learning_rate": 0.00018960932416758606,
      "loss": 0.392,
      "step": 11519
    },
    {
      "epoch": 0.05418779457557598,
      "grad_norm": 3.236551523208618,
      "learning_rate": 0.00018960838118947258,
      "loss": 0.5,
      "step": 11520
    },
    {
      "epoch": 0.054192498377188446,
      "grad_norm": 3.05181622505188,
      "learning_rate": 0.00018960743821135913,
      "loss": 0.4379,
      "step": 11521
    },
    {
      "epoch": 0.054197202178800905,
      "grad_norm": 1.623916506767273,
      "learning_rate": 0.00018960649523324565,
      "loss": 0.1445,
      "step": 11522
    },
    {
      "epoch": 0.05420190598041337,
      "grad_norm": 2.2026612758636475,
      "learning_rate": 0.00018960555225513217,
      "loss": 0.548,
      "step": 11523
    },
    {
      "epoch": 0.054206609782025836,
      "grad_norm": 3.844758987426758,
      "learning_rate": 0.00018960460927701869,
      "loss": 0.8252,
      "step": 11524
    },
    {
      "epoch": 0.054211313583638294,
      "grad_norm": 0.31250303983688354,
      "learning_rate": 0.0001896036662989052,
      "loss": 0.0413,
      "step": 11525
    },
    {
      "epoch": 0.05421601738525076,
      "grad_norm": 1.2638664245605469,
      "learning_rate": 0.00018960272332079175,
      "loss": 0.203,
      "step": 11526
    },
    {
      "epoch": 0.054220721186863226,
      "grad_norm": 2.7842612266540527,
      "learning_rate": 0.00018960178034267827,
      "loss": 0.3701,
      "step": 11527
    },
    {
      "epoch": 0.054225424988475684,
      "grad_norm": 2.678011894226074,
      "learning_rate": 0.0001896008373645648,
      "loss": 0.133,
      "step": 11528
    },
    {
      "epoch": 0.05423012879008815,
      "grad_norm": 1.4721451997756958,
      "learning_rate": 0.0001895998943864513,
      "loss": 0.1657,
      "step": 11529
    },
    {
      "epoch": 0.054234832591700616,
      "grad_norm": 2.2282350063323975,
      "learning_rate": 0.00018959895140833782,
      "loss": 0.332,
      "step": 11530
    },
    {
      "epoch": 0.054239536393313074,
      "grad_norm": 1.538509488105774,
      "learning_rate": 0.00018959800843022434,
      "loss": 0.148,
      "step": 11531
    },
    {
      "epoch": 0.05424424019492554,
      "grad_norm": 1.8120876550674438,
      "learning_rate": 0.00018959706545211086,
      "loss": 0.3101,
      "step": 11532
    },
    {
      "epoch": 0.054248943996538,
      "grad_norm": 0.8876383304595947,
      "learning_rate": 0.00018959612247399738,
      "loss": 0.1905,
      "step": 11533
    },
    {
      "epoch": 0.054253647798150464,
      "grad_norm": 1.914854645729065,
      "learning_rate": 0.0001895951794958839,
      "loss": 0.3576,
      "step": 11534
    },
    {
      "epoch": 0.05425835159976293,
      "grad_norm": 2.777803897857666,
      "learning_rate": 0.00018959423651777044,
      "loss": 0.6025,
      "step": 11535
    },
    {
      "epoch": 0.05426305540137539,
      "grad_norm": 1.4120110273361206,
      "learning_rate": 0.00018959329353965696,
      "loss": 0.186,
      "step": 11536
    },
    {
      "epoch": 0.054267759202987854,
      "grad_norm": 1.6717833280563354,
      "learning_rate": 0.00018959235056154348,
      "loss": 0.199,
      "step": 11537
    },
    {
      "epoch": 0.05427246300460032,
      "grad_norm": 1.7890267372131348,
      "learning_rate": 0.00018959140758343,
      "loss": 0.2183,
      "step": 11538
    },
    {
      "epoch": 0.05427716680621278,
      "grad_norm": 1.4486948251724243,
      "learning_rate": 0.00018959046460531652,
      "loss": 0.2346,
      "step": 11539
    },
    {
      "epoch": 0.054281870607825244,
      "grad_norm": 2.834590435028076,
      "learning_rate": 0.00018958952162720304,
      "loss": 0.4875,
      "step": 11540
    },
    {
      "epoch": 0.05428657440943771,
      "grad_norm": 2.7176272869110107,
      "learning_rate": 0.00018958857864908956,
      "loss": 0.3338,
      "step": 11541
    },
    {
      "epoch": 0.05429127821105017,
      "grad_norm": 2.504544496536255,
      "learning_rate": 0.00018958763567097607,
      "loss": 0.4733,
      "step": 11542
    },
    {
      "epoch": 0.054295982012662634,
      "grad_norm": 1.036152720451355,
      "learning_rate": 0.0001895866926928626,
      "loss": 0.1843,
      "step": 11543
    },
    {
      "epoch": 0.0543006858142751,
      "grad_norm": 0.5263709425926208,
      "learning_rate": 0.00018958574971474914,
      "loss": 0.0661,
      "step": 11544
    },
    {
      "epoch": 0.05430538961588756,
      "grad_norm": 3.2901508808135986,
      "learning_rate": 0.00018958480673663566,
      "loss": 0.7907,
      "step": 11545
    },
    {
      "epoch": 0.054310093417500024,
      "grad_norm": 2.3108069896698,
      "learning_rate": 0.00018958386375852218,
      "loss": 0.2761,
      "step": 11546
    },
    {
      "epoch": 0.05431479721911249,
      "grad_norm": 2.0075292587280273,
      "learning_rate": 0.0001895829207804087,
      "loss": 0.3327,
      "step": 11547
    },
    {
      "epoch": 0.05431950102072495,
      "grad_norm": 3.419724225997925,
      "learning_rate": 0.00018958197780229521,
      "loss": 0.5401,
      "step": 11548
    },
    {
      "epoch": 0.054324204822337414,
      "grad_norm": 2.5338242053985596,
      "learning_rate": 0.00018958103482418176,
      "loss": 0.3751,
      "step": 11549
    },
    {
      "epoch": 0.05432890862394987,
      "grad_norm": 1.6728160381317139,
      "learning_rate": 0.00018958009184606825,
      "loss": 0.2026,
      "step": 11550
    },
    {
      "epoch": 0.05433361242556234,
      "grad_norm": 1.4723716974258423,
      "learning_rate": 0.00018957914886795477,
      "loss": 0.1201,
      "step": 11551
    },
    {
      "epoch": 0.054338316227174804,
      "grad_norm": 0.3819080591201782,
      "learning_rate": 0.0001895782058898413,
      "loss": 0.0476,
      "step": 11552
    },
    {
      "epoch": 0.05434302002878726,
      "grad_norm": 1.9394012689590454,
      "learning_rate": 0.00018957726291172783,
      "loss": 0.4174,
      "step": 11553
    },
    {
      "epoch": 0.05434772383039973,
      "grad_norm": 1.7074272632598877,
      "learning_rate": 0.00018957631993361435,
      "loss": 0.1686,
      "step": 11554
    },
    {
      "epoch": 0.054352427632012194,
      "grad_norm": 3.0465140342712402,
      "learning_rate": 0.00018957537695550087,
      "loss": 0.4968,
      "step": 11555
    },
    {
      "epoch": 0.05435713143362465,
      "grad_norm": 4.2686767578125,
      "learning_rate": 0.0001895744339773874,
      "loss": 0.9527,
      "step": 11556
    },
    {
      "epoch": 0.05436183523523712,
      "grad_norm": 1.5033165216445923,
      "learning_rate": 0.0001895734909992739,
      "loss": 0.175,
      "step": 11557
    },
    {
      "epoch": 0.054366539036849584,
      "grad_norm": 2.3655612468719482,
      "learning_rate": 0.00018957254802116045,
      "loss": 0.3931,
      "step": 11558
    },
    {
      "epoch": 0.05437124283846204,
      "grad_norm": 4.679810523986816,
      "learning_rate": 0.00018957160504304697,
      "loss": 0.5949,
      "step": 11559
    },
    {
      "epoch": 0.05437594664007451,
      "grad_norm": 1.747925043106079,
      "learning_rate": 0.0001895706620649335,
      "loss": 0.2322,
      "step": 11560
    },
    {
      "epoch": 0.054380650441686974,
      "grad_norm": 1.3468952178955078,
      "learning_rate": 0.00018956971908681998,
      "loss": 0.1296,
      "step": 11561
    },
    {
      "epoch": 0.05438535424329943,
      "grad_norm": 1.373175859451294,
      "learning_rate": 0.00018956877610870653,
      "loss": 0.0859,
      "step": 11562
    },
    {
      "epoch": 0.0543900580449119,
      "grad_norm": 1.8273098468780518,
      "learning_rate": 0.00018956783313059305,
      "loss": 0.3056,
      "step": 11563
    },
    {
      "epoch": 0.054394761846524364,
      "grad_norm": 1.8249096870422363,
      "learning_rate": 0.00018956689015247957,
      "loss": 0.317,
      "step": 11564
    },
    {
      "epoch": 0.05439946564813682,
      "grad_norm": 1.3804407119750977,
      "learning_rate": 0.00018956594717436608,
      "loss": 0.2128,
      "step": 11565
    },
    {
      "epoch": 0.05440416944974929,
      "grad_norm": 1.795083999633789,
      "learning_rate": 0.0001895650041962526,
      "loss": 0.2817,
      "step": 11566
    },
    {
      "epoch": 0.05440887325136175,
      "grad_norm": 1.7728945016860962,
      "learning_rate": 0.00018956406121813915,
      "loss": 0.2378,
      "step": 11567
    },
    {
      "epoch": 0.05441357705297421,
      "grad_norm": 0.29445698857307434,
      "learning_rate": 0.00018956311824002567,
      "loss": 0.0225,
      "step": 11568
    },
    {
      "epoch": 0.05441828085458668,
      "grad_norm": 1.0655181407928467,
      "learning_rate": 0.0001895621752619122,
      "loss": 0.2116,
      "step": 11569
    },
    {
      "epoch": 0.05442298465619914,
      "grad_norm": 1.927411675453186,
      "learning_rate": 0.0001895612322837987,
      "loss": 0.3472,
      "step": 11570
    },
    {
      "epoch": 0.0544276884578116,
      "grad_norm": 1.2501336336135864,
      "learning_rate": 0.00018956028930568522,
      "loss": 0.1534,
      "step": 11571
    },
    {
      "epoch": 0.05443239225942407,
      "grad_norm": 4.799838542938232,
      "learning_rate": 0.00018955934632757174,
      "loss": 0.5518,
      "step": 11572
    },
    {
      "epoch": 0.05443709606103653,
      "grad_norm": 1.3952510356903076,
      "learning_rate": 0.00018955840334945826,
      "loss": 0.1402,
      "step": 11573
    },
    {
      "epoch": 0.05444179986264899,
      "grad_norm": 1.3281822204589844,
      "learning_rate": 0.00018955746037134478,
      "loss": 0.1043,
      "step": 11574
    },
    {
      "epoch": 0.05444650366426146,
      "grad_norm": 0.8075376749038696,
      "learning_rate": 0.0001895565173932313,
      "loss": 0.0487,
      "step": 11575
    },
    {
      "epoch": 0.05445120746587392,
      "grad_norm": 0.5778732299804688,
      "learning_rate": 0.00018955557441511784,
      "loss": 0.0616,
      "step": 11576
    },
    {
      "epoch": 0.05445591126748638,
      "grad_norm": 2.0549001693725586,
      "learning_rate": 0.00018955463143700436,
      "loss": 0.5013,
      "step": 11577
    },
    {
      "epoch": 0.05446061506909885,
      "grad_norm": 2.860104560852051,
      "learning_rate": 0.00018955368845889088,
      "loss": 0.5539,
      "step": 11578
    },
    {
      "epoch": 0.05446531887071131,
      "grad_norm": 1.9809210300445557,
      "learning_rate": 0.0001895527454807774,
      "loss": 0.3366,
      "step": 11579
    },
    {
      "epoch": 0.05447002267232377,
      "grad_norm": 0.9081730842590332,
      "learning_rate": 0.00018955180250266395,
      "loss": 0.076,
      "step": 11580
    },
    {
      "epoch": 0.05447472647393624,
      "grad_norm": 2.49100399017334,
      "learning_rate": 0.00018955085952455044,
      "loss": 0.4743,
      "step": 11581
    },
    {
      "epoch": 0.0544794302755487,
      "grad_norm": 2.1439359188079834,
      "learning_rate": 0.00018954991654643696,
      "loss": 0.2428,
      "step": 11582
    },
    {
      "epoch": 0.05448413407716116,
      "grad_norm": 1.8357994556427002,
      "learning_rate": 0.00018954897356832347,
      "loss": 0.2249,
      "step": 11583
    },
    {
      "epoch": 0.05448883787877362,
      "grad_norm": 4.2337565422058105,
      "learning_rate": 0.00018954803059021,
      "loss": 0.8575,
      "step": 11584
    },
    {
      "epoch": 0.05449354168038609,
      "grad_norm": 1.789108157157898,
      "learning_rate": 0.00018954708761209654,
      "loss": 0.1979,
      "step": 11585
    },
    {
      "epoch": 0.05449824548199855,
      "grad_norm": 1.9457261562347412,
      "learning_rate": 0.00018954614463398306,
      "loss": 0.1638,
      "step": 11586
    },
    {
      "epoch": 0.05450294928361101,
      "grad_norm": 2.720078468322754,
      "learning_rate": 0.00018954520165586958,
      "loss": 0.2767,
      "step": 11587
    },
    {
      "epoch": 0.05450765308522348,
      "grad_norm": 3.56493878364563,
      "learning_rate": 0.0001895442586777561,
      "loss": 0.3504,
      "step": 11588
    },
    {
      "epoch": 0.05451235688683594,
      "grad_norm": 1.320667028427124,
      "learning_rate": 0.00018954331569964261,
      "loss": 0.1323,
      "step": 11589
    },
    {
      "epoch": 0.0545170606884484,
      "grad_norm": 2.12351131439209,
      "learning_rate": 0.00018954237272152916,
      "loss": 0.2374,
      "step": 11590
    },
    {
      "epoch": 0.05452176449006087,
      "grad_norm": 3.1224162578582764,
      "learning_rate": 0.00018954142974341568,
      "loss": 0.29,
      "step": 11591
    },
    {
      "epoch": 0.05452646829167333,
      "grad_norm": 1.3080633878707886,
      "learning_rate": 0.00018954048676530217,
      "loss": 0.1341,
      "step": 11592
    },
    {
      "epoch": 0.05453117209328579,
      "grad_norm": 2.481374979019165,
      "learning_rate": 0.0001895395437871887,
      "loss": 0.4212,
      "step": 11593
    },
    {
      "epoch": 0.05453587589489826,
      "grad_norm": 2.686161756515503,
      "learning_rate": 0.00018953860080907523,
      "loss": 0.1883,
      "step": 11594
    },
    {
      "epoch": 0.05454057969651072,
      "grad_norm": 2.209524393081665,
      "learning_rate": 0.00018953765783096175,
      "loss": 0.3793,
      "step": 11595
    },
    {
      "epoch": 0.05454528349812318,
      "grad_norm": 3.1223738193511963,
      "learning_rate": 0.00018953671485284827,
      "loss": 0.1635,
      "step": 11596
    },
    {
      "epoch": 0.05454998729973565,
      "grad_norm": 2.0052928924560547,
      "learning_rate": 0.0001895357718747348,
      "loss": 0.1256,
      "step": 11597
    },
    {
      "epoch": 0.05455469110134811,
      "grad_norm": 2.159599781036377,
      "learning_rate": 0.0001895348288966213,
      "loss": 0.3972,
      "step": 11598
    },
    {
      "epoch": 0.05455939490296057,
      "grad_norm": 1.1189124584197998,
      "learning_rate": 0.00018953388591850785,
      "loss": 0.126,
      "step": 11599
    },
    {
      "epoch": 0.05456409870457304,
      "grad_norm": 0.7857068181037903,
      "learning_rate": 0.00018953294294039437,
      "loss": 0.0625,
      "step": 11600
    },
    {
      "epoch": 0.054568802506185496,
      "grad_norm": 2.2908010482788086,
      "learning_rate": 0.0001895319999622809,
      "loss": 0.2521,
      "step": 11601
    },
    {
      "epoch": 0.05457350630779796,
      "grad_norm": 4.678260326385498,
      "learning_rate": 0.0001895310569841674,
      "loss": 0.662,
      "step": 11602
    },
    {
      "epoch": 0.05457821010941043,
      "grad_norm": 3.461425542831421,
      "learning_rate": 0.00018953011400605393,
      "loss": 0.4452,
      "step": 11603
    },
    {
      "epoch": 0.054582913911022886,
      "grad_norm": 3.7070257663726807,
      "learning_rate": 0.00018952917102794045,
      "loss": 0.2274,
      "step": 11604
    },
    {
      "epoch": 0.05458761771263535,
      "grad_norm": 7.0499773025512695,
      "learning_rate": 0.00018952822804982697,
      "loss": 0.3777,
      "step": 11605
    },
    {
      "epoch": 0.05459232151424782,
      "grad_norm": 2.551527976989746,
      "learning_rate": 0.00018952728507171348,
      "loss": 0.205,
      "step": 11606
    },
    {
      "epoch": 0.054597025315860276,
      "grad_norm": 4.517340183258057,
      "learning_rate": 0.0001895263420936,
      "loss": 0.6339,
      "step": 11607
    },
    {
      "epoch": 0.05460172911747274,
      "grad_norm": 4.251468181610107,
      "learning_rate": 0.00018952539911548655,
      "loss": 0.5626,
      "step": 11608
    },
    {
      "epoch": 0.05460643291908521,
      "grad_norm": 0.7892964482307434,
      "learning_rate": 0.00018952445613737307,
      "loss": 0.0701,
      "step": 11609
    },
    {
      "epoch": 0.054611136720697666,
      "grad_norm": 2.853095531463623,
      "learning_rate": 0.0001895235131592596,
      "loss": 0.2705,
      "step": 11610
    },
    {
      "epoch": 0.05461584052231013,
      "grad_norm": 2.1219184398651123,
      "learning_rate": 0.0001895225701811461,
      "loss": 0.3284,
      "step": 11611
    },
    {
      "epoch": 0.0546205443239226,
      "grad_norm": 6.894754886627197,
      "learning_rate": 0.00018952162720303262,
      "loss": 0.6612,
      "step": 11612
    },
    {
      "epoch": 0.054625248125535056,
      "grad_norm": 1.267855167388916,
      "learning_rate": 0.00018952068422491914,
      "loss": 0.1477,
      "step": 11613
    },
    {
      "epoch": 0.05462995192714752,
      "grad_norm": 1.8840099573135376,
      "learning_rate": 0.00018951974124680566,
      "loss": 0.1716,
      "step": 11614
    },
    {
      "epoch": 0.05463465572875999,
      "grad_norm": 2.0328540802001953,
      "learning_rate": 0.00018951879826869218,
      "loss": 0.2301,
      "step": 11615
    },
    {
      "epoch": 0.054639359530372446,
      "grad_norm": 2.4354403018951416,
      "learning_rate": 0.0001895178552905787,
      "loss": 0.2814,
      "step": 11616
    },
    {
      "epoch": 0.05464406333198491,
      "grad_norm": 1.3754576444625854,
      "learning_rate": 0.00018951691231246524,
      "loss": 0.0654,
      "step": 11617
    },
    {
      "epoch": 0.05464876713359737,
      "grad_norm": 2.069532871246338,
      "learning_rate": 0.00018951596933435176,
      "loss": 0.2734,
      "step": 11618
    },
    {
      "epoch": 0.054653470935209836,
      "grad_norm": 3.6718413829803467,
      "learning_rate": 0.00018951502635623828,
      "loss": 0.4194,
      "step": 11619
    },
    {
      "epoch": 0.0546581747368223,
      "grad_norm": 2.6174025535583496,
      "learning_rate": 0.0001895140833781248,
      "loss": 0.5274,
      "step": 11620
    },
    {
      "epoch": 0.05466287853843476,
      "grad_norm": 0.22830741107463837,
      "learning_rate": 0.00018951314040001135,
      "loss": 0.0217,
      "step": 11621
    },
    {
      "epoch": 0.054667582340047226,
      "grad_norm": 3.1804873943328857,
      "learning_rate": 0.00018951219742189786,
      "loss": 0.4319,
      "step": 11622
    },
    {
      "epoch": 0.05467228614165969,
      "grad_norm": 0.4971902668476105,
      "learning_rate": 0.00018951125444378436,
      "loss": 0.0802,
      "step": 11623
    },
    {
      "epoch": 0.05467698994327215,
      "grad_norm": 2.4887468814849854,
      "learning_rate": 0.00018951031146567087,
      "loss": 0.2292,
      "step": 11624
    },
    {
      "epoch": 0.054681693744884616,
      "grad_norm": 1.4452557563781738,
      "learning_rate": 0.0001895093684875574,
      "loss": 0.0925,
      "step": 11625
    },
    {
      "epoch": 0.05468639754649708,
      "grad_norm": 0.6919355392456055,
      "learning_rate": 0.00018950842550944394,
      "loss": 0.0381,
      "step": 11626
    },
    {
      "epoch": 0.05469110134810954,
      "grad_norm": 2.148782253265381,
      "learning_rate": 0.00018950748253133046,
      "loss": 0.1848,
      "step": 11627
    },
    {
      "epoch": 0.054695805149722006,
      "grad_norm": 0.5583764910697937,
      "learning_rate": 0.00018950653955321698,
      "loss": 0.051,
      "step": 11628
    },
    {
      "epoch": 0.05470050895133447,
      "grad_norm": 1.9219037294387817,
      "learning_rate": 0.0001895055965751035,
      "loss": 0.347,
      "step": 11629
    },
    {
      "epoch": 0.05470521275294693,
      "grad_norm": 1.5185669660568237,
      "learning_rate": 0.00018950465359699004,
      "loss": 0.1935,
      "step": 11630
    },
    {
      "epoch": 0.054709916554559396,
      "grad_norm": 1.1721729040145874,
      "learning_rate": 0.00018950371061887656,
      "loss": 0.0752,
      "step": 11631
    },
    {
      "epoch": 0.05471462035617186,
      "grad_norm": 1.7116714715957642,
      "learning_rate": 0.00018950276764076308,
      "loss": 0.1236,
      "step": 11632
    },
    {
      "epoch": 0.05471932415778432,
      "grad_norm": 1.874290943145752,
      "learning_rate": 0.0001895018246626496,
      "loss": 0.2083,
      "step": 11633
    },
    {
      "epoch": 0.054724027959396786,
      "grad_norm": 1.6897410154342651,
      "learning_rate": 0.00018950088168453612,
      "loss": 0.2012,
      "step": 11634
    },
    {
      "epoch": 0.054728731761009244,
      "grad_norm": 3.5255050659179688,
      "learning_rate": 0.00018949993870642263,
      "loss": 0.5323,
      "step": 11635
    },
    {
      "epoch": 0.05473343556262171,
      "grad_norm": 4.918656349182129,
      "learning_rate": 0.00018949899572830915,
      "loss": 0.6548,
      "step": 11636
    },
    {
      "epoch": 0.054738139364234176,
      "grad_norm": 1.227492332458496,
      "learning_rate": 0.00018949805275019567,
      "loss": 0.108,
      "step": 11637
    },
    {
      "epoch": 0.054742843165846634,
      "grad_norm": 4.416887283325195,
      "learning_rate": 0.0001894971097720822,
      "loss": 0.4042,
      "step": 11638
    },
    {
      "epoch": 0.0547475469674591,
      "grad_norm": 2.6736485958099365,
      "learning_rate": 0.0001894961667939687,
      "loss": 0.4787,
      "step": 11639
    },
    {
      "epoch": 0.054752250769071566,
      "grad_norm": 2.788421869277954,
      "learning_rate": 0.00018949522381585525,
      "loss": 0.4555,
      "step": 11640
    },
    {
      "epoch": 0.054756954570684024,
      "grad_norm": 3.248779296875,
      "learning_rate": 0.00018949428083774177,
      "loss": 0.0852,
      "step": 11641
    },
    {
      "epoch": 0.05476165837229649,
      "grad_norm": 3.0150656700134277,
      "learning_rate": 0.0001894933378596283,
      "loss": 0.4531,
      "step": 11642
    },
    {
      "epoch": 0.054766362173908956,
      "grad_norm": 1.2757700681686401,
      "learning_rate": 0.0001894923948815148,
      "loss": 0.0842,
      "step": 11643
    },
    {
      "epoch": 0.054771065975521414,
      "grad_norm": 1.3842195272445679,
      "learning_rate": 0.00018949145190340133,
      "loss": 0.1056,
      "step": 11644
    },
    {
      "epoch": 0.05477576977713388,
      "grad_norm": 1.2926442623138428,
      "learning_rate": 0.00018949050892528785,
      "loss": 0.1517,
      "step": 11645
    },
    {
      "epoch": 0.054780473578746346,
      "grad_norm": 2.0824358463287354,
      "learning_rate": 0.00018948956594717437,
      "loss": 0.161,
      "step": 11646
    },
    {
      "epoch": 0.054785177380358804,
      "grad_norm": 1.785157561302185,
      "learning_rate": 0.00018948862296906088,
      "loss": 0.1246,
      "step": 11647
    },
    {
      "epoch": 0.05478988118197127,
      "grad_norm": 0.48712775111198425,
      "learning_rate": 0.0001894876799909474,
      "loss": 0.0711,
      "step": 11648
    },
    {
      "epoch": 0.054794584983583736,
      "grad_norm": 1.5868843793869019,
      "learning_rate": 0.00018948673701283395,
      "loss": 0.2043,
      "step": 11649
    },
    {
      "epoch": 0.054799288785196194,
      "grad_norm": 2.5048446655273438,
      "learning_rate": 0.00018948579403472047,
      "loss": 0.3659,
      "step": 11650
    },
    {
      "epoch": 0.05480399258680866,
      "grad_norm": 1.5533802509307861,
      "learning_rate": 0.000189484851056607,
      "loss": 0.0815,
      "step": 11651
    },
    {
      "epoch": 0.05480869638842112,
      "grad_norm": 0.9249840974807739,
      "learning_rate": 0.0001894839080784935,
      "loss": 0.0868,
      "step": 11652
    },
    {
      "epoch": 0.054813400190033584,
      "grad_norm": 4.160765647888184,
      "learning_rate": 0.00018948296510038005,
      "loss": 0.5678,
      "step": 11653
    },
    {
      "epoch": 0.05481810399164605,
      "grad_norm": 1.0137443542480469,
      "learning_rate": 0.00018948202212226654,
      "loss": 0.0329,
      "step": 11654
    },
    {
      "epoch": 0.05482280779325851,
      "grad_norm": 1.484474778175354,
      "learning_rate": 0.00018948107914415306,
      "loss": 0.1471,
      "step": 11655
    },
    {
      "epoch": 0.054827511594870974,
      "grad_norm": 3.111855983734131,
      "learning_rate": 0.00018948013616603958,
      "loss": 0.4458,
      "step": 11656
    },
    {
      "epoch": 0.05483221539648344,
      "grad_norm": 3.651355266571045,
      "learning_rate": 0.0001894791931879261,
      "loss": 0.4222,
      "step": 11657
    },
    {
      "epoch": 0.0548369191980959,
      "grad_norm": 0.911271333694458,
      "learning_rate": 0.00018947825020981264,
      "loss": 0.0496,
      "step": 11658
    },
    {
      "epoch": 0.054841622999708364,
      "grad_norm": 2.238340377807617,
      "learning_rate": 0.00018947730723169916,
      "loss": 0.2637,
      "step": 11659
    },
    {
      "epoch": 0.05484632680132083,
      "grad_norm": 5.120570182800293,
      "learning_rate": 0.00018947636425358568,
      "loss": 0.8289,
      "step": 11660
    },
    {
      "epoch": 0.05485103060293329,
      "grad_norm": 1.4262396097183228,
      "learning_rate": 0.0001894754212754722,
      "loss": 0.1313,
      "step": 11661
    },
    {
      "epoch": 0.054855734404545754,
      "grad_norm": 2.818012237548828,
      "learning_rate": 0.00018947447829735875,
      "loss": 0.2858,
      "step": 11662
    },
    {
      "epoch": 0.05486043820615822,
      "grad_norm": 1.897621512413025,
      "learning_rate": 0.00018947353531924526,
      "loss": 0.2178,
      "step": 11663
    },
    {
      "epoch": 0.05486514200777068,
      "grad_norm": 1.6459579467773438,
      "learning_rate": 0.00018947259234113178,
      "loss": 0.1343,
      "step": 11664
    },
    {
      "epoch": 0.054869845809383144,
      "grad_norm": 2.2989726066589355,
      "learning_rate": 0.0001894716493630183,
      "loss": 0.2965,
      "step": 11665
    },
    {
      "epoch": 0.05487454961099561,
      "grad_norm": 1.0241202116012573,
      "learning_rate": 0.0001894707063849048,
      "loss": 0.0574,
      "step": 11666
    },
    {
      "epoch": 0.05487925341260807,
      "grad_norm": 0.8220648169517517,
      "learning_rate": 0.00018946976340679134,
      "loss": 0.1159,
      "step": 11667
    },
    {
      "epoch": 0.054883957214220534,
      "grad_norm": 1.9202877283096313,
      "learning_rate": 0.00018946882042867786,
      "loss": 0.1768,
      "step": 11668
    },
    {
      "epoch": 0.05488866101583299,
      "grad_norm": 2.215459108352661,
      "learning_rate": 0.00018946787745056438,
      "loss": 0.485,
      "step": 11669
    },
    {
      "epoch": 0.05489336481744546,
      "grad_norm": 1.1911174058914185,
      "learning_rate": 0.0001894669344724509,
      "loss": 0.0988,
      "step": 11670
    },
    {
      "epoch": 0.054898068619057924,
      "grad_norm": 3.2884697914123535,
      "learning_rate": 0.00018946599149433744,
      "loss": 0.5797,
      "step": 11671
    },
    {
      "epoch": 0.05490277242067038,
      "grad_norm": 4.4937357902526855,
      "learning_rate": 0.00018946504851622396,
      "loss": 0.8276,
      "step": 11672
    },
    {
      "epoch": 0.05490747622228285,
      "grad_norm": 3.7853803634643555,
      "learning_rate": 0.00018946410553811048,
      "loss": 0.4569,
      "step": 11673
    },
    {
      "epoch": 0.054912180023895314,
      "grad_norm": 2.2665116786956787,
      "learning_rate": 0.000189463162559997,
      "loss": 0.3893,
      "step": 11674
    },
    {
      "epoch": 0.05491688382550777,
      "grad_norm": 1.4378198385238647,
      "learning_rate": 0.00018946221958188352,
      "loss": 0.113,
      "step": 11675
    },
    {
      "epoch": 0.05492158762712024,
      "grad_norm": 2.2892274856567383,
      "learning_rate": 0.00018946127660377003,
      "loss": 0.2062,
      "step": 11676
    },
    {
      "epoch": 0.054926291428732704,
      "grad_norm": 1.7611711025238037,
      "learning_rate": 0.00018946033362565655,
      "loss": 0.0984,
      "step": 11677
    },
    {
      "epoch": 0.05493099523034516,
      "grad_norm": 1.892953872680664,
      "learning_rate": 0.00018945939064754307,
      "loss": 0.3013,
      "step": 11678
    },
    {
      "epoch": 0.05493569903195763,
      "grad_norm": 0.8689135909080505,
      "learning_rate": 0.0001894584476694296,
      "loss": 0.127,
      "step": 11679
    },
    {
      "epoch": 0.054940402833570094,
      "grad_norm": 1.1858932971954346,
      "learning_rate": 0.00018945750469131614,
      "loss": 0.1013,
      "step": 11680
    },
    {
      "epoch": 0.05494510663518255,
      "grad_norm": 0.7043918371200562,
      "learning_rate": 0.00018945656171320265,
      "loss": 0.0563,
      "step": 11681
    },
    {
      "epoch": 0.05494981043679502,
      "grad_norm": 0.5751286149024963,
      "learning_rate": 0.00018945561873508917,
      "loss": 0.0931,
      "step": 11682
    },
    {
      "epoch": 0.054954514238407484,
      "grad_norm": 1.4739394187927246,
      "learning_rate": 0.0001894546757569757,
      "loss": 0.1595,
      "step": 11683
    },
    {
      "epoch": 0.05495921804001994,
      "grad_norm": 2.017343759536743,
      "learning_rate": 0.0001894537327788622,
      "loss": 0.4156,
      "step": 11684
    },
    {
      "epoch": 0.05496392184163241,
      "grad_norm": 1.6885038614273071,
      "learning_rate": 0.00018945278980074873,
      "loss": 0.2341,
      "step": 11685
    },
    {
      "epoch": 0.05496862564324487,
      "grad_norm": 0.9557134509086609,
      "learning_rate": 0.00018945184682263525,
      "loss": 0.0844,
      "step": 11686
    },
    {
      "epoch": 0.05497332944485733,
      "grad_norm": 1.5138646364212036,
      "learning_rate": 0.00018945090384452177,
      "loss": 0.1482,
      "step": 11687
    },
    {
      "epoch": 0.0549780332464698,
      "grad_norm": 4.173501014709473,
      "learning_rate": 0.00018944996086640828,
      "loss": 0.7198,
      "step": 11688
    },
    {
      "epoch": 0.05498273704808226,
      "grad_norm": 3.6112194061279297,
      "learning_rate": 0.0001894490178882948,
      "loss": 0.2861,
      "step": 11689
    },
    {
      "epoch": 0.05498744084969472,
      "grad_norm": 1.2885961532592773,
      "learning_rate": 0.00018944807491018135,
      "loss": 0.1066,
      "step": 11690
    },
    {
      "epoch": 0.05499214465130719,
      "grad_norm": 1.0787301063537598,
      "learning_rate": 0.00018944713193206787,
      "loss": 0.1695,
      "step": 11691
    },
    {
      "epoch": 0.05499684845291965,
      "grad_norm": 3.069058656692505,
      "learning_rate": 0.0001894461889539544,
      "loss": 0.5225,
      "step": 11692
    },
    {
      "epoch": 0.05500155225453211,
      "grad_norm": 0.9371784329414368,
      "learning_rate": 0.0001894452459758409,
      "loss": 0.1609,
      "step": 11693
    },
    {
      "epoch": 0.05500625605614458,
      "grad_norm": 4.411156177520752,
      "learning_rate": 0.00018944430299772745,
      "loss": 1.026,
      "step": 11694
    },
    {
      "epoch": 0.05501095985775704,
      "grad_norm": 1.2832040786743164,
      "learning_rate": 0.00018944336001961397,
      "loss": 0.1134,
      "step": 11695
    },
    {
      "epoch": 0.0550156636593695,
      "grad_norm": 0.982123076915741,
      "learning_rate": 0.0001894424170415005,
      "loss": 0.0985,
      "step": 11696
    },
    {
      "epoch": 0.05502036746098197,
      "grad_norm": 2.395730495452881,
      "learning_rate": 0.00018944147406338698,
      "loss": 0.1575,
      "step": 11697
    },
    {
      "epoch": 0.05502507126259443,
      "grad_norm": 29.221248626708984,
      "learning_rate": 0.0001894405310852735,
      "loss": 0.4066,
      "step": 11698
    },
    {
      "epoch": 0.05502977506420689,
      "grad_norm": 0.7420975565910339,
      "learning_rate": 0.00018943958810716004,
      "loss": 0.0576,
      "step": 11699
    },
    {
      "epoch": 0.05503447886581936,
      "grad_norm": 2.7048799991607666,
      "learning_rate": 0.00018943864512904656,
      "loss": 0.4053,
      "step": 11700
    },
    {
      "epoch": 0.05503918266743182,
      "grad_norm": 5.358322620391846,
      "learning_rate": 0.00018943770215093308,
      "loss": 1.0659,
      "step": 11701
    },
    {
      "epoch": 0.05504388646904428,
      "grad_norm": 2.4269707202911377,
      "learning_rate": 0.0001894367591728196,
      "loss": 0.2363,
      "step": 11702
    },
    {
      "epoch": 0.05504859027065674,
      "grad_norm": 2.4130778312683105,
      "learning_rate": 0.00018943581619470615,
      "loss": 0.1639,
      "step": 11703
    },
    {
      "epoch": 0.05505329407226921,
      "grad_norm": 0.7994467616081238,
      "learning_rate": 0.00018943487321659266,
      "loss": 0.0605,
      "step": 11704
    },
    {
      "epoch": 0.05505799787388167,
      "grad_norm": 1.2003568410873413,
      "learning_rate": 0.00018943393023847918,
      "loss": 0.1023,
      "step": 11705
    },
    {
      "epoch": 0.05506270167549413,
      "grad_norm": 0.4513697624206543,
      "learning_rate": 0.0001894329872603657,
      "loss": 0.021,
      "step": 11706
    },
    {
      "epoch": 0.0550674054771066,
      "grad_norm": 1.478308081626892,
      "learning_rate": 0.00018943204428225222,
      "loss": 0.1009,
      "step": 11707
    },
    {
      "epoch": 0.05507210927871906,
      "grad_norm": 3.4169065952301025,
      "learning_rate": 0.00018943110130413874,
      "loss": 0.2416,
      "step": 11708
    },
    {
      "epoch": 0.05507681308033152,
      "grad_norm": 0.9196101427078247,
      "learning_rate": 0.00018943015832602526,
      "loss": 0.0535,
      "step": 11709
    },
    {
      "epoch": 0.05508151688194399,
      "grad_norm": 1.2849723100662231,
      "learning_rate": 0.00018942921534791178,
      "loss": 0.0702,
      "step": 11710
    },
    {
      "epoch": 0.05508622068355645,
      "grad_norm": 5.4910688400268555,
      "learning_rate": 0.0001894282723697983,
      "loss": 1.2552,
      "step": 11711
    },
    {
      "epoch": 0.05509092448516891,
      "grad_norm": 1.9318798780441284,
      "learning_rate": 0.00018942732939168484,
      "loss": 0.209,
      "step": 11712
    },
    {
      "epoch": 0.05509562828678138,
      "grad_norm": 0.6823701858520508,
      "learning_rate": 0.00018942638641357136,
      "loss": 0.0431,
      "step": 11713
    },
    {
      "epoch": 0.05510033208839384,
      "grad_norm": 3.91874361038208,
      "learning_rate": 0.00018942544343545788,
      "loss": 0.4541,
      "step": 11714
    },
    {
      "epoch": 0.0551050358900063,
      "grad_norm": 1.776605486869812,
      "learning_rate": 0.0001894245004573444,
      "loss": 0.1173,
      "step": 11715
    },
    {
      "epoch": 0.05510973969161877,
      "grad_norm": 0.05290748178958893,
      "learning_rate": 0.00018942355747923092,
      "loss": 0.0024,
      "step": 11716
    },
    {
      "epoch": 0.05511444349323123,
      "grad_norm": 3.237031936645508,
      "learning_rate": 0.00018942261450111743,
      "loss": 0.642,
      "step": 11717
    },
    {
      "epoch": 0.05511914729484369,
      "grad_norm": 0.02986191026866436,
      "learning_rate": 0.00018942167152300395,
      "loss": 0.0012,
      "step": 11718
    },
    {
      "epoch": 0.05512385109645616,
      "grad_norm": 3.563502073287964,
      "learning_rate": 0.00018942072854489047,
      "loss": 0.2319,
      "step": 11719
    },
    {
      "epoch": 0.055128554898068616,
      "grad_norm": 3.762049436569214,
      "learning_rate": 0.000189419785566777,
      "loss": 0.2528,
      "step": 11720
    },
    {
      "epoch": 0.05513325869968108,
      "grad_norm": 6.126992225646973,
      "learning_rate": 0.00018941884258866354,
      "loss": 0.905,
      "step": 11721
    },
    {
      "epoch": 0.05513796250129355,
      "grad_norm": 3.525771379470825,
      "learning_rate": 0.00018941789961055005,
      "loss": 0.3057,
      "step": 11722
    },
    {
      "epoch": 0.055142666302906006,
      "grad_norm": 4.271509170532227,
      "learning_rate": 0.00018941695663243657,
      "loss": 0.4821,
      "step": 11723
    },
    {
      "epoch": 0.05514737010451847,
      "grad_norm": 3.1172173023223877,
      "learning_rate": 0.0001894160136543231,
      "loss": 0.3062,
      "step": 11724
    },
    {
      "epoch": 0.05515207390613094,
      "grad_norm": 2.890561580657959,
      "learning_rate": 0.0001894150706762096,
      "loss": 0.2106,
      "step": 11725
    },
    {
      "epoch": 0.055156777707743396,
      "grad_norm": 4.346848964691162,
      "learning_rate": 0.00018941412769809616,
      "loss": 0.3796,
      "step": 11726
    },
    {
      "epoch": 0.05516148150935586,
      "grad_norm": 2.5302252769470215,
      "learning_rate": 0.00018941318471998267,
      "loss": 0.2018,
      "step": 11727
    },
    {
      "epoch": 0.05516618531096833,
      "grad_norm": 6.914642810821533,
      "learning_rate": 0.00018941224174186917,
      "loss": 0.4051,
      "step": 11728
    },
    {
      "epoch": 0.055170889112580786,
      "grad_norm": 0.2557678818702698,
      "learning_rate": 0.00018941129876375568,
      "loss": 0.0217,
      "step": 11729
    },
    {
      "epoch": 0.05517559291419325,
      "grad_norm": 0.41716668009757996,
      "learning_rate": 0.00018941035578564223,
      "loss": 0.0299,
      "step": 11730
    },
    {
      "epoch": 0.05518029671580572,
      "grad_norm": 0.6291701197624207,
      "learning_rate": 0.00018940941280752875,
      "loss": 0.0564,
      "step": 11731
    },
    {
      "epoch": 0.055185000517418176,
      "grad_norm": 0.5846610069274902,
      "learning_rate": 0.00018940846982941527,
      "loss": 0.0681,
      "step": 11732
    },
    {
      "epoch": 0.05518970431903064,
      "grad_norm": 1.2944023609161377,
      "learning_rate": 0.0001894075268513018,
      "loss": 0.1339,
      "step": 11733
    },
    {
      "epoch": 0.05519440812064311,
      "grad_norm": 1.713896632194519,
      "learning_rate": 0.0001894065838731883,
      "loss": 0.1271,
      "step": 11734
    },
    {
      "epoch": 0.055199111922255566,
      "grad_norm": 1.7139590978622437,
      "learning_rate": 0.00018940564089507485,
      "loss": 0.1371,
      "step": 11735
    },
    {
      "epoch": 0.05520381572386803,
      "grad_norm": 6.282204627990723,
      "learning_rate": 0.00018940469791696137,
      "loss": 1.289,
      "step": 11736
    },
    {
      "epoch": 0.05520851952548049,
      "grad_norm": 3.6518802642822266,
      "learning_rate": 0.0001894037549388479,
      "loss": 0.4408,
      "step": 11737
    },
    {
      "epoch": 0.055213223327092956,
      "grad_norm": 0.28286054730415344,
      "learning_rate": 0.0001894028119607344,
      "loss": 0.014,
      "step": 11738
    },
    {
      "epoch": 0.05521792712870542,
      "grad_norm": 0.2323923259973526,
      "learning_rate": 0.0001894018689826209,
      "loss": 0.0263,
      "step": 11739
    },
    {
      "epoch": 0.05522263093031788,
      "grad_norm": 1.9852081537246704,
      "learning_rate": 0.00018940092600450744,
      "loss": 0.1638,
      "step": 11740
    },
    {
      "epoch": 0.055227334731930346,
      "grad_norm": 5.235513210296631,
      "learning_rate": 0.00018939998302639396,
      "loss": 0.5255,
      "step": 11741
    },
    {
      "epoch": 0.05523203853354281,
      "grad_norm": 1.6149338483810425,
      "learning_rate": 0.00018939904004828048,
      "loss": 0.1934,
      "step": 11742
    },
    {
      "epoch": 0.05523674233515527,
      "grad_norm": 1.935356855392456,
      "learning_rate": 0.000189398097070167,
      "loss": 0.1253,
      "step": 11743
    },
    {
      "epoch": 0.055241446136767736,
      "grad_norm": 2.6017115116119385,
      "learning_rate": 0.00018939715409205355,
      "loss": 0.5639,
      "step": 11744
    },
    {
      "epoch": 0.0552461499383802,
      "grad_norm": 2.3461780548095703,
      "learning_rate": 0.00018939621111394006,
      "loss": 0.2283,
      "step": 11745
    },
    {
      "epoch": 0.05525085373999266,
      "grad_norm": 1.5810014009475708,
      "learning_rate": 0.00018939526813582658,
      "loss": 0.1326,
      "step": 11746
    },
    {
      "epoch": 0.055255557541605126,
      "grad_norm": 3.1960806846618652,
      "learning_rate": 0.0001893943251577131,
      "loss": 0.537,
      "step": 11747
    },
    {
      "epoch": 0.05526026134321759,
      "grad_norm": 3.103647470474243,
      "learning_rate": 0.00018939338217959962,
      "loss": 0.4098,
      "step": 11748
    },
    {
      "epoch": 0.05526496514483005,
      "grad_norm": 2.7573282718658447,
      "learning_rate": 0.00018939243920148614,
      "loss": 0.5916,
      "step": 11749
    },
    {
      "epoch": 0.055269668946442516,
      "grad_norm": 2.220346450805664,
      "learning_rate": 0.00018939149622337266,
      "loss": 0.342,
      "step": 11750
    },
    {
      "epoch": 0.05527437274805498,
      "grad_norm": 2.1664865016937256,
      "learning_rate": 0.00018939055324525918,
      "loss": 0.0959,
      "step": 11751
    },
    {
      "epoch": 0.05527907654966744,
      "grad_norm": 1.1640844345092773,
      "learning_rate": 0.0001893896102671457,
      "loss": 0.0501,
      "step": 11752
    },
    {
      "epoch": 0.055283780351279906,
      "grad_norm": 2.831498384475708,
      "learning_rate": 0.00018938866728903224,
      "loss": 0.5172,
      "step": 11753
    },
    {
      "epoch": 0.055288484152892364,
      "grad_norm": 2.2793660163879395,
      "learning_rate": 0.00018938772431091876,
      "loss": 0.48,
      "step": 11754
    },
    {
      "epoch": 0.05529318795450483,
      "grad_norm": 6.254001617431641,
      "learning_rate": 0.00018938678133280528,
      "loss": 1.0362,
      "step": 11755
    },
    {
      "epoch": 0.055297891756117296,
      "grad_norm": 2.5689756870269775,
      "learning_rate": 0.0001893858383546918,
      "loss": 0.239,
      "step": 11756
    },
    {
      "epoch": 0.055302595557729754,
      "grad_norm": 0.7281445860862732,
      "learning_rate": 0.00018938489537657832,
      "loss": 0.0292,
      "step": 11757
    },
    {
      "epoch": 0.05530729935934222,
      "grad_norm": 1.2888402938842773,
      "learning_rate": 0.00018938395239846486,
      "loss": 0.0824,
      "step": 11758
    },
    {
      "epoch": 0.055312003160954686,
      "grad_norm": 2.9436559677124023,
      "learning_rate": 0.00018938300942035135,
      "loss": 0.2144,
      "step": 11759
    },
    {
      "epoch": 0.055316706962567144,
      "grad_norm": 1.58485746383667,
      "learning_rate": 0.00018938206644223787,
      "loss": 0.1556,
      "step": 11760
    },
    {
      "epoch": 0.05532141076417961,
      "grad_norm": 2.3339123725891113,
      "learning_rate": 0.0001893811234641244,
      "loss": 0.3994,
      "step": 11761
    },
    {
      "epoch": 0.055326114565792076,
      "grad_norm": 1.1727732419967651,
      "learning_rate": 0.00018938018048601094,
      "loss": 0.0581,
      "step": 11762
    },
    {
      "epoch": 0.055330818367404534,
      "grad_norm": 2.4987292289733887,
      "learning_rate": 0.00018937923750789745,
      "loss": 0.2687,
      "step": 11763
    },
    {
      "epoch": 0.055335522169017,
      "grad_norm": 1.123161792755127,
      "learning_rate": 0.00018937829452978397,
      "loss": 0.0935,
      "step": 11764
    },
    {
      "epoch": 0.055340225970629466,
      "grad_norm": 2.6334052085876465,
      "learning_rate": 0.0001893773515516705,
      "loss": 0.1519,
      "step": 11765
    },
    {
      "epoch": 0.055344929772241924,
      "grad_norm": 1.0309666395187378,
      "learning_rate": 0.000189376408573557,
      "loss": 0.1023,
      "step": 11766
    },
    {
      "epoch": 0.05534963357385439,
      "grad_norm": 1.5475257635116577,
      "learning_rate": 0.00018937546559544356,
      "loss": 0.249,
      "step": 11767
    },
    {
      "epoch": 0.055354337375466856,
      "grad_norm": 0.7694291472434998,
      "learning_rate": 0.00018937452261733007,
      "loss": 0.047,
      "step": 11768
    },
    {
      "epoch": 0.055359041177079314,
      "grad_norm": 2.1811110973358154,
      "learning_rate": 0.0001893735796392166,
      "loss": 0.4128,
      "step": 11769
    },
    {
      "epoch": 0.05536374497869178,
      "grad_norm": 2.021878480911255,
      "learning_rate": 0.00018937263666110308,
      "loss": 0.2798,
      "step": 11770
    },
    {
      "epoch": 0.05536844878030424,
      "grad_norm": 2.142707586288452,
      "learning_rate": 0.00018937169368298963,
      "loss": 0.3955,
      "step": 11771
    },
    {
      "epoch": 0.055373152581916704,
      "grad_norm": 1.4217426776885986,
      "learning_rate": 0.00018937075070487615,
      "loss": 0.1255,
      "step": 11772
    },
    {
      "epoch": 0.05537785638352917,
      "grad_norm": 3.241995334625244,
      "learning_rate": 0.00018936980772676267,
      "loss": 0.3111,
      "step": 11773
    },
    {
      "epoch": 0.05538256018514163,
      "grad_norm": 3.4505794048309326,
      "learning_rate": 0.0001893688647486492,
      "loss": 0.6072,
      "step": 11774
    },
    {
      "epoch": 0.055387263986754094,
      "grad_norm": 3.828977584838867,
      "learning_rate": 0.0001893679217705357,
      "loss": 1.0742,
      "step": 11775
    },
    {
      "epoch": 0.05539196778836656,
      "grad_norm": 1.1969670057296753,
      "learning_rate": 0.00018936697879242225,
      "loss": 0.1335,
      "step": 11776
    },
    {
      "epoch": 0.05539667158997902,
      "grad_norm": 7.151427268981934,
      "learning_rate": 0.00018936603581430877,
      "loss": 0.4099,
      "step": 11777
    },
    {
      "epoch": 0.055401375391591484,
      "grad_norm": 0.6409465670585632,
      "learning_rate": 0.0001893650928361953,
      "loss": 0.1059,
      "step": 11778
    },
    {
      "epoch": 0.05540607919320395,
      "grad_norm": 2.6902663707733154,
      "learning_rate": 0.0001893641498580818,
      "loss": 0.4429,
      "step": 11779
    },
    {
      "epoch": 0.05541078299481641,
      "grad_norm": 2.807827949523926,
      "learning_rate": 0.00018936320687996833,
      "loss": 0.3123,
      "step": 11780
    },
    {
      "epoch": 0.055415486796428874,
      "grad_norm": 2.740583658218384,
      "learning_rate": 0.00018936226390185484,
      "loss": 0.4718,
      "step": 11781
    },
    {
      "epoch": 0.05542019059804134,
      "grad_norm": 1.6982064247131348,
      "learning_rate": 0.00018936132092374136,
      "loss": 0.5341,
      "step": 11782
    },
    {
      "epoch": 0.0554248943996538,
      "grad_norm": 1.1419531106948853,
      "learning_rate": 0.00018936037794562788,
      "loss": 0.2507,
      "step": 11783
    },
    {
      "epoch": 0.055429598201266264,
      "grad_norm": 1.702013373374939,
      "learning_rate": 0.0001893594349675144,
      "loss": 0.2197,
      "step": 11784
    },
    {
      "epoch": 0.05543430200287873,
      "grad_norm": 1.5040702819824219,
      "learning_rate": 0.00018935849198940095,
      "loss": 0.2234,
      "step": 11785
    },
    {
      "epoch": 0.05543900580449119,
      "grad_norm": 1.326475739479065,
      "learning_rate": 0.00018935754901128746,
      "loss": 0.1799,
      "step": 11786
    },
    {
      "epoch": 0.055443709606103654,
      "grad_norm": 4.260069847106934,
      "learning_rate": 0.00018935660603317398,
      "loss": 0.8511,
      "step": 11787
    },
    {
      "epoch": 0.05544841340771611,
      "grad_norm": 1.1235110759735107,
      "learning_rate": 0.0001893556630550605,
      "loss": 0.2478,
      "step": 11788
    },
    {
      "epoch": 0.05545311720932858,
      "grad_norm": 1.531430721282959,
      "learning_rate": 0.00018935472007694705,
      "loss": 0.1828,
      "step": 11789
    },
    {
      "epoch": 0.055457821010941044,
      "grad_norm": 2.0021681785583496,
      "learning_rate": 0.00018935377709883354,
      "loss": 0.3125,
      "step": 11790
    },
    {
      "epoch": 0.0554625248125535,
      "grad_norm": 3.1587865352630615,
      "learning_rate": 0.00018935283412072006,
      "loss": 0.5505,
      "step": 11791
    },
    {
      "epoch": 0.05546722861416597,
      "grad_norm": 3.8805503845214844,
      "learning_rate": 0.00018935189114260658,
      "loss": 0.4348,
      "step": 11792
    },
    {
      "epoch": 0.055471932415778434,
      "grad_norm": 0.9437567591667175,
      "learning_rate": 0.0001893509481644931,
      "loss": 0.207,
      "step": 11793
    },
    {
      "epoch": 0.05547663621739089,
      "grad_norm": 1.944477915763855,
      "learning_rate": 0.00018935000518637964,
      "loss": 0.3509,
      "step": 11794
    },
    {
      "epoch": 0.05548134001900336,
      "grad_norm": 1.6929519176483154,
      "learning_rate": 0.00018934906220826616,
      "loss": 0.4357,
      "step": 11795
    },
    {
      "epoch": 0.055486043820615824,
      "grad_norm": 1.9427026510238647,
      "learning_rate": 0.00018934811923015268,
      "loss": 0.4886,
      "step": 11796
    },
    {
      "epoch": 0.05549074762222828,
      "grad_norm": 0.9235984086990356,
      "learning_rate": 0.0001893471762520392,
      "loss": 0.1482,
      "step": 11797
    },
    {
      "epoch": 0.05549545142384075,
      "grad_norm": 1.425410509109497,
      "learning_rate": 0.00018934623327392572,
      "loss": 0.2929,
      "step": 11798
    },
    {
      "epoch": 0.055500155225453214,
      "grad_norm": 0.924268364906311,
      "learning_rate": 0.00018934529029581226,
      "loss": 0.103,
      "step": 11799
    },
    {
      "epoch": 0.05550485902706567,
      "grad_norm": 1.0919078588485718,
      "learning_rate": 0.00018934434731769878,
      "loss": 0.2396,
      "step": 11800
    },
    {
      "epoch": 0.05550956282867814,
      "grad_norm": 5.155836582183838,
      "learning_rate": 0.00018934340433958527,
      "loss": 0.4033,
      "step": 11801
    },
    {
      "epoch": 0.055514266630290604,
      "grad_norm": 1.431441068649292,
      "learning_rate": 0.0001893424613614718,
      "loss": 0.158,
      "step": 11802
    },
    {
      "epoch": 0.05551897043190306,
      "grad_norm": 5.1667327880859375,
      "learning_rate": 0.00018934151838335834,
      "loss": 0.3877,
      "step": 11803
    },
    {
      "epoch": 0.05552367423351553,
      "grad_norm": 2.7474184036254883,
      "learning_rate": 0.00018934057540524485,
      "loss": 0.2975,
      "step": 11804
    },
    {
      "epoch": 0.05552837803512799,
      "grad_norm": 1.5487526655197144,
      "learning_rate": 0.00018933963242713137,
      "loss": 0.1953,
      "step": 11805
    },
    {
      "epoch": 0.05553308183674045,
      "grad_norm": 2.786720037460327,
      "learning_rate": 0.0001893386894490179,
      "loss": 0.2945,
      "step": 11806
    },
    {
      "epoch": 0.05553778563835292,
      "grad_norm": 4.1158342361450195,
      "learning_rate": 0.0001893377464709044,
      "loss": 0.5219,
      "step": 11807
    },
    {
      "epoch": 0.05554248943996538,
      "grad_norm": 2.124753713607788,
      "learning_rate": 0.00018933680349279096,
      "loss": 0.1396,
      "step": 11808
    },
    {
      "epoch": 0.05554719324157784,
      "grad_norm": 1.0577303171157837,
      "learning_rate": 0.00018933586051467747,
      "loss": 0.1392,
      "step": 11809
    },
    {
      "epoch": 0.05555189704319031,
      "grad_norm": 5.118373394012451,
      "learning_rate": 0.000189334917536564,
      "loss": 0.5704,
      "step": 11810
    },
    {
      "epoch": 0.05555660084480277,
      "grad_norm": 0.6949080228805542,
      "learning_rate": 0.0001893339745584505,
      "loss": 0.0581,
      "step": 11811
    },
    {
      "epoch": 0.05556130464641523,
      "grad_norm": 7.777693271636963,
      "learning_rate": 0.00018933303158033703,
      "loss": 0.536,
      "step": 11812
    },
    {
      "epoch": 0.0555660084480277,
      "grad_norm": 3.519749402999878,
      "learning_rate": 0.00018933208860222355,
      "loss": 0.5232,
      "step": 11813
    },
    {
      "epoch": 0.05557071224964016,
      "grad_norm": 4.220903396606445,
      "learning_rate": 0.00018933114562411007,
      "loss": 0.8673,
      "step": 11814
    },
    {
      "epoch": 0.05557541605125262,
      "grad_norm": 1.9714832305908203,
      "learning_rate": 0.0001893302026459966,
      "loss": 0.2957,
      "step": 11815
    },
    {
      "epoch": 0.05558011985286509,
      "grad_norm": 2.7409980297088623,
      "learning_rate": 0.0001893292596678831,
      "loss": 0.5501,
      "step": 11816
    },
    {
      "epoch": 0.05558482365447755,
      "grad_norm": 3.5771496295928955,
      "learning_rate": 0.00018932831668976965,
      "loss": 0.6162,
      "step": 11817
    },
    {
      "epoch": 0.05558952745609001,
      "grad_norm": 2.9000296592712402,
      "learning_rate": 0.00018932737371165617,
      "loss": 0.16,
      "step": 11818
    },
    {
      "epoch": 0.05559423125770248,
      "grad_norm": 2.8183109760284424,
      "learning_rate": 0.0001893264307335427,
      "loss": 0.3213,
      "step": 11819
    },
    {
      "epoch": 0.05559893505931494,
      "grad_norm": 0.4331408739089966,
      "learning_rate": 0.0001893254877554292,
      "loss": 0.0441,
      "step": 11820
    },
    {
      "epoch": 0.0556036388609274,
      "grad_norm": 2.8964998722076416,
      "learning_rate": 0.00018932454477731573,
      "loss": 0.3961,
      "step": 11821
    },
    {
      "epoch": 0.05560834266253986,
      "grad_norm": 2.564934492111206,
      "learning_rate": 0.00018932360179920224,
      "loss": 0.387,
      "step": 11822
    },
    {
      "epoch": 0.05561304646415233,
      "grad_norm": 1.6106797456741333,
      "learning_rate": 0.00018932265882108876,
      "loss": 0.2571,
      "step": 11823
    },
    {
      "epoch": 0.05561775026576479,
      "grad_norm": 1.036907434463501,
      "learning_rate": 0.00018932171584297528,
      "loss": 0.208,
      "step": 11824
    },
    {
      "epoch": 0.05562245406737725,
      "grad_norm": 2.071385622024536,
      "learning_rate": 0.0001893207728648618,
      "loss": 0.3574,
      "step": 11825
    },
    {
      "epoch": 0.05562715786898972,
      "grad_norm": 1.0084823369979858,
      "learning_rate": 0.00018931982988674835,
      "loss": 0.1036,
      "step": 11826
    },
    {
      "epoch": 0.05563186167060218,
      "grad_norm": 3.0161514282226562,
      "learning_rate": 0.00018931888690863486,
      "loss": 0.4398,
      "step": 11827
    },
    {
      "epoch": 0.05563656547221464,
      "grad_norm": 3.315706968307495,
      "learning_rate": 0.00018931794393052138,
      "loss": 0.9186,
      "step": 11828
    },
    {
      "epoch": 0.05564126927382711,
      "grad_norm": 2.3237781524658203,
      "learning_rate": 0.0001893170009524079,
      "loss": 0.4055,
      "step": 11829
    },
    {
      "epoch": 0.05564597307543957,
      "grad_norm": 2.679149866104126,
      "learning_rate": 0.00018931605797429445,
      "loss": 0.5313,
      "step": 11830
    },
    {
      "epoch": 0.05565067687705203,
      "grad_norm": 1.5855125188827515,
      "learning_rate": 0.00018931511499618097,
      "loss": 0.3699,
      "step": 11831
    },
    {
      "epoch": 0.0556553806786645,
      "grad_norm": 1.1751543283462524,
      "learning_rate": 0.00018931417201806746,
      "loss": 0.2234,
      "step": 11832
    },
    {
      "epoch": 0.05566008448027696,
      "grad_norm": 1.9509541988372803,
      "learning_rate": 0.00018931322903995398,
      "loss": 0.2421,
      "step": 11833
    },
    {
      "epoch": 0.05566478828188942,
      "grad_norm": 1.5977834463119507,
      "learning_rate": 0.0001893122860618405,
      "loss": 0.1922,
      "step": 11834
    },
    {
      "epoch": 0.05566949208350189,
      "grad_norm": 1.5662524700164795,
      "learning_rate": 0.00018931134308372704,
      "loss": 0.2518,
      "step": 11835
    },
    {
      "epoch": 0.05567419588511435,
      "grad_norm": 0.613412082195282,
      "learning_rate": 0.00018931040010561356,
      "loss": 0.0932,
      "step": 11836
    },
    {
      "epoch": 0.05567889968672681,
      "grad_norm": 2.5870213508605957,
      "learning_rate": 0.00018930945712750008,
      "loss": 0.443,
      "step": 11837
    },
    {
      "epoch": 0.05568360348833928,
      "grad_norm": 0.6493396759033203,
      "learning_rate": 0.0001893085141493866,
      "loss": 0.1034,
      "step": 11838
    },
    {
      "epoch": 0.055688307289951736,
      "grad_norm": 0.557840883731842,
      "learning_rate": 0.00018930757117127314,
      "loss": 0.0704,
      "step": 11839
    },
    {
      "epoch": 0.0556930110915642,
      "grad_norm": 0.9682518839836121,
      "learning_rate": 0.00018930662819315966,
      "loss": 0.1547,
      "step": 11840
    },
    {
      "epoch": 0.05569771489317667,
      "grad_norm": 0.794548749923706,
      "learning_rate": 0.00018930568521504618,
      "loss": 0.1047,
      "step": 11841
    },
    {
      "epoch": 0.055702418694789126,
      "grad_norm": 4.858740329742432,
      "learning_rate": 0.0001893047422369327,
      "loss": 0.348,
      "step": 11842
    },
    {
      "epoch": 0.05570712249640159,
      "grad_norm": 0.40231853723526,
      "learning_rate": 0.0001893037992588192,
      "loss": 0.0416,
      "step": 11843
    },
    {
      "epoch": 0.05571182629801406,
      "grad_norm": 1.3890997171401978,
      "learning_rate": 0.00018930285628070574,
      "loss": 0.3107,
      "step": 11844
    },
    {
      "epoch": 0.055716530099626516,
      "grad_norm": 1.9101712703704834,
      "learning_rate": 0.00018930191330259225,
      "loss": 0.3597,
      "step": 11845
    },
    {
      "epoch": 0.05572123390123898,
      "grad_norm": 2.1234469413757324,
      "learning_rate": 0.00018930097032447877,
      "loss": 0.4132,
      "step": 11846
    },
    {
      "epoch": 0.05572593770285145,
      "grad_norm": 0.6058177351951599,
      "learning_rate": 0.0001893000273463653,
      "loss": 0.0596,
      "step": 11847
    },
    {
      "epoch": 0.055730641504463906,
      "grad_norm": 2.4262444972991943,
      "learning_rate": 0.0001892990843682518,
      "loss": 0.4855,
      "step": 11848
    },
    {
      "epoch": 0.05573534530607637,
      "grad_norm": 1.310768485069275,
      "learning_rate": 0.00018929814139013836,
      "loss": 0.1693,
      "step": 11849
    },
    {
      "epoch": 0.05574004910768884,
      "grad_norm": 1.783402919769287,
      "learning_rate": 0.00018929719841202487,
      "loss": 0.2683,
      "step": 11850
    },
    {
      "epoch": 0.055744752909301296,
      "grad_norm": 2.650930881500244,
      "learning_rate": 0.0001892962554339114,
      "loss": 0.2083,
      "step": 11851
    },
    {
      "epoch": 0.05574945671091376,
      "grad_norm": 2.47684907913208,
      "learning_rate": 0.0001892953124557979,
      "loss": 0.2388,
      "step": 11852
    },
    {
      "epoch": 0.05575416051252623,
      "grad_norm": 0.9857344031333923,
      "learning_rate": 0.00018929436947768443,
      "loss": 0.1095,
      "step": 11853
    },
    {
      "epoch": 0.055758864314138686,
      "grad_norm": 0.5078587532043457,
      "learning_rate": 0.00018929342649957095,
      "loss": 0.0233,
      "step": 11854
    },
    {
      "epoch": 0.05576356811575115,
      "grad_norm": 2.5735583305358887,
      "learning_rate": 0.00018929248352145747,
      "loss": 0.2512,
      "step": 11855
    },
    {
      "epoch": 0.05576827191736361,
      "grad_norm": 4.143693447113037,
      "learning_rate": 0.00018929154054334399,
      "loss": 0.4904,
      "step": 11856
    },
    {
      "epoch": 0.055772975718976076,
      "grad_norm": 0.2156592756509781,
      "learning_rate": 0.0001892905975652305,
      "loss": 0.0135,
      "step": 11857
    },
    {
      "epoch": 0.05577767952058854,
      "grad_norm": 0.8783156871795654,
      "learning_rate": 0.00018928965458711705,
      "loss": 0.0759,
      "step": 11858
    },
    {
      "epoch": 0.055782383322201,
      "grad_norm": 3.439147710800171,
      "learning_rate": 0.00018928871160900357,
      "loss": 0.2902,
      "step": 11859
    },
    {
      "epoch": 0.055787087123813466,
      "grad_norm": 3.2224044799804688,
      "learning_rate": 0.0001892877686308901,
      "loss": 0.332,
      "step": 11860
    },
    {
      "epoch": 0.05579179092542593,
      "grad_norm": 4.167941093444824,
      "learning_rate": 0.0001892868256527766,
      "loss": 0.557,
      "step": 11861
    },
    {
      "epoch": 0.05579649472703839,
      "grad_norm": 2.427269458770752,
      "learning_rate": 0.00018928588267466315,
      "loss": 0.3089,
      "step": 11862
    },
    {
      "epoch": 0.055801198528650856,
      "grad_norm": 1.8778059482574463,
      "learning_rate": 0.00018928493969654964,
      "loss": 0.0941,
      "step": 11863
    },
    {
      "epoch": 0.05580590233026332,
      "grad_norm": 1.786379337310791,
      "learning_rate": 0.00018928399671843616,
      "loss": 0.1784,
      "step": 11864
    },
    {
      "epoch": 0.05581060613187578,
      "grad_norm": 1.9714187383651733,
      "learning_rate": 0.00018928305374032268,
      "loss": 0.2825,
      "step": 11865
    },
    {
      "epoch": 0.055815309933488245,
      "grad_norm": 3.0862698554992676,
      "learning_rate": 0.0001892821107622092,
      "loss": 0.2771,
      "step": 11866
    },
    {
      "epoch": 0.05582001373510071,
      "grad_norm": 2.100478410720825,
      "learning_rate": 0.00018928116778409575,
      "loss": 0.185,
      "step": 11867
    },
    {
      "epoch": 0.05582471753671317,
      "grad_norm": 0.12070785462856293,
      "learning_rate": 0.00018928022480598226,
      "loss": 0.0059,
      "step": 11868
    },
    {
      "epoch": 0.055829421338325635,
      "grad_norm": 4.132809162139893,
      "learning_rate": 0.00018927928182786878,
      "loss": 0.3705,
      "step": 11869
    },
    {
      "epoch": 0.0558341251399381,
      "grad_norm": 2.223961114883423,
      "learning_rate": 0.0001892783388497553,
      "loss": 0.4787,
      "step": 11870
    },
    {
      "epoch": 0.05583882894155056,
      "grad_norm": 3.541870355606079,
      "learning_rate": 0.00018927739587164185,
      "loss": 0.7027,
      "step": 11871
    },
    {
      "epoch": 0.055843532743163025,
      "grad_norm": 2.514425039291382,
      "learning_rate": 0.00018927645289352837,
      "loss": 0.2325,
      "step": 11872
    },
    {
      "epoch": 0.055848236544775484,
      "grad_norm": 1.184518575668335,
      "learning_rate": 0.00018927550991541488,
      "loss": 0.1037,
      "step": 11873
    },
    {
      "epoch": 0.05585294034638795,
      "grad_norm": 3.5946602821350098,
      "learning_rate": 0.00018927456693730138,
      "loss": 0.2864,
      "step": 11874
    },
    {
      "epoch": 0.055857644148000415,
      "grad_norm": 3.34698224067688,
      "learning_rate": 0.0001892736239591879,
      "loss": 0.1966,
      "step": 11875
    },
    {
      "epoch": 0.055862347949612874,
      "grad_norm": 3.1322202682495117,
      "learning_rate": 0.00018927268098107444,
      "loss": 0.3619,
      "step": 11876
    },
    {
      "epoch": 0.05586705175122534,
      "grad_norm": 3.9306528568267822,
      "learning_rate": 0.00018927173800296096,
      "loss": 0.2124,
      "step": 11877
    },
    {
      "epoch": 0.055871755552837805,
      "grad_norm": 0.38862332701683044,
      "learning_rate": 0.00018927079502484748,
      "loss": 0.0409,
      "step": 11878
    },
    {
      "epoch": 0.055876459354450264,
      "grad_norm": 3.1915063858032227,
      "learning_rate": 0.000189269852046734,
      "loss": 0.2523,
      "step": 11879
    },
    {
      "epoch": 0.05588116315606273,
      "grad_norm": 1.8642933368682861,
      "learning_rate": 0.00018926890906862054,
      "loss": 0.3321,
      "step": 11880
    },
    {
      "epoch": 0.055885866957675195,
      "grad_norm": 1.6180529594421387,
      "learning_rate": 0.00018926796609050706,
      "loss": 0.1781,
      "step": 11881
    },
    {
      "epoch": 0.055890570759287654,
      "grad_norm": 2.0190322399139404,
      "learning_rate": 0.00018926702311239358,
      "loss": 0.2843,
      "step": 11882
    },
    {
      "epoch": 0.05589527456090012,
      "grad_norm": 1.3582254648208618,
      "learning_rate": 0.0001892660801342801,
      "loss": 0.1058,
      "step": 11883
    },
    {
      "epoch": 0.055899978362512585,
      "grad_norm": 2.52194881439209,
      "learning_rate": 0.00018926513715616662,
      "loss": 0.2217,
      "step": 11884
    },
    {
      "epoch": 0.055904682164125044,
      "grad_norm": 0.611831784248352,
      "learning_rate": 0.00018926419417805314,
      "loss": 0.0606,
      "step": 11885
    },
    {
      "epoch": 0.05590938596573751,
      "grad_norm": 0.495450496673584,
      "learning_rate": 0.00018926325119993965,
      "loss": 0.0524,
      "step": 11886
    },
    {
      "epoch": 0.055914089767349975,
      "grad_norm": 1.9002656936645508,
      "learning_rate": 0.00018926230822182617,
      "loss": 0.117,
      "step": 11887
    },
    {
      "epoch": 0.055918793568962434,
      "grad_norm": 1.8262650966644287,
      "learning_rate": 0.0001892613652437127,
      "loss": 0.26,
      "step": 11888
    },
    {
      "epoch": 0.0559234973705749,
      "grad_norm": 1.046026349067688,
      "learning_rate": 0.00018926042226559924,
      "loss": 0.1593,
      "step": 11889
    },
    {
      "epoch": 0.05592820117218736,
      "grad_norm": 2.2516605854034424,
      "learning_rate": 0.00018925947928748576,
      "loss": 0.3317,
      "step": 11890
    },
    {
      "epoch": 0.055932904973799824,
      "grad_norm": 0.2600075900554657,
      "learning_rate": 0.00018925853630937227,
      "loss": 0.0176,
      "step": 11891
    },
    {
      "epoch": 0.05593760877541229,
      "grad_norm": 1.4104984998703003,
      "learning_rate": 0.0001892575933312588,
      "loss": 0.1102,
      "step": 11892
    },
    {
      "epoch": 0.05594231257702475,
      "grad_norm": 1.1895806789398193,
      "learning_rate": 0.0001892566503531453,
      "loss": 0.1248,
      "step": 11893
    },
    {
      "epoch": 0.055947016378637214,
      "grad_norm": 2.2627580165863037,
      "learning_rate": 0.00018925570737503183,
      "loss": 0.3252,
      "step": 11894
    },
    {
      "epoch": 0.05595172018024968,
      "grad_norm": 2.1509923934936523,
      "learning_rate": 0.00018925476439691835,
      "loss": 0.4361,
      "step": 11895
    },
    {
      "epoch": 0.05595642398186214,
      "grad_norm": 0.9220050573348999,
      "learning_rate": 0.00018925382141880487,
      "loss": 0.1108,
      "step": 11896
    },
    {
      "epoch": 0.055961127783474604,
      "grad_norm": 3.957624912261963,
      "learning_rate": 0.00018925287844069139,
      "loss": 0.3307,
      "step": 11897
    },
    {
      "epoch": 0.05596583158508707,
      "grad_norm": 4.234726428985596,
      "learning_rate": 0.0001892519354625779,
      "loss": 0.875,
      "step": 11898
    },
    {
      "epoch": 0.05597053538669953,
      "grad_norm": 1.7958787679672241,
      "learning_rate": 0.00018925099248446445,
      "loss": 0.1588,
      "step": 11899
    },
    {
      "epoch": 0.055975239188311994,
      "grad_norm": 3.9403300285339355,
      "learning_rate": 0.00018925004950635097,
      "loss": 0.2813,
      "step": 11900
    },
    {
      "epoch": 0.05597994298992446,
      "grad_norm": 6.549342632293701,
      "learning_rate": 0.0001892491065282375,
      "loss": 0.6106,
      "step": 11901
    },
    {
      "epoch": 0.05598464679153692,
      "grad_norm": 2.156747817993164,
      "learning_rate": 0.000189248163550124,
      "loss": 0.0823,
      "step": 11902
    },
    {
      "epoch": 0.055989350593149384,
      "grad_norm": 5.680121421813965,
      "learning_rate": 0.00018924722057201055,
      "loss": 0.4532,
      "step": 11903
    },
    {
      "epoch": 0.05599405439476185,
      "grad_norm": 2.874948024749756,
      "learning_rate": 0.00018924627759389707,
      "loss": 0.285,
      "step": 11904
    },
    {
      "epoch": 0.05599875819637431,
      "grad_norm": 3.312227487564087,
      "learning_rate": 0.00018924533461578356,
      "loss": 0.3173,
      "step": 11905
    },
    {
      "epoch": 0.056003461997986774,
      "grad_norm": 2.5492043495178223,
      "learning_rate": 0.00018924439163767008,
      "loss": 0.1859,
      "step": 11906
    },
    {
      "epoch": 0.05600816579959923,
      "grad_norm": 3.4358248710632324,
      "learning_rate": 0.0001892434486595566,
      "loss": 0.5114,
      "step": 11907
    },
    {
      "epoch": 0.0560128696012117,
      "grad_norm": 2.677370548248291,
      "learning_rate": 0.00018924250568144315,
      "loss": 0.3375,
      "step": 11908
    },
    {
      "epoch": 0.056017573402824164,
      "grad_norm": 2.30234694480896,
      "learning_rate": 0.00018924156270332966,
      "loss": 0.1744,
      "step": 11909
    },
    {
      "epoch": 0.05602227720443662,
      "grad_norm": 1.2859503030776978,
      "learning_rate": 0.00018924061972521618,
      "loss": 0.1197,
      "step": 11910
    },
    {
      "epoch": 0.05602698100604909,
      "grad_norm": 3.864651679992676,
      "learning_rate": 0.0001892396767471027,
      "loss": 0.4274,
      "step": 11911
    },
    {
      "epoch": 0.056031684807661554,
      "grad_norm": 5.6823410987854,
      "learning_rate": 0.00018923873376898925,
      "loss": 0.3822,
      "step": 11912
    },
    {
      "epoch": 0.05603638860927401,
      "grad_norm": 2.8137366771698,
      "learning_rate": 0.00018923779079087577,
      "loss": 0.313,
      "step": 11913
    },
    {
      "epoch": 0.05604109241088648,
      "grad_norm": 3.3678557872772217,
      "learning_rate": 0.00018923684781276228,
      "loss": 0.8623,
      "step": 11914
    },
    {
      "epoch": 0.056045796212498944,
      "grad_norm": 3.354006052017212,
      "learning_rate": 0.0001892359048346488,
      "loss": 0.3674,
      "step": 11915
    },
    {
      "epoch": 0.0560505000141114,
      "grad_norm": 1.3303941488265991,
      "learning_rate": 0.00018923496185653532,
      "loss": 0.0701,
      "step": 11916
    },
    {
      "epoch": 0.05605520381572387,
      "grad_norm": 2.3660788536071777,
      "learning_rate": 0.00018923401887842184,
      "loss": 0.2598,
      "step": 11917
    },
    {
      "epoch": 0.056059907617336334,
      "grad_norm": 5.000446319580078,
      "learning_rate": 0.00018923307590030836,
      "loss": 0.7392,
      "step": 11918
    },
    {
      "epoch": 0.05606461141894879,
      "grad_norm": 4.186976432800293,
      "learning_rate": 0.00018923213292219488,
      "loss": 0.3915,
      "step": 11919
    },
    {
      "epoch": 0.05606931522056126,
      "grad_norm": 1.949267029762268,
      "learning_rate": 0.0001892311899440814,
      "loss": 0.3159,
      "step": 11920
    },
    {
      "epoch": 0.056074019022173724,
      "grad_norm": 0.8944299817085266,
      "learning_rate": 0.00018923024696596794,
      "loss": 0.0845,
      "step": 11921
    },
    {
      "epoch": 0.05607872282378618,
      "grad_norm": 0.8225506544113159,
      "learning_rate": 0.00018922930398785446,
      "loss": 0.0803,
      "step": 11922
    },
    {
      "epoch": 0.05608342662539865,
      "grad_norm": 1.707695722579956,
      "learning_rate": 0.00018922836100974098,
      "loss": 0.1503,
      "step": 11923
    },
    {
      "epoch": 0.05608813042701111,
      "grad_norm": 0.26940712332725525,
      "learning_rate": 0.0001892274180316275,
      "loss": 0.0216,
      "step": 11924
    },
    {
      "epoch": 0.05609283422862357,
      "grad_norm": 1.5155458450317383,
      "learning_rate": 0.00018922647505351402,
      "loss": 0.1204,
      "step": 11925
    },
    {
      "epoch": 0.05609753803023604,
      "grad_norm": 0.9573044776916504,
      "learning_rate": 0.00018922553207540054,
      "loss": 0.1208,
      "step": 11926
    },
    {
      "epoch": 0.0561022418318485,
      "grad_norm": 3.642775297164917,
      "learning_rate": 0.00018922458909728705,
      "loss": 0.2486,
      "step": 11927
    },
    {
      "epoch": 0.05610694563346096,
      "grad_norm": 2.82430100440979,
      "learning_rate": 0.00018922364611917357,
      "loss": 0.6691,
      "step": 11928
    },
    {
      "epoch": 0.05611164943507343,
      "grad_norm": 1.6174710988998413,
      "learning_rate": 0.0001892227031410601,
      "loss": 0.0678,
      "step": 11929
    },
    {
      "epoch": 0.05611635323668589,
      "grad_norm": 0.3893440365791321,
      "learning_rate": 0.00018922176016294664,
      "loss": 0.0272,
      "step": 11930
    },
    {
      "epoch": 0.05612105703829835,
      "grad_norm": 1.2319523096084595,
      "learning_rate": 0.00018922081718483316,
      "loss": 0.1575,
      "step": 11931
    },
    {
      "epoch": 0.05612576083991082,
      "grad_norm": 2.1646084785461426,
      "learning_rate": 0.00018921987420671967,
      "loss": 0.2032,
      "step": 11932
    },
    {
      "epoch": 0.05613046464152328,
      "grad_norm": 1.6953390836715698,
      "learning_rate": 0.0001892189312286062,
      "loss": 0.3125,
      "step": 11933
    },
    {
      "epoch": 0.05613516844313574,
      "grad_norm": 1.8722593784332275,
      "learning_rate": 0.0001892179882504927,
      "loss": 0.1756,
      "step": 11934
    },
    {
      "epoch": 0.05613987224474821,
      "grad_norm": 1.3801389932632446,
      "learning_rate": 0.00018921704527237926,
      "loss": 0.0679,
      "step": 11935
    },
    {
      "epoch": 0.05614457604636067,
      "grad_norm": 0.3653991222381592,
      "learning_rate": 0.00018921610229426575,
      "loss": 0.0219,
      "step": 11936
    },
    {
      "epoch": 0.05614927984797313,
      "grad_norm": 3.078226089477539,
      "learning_rate": 0.00018921515931615227,
      "loss": 0.3868,
      "step": 11937
    },
    {
      "epoch": 0.0561539836495856,
      "grad_norm": 1.2656474113464355,
      "learning_rate": 0.00018921421633803879,
      "loss": 0.1035,
      "step": 11938
    },
    {
      "epoch": 0.05615868745119806,
      "grad_norm": 2.8455233573913574,
      "learning_rate": 0.00018921327335992533,
      "loss": 0.702,
      "step": 11939
    },
    {
      "epoch": 0.05616339125281052,
      "grad_norm": 2.4323787689208984,
      "learning_rate": 0.00018921233038181185,
      "loss": 0.3337,
      "step": 11940
    },
    {
      "epoch": 0.05616809505442298,
      "grad_norm": 1.0295346975326538,
      "learning_rate": 0.00018921138740369837,
      "loss": 0.0874,
      "step": 11941
    },
    {
      "epoch": 0.05617279885603545,
      "grad_norm": 4.354055881500244,
      "learning_rate": 0.0001892104444255849,
      "loss": 1.0792,
      "step": 11942
    },
    {
      "epoch": 0.05617750265764791,
      "grad_norm": 2.2722959518432617,
      "learning_rate": 0.0001892095014474714,
      "loss": 0.2384,
      "step": 11943
    },
    {
      "epoch": 0.05618220645926037,
      "grad_norm": 2.6896328926086426,
      "learning_rate": 0.00018920855846935795,
      "loss": 0.2691,
      "step": 11944
    },
    {
      "epoch": 0.05618691026087284,
      "grad_norm": 1.003901481628418,
      "learning_rate": 0.00018920761549124447,
      "loss": 0.0855,
      "step": 11945
    },
    {
      "epoch": 0.0561916140624853,
      "grad_norm": 0.5005442500114441,
      "learning_rate": 0.000189206672513131,
      "loss": 0.0724,
      "step": 11946
    },
    {
      "epoch": 0.05619631786409776,
      "grad_norm": 2.6487629413604736,
      "learning_rate": 0.0001892057295350175,
      "loss": 0.3977,
      "step": 11947
    },
    {
      "epoch": 0.05620102166571023,
      "grad_norm": 0.9554973244667053,
      "learning_rate": 0.000189204786556904,
      "loss": 0.0683,
      "step": 11948
    },
    {
      "epoch": 0.05620572546732269,
      "grad_norm": 2.466691493988037,
      "learning_rate": 0.00018920384357879055,
      "loss": 0.4617,
      "step": 11949
    },
    {
      "epoch": 0.05621042926893515,
      "grad_norm": 2.4276888370513916,
      "learning_rate": 0.00018920290060067706,
      "loss": 0.2289,
      "step": 11950
    },
    {
      "epoch": 0.05621513307054762,
      "grad_norm": 6.712893962860107,
      "learning_rate": 0.00018920195762256358,
      "loss": 0.569,
      "step": 11951
    },
    {
      "epoch": 0.05621983687216008,
      "grad_norm": 0.7320441603660583,
      "learning_rate": 0.0001892010146444501,
      "loss": 0.0247,
      "step": 11952
    },
    {
      "epoch": 0.05622454067377254,
      "grad_norm": 5.52238655090332,
      "learning_rate": 0.00018920007166633665,
      "loss": 0.6886,
      "step": 11953
    },
    {
      "epoch": 0.05622924447538501,
      "grad_norm": 3.650735378265381,
      "learning_rate": 0.00018919912868822317,
      "loss": 0.2412,
      "step": 11954
    },
    {
      "epoch": 0.05623394827699747,
      "grad_norm": 0.7841294407844543,
      "learning_rate": 0.00018919818571010968,
      "loss": 0.0734,
      "step": 11955
    },
    {
      "epoch": 0.05623865207860993,
      "grad_norm": 1.9464696645736694,
      "learning_rate": 0.0001891972427319962,
      "loss": 0.2603,
      "step": 11956
    },
    {
      "epoch": 0.0562433558802224,
      "grad_norm": 2.5576798915863037,
      "learning_rate": 0.00018919629975388272,
      "loss": 0.2127,
      "step": 11957
    },
    {
      "epoch": 0.056248059681834855,
      "grad_norm": 0.8818584680557251,
      "learning_rate": 0.00018919535677576924,
      "loss": 0.0927,
      "step": 11958
    },
    {
      "epoch": 0.05625276348344732,
      "grad_norm": 2.1583218574523926,
      "learning_rate": 0.00018919441379765576,
      "loss": 0.1209,
      "step": 11959
    },
    {
      "epoch": 0.05625746728505979,
      "grad_norm": 3.375190496444702,
      "learning_rate": 0.00018919347081954228,
      "loss": 0.2703,
      "step": 11960
    },
    {
      "epoch": 0.056262171086672245,
      "grad_norm": 1.8239108324050903,
      "learning_rate": 0.0001891925278414288,
      "loss": 0.1295,
      "step": 11961
    },
    {
      "epoch": 0.05626687488828471,
      "grad_norm": 1.5942726135253906,
      "learning_rate": 0.00018919158486331534,
      "loss": 0.1442,
      "step": 11962
    },
    {
      "epoch": 0.05627157868989718,
      "grad_norm": 3.3880155086517334,
      "learning_rate": 0.00018919064188520186,
      "loss": 0.4806,
      "step": 11963
    },
    {
      "epoch": 0.056276282491509635,
      "grad_norm": 1.2610247135162354,
      "learning_rate": 0.00018918969890708838,
      "loss": 0.1002,
      "step": 11964
    },
    {
      "epoch": 0.0562809862931221,
      "grad_norm": 4.959774971008301,
      "learning_rate": 0.0001891887559289749,
      "loss": 0.5696,
      "step": 11965
    },
    {
      "epoch": 0.05628569009473457,
      "grad_norm": 2.4804861545562744,
      "learning_rate": 0.00018918781295086142,
      "loss": 0.3149,
      "step": 11966
    },
    {
      "epoch": 0.056290393896347025,
      "grad_norm": 2.5261588096618652,
      "learning_rate": 0.00018918686997274794,
      "loss": 0.2383,
      "step": 11967
    },
    {
      "epoch": 0.05629509769795949,
      "grad_norm": 0.1017460897564888,
      "learning_rate": 0.00018918592699463445,
      "loss": 0.0053,
      "step": 11968
    },
    {
      "epoch": 0.05629980149957196,
      "grad_norm": 2.9506309032440186,
      "learning_rate": 0.00018918498401652097,
      "loss": 0.5265,
      "step": 11969
    },
    {
      "epoch": 0.056304505301184415,
      "grad_norm": 0.8056451082229614,
      "learning_rate": 0.0001891840410384075,
      "loss": 0.057,
      "step": 11970
    },
    {
      "epoch": 0.05630920910279688,
      "grad_norm": 1.730290174484253,
      "learning_rate": 0.00018918309806029404,
      "loss": 0.1287,
      "step": 11971
    },
    {
      "epoch": 0.05631391290440935,
      "grad_norm": 0.19082757830619812,
      "learning_rate": 0.00018918215508218056,
      "loss": 0.0121,
      "step": 11972
    },
    {
      "epoch": 0.056318616706021805,
      "grad_norm": 1.0608181953430176,
      "learning_rate": 0.00018918121210406707,
      "loss": 0.0821,
      "step": 11973
    },
    {
      "epoch": 0.05632332050763427,
      "grad_norm": 3.0469841957092285,
      "learning_rate": 0.0001891802691259536,
      "loss": 0.2472,
      "step": 11974
    },
    {
      "epoch": 0.05632802430924673,
      "grad_norm": 3.678570032119751,
      "learning_rate": 0.0001891793261478401,
      "loss": 0.1714,
      "step": 11975
    },
    {
      "epoch": 0.056332728110859195,
      "grad_norm": 0.20930351316928864,
      "learning_rate": 0.00018917838316972666,
      "loss": 0.0192,
      "step": 11976
    },
    {
      "epoch": 0.05633743191247166,
      "grad_norm": 3.4157345294952393,
      "learning_rate": 0.00018917744019161318,
      "loss": 0.3616,
      "step": 11977
    },
    {
      "epoch": 0.05634213571408412,
      "grad_norm": 0.49255308508872986,
      "learning_rate": 0.0001891764972134997,
      "loss": 0.0454,
      "step": 11978
    },
    {
      "epoch": 0.056346839515696585,
      "grad_norm": 3.192336320877075,
      "learning_rate": 0.00018917555423538619,
      "loss": 0.4148,
      "step": 11979
    },
    {
      "epoch": 0.05635154331730905,
      "grad_norm": 2.836367607116699,
      "learning_rate": 0.00018917461125727273,
      "loss": 0.3298,
      "step": 11980
    },
    {
      "epoch": 0.05635624711892151,
      "grad_norm": 5.488743305206299,
      "learning_rate": 0.00018917366827915925,
      "loss": 0.9132,
      "step": 11981
    },
    {
      "epoch": 0.056360950920533975,
      "grad_norm": 4.922192573547363,
      "learning_rate": 0.00018917272530104577,
      "loss": 0.5631,
      "step": 11982
    },
    {
      "epoch": 0.05636565472214644,
      "grad_norm": 3.9964559078216553,
      "learning_rate": 0.0001891717823229323,
      "loss": 0.4301,
      "step": 11983
    },
    {
      "epoch": 0.0563703585237589,
      "grad_norm": 1.6015594005584717,
      "learning_rate": 0.0001891708393448188,
      "loss": 0.1477,
      "step": 11984
    },
    {
      "epoch": 0.056375062325371365,
      "grad_norm": 3.709592342376709,
      "learning_rate": 0.00018916989636670535,
      "loss": 0.8225,
      "step": 11985
    },
    {
      "epoch": 0.05637976612698383,
      "grad_norm": 2.2368898391723633,
      "learning_rate": 0.00018916895338859187,
      "loss": 0.2479,
      "step": 11986
    },
    {
      "epoch": 0.05638446992859629,
      "grad_norm": 3.3098089694976807,
      "learning_rate": 0.0001891680104104784,
      "loss": 0.4342,
      "step": 11987
    },
    {
      "epoch": 0.056389173730208755,
      "grad_norm": 3.240077018737793,
      "learning_rate": 0.0001891670674323649,
      "loss": 0.3151,
      "step": 11988
    },
    {
      "epoch": 0.05639387753182122,
      "grad_norm": 2.182882070541382,
      "learning_rate": 0.00018916612445425143,
      "loss": 0.136,
      "step": 11989
    },
    {
      "epoch": 0.05639858133343368,
      "grad_norm": 2.359705686569214,
      "learning_rate": 0.00018916518147613795,
      "loss": 0.3847,
      "step": 11990
    },
    {
      "epoch": 0.056403285135046145,
      "grad_norm": 2.5173771381378174,
      "learning_rate": 0.00018916423849802446,
      "loss": 0.469,
      "step": 11991
    },
    {
      "epoch": 0.056407988936658604,
      "grad_norm": 3.145897150039673,
      "learning_rate": 0.00018916329551991098,
      "loss": 0.5901,
      "step": 11992
    },
    {
      "epoch": 0.05641269273827107,
      "grad_norm": 2.8257968425750732,
      "learning_rate": 0.0001891623525417975,
      "loss": 0.1884,
      "step": 11993
    },
    {
      "epoch": 0.056417396539883535,
      "grad_norm": 3.5953874588012695,
      "learning_rate": 0.00018916140956368405,
      "loss": 0.5737,
      "step": 11994
    },
    {
      "epoch": 0.056422100341495994,
      "grad_norm": 0.9925166368484497,
      "learning_rate": 0.00018916046658557057,
      "loss": 0.1507,
      "step": 11995
    },
    {
      "epoch": 0.05642680414310846,
      "grad_norm": 1.5482444763183594,
      "learning_rate": 0.00018915952360745708,
      "loss": 0.2379,
      "step": 11996
    },
    {
      "epoch": 0.056431507944720925,
      "grad_norm": 1.6711571216583252,
      "learning_rate": 0.0001891585806293436,
      "loss": 0.2062,
      "step": 11997
    },
    {
      "epoch": 0.056436211746333384,
      "grad_norm": 1.6530715227127075,
      "learning_rate": 0.00018915763765123012,
      "loss": 0.2447,
      "step": 11998
    },
    {
      "epoch": 0.05644091554794585,
      "grad_norm": 1.0652763843536377,
      "learning_rate": 0.00018915669467311664,
      "loss": 0.2263,
      "step": 11999
    },
    {
      "epoch": 0.056445619349558315,
      "grad_norm": 2.5780723094940186,
      "learning_rate": 0.00018915575169500316,
      "loss": 0.4966,
      "step": 12000
    },
    {
      "epoch": 0.056450323151170774,
      "grad_norm": 0.9220056533813477,
      "learning_rate": 0.00018915480871688968,
      "loss": 0.0954,
      "step": 12001
    },
    {
      "epoch": 0.05645502695278324,
      "grad_norm": 5.072256088256836,
      "learning_rate": 0.0001891538657387762,
      "loss": 1.2062,
      "step": 12002
    },
    {
      "epoch": 0.056459730754395705,
      "grad_norm": 3.384075880050659,
      "learning_rate": 0.00018915292276066274,
      "loss": 0.2577,
      "step": 12003
    },
    {
      "epoch": 0.056464434556008164,
      "grad_norm": 2.6200873851776123,
      "learning_rate": 0.00018915197978254926,
      "loss": 0.3378,
      "step": 12004
    },
    {
      "epoch": 0.05646913835762063,
      "grad_norm": 4.535585403442383,
      "learning_rate": 0.00018915103680443578,
      "loss": 0.7424,
      "step": 12005
    },
    {
      "epoch": 0.056473842159233095,
      "grad_norm": 1.7546604871749878,
      "learning_rate": 0.0001891500938263223,
      "loss": 0.3132,
      "step": 12006
    },
    {
      "epoch": 0.056478545960845554,
      "grad_norm": 0.47229886054992676,
      "learning_rate": 0.00018914915084820882,
      "loss": 0.0543,
      "step": 12007
    },
    {
      "epoch": 0.05648324976245802,
      "grad_norm": 2.276268482208252,
      "learning_rate": 0.00018914820787009536,
      "loss": 0.2638,
      "step": 12008
    },
    {
      "epoch": 0.05648795356407048,
      "grad_norm": 1.4955841302871704,
      "learning_rate": 0.00018914726489198188,
      "loss": 0.1541,
      "step": 12009
    },
    {
      "epoch": 0.056492657365682944,
      "grad_norm": 1.9307749271392822,
      "learning_rate": 0.00018914632191386837,
      "loss": 0.3301,
      "step": 12010
    },
    {
      "epoch": 0.05649736116729541,
      "grad_norm": 0.9078052043914795,
      "learning_rate": 0.0001891453789357549,
      "loss": 0.1314,
      "step": 12011
    },
    {
      "epoch": 0.05650206496890787,
      "grad_norm": 0.474296510219574,
      "learning_rate": 0.00018914443595764144,
      "loss": 0.0645,
      "step": 12012
    },
    {
      "epoch": 0.056506768770520334,
      "grad_norm": 2.5329232215881348,
      "learning_rate": 0.00018914349297952796,
      "loss": 0.2399,
      "step": 12013
    },
    {
      "epoch": 0.0565114725721328,
      "grad_norm": 1.7086820602416992,
      "learning_rate": 0.00018914255000141447,
      "loss": 0.1404,
      "step": 12014
    },
    {
      "epoch": 0.05651617637374526,
      "grad_norm": 4.314594745635986,
      "learning_rate": 0.000189141607023301,
      "loss": 0.274,
      "step": 12015
    },
    {
      "epoch": 0.056520880175357724,
      "grad_norm": 0.5429627895355225,
      "learning_rate": 0.0001891406640451875,
      "loss": 0.0857,
      "step": 12016
    },
    {
      "epoch": 0.05652558397697019,
      "grad_norm": 3.029616594314575,
      "learning_rate": 0.00018913972106707406,
      "loss": 0.3581,
      "step": 12017
    },
    {
      "epoch": 0.05653028777858265,
      "grad_norm": 0.9177241325378418,
      "learning_rate": 0.00018913877808896058,
      "loss": 0.0321,
      "step": 12018
    },
    {
      "epoch": 0.056534991580195114,
      "grad_norm": 1.9462010860443115,
      "learning_rate": 0.0001891378351108471,
      "loss": 0.2092,
      "step": 12019
    },
    {
      "epoch": 0.05653969538180758,
      "grad_norm": 1.1105189323425293,
      "learning_rate": 0.0001891368921327336,
      "loss": 0.0584,
      "step": 12020
    },
    {
      "epoch": 0.05654439918342004,
      "grad_norm": 0.6998117566108704,
      "learning_rate": 0.00018913594915462013,
      "loss": 0.0655,
      "step": 12021
    },
    {
      "epoch": 0.056549102985032504,
      "grad_norm": 2.561328649520874,
      "learning_rate": 0.00018913500617650665,
      "loss": 0.6727,
      "step": 12022
    },
    {
      "epoch": 0.05655380678664497,
      "grad_norm": 0.8145323991775513,
      "learning_rate": 0.00018913406319839317,
      "loss": 0.1057,
      "step": 12023
    },
    {
      "epoch": 0.05655851058825743,
      "grad_norm": 2.810324192047119,
      "learning_rate": 0.0001891331202202797,
      "loss": 0.43,
      "step": 12024
    },
    {
      "epoch": 0.056563214389869894,
      "grad_norm": 2.506840229034424,
      "learning_rate": 0.0001891321772421662,
      "loss": 0.4291,
      "step": 12025
    },
    {
      "epoch": 0.05656791819148236,
      "grad_norm": 5.013891220092773,
      "learning_rate": 0.00018913123426405275,
      "loss": 0.8641,
      "step": 12026
    },
    {
      "epoch": 0.05657262199309482,
      "grad_norm": 0.9062587022781372,
      "learning_rate": 0.00018913029128593927,
      "loss": 0.0698,
      "step": 12027
    },
    {
      "epoch": 0.056577325794707284,
      "grad_norm": 1.5870040655136108,
      "learning_rate": 0.0001891293483078258,
      "loss": 0.1189,
      "step": 12028
    },
    {
      "epoch": 0.05658202959631974,
      "grad_norm": 1.4906567335128784,
      "learning_rate": 0.0001891284053297123,
      "loss": 0.0965,
      "step": 12029
    },
    {
      "epoch": 0.05658673339793221,
      "grad_norm": 5.88347053527832,
      "learning_rate": 0.00018912746235159883,
      "loss": 0.3566,
      "step": 12030
    },
    {
      "epoch": 0.056591437199544674,
      "grad_norm": 1.8311123847961426,
      "learning_rate": 0.00018912651937348535,
      "loss": 0.3379,
      "step": 12031
    },
    {
      "epoch": 0.05659614100115713,
      "grad_norm": 0.6447319984436035,
      "learning_rate": 0.00018912557639537186,
      "loss": 0.0765,
      "step": 12032
    },
    {
      "epoch": 0.0566008448027696,
      "grad_norm": 2.2371509075164795,
      "learning_rate": 0.00018912463341725838,
      "loss": 0.3451,
      "step": 12033
    },
    {
      "epoch": 0.056605548604382064,
      "grad_norm": 3.8215298652648926,
      "learning_rate": 0.0001891236904391449,
      "loss": 0.3709,
      "step": 12034
    },
    {
      "epoch": 0.05661025240599452,
      "grad_norm": 2.324650526046753,
      "learning_rate": 0.00018912274746103145,
      "loss": 0.3723,
      "step": 12035
    },
    {
      "epoch": 0.05661495620760699,
      "grad_norm": 1.8681837320327759,
      "learning_rate": 0.00018912180448291797,
      "loss": 0.2263,
      "step": 12036
    },
    {
      "epoch": 0.056619660009219454,
      "grad_norm": 1.040029525756836,
      "learning_rate": 0.00018912086150480448,
      "loss": 0.1287,
      "step": 12037
    },
    {
      "epoch": 0.05662436381083191,
      "grad_norm": 0.5955957770347595,
      "learning_rate": 0.000189119918526691,
      "loss": 0.0679,
      "step": 12038
    },
    {
      "epoch": 0.05662906761244438,
      "grad_norm": 3.3724539279937744,
      "learning_rate": 0.00018911897554857755,
      "loss": 0.6355,
      "step": 12039
    },
    {
      "epoch": 0.056633771414056844,
      "grad_norm": 0.7112323045730591,
      "learning_rate": 0.00018911803257046407,
      "loss": 0.0507,
      "step": 12040
    },
    {
      "epoch": 0.0566384752156693,
      "grad_norm": 1.3260903358459473,
      "learning_rate": 0.00018911708959235056,
      "loss": 0.0869,
      "step": 12041
    },
    {
      "epoch": 0.05664317901728177,
      "grad_norm": 2.3145639896392822,
      "learning_rate": 0.00018911614661423708,
      "loss": 0.1639,
      "step": 12042
    },
    {
      "epoch": 0.056647882818894234,
      "grad_norm": 0.49381551146507263,
      "learning_rate": 0.0001891152036361236,
      "loss": 0.0507,
      "step": 12043
    },
    {
      "epoch": 0.05665258662050669,
      "grad_norm": 1.7993385791778564,
      "learning_rate": 0.00018911426065801014,
      "loss": 0.1912,
      "step": 12044
    },
    {
      "epoch": 0.05665729042211916,
      "grad_norm": 0.9650538563728333,
      "learning_rate": 0.00018911331767989666,
      "loss": 0.106,
      "step": 12045
    },
    {
      "epoch": 0.05666199422373162,
      "grad_norm": 3.0264742374420166,
      "learning_rate": 0.00018911237470178318,
      "loss": 0.2926,
      "step": 12046
    },
    {
      "epoch": 0.05666669802534408,
      "grad_norm": 0.5309396386146545,
      "learning_rate": 0.0001891114317236697,
      "loss": 0.0366,
      "step": 12047
    },
    {
      "epoch": 0.05667140182695655,
      "grad_norm": 2.9967501163482666,
      "learning_rate": 0.00018911048874555624,
      "loss": 0.4039,
      "step": 12048
    },
    {
      "epoch": 0.05667610562856901,
      "grad_norm": 2.508338212966919,
      "learning_rate": 0.00018910954576744276,
      "loss": 0.754,
      "step": 12049
    },
    {
      "epoch": 0.05668080943018147,
      "grad_norm": 0.44561687111854553,
      "learning_rate": 0.00018910860278932928,
      "loss": 0.0327,
      "step": 12050
    },
    {
      "epoch": 0.05668551323179394,
      "grad_norm": 1.162219762802124,
      "learning_rate": 0.0001891076598112158,
      "loss": 0.1156,
      "step": 12051
    },
    {
      "epoch": 0.0566902170334064,
      "grad_norm": 2.6822261810302734,
      "learning_rate": 0.0001891067168331023,
      "loss": 0.349,
      "step": 12052
    },
    {
      "epoch": 0.05669492083501886,
      "grad_norm": 2.2081120014190674,
      "learning_rate": 0.00018910577385498884,
      "loss": 0.0926,
      "step": 12053
    },
    {
      "epoch": 0.05669962463663133,
      "grad_norm": 2.751605987548828,
      "learning_rate": 0.00018910483087687536,
      "loss": 0.2562,
      "step": 12054
    },
    {
      "epoch": 0.05670432843824379,
      "grad_norm": 3.957200288772583,
      "learning_rate": 0.00018910388789876187,
      "loss": 0.3035,
      "step": 12055
    },
    {
      "epoch": 0.05670903223985625,
      "grad_norm": 3.0942981243133545,
      "learning_rate": 0.0001891029449206484,
      "loss": 0.3646,
      "step": 12056
    },
    {
      "epoch": 0.05671373604146872,
      "grad_norm": 3.621302366256714,
      "learning_rate": 0.0001891020019425349,
      "loss": 0.7207,
      "step": 12057
    },
    {
      "epoch": 0.05671843984308118,
      "grad_norm": 4.392838001251221,
      "learning_rate": 0.00018910105896442146,
      "loss": 0.5743,
      "step": 12058
    },
    {
      "epoch": 0.05672314364469364,
      "grad_norm": 2.063491106033325,
      "learning_rate": 0.00018910011598630798,
      "loss": 0.1814,
      "step": 12059
    },
    {
      "epoch": 0.05672784744630611,
      "grad_norm": 4.699255466461182,
      "learning_rate": 0.0001890991730081945,
      "loss": 1.0365,
      "step": 12060
    },
    {
      "epoch": 0.05673255124791857,
      "grad_norm": 4.595598220825195,
      "learning_rate": 0.000189098230030081,
      "loss": 0.4933,
      "step": 12061
    },
    {
      "epoch": 0.05673725504953103,
      "grad_norm": 0.1651470810174942,
      "learning_rate": 0.00018909728705196753,
      "loss": 0.0135,
      "step": 12062
    },
    {
      "epoch": 0.05674195885114349,
      "grad_norm": 3.3309996128082275,
      "learning_rate": 0.00018909634407385405,
      "loss": 0.6541,
      "step": 12063
    },
    {
      "epoch": 0.05674666265275596,
      "grad_norm": 5.007522106170654,
      "learning_rate": 0.00018909540109574057,
      "loss": 0.627,
      "step": 12064
    },
    {
      "epoch": 0.05675136645436842,
      "grad_norm": 0.6978126764297485,
      "learning_rate": 0.0001890944581176271,
      "loss": 0.0802,
      "step": 12065
    },
    {
      "epoch": 0.05675607025598088,
      "grad_norm": 1.402145504951477,
      "learning_rate": 0.0001890935151395136,
      "loss": 0.1617,
      "step": 12066
    },
    {
      "epoch": 0.05676077405759335,
      "grad_norm": 1.7167747020721436,
      "learning_rate": 0.00018909257216140015,
      "loss": 0.1083,
      "step": 12067
    },
    {
      "epoch": 0.05676547785920581,
      "grad_norm": 3.49747371673584,
      "learning_rate": 0.00018909162918328667,
      "loss": 0.274,
      "step": 12068
    },
    {
      "epoch": 0.05677018166081827,
      "grad_norm": 0.5254803895950317,
      "learning_rate": 0.0001890906862051732,
      "loss": 0.0465,
      "step": 12069
    },
    {
      "epoch": 0.05677488546243074,
      "grad_norm": 1.9901655912399292,
      "learning_rate": 0.0001890897432270597,
      "loss": 0.1355,
      "step": 12070
    },
    {
      "epoch": 0.0567795892640432,
      "grad_norm": 3.3084628582000732,
      "learning_rate": 0.00018908880024894625,
      "loss": 0.4777,
      "step": 12071
    },
    {
      "epoch": 0.05678429306565566,
      "grad_norm": 2.5538125038146973,
      "learning_rate": 0.00018908785727083275,
      "loss": 0.4967,
      "step": 12072
    },
    {
      "epoch": 0.05678899686726813,
      "grad_norm": 2.5703065395355225,
      "learning_rate": 0.00018908691429271926,
      "loss": 0.2381,
      "step": 12073
    },
    {
      "epoch": 0.05679370066888059,
      "grad_norm": 2.717658758163452,
      "learning_rate": 0.00018908597131460578,
      "loss": 0.4986,
      "step": 12074
    },
    {
      "epoch": 0.05679840447049305,
      "grad_norm": 0.8152623772621155,
      "learning_rate": 0.0001890850283364923,
      "loss": 0.07,
      "step": 12075
    },
    {
      "epoch": 0.05680310827210552,
      "grad_norm": 1.487663984298706,
      "learning_rate": 0.00018908408535837885,
      "loss": 0.1631,
      "step": 12076
    },
    {
      "epoch": 0.05680781207371798,
      "grad_norm": 1.326290488243103,
      "learning_rate": 0.00018908314238026537,
      "loss": 0.1309,
      "step": 12077
    },
    {
      "epoch": 0.05681251587533044,
      "grad_norm": 2.498150110244751,
      "learning_rate": 0.00018908219940215188,
      "loss": 0.304,
      "step": 12078
    },
    {
      "epoch": 0.05681721967694291,
      "grad_norm": 1.444624900817871,
      "learning_rate": 0.0001890812564240384,
      "loss": 0.1664,
      "step": 12079
    },
    {
      "epoch": 0.056821923478555365,
      "grad_norm": 2.1287364959716797,
      "learning_rate": 0.00018908031344592495,
      "loss": 0.2175,
      "step": 12080
    },
    {
      "epoch": 0.05682662728016783,
      "grad_norm": 4.241260528564453,
      "learning_rate": 0.00018907937046781147,
      "loss": 1.002,
      "step": 12081
    },
    {
      "epoch": 0.0568313310817803,
      "grad_norm": 2.7076847553253174,
      "learning_rate": 0.00018907842748969799,
      "loss": 0.3785,
      "step": 12082
    },
    {
      "epoch": 0.056836034883392755,
      "grad_norm": 0.39411935210227966,
      "learning_rate": 0.00018907748451158448,
      "loss": 0.0402,
      "step": 12083
    },
    {
      "epoch": 0.05684073868500522,
      "grad_norm": 3.984973907470703,
      "learning_rate": 0.000189076541533471,
      "loss": 0.7653,
      "step": 12084
    },
    {
      "epoch": 0.05684544248661769,
      "grad_norm": 2.7733795642852783,
      "learning_rate": 0.00018907559855535754,
      "loss": 0.6239,
      "step": 12085
    },
    {
      "epoch": 0.056850146288230145,
      "grad_norm": 1.1823855638504028,
      "learning_rate": 0.00018907465557724406,
      "loss": 0.1519,
      "step": 12086
    },
    {
      "epoch": 0.05685485008984261,
      "grad_norm": 3.1319069862365723,
      "learning_rate": 0.00018907371259913058,
      "loss": 0.4787,
      "step": 12087
    },
    {
      "epoch": 0.05685955389145508,
      "grad_norm": 1.4752117395401,
      "learning_rate": 0.0001890727696210171,
      "loss": 0.1638,
      "step": 12088
    },
    {
      "epoch": 0.056864257693067535,
      "grad_norm": 1.9709792137145996,
      "learning_rate": 0.00018907182664290364,
      "loss": 0.2733,
      "step": 12089
    },
    {
      "epoch": 0.05686896149468,
      "grad_norm": 4.837705135345459,
      "learning_rate": 0.00018907088366479016,
      "loss": 0.2104,
      "step": 12090
    },
    {
      "epoch": 0.05687366529629247,
      "grad_norm": 0.9242179989814758,
      "learning_rate": 0.00018906994068667668,
      "loss": 0.0445,
      "step": 12091
    },
    {
      "epoch": 0.056878369097904925,
      "grad_norm": 1.0654973983764648,
      "learning_rate": 0.0001890689977085632,
      "loss": 0.1243,
      "step": 12092
    },
    {
      "epoch": 0.05688307289951739,
      "grad_norm": 1.36408531665802,
      "learning_rate": 0.00018906805473044972,
      "loss": 0.1535,
      "step": 12093
    },
    {
      "epoch": 0.05688777670112986,
      "grad_norm": 1.6546986103057861,
      "learning_rate": 0.00018906711175233624,
      "loss": 0.2672,
      "step": 12094
    },
    {
      "epoch": 0.056892480502742315,
      "grad_norm": 2.020313024520874,
      "learning_rate": 0.00018906616877422276,
      "loss": 0.2442,
      "step": 12095
    },
    {
      "epoch": 0.05689718430435478,
      "grad_norm": 1.7355092763900757,
      "learning_rate": 0.00018906522579610927,
      "loss": 0.2775,
      "step": 12096
    },
    {
      "epoch": 0.05690188810596724,
      "grad_norm": 0.7065630555152893,
      "learning_rate": 0.0001890642828179958,
      "loss": 0.0562,
      "step": 12097
    },
    {
      "epoch": 0.056906591907579705,
      "grad_norm": 1.5428249835968018,
      "learning_rate": 0.00018906333983988234,
      "loss": 0.1983,
      "step": 12098
    },
    {
      "epoch": 0.05691129570919217,
      "grad_norm": 2.6261417865753174,
      "learning_rate": 0.00018906239686176886,
      "loss": 0.4073,
      "step": 12099
    },
    {
      "epoch": 0.05691599951080463,
      "grad_norm": 2.382506847381592,
      "learning_rate": 0.00018906145388365538,
      "loss": 0.3893,
      "step": 12100
    },
    {
      "epoch": 0.056920703312417095,
      "grad_norm": 2.8740503787994385,
      "learning_rate": 0.0001890605109055419,
      "loss": 0.5544,
      "step": 12101
    },
    {
      "epoch": 0.05692540711402956,
      "grad_norm": 2.270562171936035,
      "learning_rate": 0.0001890595679274284,
      "loss": 0.3394,
      "step": 12102
    },
    {
      "epoch": 0.05693011091564202,
      "grad_norm": 2.6226718425750732,
      "learning_rate": 0.00018905862494931493,
      "loss": 0.2703,
      "step": 12103
    },
    {
      "epoch": 0.056934814717254485,
      "grad_norm": 1.0883090496063232,
      "learning_rate": 0.00018905768197120145,
      "loss": 0.0361,
      "step": 12104
    },
    {
      "epoch": 0.05693951851886695,
      "grad_norm": 2.7956154346466064,
      "learning_rate": 0.00018905673899308797,
      "loss": 0.4874,
      "step": 12105
    },
    {
      "epoch": 0.05694422232047941,
      "grad_norm": 3.4255383014678955,
      "learning_rate": 0.0001890557960149745,
      "loss": 0.2237,
      "step": 12106
    },
    {
      "epoch": 0.056948926122091875,
      "grad_norm": 3.0493052005767822,
      "learning_rate": 0.000189054853036861,
      "loss": 0.4045,
      "step": 12107
    },
    {
      "epoch": 0.05695362992370434,
      "grad_norm": 1.9744764566421509,
      "learning_rate": 0.00018905391005874755,
      "loss": 0.2666,
      "step": 12108
    },
    {
      "epoch": 0.0569583337253168,
      "grad_norm": 2.3823819160461426,
      "learning_rate": 0.00018905296708063407,
      "loss": 0.2381,
      "step": 12109
    },
    {
      "epoch": 0.056963037526929265,
      "grad_norm": 1.084886074066162,
      "learning_rate": 0.0001890520241025206,
      "loss": 0.0881,
      "step": 12110
    },
    {
      "epoch": 0.05696774132854173,
      "grad_norm": 0.8629797697067261,
      "learning_rate": 0.0001890510811244071,
      "loss": 0.0775,
      "step": 12111
    },
    {
      "epoch": 0.05697244513015419,
      "grad_norm": 1.0813995599746704,
      "learning_rate": 0.00018905013814629365,
      "loss": 0.0861,
      "step": 12112
    },
    {
      "epoch": 0.056977148931766655,
      "grad_norm": 1.9670687913894653,
      "learning_rate": 0.00018904919516818017,
      "loss": 0.3648,
      "step": 12113
    },
    {
      "epoch": 0.056981852733379114,
      "grad_norm": 0.8795929551124573,
      "learning_rate": 0.00018904825219006666,
      "loss": 0.0992,
      "step": 12114
    },
    {
      "epoch": 0.05698655653499158,
      "grad_norm": 1.0950937271118164,
      "learning_rate": 0.00018904730921195318,
      "loss": 0.127,
      "step": 12115
    },
    {
      "epoch": 0.056991260336604045,
      "grad_norm": 0.03380267694592476,
      "learning_rate": 0.0001890463662338397,
      "loss": 0.0016,
      "step": 12116
    },
    {
      "epoch": 0.056995964138216504,
      "grad_norm": 1.7154275178909302,
      "learning_rate": 0.00018904542325572625,
      "loss": 0.2493,
      "step": 12117
    },
    {
      "epoch": 0.05700066793982897,
      "grad_norm": 2.002863883972168,
      "learning_rate": 0.00018904448027761277,
      "loss": 0.2559,
      "step": 12118
    },
    {
      "epoch": 0.057005371741441435,
      "grad_norm": 0.7349507212638855,
      "learning_rate": 0.00018904353729949928,
      "loss": 0.0714,
      "step": 12119
    },
    {
      "epoch": 0.057010075543053894,
      "grad_norm": 4.076197147369385,
      "learning_rate": 0.0001890425943213858,
      "loss": 0.7976,
      "step": 12120
    },
    {
      "epoch": 0.05701477934466636,
      "grad_norm": 2.711700201034546,
      "learning_rate": 0.00018904165134327235,
      "loss": 0.3426,
      "step": 12121
    },
    {
      "epoch": 0.057019483146278825,
      "grad_norm": 1.2873265743255615,
      "learning_rate": 0.00018904070836515887,
      "loss": 0.1646,
      "step": 12122
    },
    {
      "epoch": 0.057024186947891284,
      "grad_norm": 1.9210638999938965,
      "learning_rate": 0.00018903976538704539,
      "loss": 0.2439,
      "step": 12123
    },
    {
      "epoch": 0.05702889074950375,
      "grad_norm": 0.9485289454460144,
      "learning_rate": 0.0001890388224089319,
      "loss": 0.0802,
      "step": 12124
    },
    {
      "epoch": 0.057033594551116215,
      "grad_norm": 2.221832036972046,
      "learning_rate": 0.0001890378794308184,
      "loss": 0.2385,
      "step": 12125
    },
    {
      "epoch": 0.057038298352728674,
      "grad_norm": 2.863940954208374,
      "learning_rate": 0.00018903693645270494,
      "loss": 0.7287,
      "step": 12126
    },
    {
      "epoch": 0.05704300215434114,
      "grad_norm": 4.701204299926758,
      "learning_rate": 0.00018903599347459146,
      "loss": 0.5928,
      "step": 12127
    },
    {
      "epoch": 0.057047705955953605,
      "grad_norm": 3.645581007003784,
      "learning_rate": 0.00018903505049647798,
      "loss": 0.2672,
      "step": 12128
    },
    {
      "epoch": 0.057052409757566064,
      "grad_norm": 0.5442382097244263,
      "learning_rate": 0.0001890341075183645,
      "loss": 0.0707,
      "step": 12129
    },
    {
      "epoch": 0.05705711355917853,
      "grad_norm": 2.560584783554077,
      "learning_rate": 0.00018903316454025104,
      "loss": 0.5528,
      "step": 12130
    },
    {
      "epoch": 0.05706181736079099,
      "grad_norm": 0.24224510788917542,
      "learning_rate": 0.00018903222156213756,
      "loss": 0.0164,
      "step": 12131
    },
    {
      "epoch": 0.057066521162403454,
      "grad_norm": 5.796814918518066,
      "learning_rate": 0.00018903127858402408,
      "loss": 1.3631,
      "step": 12132
    },
    {
      "epoch": 0.05707122496401592,
      "grad_norm": 2.703890562057495,
      "learning_rate": 0.0001890303356059106,
      "loss": 0.2314,
      "step": 12133
    },
    {
      "epoch": 0.05707592876562838,
      "grad_norm": 1.164912223815918,
      "learning_rate": 0.00018902939262779712,
      "loss": 0.1363,
      "step": 12134
    },
    {
      "epoch": 0.057080632567240844,
      "grad_norm": 0.9101374745368958,
      "learning_rate": 0.00018902844964968364,
      "loss": 0.1045,
      "step": 12135
    },
    {
      "epoch": 0.05708533636885331,
      "grad_norm": 0.12887829542160034,
      "learning_rate": 0.00018902750667157016,
      "loss": 0.0086,
      "step": 12136
    },
    {
      "epoch": 0.05709004017046577,
      "grad_norm": 1.2529362440109253,
      "learning_rate": 0.00018902656369345667,
      "loss": 0.2108,
      "step": 12137
    },
    {
      "epoch": 0.057094743972078234,
      "grad_norm": 0.8053085803985596,
      "learning_rate": 0.0001890256207153432,
      "loss": 0.1227,
      "step": 12138
    },
    {
      "epoch": 0.0570994477736907,
      "grad_norm": 0.9924941658973694,
      "learning_rate": 0.00018902467773722974,
      "loss": 0.1309,
      "step": 12139
    },
    {
      "epoch": 0.05710415157530316,
      "grad_norm": 1.073637843132019,
      "learning_rate": 0.00018902373475911626,
      "loss": 0.1558,
      "step": 12140
    },
    {
      "epoch": 0.057108855376915624,
      "grad_norm": 1.8373323678970337,
      "learning_rate": 0.00018902279178100278,
      "loss": 0.2402,
      "step": 12141
    },
    {
      "epoch": 0.05711355917852809,
      "grad_norm": 3.491183042526245,
      "learning_rate": 0.0001890218488028893,
      "loss": 0.8442,
      "step": 12142
    },
    {
      "epoch": 0.05711826298014055,
      "grad_norm": 2.95100736618042,
      "learning_rate": 0.0001890209058247758,
      "loss": 0.7557,
      "step": 12143
    },
    {
      "epoch": 0.057122966781753014,
      "grad_norm": 2.530763626098633,
      "learning_rate": 0.00018901996284666236,
      "loss": 0.2662,
      "step": 12144
    },
    {
      "epoch": 0.05712767058336548,
      "grad_norm": 0.5712425112724304,
      "learning_rate": 0.00018901901986854885,
      "loss": 0.0409,
      "step": 12145
    },
    {
      "epoch": 0.05713237438497794,
      "grad_norm": 1.5064489841461182,
      "learning_rate": 0.00018901807689043537,
      "loss": 0.1629,
      "step": 12146
    },
    {
      "epoch": 0.057137078186590404,
      "grad_norm": 0.9801156520843506,
      "learning_rate": 0.0001890171339123219,
      "loss": 0.1148,
      "step": 12147
    },
    {
      "epoch": 0.05714178198820286,
      "grad_norm": 1.463845133781433,
      "learning_rate": 0.00018901619093420843,
      "loss": 0.4455,
      "step": 12148
    },
    {
      "epoch": 0.05714648578981533,
      "grad_norm": 0.4608185887336731,
      "learning_rate": 0.00018901524795609495,
      "loss": 0.0419,
      "step": 12149
    },
    {
      "epoch": 0.057151189591427794,
      "grad_norm": 0.5198948383331299,
      "learning_rate": 0.00018901430497798147,
      "loss": 0.0328,
      "step": 12150
    },
    {
      "epoch": 0.05715589339304025,
      "grad_norm": 2.2489802837371826,
      "learning_rate": 0.000189013361999868,
      "loss": 0.3579,
      "step": 12151
    },
    {
      "epoch": 0.05716059719465272,
      "grad_norm": 1.40504789352417,
      "learning_rate": 0.0001890124190217545,
      "loss": 0.0989,
      "step": 12152
    },
    {
      "epoch": 0.057165300996265184,
      "grad_norm": 2.3233258724212646,
      "learning_rate": 0.00018901147604364105,
      "loss": 0.2358,
      "step": 12153
    },
    {
      "epoch": 0.05717000479787764,
      "grad_norm": 0.36622753739356995,
      "learning_rate": 0.00018901053306552757,
      "loss": 0.048,
      "step": 12154
    },
    {
      "epoch": 0.05717470859949011,
      "grad_norm": 2.3795664310455322,
      "learning_rate": 0.0001890095900874141,
      "loss": 0.2586,
      "step": 12155
    },
    {
      "epoch": 0.057179412401102574,
      "grad_norm": 2.0902211666107178,
      "learning_rate": 0.00018900864710930058,
      "loss": 0.2183,
      "step": 12156
    },
    {
      "epoch": 0.05718411620271503,
      "grad_norm": 0.22805209457874298,
      "learning_rate": 0.0001890077041311871,
      "loss": 0.0143,
      "step": 12157
    },
    {
      "epoch": 0.0571888200043275,
      "grad_norm": 1.8908982276916504,
      "learning_rate": 0.00018900676115307365,
      "loss": 0.2102,
      "step": 12158
    },
    {
      "epoch": 0.057193523805939964,
      "grad_norm": 3.445986270904541,
      "learning_rate": 0.00018900581817496017,
      "loss": 0.221,
      "step": 12159
    },
    {
      "epoch": 0.05719822760755242,
      "grad_norm": 2.3357138633728027,
      "learning_rate": 0.00018900487519684668,
      "loss": 0.2859,
      "step": 12160
    },
    {
      "epoch": 0.05720293140916489,
      "grad_norm": 4.188645362854004,
      "learning_rate": 0.0001890039322187332,
      "loss": 0.181,
      "step": 12161
    },
    {
      "epoch": 0.057207635210777354,
      "grad_norm": 0.5113160014152527,
      "learning_rate": 0.00018900298924061975,
      "loss": 0.0821,
      "step": 12162
    },
    {
      "epoch": 0.05721233901238981,
      "grad_norm": 3.0531811714172363,
      "learning_rate": 0.00018900204626250627,
      "loss": 0.2582,
      "step": 12163
    },
    {
      "epoch": 0.05721704281400228,
      "grad_norm": 1.5766868591308594,
      "learning_rate": 0.00018900110328439279,
      "loss": 0.1378,
      "step": 12164
    },
    {
      "epoch": 0.05722174661561474,
      "grad_norm": 3.394317388534546,
      "learning_rate": 0.0001890001603062793,
      "loss": 0.7101,
      "step": 12165
    },
    {
      "epoch": 0.0572264504172272,
      "grad_norm": 2.2611327171325684,
      "learning_rate": 0.00018899921732816582,
      "loss": 0.3597,
      "step": 12166
    },
    {
      "epoch": 0.05723115421883967,
      "grad_norm": 2.529404878616333,
      "learning_rate": 0.00018899827435005234,
      "loss": 0.3265,
      "step": 12167
    },
    {
      "epoch": 0.05723585802045213,
      "grad_norm": 0.7675556540489197,
      "learning_rate": 0.00018899733137193886,
      "loss": 0.0866,
      "step": 12168
    },
    {
      "epoch": 0.05724056182206459,
      "grad_norm": 5.99297571182251,
      "learning_rate": 0.00018899638839382538,
      "loss": 0.2221,
      "step": 12169
    },
    {
      "epoch": 0.05724526562367706,
      "grad_norm": 3.0969390869140625,
      "learning_rate": 0.0001889954454157119,
      "loss": 0.5224,
      "step": 12170
    },
    {
      "epoch": 0.05724996942528952,
      "grad_norm": 0.5573925375938416,
      "learning_rate": 0.00018899450243759844,
      "loss": 0.0689,
      "step": 12171
    },
    {
      "epoch": 0.05725467322690198,
      "grad_norm": 2.1863505840301514,
      "learning_rate": 0.00018899355945948496,
      "loss": 0.3057,
      "step": 12172
    },
    {
      "epoch": 0.05725937702851445,
      "grad_norm": 1.590328335762024,
      "learning_rate": 0.00018899261648137148,
      "loss": 0.1962,
      "step": 12173
    },
    {
      "epoch": 0.05726408083012691,
      "grad_norm": 0.372578501701355,
      "learning_rate": 0.000188991673503258,
      "loss": 0.0173,
      "step": 12174
    },
    {
      "epoch": 0.05726878463173937,
      "grad_norm": 2.637223720550537,
      "learning_rate": 0.00018899073052514452,
      "loss": 0.3488,
      "step": 12175
    },
    {
      "epoch": 0.05727348843335184,
      "grad_norm": 1.0547516345977783,
      "learning_rate": 0.00018898978754703104,
      "loss": 0.0728,
      "step": 12176
    },
    {
      "epoch": 0.0572781922349643,
      "grad_norm": 2.006286859512329,
      "learning_rate": 0.00018898884456891756,
      "loss": 0.3629,
      "step": 12177
    },
    {
      "epoch": 0.05728289603657676,
      "grad_norm": 0.694240152835846,
      "learning_rate": 0.00018898790159080407,
      "loss": 0.0559,
      "step": 12178
    },
    {
      "epoch": 0.05728759983818923,
      "grad_norm": 0.6607364416122437,
      "learning_rate": 0.0001889869586126906,
      "loss": 0.0899,
      "step": 12179
    },
    {
      "epoch": 0.05729230363980169,
      "grad_norm": 3.403282880783081,
      "learning_rate": 0.00018898601563457714,
      "loss": 0.2319,
      "step": 12180
    },
    {
      "epoch": 0.05729700744141415,
      "grad_norm": 0.8178000450134277,
      "learning_rate": 0.00018898507265646366,
      "loss": 0.0767,
      "step": 12181
    },
    {
      "epoch": 0.05730171124302661,
      "grad_norm": 1.1083327531814575,
      "learning_rate": 0.00018898412967835018,
      "loss": 0.1099,
      "step": 12182
    },
    {
      "epoch": 0.05730641504463908,
      "grad_norm": 0.37804973125457764,
      "learning_rate": 0.0001889831867002367,
      "loss": 0.0312,
      "step": 12183
    },
    {
      "epoch": 0.05731111884625154,
      "grad_norm": 3.6058809757232666,
      "learning_rate": 0.0001889822437221232,
      "loss": 0.3951,
      "step": 12184
    },
    {
      "epoch": 0.057315822647864,
      "grad_norm": 0.9452706575393677,
      "learning_rate": 0.00018898130074400976,
      "loss": 0.0863,
      "step": 12185
    },
    {
      "epoch": 0.05732052644947647,
      "grad_norm": 4.116334915161133,
      "learning_rate": 0.00018898035776589628,
      "loss": 0.8609,
      "step": 12186
    },
    {
      "epoch": 0.05732523025108893,
      "grad_norm": 1.6806581020355225,
      "learning_rate": 0.00018897941478778277,
      "loss": 0.1363,
      "step": 12187
    },
    {
      "epoch": 0.05732993405270139,
      "grad_norm": 3.253227472305298,
      "learning_rate": 0.0001889784718096693,
      "loss": 0.4913,
      "step": 12188
    },
    {
      "epoch": 0.05733463785431386,
      "grad_norm": 0.41041100025177,
      "learning_rate": 0.00018897752883155583,
      "loss": 0.0173,
      "step": 12189
    },
    {
      "epoch": 0.05733934165592632,
      "grad_norm": 2.2479376792907715,
      "learning_rate": 0.00018897658585344235,
      "loss": 0.4023,
      "step": 12190
    },
    {
      "epoch": 0.05734404545753878,
      "grad_norm": 3.8341007232666016,
      "learning_rate": 0.00018897564287532887,
      "loss": 0.3512,
      "step": 12191
    },
    {
      "epoch": 0.05734874925915125,
      "grad_norm": 5.60495662689209,
      "learning_rate": 0.0001889746998972154,
      "loss": 0.7554,
      "step": 12192
    },
    {
      "epoch": 0.05735345306076371,
      "grad_norm": 2.892040967941284,
      "learning_rate": 0.0001889737569191019,
      "loss": 0.2703,
      "step": 12193
    },
    {
      "epoch": 0.05735815686237617,
      "grad_norm": 1.2281571626663208,
      "learning_rate": 0.00018897281394098845,
      "loss": 0.0641,
      "step": 12194
    },
    {
      "epoch": 0.05736286066398864,
      "grad_norm": 1.2098476886749268,
      "learning_rate": 0.00018897187096287497,
      "loss": 0.1294,
      "step": 12195
    },
    {
      "epoch": 0.0573675644656011,
      "grad_norm": 1.7814971208572388,
      "learning_rate": 0.0001889709279847615,
      "loss": 0.1991,
      "step": 12196
    },
    {
      "epoch": 0.05737226826721356,
      "grad_norm": 1.4418830871582031,
      "learning_rate": 0.000188969985006648,
      "loss": 0.1854,
      "step": 12197
    },
    {
      "epoch": 0.057376972068826027,
      "grad_norm": 1.2913100719451904,
      "learning_rate": 0.00018896904202853453,
      "loss": 0.1375,
      "step": 12198
    },
    {
      "epoch": 0.057381675870438485,
      "grad_norm": 4.500205993652344,
      "learning_rate": 0.00018896809905042105,
      "loss": 0.8783,
      "step": 12199
    },
    {
      "epoch": 0.05738637967205095,
      "grad_norm": 2.466590404510498,
      "learning_rate": 0.00018896715607230757,
      "loss": 0.2566,
      "step": 12200
    },
    {
      "epoch": 0.057391083473663417,
      "grad_norm": 4.496293544769287,
      "learning_rate": 0.00018896621309419408,
      "loss": 0.6026,
      "step": 12201
    },
    {
      "epoch": 0.057395787275275875,
      "grad_norm": 0.7367803454399109,
      "learning_rate": 0.0001889652701160806,
      "loss": 0.0349,
      "step": 12202
    },
    {
      "epoch": 0.05740049107688834,
      "grad_norm": 6.832284450531006,
      "learning_rate": 0.00018896432713796715,
      "loss": 1.1431,
      "step": 12203
    },
    {
      "epoch": 0.057405194878500806,
      "grad_norm": 5.082470417022705,
      "learning_rate": 0.00018896338415985367,
      "loss": 0.2267,
      "step": 12204
    },
    {
      "epoch": 0.057409898680113265,
      "grad_norm": 1.91672682762146,
      "learning_rate": 0.00018896244118174019,
      "loss": 0.1379,
      "step": 12205
    },
    {
      "epoch": 0.05741460248172573,
      "grad_norm": 1.7991195917129517,
      "learning_rate": 0.0001889614982036267,
      "loss": 0.2,
      "step": 12206
    },
    {
      "epoch": 0.057419306283338196,
      "grad_norm": 0.8860871195793152,
      "learning_rate": 0.00018896055522551322,
      "loss": 0.0538,
      "step": 12207
    },
    {
      "epoch": 0.057424010084950655,
      "grad_norm": 3.4134786128997803,
      "learning_rate": 0.00018895961224739974,
      "loss": 0.2325,
      "step": 12208
    },
    {
      "epoch": 0.05742871388656312,
      "grad_norm": 2.5179965496063232,
      "learning_rate": 0.00018895866926928626,
      "loss": 0.1563,
      "step": 12209
    },
    {
      "epoch": 0.057433417688175586,
      "grad_norm": 0.8266268372535706,
      "learning_rate": 0.00018895772629117278,
      "loss": 0.0681,
      "step": 12210
    },
    {
      "epoch": 0.057438121489788045,
      "grad_norm": 7.363483428955078,
      "learning_rate": 0.0001889567833130593,
      "loss": 1.1534,
      "step": 12211
    },
    {
      "epoch": 0.05744282529140051,
      "grad_norm": 6.086527347564697,
      "learning_rate": 0.00018895584033494584,
      "loss": 0.3277,
      "step": 12212
    },
    {
      "epoch": 0.057447529093012976,
      "grad_norm": 3.352879047393799,
      "learning_rate": 0.00018895489735683236,
      "loss": 0.7797,
      "step": 12213
    },
    {
      "epoch": 0.057452232894625435,
      "grad_norm": 2.125213623046875,
      "learning_rate": 0.00018895395437871888,
      "loss": 0.1769,
      "step": 12214
    },
    {
      "epoch": 0.0574569366962379,
      "grad_norm": 1.8154919147491455,
      "learning_rate": 0.0001889530114006054,
      "loss": 0.1649,
      "step": 12215
    },
    {
      "epoch": 0.05746164049785036,
      "grad_norm": 1.6508557796478271,
      "learning_rate": 0.00018895206842249192,
      "loss": 0.1264,
      "step": 12216
    },
    {
      "epoch": 0.057466344299462825,
      "grad_norm": 0.301710307598114,
      "learning_rate": 0.00018895112544437846,
      "loss": 0.0181,
      "step": 12217
    },
    {
      "epoch": 0.05747104810107529,
      "grad_norm": 0.4437215328216553,
      "learning_rate": 0.00018895018246626496,
      "loss": 0.0249,
      "step": 12218
    },
    {
      "epoch": 0.05747575190268775,
      "grad_norm": 6.241321086883545,
      "learning_rate": 0.00018894923948815147,
      "loss": 1.2319,
      "step": 12219
    },
    {
      "epoch": 0.057480455704300215,
      "grad_norm": 2.9555795192718506,
      "learning_rate": 0.000188948296510038,
      "loss": 0.2953,
      "step": 12220
    },
    {
      "epoch": 0.05748515950591268,
      "grad_norm": 0.31867390871047974,
      "learning_rate": 0.00018894735353192454,
      "loss": 0.0199,
      "step": 12221
    },
    {
      "epoch": 0.05748986330752514,
      "grad_norm": 0.1972627490758896,
      "learning_rate": 0.00018894641055381106,
      "loss": 0.0109,
      "step": 12222
    },
    {
      "epoch": 0.057494567109137605,
      "grad_norm": 5.174636363983154,
      "learning_rate": 0.00018894546757569758,
      "loss": 0.1449,
      "step": 12223
    },
    {
      "epoch": 0.05749927091075007,
      "grad_norm": 4.14119291305542,
      "learning_rate": 0.0001889445245975841,
      "loss": 0.2703,
      "step": 12224
    },
    {
      "epoch": 0.05750397471236253,
      "grad_norm": 2.72839093208313,
      "learning_rate": 0.0001889435816194706,
      "loss": 0.4176,
      "step": 12225
    },
    {
      "epoch": 0.057508678513974995,
      "grad_norm": 5.146823883056641,
      "learning_rate": 0.00018894263864135716,
      "loss": 0.7613,
      "step": 12226
    },
    {
      "epoch": 0.05751338231558746,
      "grad_norm": 1.212465763092041,
      "learning_rate": 0.00018894169566324368,
      "loss": 0.0828,
      "step": 12227
    },
    {
      "epoch": 0.05751808611719992,
      "grad_norm": 4.406861305236816,
      "learning_rate": 0.0001889407526851302,
      "loss": 0.9708,
      "step": 12228
    },
    {
      "epoch": 0.057522789918812385,
      "grad_norm": 1.2279359102249146,
      "learning_rate": 0.00018893980970701671,
      "loss": 0.0899,
      "step": 12229
    },
    {
      "epoch": 0.05752749372042485,
      "grad_norm": 3.9261505603790283,
      "learning_rate": 0.00018893886672890323,
      "loss": 0.6547,
      "step": 12230
    },
    {
      "epoch": 0.05753219752203731,
      "grad_norm": 3.654014825820923,
      "learning_rate": 0.00018893792375078975,
      "loss": 0.9747,
      "step": 12231
    },
    {
      "epoch": 0.057536901323649775,
      "grad_norm": 0.35706594586372375,
      "learning_rate": 0.00018893698077267627,
      "loss": 0.0266,
      "step": 12232
    },
    {
      "epoch": 0.057541605125262234,
      "grad_norm": 1.5953280925750732,
      "learning_rate": 0.0001889360377945628,
      "loss": 0.1785,
      "step": 12233
    },
    {
      "epoch": 0.0575463089268747,
      "grad_norm": 4.116952419281006,
      "learning_rate": 0.0001889350948164493,
      "loss": 0.7639,
      "step": 12234
    },
    {
      "epoch": 0.057551012728487165,
      "grad_norm": 1.8037539720535278,
      "learning_rate": 0.00018893415183833585,
      "loss": 0.3493,
      "step": 12235
    },
    {
      "epoch": 0.057555716530099624,
      "grad_norm": 4.577200889587402,
      "learning_rate": 0.00018893320886022237,
      "loss": 0.8166,
      "step": 12236
    },
    {
      "epoch": 0.05756042033171209,
      "grad_norm": 3.282308340072632,
      "learning_rate": 0.0001889322658821089,
      "loss": 0.383,
      "step": 12237
    },
    {
      "epoch": 0.057565124133324555,
      "grad_norm": 3.083921432495117,
      "learning_rate": 0.0001889313229039954,
      "loss": 0.5997,
      "step": 12238
    },
    {
      "epoch": 0.057569827934937014,
      "grad_norm": 0.4639907777309418,
      "learning_rate": 0.00018893037992588193,
      "loss": 0.0611,
      "step": 12239
    },
    {
      "epoch": 0.05757453173654948,
      "grad_norm": 3.243767738342285,
      "learning_rate": 0.00018892943694776845,
      "loss": 0.5481,
      "step": 12240
    },
    {
      "epoch": 0.057579235538161945,
      "grad_norm": 0.9361175894737244,
      "learning_rate": 0.00018892849396965497,
      "loss": 0.1213,
      "step": 12241
    },
    {
      "epoch": 0.057583939339774404,
      "grad_norm": 1.9123603105545044,
      "learning_rate": 0.00018892755099154148,
      "loss": 0.1635,
      "step": 12242
    },
    {
      "epoch": 0.05758864314138687,
      "grad_norm": 2.335278272628784,
      "learning_rate": 0.000188926608013428,
      "loss": 0.442,
      "step": 12243
    },
    {
      "epoch": 0.057593346942999335,
      "grad_norm": 2.9103786945343018,
      "learning_rate": 0.00018892566503531455,
      "loss": 0.4913,
      "step": 12244
    },
    {
      "epoch": 0.057598050744611794,
      "grad_norm": 1.675089716911316,
      "learning_rate": 0.00018892472205720107,
      "loss": 0.1838,
      "step": 12245
    },
    {
      "epoch": 0.05760275454622426,
      "grad_norm": 1.3073852062225342,
      "learning_rate": 0.00018892377907908759,
      "loss": 0.1449,
      "step": 12246
    },
    {
      "epoch": 0.057607458347836725,
      "grad_norm": 1.3795270919799805,
      "learning_rate": 0.0001889228361009741,
      "loss": 0.1259,
      "step": 12247
    },
    {
      "epoch": 0.057612162149449184,
      "grad_norm": 1.6758466958999634,
      "learning_rate": 0.00018892189312286065,
      "loss": 0.2117,
      "step": 12248
    },
    {
      "epoch": 0.05761686595106165,
      "grad_norm": 0.7190560102462769,
      "learning_rate": 0.00018892095014474714,
      "loss": 0.0719,
      "step": 12249
    },
    {
      "epoch": 0.05762156975267411,
      "grad_norm": 0.9405825138092041,
      "learning_rate": 0.00018892000716663366,
      "loss": 0.1484,
      "step": 12250
    },
    {
      "epoch": 0.057626273554286574,
      "grad_norm": 1.829602599143982,
      "learning_rate": 0.00018891906418852018,
      "loss": 0.2509,
      "step": 12251
    },
    {
      "epoch": 0.05763097735589904,
      "grad_norm": 1.5251654386520386,
      "learning_rate": 0.0001889181212104067,
      "loss": 0.168,
      "step": 12252
    },
    {
      "epoch": 0.0576356811575115,
      "grad_norm": 4.99120569229126,
      "learning_rate": 0.00018891717823229324,
      "loss": 0.6018,
      "step": 12253
    },
    {
      "epoch": 0.057640384959123964,
      "grad_norm": 0.36813098192214966,
      "learning_rate": 0.00018891623525417976,
      "loss": 0.0436,
      "step": 12254
    },
    {
      "epoch": 0.05764508876073643,
      "grad_norm": 2.349167823791504,
      "learning_rate": 0.00018891529227606628,
      "loss": 0.4183,
      "step": 12255
    },
    {
      "epoch": 0.05764979256234889,
      "grad_norm": 1.210328221321106,
      "learning_rate": 0.0001889143492979528,
      "loss": 0.1435,
      "step": 12256
    },
    {
      "epoch": 0.057654496363961354,
      "grad_norm": 1.8520703315734863,
      "learning_rate": 0.00018891340631983935,
      "loss": 0.3007,
      "step": 12257
    },
    {
      "epoch": 0.05765920016557382,
      "grad_norm": 2.500663995742798,
      "learning_rate": 0.00018891246334172586,
      "loss": 0.2896,
      "step": 12258
    },
    {
      "epoch": 0.05766390396718628,
      "grad_norm": 1.2861573696136475,
      "learning_rate": 0.00018891152036361238,
      "loss": 0.1182,
      "step": 12259
    },
    {
      "epoch": 0.057668607768798744,
      "grad_norm": 3.971820116043091,
      "learning_rate": 0.0001889105773854989,
      "loss": 0.7444,
      "step": 12260
    },
    {
      "epoch": 0.05767331157041121,
      "grad_norm": 3.754631280899048,
      "learning_rate": 0.0001889096344073854,
      "loss": 0.3461,
      "step": 12261
    },
    {
      "epoch": 0.05767801537202367,
      "grad_norm": 3.5631322860717773,
      "learning_rate": 0.00018890869142927194,
      "loss": 0.2423,
      "step": 12262
    },
    {
      "epoch": 0.057682719173636134,
      "grad_norm": 2.5021097660064697,
      "learning_rate": 0.00018890774845115846,
      "loss": 0.4202,
      "step": 12263
    },
    {
      "epoch": 0.0576874229752486,
      "grad_norm": 2.5685954093933105,
      "learning_rate": 0.00018890680547304498,
      "loss": 0.3358,
      "step": 12264
    },
    {
      "epoch": 0.05769212677686106,
      "grad_norm": 0.4316926598548889,
      "learning_rate": 0.0001889058624949315,
      "loss": 0.062,
      "step": 12265
    },
    {
      "epoch": 0.057696830578473524,
      "grad_norm": 2.6519253253936768,
      "learning_rate": 0.000188904919516818,
      "loss": 0.3415,
      "step": 12266
    },
    {
      "epoch": 0.05770153438008598,
      "grad_norm": 1.1488025188446045,
      "learning_rate": 0.00018890397653870456,
      "loss": 0.1237,
      "step": 12267
    },
    {
      "epoch": 0.05770623818169845,
      "grad_norm": 1.874145269393921,
      "learning_rate": 0.00018890303356059108,
      "loss": 0.1318,
      "step": 12268
    },
    {
      "epoch": 0.057710941983310914,
      "grad_norm": 1.1574854850769043,
      "learning_rate": 0.0001889020905824776,
      "loss": 0.1437,
      "step": 12269
    },
    {
      "epoch": 0.05771564578492337,
      "grad_norm": 2.2246716022491455,
      "learning_rate": 0.00018890114760436411,
      "loss": 0.1694,
      "step": 12270
    },
    {
      "epoch": 0.05772034958653584,
      "grad_norm": 8.807146072387695,
      "learning_rate": 0.00018890020462625063,
      "loss": 0.1909,
      "step": 12271
    },
    {
      "epoch": 0.057725053388148304,
      "grad_norm": 0.7091254591941833,
      "learning_rate": 0.00018889926164813715,
      "loss": 0.081,
      "step": 12272
    },
    {
      "epoch": 0.05772975718976076,
      "grad_norm": 0.1804647594690323,
      "learning_rate": 0.00018889831867002367,
      "loss": 0.0101,
      "step": 12273
    },
    {
      "epoch": 0.05773446099137323,
      "grad_norm": 1.3859399557113647,
      "learning_rate": 0.0001888973756919102,
      "loss": 0.1373,
      "step": 12274
    },
    {
      "epoch": 0.057739164792985694,
      "grad_norm": 1.8098310232162476,
      "learning_rate": 0.0001888964327137967,
      "loss": 0.1861,
      "step": 12275
    },
    {
      "epoch": 0.05774386859459815,
      "grad_norm": 2.294220447540283,
      "learning_rate": 0.00018889548973568325,
      "loss": 0.4145,
      "step": 12276
    },
    {
      "epoch": 0.05774857239621062,
      "grad_norm": 0.28172755241394043,
      "learning_rate": 0.00018889454675756977,
      "loss": 0.0184,
      "step": 12277
    },
    {
      "epoch": 0.057753276197823084,
      "grad_norm": 1.9803110361099243,
      "learning_rate": 0.0001888936037794563,
      "loss": 0.1516,
      "step": 12278
    },
    {
      "epoch": 0.05775797999943554,
      "grad_norm": 2.418194055557251,
      "learning_rate": 0.0001888926608013428,
      "loss": 0.5466,
      "step": 12279
    },
    {
      "epoch": 0.05776268380104801,
      "grad_norm": 2.5155210494995117,
      "learning_rate": 0.00018889171782322933,
      "loss": 0.3286,
      "step": 12280
    },
    {
      "epoch": 0.057767387602660474,
      "grad_norm": 3.821201801300049,
      "learning_rate": 0.00018889077484511585,
      "loss": 0.5754,
      "step": 12281
    },
    {
      "epoch": 0.05777209140427293,
      "grad_norm": 2.928828001022339,
      "learning_rate": 0.00018888983186700237,
      "loss": 0.7552,
      "step": 12282
    },
    {
      "epoch": 0.0577767952058854,
      "grad_norm": 0.6409475207328796,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.0463,
      "step": 12283
    },
    {
      "epoch": 0.05778149900749786,
      "grad_norm": 0.20911425352096558,
      "learning_rate": 0.0001888879459107754,
      "loss": 0.0157,
      "step": 12284
    },
    {
      "epoch": 0.05778620280911032,
      "grad_norm": 1.9685412645339966,
      "learning_rate": 0.00018888700293266195,
      "loss": 0.2669,
      "step": 12285
    },
    {
      "epoch": 0.05779090661072279,
      "grad_norm": 1.6718108654022217,
      "learning_rate": 0.00018888605995454847,
      "loss": 0.2424,
      "step": 12286
    },
    {
      "epoch": 0.05779561041233525,
      "grad_norm": 4.343360424041748,
      "learning_rate": 0.00018888511697643499,
      "loss": 0.6883,
      "step": 12287
    },
    {
      "epoch": 0.05780031421394771,
      "grad_norm": 0.8880170583724976,
      "learning_rate": 0.0001888841739983215,
      "loss": 0.0945,
      "step": 12288
    },
    {
      "epoch": 0.05780501801556018,
      "grad_norm": 3.117854595184326,
      "learning_rate": 0.00018888323102020805,
      "loss": 0.5378,
      "step": 12289
    },
    {
      "epoch": 0.057809721817172637,
      "grad_norm": 1.6436251401901245,
      "learning_rate": 0.00018888228804209457,
      "loss": 0.4289,
      "step": 12290
    },
    {
      "epoch": 0.0578144256187851,
      "grad_norm": 1.1371175050735474,
      "learning_rate": 0.0001888813450639811,
      "loss": 0.0888,
      "step": 12291
    },
    {
      "epoch": 0.05781912942039757,
      "grad_norm": 5.876226902008057,
      "learning_rate": 0.00018888040208586758,
      "loss": 0.9969,
      "step": 12292
    },
    {
      "epoch": 0.057823833222010027,
      "grad_norm": 1.6482536792755127,
      "learning_rate": 0.0001888794591077541,
      "loss": 0.1509,
      "step": 12293
    },
    {
      "epoch": 0.05782853702362249,
      "grad_norm": 1.5335568189620972,
      "learning_rate": 0.00018887851612964064,
      "loss": 0.2518,
      "step": 12294
    },
    {
      "epoch": 0.05783324082523496,
      "grad_norm": 2.582181215286255,
      "learning_rate": 0.00018887757315152716,
      "loss": 0.514,
      "step": 12295
    },
    {
      "epoch": 0.057837944626847416,
      "grad_norm": 2.4017584323883057,
      "learning_rate": 0.00018887663017341368,
      "loss": 0.6627,
      "step": 12296
    },
    {
      "epoch": 0.05784264842845988,
      "grad_norm": 3.171731472015381,
      "learning_rate": 0.0001888756871953002,
      "loss": 0.3883,
      "step": 12297
    },
    {
      "epoch": 0.05784735223007235,
      "grad_norm": 2.5960752964019775,
      "learning_rate": 0.00018887474421718675,
      "loss": 0.7364,
      "step": 12298
    },
    {
      "epoch": 0.057852056031684806,
      "grad_norm": 1.6646640300750732,
      "learning_rate": 0.00018887380123907326,
      "loss": 0.1193,
      "step": 12299
    },
    {
      "epoch": 0.05785675983329727,
      "grad_norm": 2.5239765644073486,
      "learning_rate": 0.00018887285826095978,
      "loss": 0.3188,
      "step": 12300
    },
    {
      "epoch": 0.05786146363490973,
      "grad_norm": 2.458371639251709,
      "learning_rate": 0.0001888719152828463,
      "loss": 0.281,
      "step": 12301
    },
    {
      "epoch": 0.057866167436522196,
      "grad_norm": 0.9665080904960632,
      "learning_rate": 0.00018887097230473282,
      "loss": 0.1443,
      "step": 12302
    },
    {
      "epoch": 0.05787087123813466,
      "grad_norm": 4.2963433265686035,
      "learning_rate": 0.00018887002932661934,
      "loss": 0.5148,
      "step": 12303
    },
    {
      "epoch": 0.05787557503974712,
      "grad_norm": 2.8777811527252197,
      "learning_rate": 0.00018886908634850586,
      "loss": 0.5292,
      "step": 12304
    },
    {
      "epoch": 0.057880278841359586,
      "grad_norm": 1.3486318588256836,
      "learning_rate": 0.00018886814337039238,
      "loss": 0.1787,
      "step": 12305
    },
    {
      "epoch": 0.05788498264297205,
      "grad_norm": 1.488542079925537,
      "learning_rate": 0.0001888672003922789,
      "loss": 0.2108,
      "step": 12306
    },
    {
      "epoch": 0.05788968644458451,
      "grad_norm": 1.2032644748687744,
      "learning_rate": 0.00018886625741416544,
      "loss": 0.1777,
      "step": 12307
    },
    {
      "epoch": 0.057894390246196976,
      "grad_norm": 7.382757186889648,
      "learning_rate": 0.00018886531443605196,
      "loss": 0.4265,
      "step": 12308
    },
    {
      "epoch": 0.05789909404780944,
      "grad_norm": 1.7320088148117065,
      "learning_rate": 0.00018886437145793848,
      "loss": 0.2085,
      "step": 12309
    },
    {
      "epoch": 0.0579037978494219,
      "grad_norm": 4.18495512008667,
      "learning_rate": 0.000188863428479825,
      "loss": 0.8393,
      "step": 12310
    },
    {
      "epoch": 0.057908501651034366,
      "grad_norm": 1.631996989250183,
      "learning_rate": 0.00018886248550171151,
      "loss": 0.1431,
      "step": 12311
    },
    {
      "epoch": 0.05791320545264683,
      "grad_norm": 2.8643572330474854,
      "learning_rate": 0.00018886154252359803,
      "loss": 0.316,
      "step": 12312
    },
    {
      "epoch": 0.05791790925425929,
      "grad_norm": 1.534167766571045,
      "learning_rate": 0.00018886059954548455,
      "loss": 0.111,
      "step": 12313
    },
    {
      "epoch": 0.057922613055871756,
      "grad_norm": 3.1297433376312256,
      "learning_rate": 0.00018885965656737107,
      "loss": 0.5665,
      "step": 12314
    },
    {
      "epoch": 0.05792731685748422,
      "grad_norm": 3.109661102294922,
      "learning_rate": 0.0001888587135892576,
      "loss": 0.5271,
      "step": 12315
    },
    {
      "epoch": 0.05793202065909668,
      "grad_norm": 3.167935371398926,
      "learning_rate": 0.0001888577706111441,
      "loss": 0.2843,
      "step": 12316
    },
    {
      "epoch": 0.057936724460709146,
      "grad_norm": 2.874526262283325,
      "learning_rate": 0.00018885682763303065,
      "loss": 0.5445,
      "step": 12317
    },
    {
      "epoch": 0.057941428262321605,
      "grad_norm": 2.8376705646514893,
      "learning_rate": 0.00018885588465491717,
      "loss": 0.2744,
      "step": 12318
    },
    {
      "epoch": 0.05794613206393407,
      "grad_norm": 2.708061933517456,
      "learning_rate": 0.0001888549416768037,
      "loss": 0.4044,
      "step": 12319
    },
    {
      "epoch": 0.057950835865546536,
      "grad_norm": 2.617288827896118,
      "learning_rate": 0.0001888539986986902,
      "loss": 0.5674,
      "step": 12320
    },
    {
      "epoch": 0.057955539667158995,
      "grad_norm": 1.561157464981079,
      "learning_rate": 0.00018885305572057676,
      "loss": 0.232,
      "step": 12321
    },
    {
      "epoch": 0.05796024346877146,
      "grad_norm": 1.1933430433273315,
      "learning_rate": 0.00018885211274246327,
      "loss": 0.1663,
      "step": 12322
    },
    {
      "epoch": 0.057964947270383926,
      "grad_norm": 1.7210196256637573,
      "learning_rate": 0.00018885116976434977,
      "loss": 0.2214,
      "step": 12323
    },
    {
      "epoch": 0.057969651071996385,
      "grad_norm": 0.6520243287086487,
      "learning_rate": 0.00018885022678623628,
      "loss": 0.0754,
      "step": 12324
    },
    {
      "epoch": 0.05797435487360885,
      "grad_norm": 1.876582145690918,
      "learning_rate": 0.0001888492838081228,
      "loss": 0.1976,
      "step": 12325
    },
    {
      "epoch": 0.057979058675221316,
      "grad_norm": 1.3799936771392822,
      "learning_rate": 0.00018884834083000935,
      "loss": 0.1558,
      "step": 12326
    },
    {
      "epoch": 0.057983762476833775,
      "grad_norm": 0.4707706868648529,
      "learning_rate": 0.00018884739785189587,
      "loss": 0.0366,
      "step": 12327
    },
    {
      "epoch": 0.05798846627844624,
      "grad_norm": 2.5283820629119873,
      "learning_rate": 0.00018884645487378239,
      "loss": 0.3958,
      "step": 12328
    },
    {
      "epoch": 0.057993170080058706,
      "grad_norm": 3.9364750385284424,
      "learning_rate": 0.0001888455118956689,
      "loss": 0.5774,
      "step": 12329
    },
    {
      "epoch": 0.057997873881671165,
      "grad_norm": 1.8420900106430054,
      "learning_rate": 0.00018884456891755545,
      "loss": 0.1874,
      "step": 12330
    },
    {
      "epoch": 0.05800257768328363,
      "grad_norm": 1.5900344848632812,
      "learning_rate": 0.00018884362593944197,
      "loss": 0.2949,
      "step": 12331
    },
    {
      "epoch": 0.058007281484896096,
      "grad_norm": 2.1099579334259033,
      "learning_rate": 0.0001888426829613285,
      "loss": 0.3421,
      "step": 12332
    },
    {
      "epoch": 0.058011985286508555,
      "grad_norm": 4.290627479553223,
      "learning_rate": 0.000188841739983215,
      "loss": 0.6881,
      "step": 12333
    },
    {
      "epoch": 0.05801668908812102,
      "grad_norm": 0.9639856815338135,
      "learning_rate": 0.0001888407970051015,
      "loss": 0.1214,
      "step": 12334
    },
    {
      "epoch": 0.05802139288973348,
      "grad_norm": 0.8734665513038635,
      "learning_rate": 0.00018883985402698804,
      "loss": 0.139,
      "step": 12335
    },
    {
      "epoch": 0.058026096691345945,
      "grad_norm": 3.053687572479248,
      "learning_rate": 0.00018883891104887456,
      "loss": 0.1103,
      "step": 12336
    },
    {
      "epoch": 0.05803080049295841,
      "grad_norm": 0.4609529972076416,
      "learning_rate": 0.00018883796807076108,
      "loss": 0.0631,
      "step": 12337
    },
    {
      "epoch": 0.05803550429457087,
      "grad_norm": 2.1929380893707275,
      "learning_rate": 0.0001888370250926476,
      "loss": 0.2656,
      "step": 12338
    },
    {
      "epoch": 0.058040208096183335,
      "grad_norm": 2.5654006004333496,
      "learning_rate": 0.00018883608211453415,
      "loss": 0.3768,
      "step": 12339
    },
    {
      "epoch": 0.0580449118977958,
      "grad_norm": 1.9077346324920654,
      "learning_rate": 0.00018883513913642066,
      "loss": 0.4061,
      "step": 12340
    },
    {
      "epoch": 0.05804961569940826,
      "grad_norm": 1.8087655305862427,
      "learning_rate": 0.00018883419615830718,
      "loss": 0.2622,
      "step": 12341
    },
    {
      "epoch": 0.058054319501020725,
      "grad_norm": 1.8338104486465454,
      "learning_rate": 0.0001888332531801937,
      "loss": 0.3548,
      "step": 12342
    },
    {
      "epoch": 0.05805902330263319,
      "grad_norm": 0.9544261693954468,
      "learning_rate": 0.00018883231020208022,
      "loss": 0.0496,
      "step": 12343
    },
    {
      "epoch": 0.05806372710424565,
      "grad_norm": 3.6367690563201904,
      "learning_rate": 0.00018883136722396674,
      "loss": 0.5481,
      "step": 12344
    },
    {
      "epoch": 0.058068430905858115,
      "grad_norm": 0.6376535892486572,
      "learning_rate": 0.00018883042424585326,
      "loss": 0.0504,
      "step": 12345
    },
    {
      "epoch": 0.05807313470747058,
      "grad_norm": 0.49197494983673096,
      "learning_rate": 0.00018882948126773978,
      "loss": 0.065,
      "step": 12346
    },
    {
      "epoch": 0.05807783850908304,
      "grad_norm": 1.1138447523117065,
      "learning_rate": 0.0001888285382896263,
      "loss": 0.0788,
      "step": 12347
    },
    {
      "epoch": 0.058082542310695505,
      "grad_norm": 2.2432477474212646,
      "learning_rate": 0.00018882759531151284,
      "loss": 0.3598,
      "step": 12348
    },
    {
      "epoch": 0.05808724611230797,
      "grad_norm": 0.851193904876709,
      "learning_rate": 0.00018882665233339936,
      "loss": 0.0755,
      "step": 12349
    },
    {
      "epoch": 0.05809194991392043,
      "grad_norm": 1.6657124757766724,
      "learning_rate": 0.00018882570935528588,
      "loss": 0.1579,
      "step": 12350
    },
    {
      "epoch": 0.058096653715532895,
      "grad_norm": 3.7223117351531982,
      "learning_rate": 0.0001888247663771724,
      "loss": 0.3557,
      "step": 12351
    },
    {
      "epoch": 0.058101357517145354,
      "grad_norm": 3.0348737239837646,
      "learning_rate": 0.00018882382339905891,
      "loss": 0.4316,
      "step": 12352
    },
    {
      "epoch": 0.05810606131875782,
      "grad_norm": 1.946732997894287,
      "learning_rate": 0.00018882288042094546,
      "loss": 0.0931,
      "step": 12353
    },
    {
      "epoch": 0.058110765120370285,
      "grad_norm": 3.4047513008117676,
      "learning_rate": 0.00018882193744283195,
      "loss": 0.5639,
      "step": 12354
    },
    {
      "epoch": 0.058115468921982744,
      "grad_norm": 2.9371097087860107,
      "learning_rate": 0.00018882099446471847,
      "loss": 0.4047,
      "step": 12355
    },
    {
      "epoch": 0.05812017272359521,
      "grad_norm": 4.789393424987793,
      "learning_rate": 0.000188820051486605,
      "loss": 0.872,
      "step": 12356
    },
    {
      "epoch": 0.058124876525207675,
      "grad_norm": 2.249117851257324,
      "learning_rate": 0.00018881910850849154,
      "loss": 0.1591,
      "step": 12357
    },
    {
      "epoch": 0.058129580326820134,
      "grad_norm": 1.3745158910751343,
      "learning_rate": 0.00018881816553037805,
      "loss": 0.1098,
      "step": 12358
    },
    {
      "epoch": 0.0581342841284326,
      "grad_norm": 2.4566915035247803,
      "learning_rate": 0.00018881722255226457,
      "loss": 0.2326,
      "step": 12359
    },
    {
      "epoch": 0.058138987930045065,
      "grad_norm": 0.6096935868263245,
      "learning_rate": 0.0001888162795741511,
      "loss": 0.0502,
      "step": 12360
    },
    {
      "epoch": 0.058143691731657524,
      "grad_norm": 2.051206111907959,
      "learning_rate": 0.0001888153365960376,
      "loss": 0.202,
      "step": 12361
    },
    {
      "epoch": 0.05814839553326999,
      "grad_norm": 3.7206947803497314,
      "learning_rate": 0.00018881439361792416,
      "loss": 0.3473,
      "step": 12362
    },
    {
      "epoch": 0.058153099334882455,
      "grad_norm": 5.433930397033691,
      "learning_rate": 0.00018881345063981067,
      "loss": 0.1713,
      "step": 12363
    },
    {
      "epoch": 0.058157803136494914,
      "grad_norm": 2.9780468940734863,
      "learning_rate": 0.0001888125076616972,
      "loss": 0.4881,
      "step": 12364
    },
    {
      "epoch": 0.05816250693810738,
      "grad_norm": 1.8734456300735474,
      "learning_rate": 0.00018881156468358368,
      "loss": 0.1515,
      "step": 12365
    },
    {
      "epoch": 0.058167210739719845,
      "grad_norm": 2.974874258041382,
      "learning_rate": 0.0001888106217054702,
      "loss": 0.4553,
      "step": 12366
    },
    {
      "epoch": 0.058171914541332304,
      "grad_norm": 3.4331510066986084,
      "learning_rate": 0.00018880967872735675,
      "loss": 0.554,
      "step": 12367
    },
    {
      "epoch": 0.05817661834294477,
      "grad_norm": 2.4283151626586914,
      "learning_rate": 0.00018880873574924327,
      "loss": 0.3839,
      "step": 12368
    },
    {
      "epoch": 0.05818132214455723,
      "grad_norm": 1.5652137994766235,
      "learning_rate": 0.00018880779277112979,
      "loss": 0.1711,
      "step": 12369
    },
    {
      "epoch": 0.058186025946169694,
      "grad_norm": 2.1299996376037598,
      "learning_rate": 0.0001888068497930163,
      "loss": 0.1563,
      "step": 12370
    },
    {
      "epoch": 0.05819072974778216,
      "grad_norm": 4.461215972900391,
      "learning_rate": 0.00018880590681490285,
      "loss": 0.5061,
      "step": 12371
    },
    {
      "epoch": 0.05819543354939462,
      "grad_norm": 0.5384449362754822,
      "learning_rate": 0.00018880496383678937,
      "loss": 0.0672,
      "step": 12372
    },
    {
      "epoch": 0.058200137351007084,
      "grad_norm": 1.9282087087631226,
      "learning_rate": 0.0001888040208586759,
      "loss": 0.218,
      "step": 12373
    },
    {
      "epoch": 0.05820484115261955,
      "grad_norm": 1.2606505155563354,
      "learning_rate": 0.0001888030778805624,
      "loss": 0.1211,
      "step": 12374
    },
    {
      "epoch": 0.05820954495423201,
      "grad_norm": 1.7877767086029053,
      "learning_rate": 0.00018880213490244892,
      "loss": 0.1245,
      "step": 12375
    },
    {
      "epoch": 0.058214248755844474,
      "grad_norm": 1.8300769329071045,
      "learning_rate": 0.00018880119192433544,
      "loss": 0.307,
      "step": 12376
    },
    {
      "epoch": 0.05821895255745694,
      "grad_norm": 1.222193956375122,
      "learning_rate": 0.00018880024894622196,
      "loss": 0.1269,
      "step": 12377
    },
    {
      "epoch": 0.0582236563590694,
      "grad_norm": 3.582249879837036,
      "learning_rate": 0.00018879930596810848,
      "loss": 0.3968,
      "step": 12378
    },
    {
      "epoch": 0.058228360160681863,
      "grad_norm": 0.963850736618042,
      "learning_rate": 0.000188798362989995,
      "loss": 0.1034,
      "step": 12379
    },
    {
      "epoch": 0.05823306396229433,
      "grad_norm": 3.2185909748077393,
      "learning_rate": 0.00018879742001188155,
      "loss": 0.6613,
      "step": 12380
    },
    {
      "epoch": 0.05823776776390679,
      "grad_norm": 1.4254436492919922,
      "learning_rate": 0.00018879647703376806,
      "loss": 0.1284,
      "step": 12381
    },
    {
      "epoch": 0.058242471565519253,
      "grad_norm": 7.966659069061279,
      "learning_rate": 0.00018879553405565458,
      "loss": 0.6316,
      "step": 12382
    },
    {
      "epoch": 0.05824717536713172,
      "grad_norm": 1.649262547492981,
      "learning_rate": 0.0001887945910775411,
      "loss": 0.0794,
      "step": 12383
    },
    {
      "epoch": 0.05825187916874418,
      "grad_norm": 3.0018959045410156,
      "learning_rate": 0.00018879364809942762,
      "loss": 0.4141,
      "step": 12384
    },
    {
      "epoch": 0.05825658297035664,
      "grad_norm": 2.886824131011963,
      "learning_rate": 0.00018879270512131414,
      "loss": 0.3457,
      "step": 12385
    },
    {
      "epoch": 0.0582612867719691,
      "grad_norm": 1.21197509765625,
      "learning_rate": 0.00018879176214320066,
      "loss": 0.158,
      "step": 12386
    },
    {
      "epoch": 0.05826599057358157,
      "grad_norm": 3.9448423385620117,
      "learning_rate": 0.00018879081916508718,
      "loss": 0.4385,
      "step": 12387
    },
    {
      "epoch": 0.05827069437519403,
      "grad_norm": 1.803242802619934,
      "learning_rate": 0.0001887898761869737,
      "loss": 0.2343,
      "step": 12388
    },
    {
      "epoch": 0.05827539817680649,
      "grad_norm": 1.2243653535842896,
      "learning_rate": 0.00018878893320886024,
      "loss": 0.1185,
      "step": 12389
    },
    {
      "epoch": 0.05828010197841896,
      "grad_norm": 3.7775373458862305,
      "learning_rate": 0.00018878799023074676,
      "loss": 0.5803,
      "step": 12390
    },
    {
      "epoch": 0.05828480578003142,
      "grad_norm": 3.36704421043396,
      "learning_rate": 0.00018878704725263328,
      "loss": 0.2124,
      "step": 12391
    },
    {
      "epoch": 0.05828950958164388,
      "grad_norm": 1.2987914085388184,
      "learning_rate": 0.0001887861042745198,
      "loss": 0.1041,
      "step": 12392
    },
    {
      "epoch": 0.05829421338325635,
      "grad_norm": 4.909643650054932,
      "learning_rate": 0.00018878516129640631,
      "loss": 0.9152,
      "step": 12393
    },
    {
      "epoch": 0.05829891718486881,
      "grad_norm": 1.1035202741622925,
      "learning_rate": 0.00018878421831829286,
      "loss": 0.069,
      "step": 12394
    },
    {
      "epoch": 0.05830362098648127,
      "grad_norm": 0.33386245369911194,
      "learning_rate": 0.00018878327534017938,
      "loss": 0.0243,
      "step": 12395
    },
    {
      "epoch": 0.05830832478809374,
      "grad_norm": 1.9766249656677246,
      "learning_rate": 0.00018878233236206587,
      "loss": 0.2145,
      "step": 12396
    },
    {
      "epoch": 0.0583130285897062,
      "grad_norm": 7.476847171783447,
      "learning_rate": 0.0001887813893839524,
      "loss": 0.7788,
      "step": 12397
    },
    {
      "epoch": 0.05831773239131866,
      "grad_norm": 4.210933208465576,
      "learning_rate": 0.00018878044640583894,
      "loss": 0.345,
      "step": 12398
    },
    {
      "epoch": 0.05832243619293113,
      "grad_norm": 1.7160059213638306,
      "learning_rate": 0.00018877950342772545,
      "loss": 0.2466,
      "step": 12399
    },
    {
      "epoch": 0.05832713999454359,
      "grad_norm": 2.0155529975891113,
      "learning_rate": 0.00018877856044961197,
      "loss": 0.1948,
      "step": 12400
    },
    {
      "epoch": 0.05833184379615605,
      "grad_norm": 8.793659210205078,
      "learning_rate": 0.0001887776174714985,
      "loss": 0.4705,
      "step": 12401
    },
    {
      "epoch": 0.05833654759776852,
      "grad_norm": 2.0721435546875,
      "learning_rate": 0.000188776674493385,
      "loss": 0.179,
      "step": 12402
    },
    {
      "epoch": 0.058341251399380976,
      "grad_norm": 2.734407663345337,
      "learning_rate": 0.00018877573151527156,
      "loss": 0.3081,
      "step": 12403
    },
    {
      "epoch": 0.05834595520099344,
      "grad_norm": 4.537439823150635,
      "learning_rate": 0.00018877478853715807,
      "loss": 0.2963,
      "step": 12404
    },
    {
      "epoch": 0.05835065900260591,
      "grad_norm": 0.44437873363494873,
      "learning_rate": 0.0001887738455590446,
      "loss": 0.0242,
      "step": 12405
    },
    {
      "epoch": 0.058355362804218366,
      "grad_norm": 0.7508643269538879,
      "learning_rate": 0.0001887729025809311,
      "loss": 0.05,
      "step": 12406
    },
    {
      "epoch": 0.05836006660583083,
      "grad_norm": 4.222899436950684,
      "learning_rate": 0.00018877195960281763,
      "loss": 0.5431,
      "step": 12407
    },
    {
      "epoch": 0.0583647704074433,
      "grad_norm": 2.0439183712005615,
      "learning_rate": 0.00018877101662470415,
      "loss": 0.1608,
      "step": 12408
    },
    {
      "epoch": 0.058369474209055756,
      "grad_norm": 7.19006872177124,
      "learning_rate": 0.00018877007364659067,
      "loss": 0.834,
      "step": 12409
    },
    {
      "epoch": 0.05837417801066822,
      "grad_norm": 3.345991849899292,
      "learning_rate": 0.00018876913066847719,
      "loss": 0.5292,
      "step": 12410
    },
    {
      "epoch": 0.05837888181228069,
      "grad_norm": 1.6289643049240112,
      "learning_rate": 0.0001887681876903637,
      "loss": 0.1827,
      "step": 12411
    },
    {
      "epoch": 0.058383585613893146,
      "grad_norm": 2.428678274154663,
      "learning_rate": 0.00018876724471225025,
      "loss": 0.4078,
      "step": 12412
    },
    {
      "epoch": 0.05838828941550561,
      "grad_norm": 2.8962724208831787,
      "learning_rate": 0.00018876630173413677,
      "loss": 0.3499,
      "step": 12413
    },
    {
      "epoch": 0.05839299321711808,
      "grad_norm": 1.3961703777313232,
      "learning_rate": 0.0001887653587560233,
      "loss": 0.1336,
      "step": 12414
    },
    {
      "epoch": 0.058397697018730536,
      "grad_norm": 1.1058990955352783,
      "learning_rate": 0.0001887644157779098,
      "loss": 0.1124,
      "step": 12415
    },
    {
      "epoch": 0.058402400820343,
      "grad_norm": 1.8711916208267212,
      "learning_rate": 0.00018876347279979632,
      "loss": 0.1916,
      "step": 12416
    },
    {
      "epoch": 0.05840710462195547,
      "grad_norm": 0.512458324432373,
      "learning_rate": 0.00018876252982168284,
      "loss": 0.0477,
      "step": 12417
    },
    {
      "epoch": 0.058411808423567926,
      "grad_norm": 1.0348693132400513,
      "learning_rate": 0.00018876158684356936,
      "loss": 0.08,
      "step": 12418
    },
    {
      "epoch": 0.05841651222518039,
      "grad_norm": 2.3901748657226562,
      "learning_rate": 0.00018876064386545588,
      "loss": 0.3539,
      "step": 12419
    },
    {
      "epoch": 0.05842121602679285,
      "grad_norm": 1.9785337448120117,
      "learning_rate": 0.0001887597008873424,
      "loss": 0.1362,
      "step": 12420
    },
    {
      "epoch": 0.058425919828405316,
      "grad_norm": 3.5426695346832275,
      "learning_rate": 0.00018875875790922895,
      "loss": 0.5396,
      "step": 12421
    },
    {
      "epoch": 0.05843062363001778,
      "grad_norm": 3.725585460662842,
      "learning_rate": 0.00018875781493111546,
      "loss": 0.4827,
      "step": 12422
    },
    {
      "epoch": 0.05843532743163024,
      "grad_norm": 2.814354419708252,
      "learning_rate": 0.00018875687195300198,
      "loss": 0.4156,
      "step": 12423
    },
    {
      "epoch": 0.058440031233242706,
      "grad_norm": 3.7749485969543457,
      "learning_rate": 0.0001887559289748885,
      "loss": 0.6631,
      "step": 12424
    },
    {
      "epoch": 0.05844473503485517,
      "grad_norm": 2.668980121612549,
      "learning_rate": 0.00018875498599677502,
      "loss": 0.2747,
      "step": 12425
    },
    {
      "epoch": 0.05844943883646763,
      "grad_norm": 1.32599675655365,
      "learning_rate": 0.00018875404301866157,
      "loss": 0.1379,
      "step": 12426
    },
    {
      "epoch": 0.058454142638080096,
      "grad_norm": 2.0534703731536865,
      "learning_rate": 0.00018875310004054806,
      "loss": 0.337,
      "step": 12427
    },
    {
      "epoch": 0.05845884643969256,
      "grad_norm": 3.6841797828674316,
      "learning_rate": 0.00018875215706243458,
      "loss": 0.6022,
      "step": 12428
    },
    {
      "epoch": 0.05846355024130502,
      "grad_norm": 3.1994049549102783,
      "learning_rate": 0.0001887512140843211,
      "loss": 0.7108,
      "step": 12429
    },
    {
      "epoch": 0.058468254042917486,
      "grad_norm": 0.4113824963569641,
      "learning_rate": 0.00018875027110620764,
      "loss": 0.0503,
      "step": 12430
    },
    {
      "epoch": 0.05847295784452995,
      "grad_norm": 0.9763739109039307,
      "learning_rate": 0.00018874932812809416,
      "loss": 0.0939,
      "step": 12431
    },
    {
      "epoch": 0.05847766164614241,
      "grad_norm": 3.5051515102386475,
      "learning_rate": 0.00018874838514998068,
      "loss": 0.7925,
      "step": 12432
    },
    {
      "epoch": 0.058482365447754876,
      "grad_norm": 2.694174289703369,
      "learning_rate": 0.0001887474421718672,
      "loss": 0.4531,
      "step": 12433
    },
    {
      "epoch": 0.05848706924936734,
      "grad_norm": 1.770894169807434,
      "learning_rate": 0.00018874649919375371,
      "loss": 0.2903,
      "step": 12434
    },
    {
      "epoch": 0.0584917730509798,
      "grad_norm": 3.0012707710266113,
      "learning_rate": 0.00018874555621564026,
      "loss": 0.5061,
      "step": 12435
    },
    {
      "epoch": 0.058496476852592266,
      "grad_norm": 2.1549124717712402,
      "learning_rate": 0.00018874461323752678,
      "loss": 0.2976,
      "step": 12436
    },
    {
      "epoch": 0.058501180654204725,
      "grad_norm": 1.2205382585525513,
      "learning_rate": 0.0001887436702594133,
      "loss": 0.1629,
      "step": 12437
    },
    {
      "epoch": 0.05850588445581719,
      "grad_norm": 4.175025463104248,
      "learning_rate": 0.0001887427272812998,
      "loss": 0.4221,
      "step": 12438
    },
    {
      "epoch": 0.058510588257429656,
      "grad_norm": 1.6801118850708008,
      "learning_rate": 0.00018874178430318634,
      "loss": 0.2376,
      "step": 12439
    },
    {
      "epoch": 0.058515292059042115,
      "grad_norm": 2.070481777191162,
      "learning_rate": 0.00018874084132507285,
      "loss": 0.4918,
      "step": 12440
    },
    {
      "epoch": 0.05851999586065458,
      "grad_norm": 3.1593620777130127,
      "learning_rate": 0.00018873989834695937,
      "loss": 0.5003,
      "step": 12441
    },
    {
      "epoch": 0.058524699662267046,
      "grad_norm": 1.3969262838363647,
      "learning_rate": 0.0001887389553688459,
      "loss": 0.1348,
      "step": 12442
    },
    {
      "epoch": 0.058529403463879505,
      "grad_norm": 4.609731197357178,
      "learning_rate": 0.0001887380123907324,
      "loss": 0.3741,
      "step": 12443
    },
    {
      "epoch": 0.05853410726549197,
      "grad_norm": 0.7270306348800659,
      "learning_rate": 0.00018873706941261896,
      "loss": 0.0843,
      "step": 12444
    },
    {
      "epoch": 0.058538811067104436,
      "grad_norm": 1.936377763748169,
      "learning_rate": 0.00018873612643450547,
      "loss": 0.4043,
      "step": 12445
    },
    {
      "epoch": 0.058543514868716895,
      "grad_norm": 1.5514411926269531,
      "learning_rate": 0.000188735183456392,
      "loss": 0.1302,
      "step": 12446
    },
    {
      "epoch": 0.05854821867032936,
      "grad_norm": 2.423489809036255,
      "learning_rate": 0.0001887342404782785,
      "loss": 0.6667,
      "step": 12447
    },
    {
      "epoch": 0.058552922471941826,
      "grad_norm": 2.221444606781006,
      "learning_rate": 0.00018873329750016503,
      "loss": 0.4987,
      "step": 12448
    },
    {
      "epoch": 0.058557626273554285,
      "grad_norm": 0.8279403448104858,
      "learning_rate": 0.00018873235452205155,
      "loss": 0.167,
      "step": 12449
    },
    {
      "epoch": 0.05856233007516675,
      "grad_norm": 1.6922861337661743,
      "learning_rate": 0.00018873141154393807,
      "loss": 0.2292,
      "step": 12450
    },
    {
      "epoch": 0.058567033876779216,
      "grad_norm": 2.0306785106658936,
      "learning_rate": 0.00018873046856582459,
      "loss": 0.2448,
      "step": 12451
    },
    {
      "epoch": 0.058571737678391675,
      "grad_norm": 2.271388530731201,
      "learning_rate": 0.0001887295255877111,
      "loss": 0.1975,
      "step": 12452
    },
    {
      "epoch": 0.05857644148000414,
      "grad_norm": 2.4452316761016846,
      "learning_rate": 0.00018872858260959765,
      "loss": 0.2053,
      "step": 12453
    },
    {
      "epoch": 0.0585811452816166,
      "grad_norm": 2.1787164211273193,
      "learning_rate": 0.00018872763963148417,
      "loss": 0.3485,
      "step": 12454
    },
    {
      "epoch": 0.058585849083229065,
      "grad_norm": 2.270599603652954,
      "learning_rate": 0.0001887266966533707,
      "loss": 0.3828,
      "step": 12455
    },
    {
      "epoch": 0.05859055288484153,
      "grad_norm": 2.2482314109802246,
      "learning_rate": 0.0001887257536752572,
      "loss": 0.4075,
      "step": 12456
    },
    {
      "epoch": 0.05859525668645399,
      "grad_norm": 1.0661994218826294,
      "learning_rate": 0.00018872481069714375,
      "loss": 0.1213,
      "step": 12457
    },
    {
      "epoch": 0.058599960488066455,
      "grad_norm": 2.929049253463745,
      "learning_rate": 0.00018872386771903024,
      "loss": 0.4528,
      "step": 12458
    },
    {
      "epoch": 0.05860466428967892,
      "grad_norm": 0.3652380108833313,
      "learning_rate": 0.00018872292474091676,
      "loss": 0.0225,
      "step": 12459
    },
    {
      "epoch": 0.05860936809129138,
      "grad_norm": 2.6974036693573,
      "learning_rate": 0.00018872198176280328,
      "loss": 0.4159,
      "step": 12460
    },
    {
      "epoch": 0.058614071892903845,
      "grad_norm": 1.6336041688919067,
      "learning_rate": 0.0001887210387846898,
      "loss": 0.249,
      "step": 12461
    },
    {
      "epoch": 0.05861877569451631,
      "grad_norm": 1.3976908922195435,
      "learning_rate": 0.00018872009580657635,
      "loss": 0.1292,
      "step": 12462
    },
    {
      "epoch": 0.05862347949612877,
      "grad_norm": 2.451326847076416,
      "learning_rate": 0.00018871915282846286,
      "loss": 0.3196,
      "step": 12463
    },
    {
      "epoch": 0.058628183297741235,
      "grad_norm": 1.370923638343811,
      "learning_rate": 0.00018871820985034938,
      "loss": 0.1857,
      "step": 12464
    },
    {
      "epoch": 0.0586328870993537,
      "grad_norm": 1.5296615362167358,
      "learning_rate": 0.0001887172668722359,
      "loss": 0.229,
      "step": 12465
    },
    {
      "epoch": 0.05863759090096616,
      "grad_norm": 1.928049921989441,
      "learning_rate": 0.00018871632389412242,
      "loss": 0.1018,
      "step": 12466
    },
    {
      "epoch": 0.058642294702578625,
      "grad_norm": 1.3982234001159668,
      "learning_rate": 0.00018871538091600897,
      "loss": 0.1632,
      "step": 12467
    },
    {
      "epoch": 0.05864699850419109,
      "grad_norm": 1.5734456777572632,
      "learning_rate": 0.00018871443793789548,
      "loss": 0.1294,
      "step": 12468
    },
    {
      "epoch": 0.05865170230580355,
      "grad_norm": 0.8673866391181946,
      "learning_rate": 0.00018871349495978198,
      "loss": 0.1352,
      "step": 12469
    },
    {
      "epoch": 0.058656406107416015,
      "grad_norm": 2.7374675273895264,
      "learning_rate": 0.0001887125519816685,
      "loss": 0.5393,
      "step": 12470
    },
    {
      "epoch": 0.058661109909028473,
      "grad_norm": 2.510385274887085,
      "learning_rate": 0.00018871160900355504,
      "loss": 0.3862,
      "step": 12471
    },
    {
      "epoch": 0.05866581371064094,
      "grad_norm": 1.6925818920135498,
      "learning_rate": 0.00018871066602544156,
      "loss": 0.2707,
      "step": 12472
    },
    {
      "epoch": 0.058670517512253405,
      "grad_norm": 2.96488881111145,
      "learning_rate": 0.00018870972304732808,
      "loss": 0.4343,
      "step": 12473
    },
    {
      "epoch": 0.058675221313865863,
      "grad_norm": 0.3495211601257324,
      "learning_rate": 0.0001887087800692146,
      "loss": 0.0265,
      "step": 12474
    },
    {
      "epoch": 0.05867992511547833,
      "grad_norm": 2.464820623397827,
      "learning_rate": 0.00018870783709110111,
      "loss": 0.1048,
      "step": 12475
    },
    {
      "epoch": 0.058684628917090795,
      "grad_norm": 1.0912092924118042,
      "learning_rate": 0.00018870689411298766,
      "loss": 0.1516,
      "step": 12476
    },
    {
      "epoch": 0.05868933271870325,
      "grad_norm": 2.0480144023895264,
      "learning_rate": 0.00018870595113487418,
      "loss": 0.2045,
      "step": 12477
    },
    {
      "epoch": 0.05869403652031572,
      "grad_norm": 2.3814239501953125,
      "learning_rate": 0.0001887050081567607,
      "loss": 0.5868,
      "step": 12478
    },
    {
      "epoch": 0.058698740321928185,
      "grad_norm": 4.676751613616943,
      "learning_rate": 0.00018870406517864722,
      "loss": 0.5153,
      "step": 12479
    },
    {
      "epoch": 0.05870344412354064,
      "grad_norm": 1.4512916803359985,
      "learning_rate": 0.00018870312220053374,
      "loss": 0.1327,
      "step": 12480
    },
    {
      "epoch": 0.05870814792515311,
      "grad_norm": 0.30024322867393494,
      "learning_rate": 0.00018870217922242025,
      "loss": 0.0207,
      "step": 12481
    },
    {
      "epoch": 0.058712851726765575,
      "grad_norm": 3.4536941051483154,
      "learning_rate": 0.00018870123624430677,
      "loss": 0.7787,
      "step": 12482
    },
    {
      "epoch": 0.05871755552837803,
      "grad_norm": 2.3110969066619873,
      "learning_rate": 0.0001887002932661933,
      "loss": 0.4698,
      "step": 12483
    },
    {
      "epoch": 0.0587222593299905,
      "grad_norm": 1.2498363256454468,
      "learning_rate": 0.0001886993502880798,
      "loss": 0.1837,
      "step": 12484
    },
    {
      "epoch": 0.058726963131602965,
      "grad_norm": 0.6669474840164185,
      "learning_rate": 0.00018869840730996636,
      "loss": 0.0646,
      "step": 12485
    },
    {
      "epoch": 0.05873166693321542,
      "grad_norm": 0.8484198451042175,
      "learning_rate": 0.00018869746433185287,
      "loss": 0.0761,
      "step": 12486
    },
    {
      "epoch": 0.05873637073482789,
      "grad_norm": 2.529808759689331,
      "learning_rate": 0.0001886965213537394,
      "loss": 0.4705,
      "step": 12487
    },
    {
      "epoch": 0.05874107453644035,
      "grad_norm": 2.4570131301879883,
      "learning_rate": 0.0001886955783756259,
      "loss": 0.5188,
      "step": 12488
    },
    {
      "epoch": 0.05874577833805281,
      "grad_norm": 2.961620330810547,
      "learning_rate": 0.00018869463539751243,
      "loss": 0.7133,
      "step": 12489
    },
    {
      "epoch": 0.05875048213966528,
      "grad_norm": 0.886397123336792,
      "learning_rate": 0.00018869369241939895,
      "loss": 0.0989,
      "step": 12490
    },
    {
      "epoch": 0.05875518594127774,
      "grad_norm": 2.7821004390716553,
      "learning_rate": 0.00018869274944128547,
      "loss": 0.4357,
      "step": 12491
    },
    {
      "epoch": 0.0587598897428902,
      "grad_norm": 7.585535526275635,
      "learning_rate": 0.00018869180646317199,
      "loss": 0.3256,
      "step": 12492
    },
    {
      "epoch": 0.05876459354450267,
      "grad_norm": 2.9076085090637207,
      "learning_rate": 0.0001886908634850585,
      "loss": 0.5249,
      "step": 12493
    },
    {
      "epoch": 0.05876929734611513,
      "grad_norm": 2.3432888984680176,
      "learning_rate": 0.00018868992050694505,
      "loss": 0.3615,
      "step": 12494
    },
    {
      "epoch": 0.05877400114772759,
      "grad_norm": 0.9678130745887756,
      "learning_rate": 0.00018868897752883157,
      "loss": 0.1041,
      "step": 12495
    },
    {
      "epoch": 0.05877870494934006,
      "grad_norm": 0.767714262008667,
      "learning_rate": 0.0001886880345507181,
      "loss": 0.0635,
      "step": 12496
    },
    {
      "epoch": 0.05878340875095252,
      "grad_norm": 0.5038768649101257,
      "learning_rate": 0.0001886870915726046,
      "loss": 0.0474,
      "step": 12497
    },
    {
      "epoch": 0.05878811255256498,
      "grad_norm": 0.5920445919036865,
      "learning_rate": 0.00018868614859449115,
      "loss": 0.0524,
      "step": 12498
    },
    {
      "epoch": 0.05879281635417745,
      "grad_norm": 2.0779881477355957,
      "learning_rate": 0.00018868520561637767,
      "loss": 0.333,
      "step": 12499
    },
    {
      "epoch": 0.05879752015578991,
      "grad_norm": 1.605686902999878,
      "learning_rate": 0.00018868426263826416,
      "loss": 0.2271,
      "step": 12500
    },
    {
      "epoch": 0.05880222395740237,
      "grad_norm": 4.43782901763916,
      "learning_rate": 0.00018868331966015068,
      "loss": 0.4873,
      "step": 12501
    },
    {
      "epoch": 0.05880692775901484,
      "grad_norm": 5.8739752769470215,
      "learning_rate": 0.0001886823766820372,
      "loss": 0.6203,
      "step": 12502
    },
    {
      "epoch": 0.0588116315606273,
      "grad_norm": 1.8831850290298462,
      "learning_rate": 0.00018868143370392375,
      "loss": 0.168,
      "step": 12503
    },
    {
      "epoch": 0.05881633536223976,
      "grad_norm": 2.1277658939361572,
      "learning_rate": 0.00018868049072581026,
      "loss": 0.2503,
      "step": 12504
    },
    {
      "epoch": 0.05882103916385222,
      "grad_norm": 0.9502758979797363,
      "learning_rate": 0.00018867954774769678,
      "loss": 0.082,
      "step": 12505
    },
    {
      "epoch": 0.05882574296546469,
      "grad_norm": 1.594075322151184,
      "learning_rate": 0.0001886786047695833,
      "loss": 0.1465,
      "step": 12506
    },
    {
      "epoch": 0.05883044676707715,
      "grad_norm": 0.43321800231933594,
      "learning_rate": 0.00018867766179146985,
      "loss": 0.0295,
      "step": 12507
    },
    {
      "epoch": 0.05883515056868961,
      "grad_norm": 2.0290796756744385,
      "learning_rate": 0.00018867671881335637,
      "loss": 0.2218,
      "step": 12508
    },
    {
      "epoch": 0.05883985437030208,
      "grad_norm": 1.987714171409607,
      "learning_rate": 0.00018867577583524288,
      "loss": 0.3237,
      "step": 12509
    },
    {
      "epoch": 0.05884455817191454,
      "grad_norm": 5.514799118041992,
      "learning_rate": 0.0001886748328571294,
      "loss": 0.5082,
      "step": 12510
    },
    {
      "epoch": 0.058849261973527,
      "grad_norm": 2.076240062713623,
      "learning_rate": 0.00018867388987901592,
      "loss": 0.1524,
      "step": 12511
    },
    {
      "epoch": 0.05885396577513947,
      "grad_norm": 2.3037102222442627,
      "learning_rate": 0.00018867294690090244,
      "loss": 0.343,
      "step": 12512
    },
    {
      "epoch": 0.05885866957675193,
      "grad_norm": 2.3248863220214844,
      "learning_rate": 0.00018867200392278896,
      "loss": 0.1486,
      "step": 12513
    },
    {
      "epoch": 0.05886337337836439,
      "grad_norm": 4.260049343109131,
      "learning_rate": 0.00018867106094467548,
      "loss": 0.6709,
      "step": 12514
    },
    {
      "epoch": 0.05886807717997686,
      "grad_norm": 2.7734973430633545,
      "learning_rate": 0.000188670117966562,
      "loss": 0.2847,
      "step": 12515
    },
    {
      "epoch": 0.05887278098158932,
      "grad_norm": 3.1770293712615967,
      "learning_rate": 0.00018866917498844854,
      "loss": 0.5251,
      "step": 12516
    },
    {
      "epoch": 0.05887748478320178,
      "grad_norm": 0.8328854441642761,
      "learning_rate": 0.00018866823201033506,
      "loss": 0.0978,
      "step": 12517
    },
    {
      "epoch": 0.05888218858481425,
      "grad_norm": 2.246692657470703,
      "learning_rate": 0.00018866728903222158,
      "loss": 0.316,
      "step": 12518
    },
    {
      "epoch": 0.05888689238642671,
      "grad_norm": 0.6507831811904907,
      "learning_rate": 0.0001886663460541081,
      "loss": 0.0625,
      "step": 12519
    },
    {
      "epoch": 0.05889159618803917,
      "grad_norm": 5.3447418212890625,
      "learning_rate": 0.00018866540307599462,
      "loss": 0.4049,
      "step": 12520
    },
    {
      "epoch": 0.05889629998965164,
      "grad_norm": 2.636301040649414,
      "learning_rate": 0.00018866446009788114,
      "loss": 0.2961,
      "step": 12521
    },
    {
      "epoch": 0.058901003791264096,
      "grad_norm": 4.513004779815674,
      "learning_rate": 0.00018866351711976765,
      "loss": 0.473,
      "step": 12522
    },
    {
      "epoch": 0.05890570759287656,
      "grad_norm": 0.5620929598808289,
      "learning_rate": 0.00018866257414165417,
      "loss": 0.0708,
      "step": 12523
    },
    {
      "epoch": 0.05891041139448903,
      "grad_norm": 4.172022342681885,
      "learning_rate": 0.0001886616311635407,
      "loss": 0.7116,
      "step": 12524
    },
    {
      "epoch": 0.058915115196101486,
      "grad_norm": 4.143495559692383,
      "learning_rate": 0.0001886606881854272,
      "loss": 0.7457,
      "step": 12525
    },
    {
      "epoch": 0.05891981899771395,
      "grad_norm": 2.063918113708496,
      "learning_rate": 0.00018865974520731376,
      "loss": 0.1964,
      "step": 12526
    },
    {
      "epoch": 0.05892452279932642,
      "grad_norm": 2.083526134490967,
      "learning_rate": 0.00018865880222920027,
      "loss": 0.2422,
      "step": 12527
    },
    {
      "epoch": 0.058929226600938876,
      "grad_norm": 0.7358397841453552,
      "learning_rate": 0.0001886578592510868,
      "loss": 0.0712,
      "step": 12528
    },
    {
      "epoch": 0.05893393040255134,
      "grad_norm": 4.784865856170654,
      "learning_rate": 0.0001886569162729733,
      "loss": 0.1874,
      "step": 12529
    },
    {
      "epoch": 0.05893863420416381,
      "grad_norm": 1.3582602739334106,
      "learning_rate": 0.00018865597329485986,
      "loss": 0.1418,
      "step": 12530
    },
    {
      "epoch": 0.058943338005776266,
      "grad_norm": 2.397003173828125,
      "learning_rate": 0.00018865503031674635,
      "loss": 0.3855,
      "step": 12531
    },
    {
      "epoch": 0.05894804180738873,
      "grad_norm": 1.2629469633102417,
      "learning_rate": 0.00018865408733863287,
      "loss": 0.1397,
      "step": 12532
    },
    {
      "epoch": 0.0589527456090012,
      "grad_norm": 3.7710940837860107,
      "learning_rate": 0.00018865314436051939,
      "loss": 0.9798,
      "step": 12533
    },
    {
      "epoch": 0.058957449410613656,
      "grad_norm": 1.148005723953247,
      "learning_rate": 0.0001886522013824059,
      "loss": 0.1585,
      "step": 12534
    },
    {
      "epoch": 0.05896215321222612,
      "grad_norm": 1.3978400230407715,
      "learning_rate": 0.00018865125840429245,
      "loss": 0.1457,
      "step": 12535
    },
    {
      "epoch": 0.05896685701383859,
      "grad_norm": 2.5156736373901367,
      "learning_rate": 0.00018865031542617897,
      "loss": 0.2314,
      "step": 12536
    },
    {
      "epoch": 0.058971560815451046,
      "grad_norm": 4.928576946258545,
      "learning_rate": 0.0001886493724480655,
      "loss": 0.5655,
      "step": 12537
    },
    {
      "epoch": 0.05897626461706351,
      "grad_norm": 0.6910154223442078,
      "learning_rate": 0.000188648429469952,
      "loss": 0.0822,
      "step": 12538
    },
    {
      "epoch": 0.05898096841867597,
      "grad_norm": 2.2061150074005127,
      "learning_rate": 0.00018864748649183855,
      "loss": 0.2243,
      "step": 12539
    },
    {
      "epoch": 0.058985672220288436,
      "grad_norm": 1.0048450231552124,
      "learning_rate": 0.00018864654351372507,
      "loss": 0.0885,
      "step": 12540
    },
    {
      "epoch": 0.0589903760219009,
      "grad_norm": 1.8681352138519287,
      "learning_rate": 0.0001886456005356116,
      "loss": 0.3447,
      "step": 12541
    },
    {
      "epoch": 0.05899507982351336,
      "grad_norm": 1.4110342264175415,
      "learning_rate": 0.0001886446575574981,
      "loss": 0.0993,
      "step": 12542
    },
    {
      "epoch": 0.058999783625125826,
      "grad_norm": 0.9990540146827698,
      "learning_rate": 0.0001886437145793846,
      "loss": 0.1106,
      "step": 12543
    },
    {
      "epoch": 0.05900448742673829,
      "grad_norm": 1.1650567054748535,
      "learning_rate": 0.00018864277160127115,
      "loss": 0.1062,
      "step": 12544
    },
    {
      "epoch": 0.05900919122835075,
      "grad_norm": 2.189272165298462,
      "learning_rate": 0.00018864182862315766,
      "loss": 0.3041,
      "step": 12545
    },
    {
      "epoch": 0.059013895029963216,
      "grad_norm": 1.0730568170547485,
      "learning_rate": 0.00018864088564504418,
      "loss": 0.1353,
      "step": 12546
    },
    {
      "epoch": 0.05901859883157568,
      "grad_norm": 1.51551353931427,
      "learning_rate": 0.0001886399426669307,
      "loss": 0.1654,
      "step": 12547
    },
    {
      "epoch": 0.05902330263318814,
      "grad_norm": 3.628513813018799,
      "learning_rate": 0.00018863899968881725,
      "loss": 0.4788,
      "step": 12548
    },
    {
      "epoch": 0.059028006434800606,
      "grad_norm": 0.7101819515228271,
      "learning_rate": 0.00018863805671070377,
      "loss": 0.0989,
      "step": 12549
    },
    {
      "epoch": 0.05903271023641307,
      "grad_norm": 1.7758984565734863,
      "learning_rate": 0.00018863711373259028,
      "loss": 0.369,
      "step": 12550
    },
    {
      "epoch": 0.05903741403802553,
      "grad_norm": 2.33695125579834,
      "learning_rate": 0.0001886361707544768,
      "loss": 0.2989,
      "step": 12551
    },
    {
      "epoch": 0.059042117839637996,
      "grad_norm": 4.255988121032715,
      "learning_rate": 0.00018863522777636332,
      "loss": 0.5472,
      "step": 12552
    },
    {
      "epoch": 0.05904682164125046,
      "grad_norm": 3.4637539386749268,
      "learning_rate": 0.00018863428479824984,
      "loss": 0.2551,
      "step": 12553
    },
    {
      "epoch": 0.05905152544286292,
      "grad_norm": 1.5088270902633667,
      "learning_rate": 0.00018863334182013636,
      "loss": 0.1162,
      "step": 12554
    },
    {
      "epoch": 0.059056229244475386,
      "grad_norm": 6.931269645690918,
      "learning_rate": 0.00018863239884202288,
      "loss": 0.3917,
      "step": 12555
    },
    {
      "epoch": 0.059060933046087845,
      "grad_norm": 2.1839449405670166,
      "learning_rate": 0.0001886314558639094,
      "loss": 0.2032,
      "step": 12556
    },
    {
      "epoch": 0.05906563684770031,
      "grad_norm": 8.317913055419922,
      "learning_rate": 0.00018863051288579594,
      "loss": 0.6226,
      "step": 12557
    },
    {
      "epoch": 0.059070340649312776,
      "grad_norm": 10.735711097717285,
      "learning_rate": 0.00018862956990768246,
      "loss": 0.6752,
      "step": 12558
    },
    {
      "epoch": 0.059075044450925235,
      "grad_norm": 0.9608107805252075,
      "learning_rate": 0.00018862862692956898,
      "loss": 0.0923,
      "step": 12559
    },
    {
      "epoch": 0.0590797482525377,
      "grad_norm": 1.192320704460144,
      "learning_rate": 0.0001886276839514555,
      "loss": 0.0906,
      "step": 12560
    },
    {
      "epoch": 0.059084452054150166,
      "grad_norm": 3.7312850952148438,
      "learning_rate": 0.00018862674097334202,
      "loss": 0.3684,
      "step": 12561
    },
    {
      "epoch": 0.059089155855762625,
      "grad_norm": 2.7696027755737305,
      "learning_rate": 0.00018862579799522853,
      "loss": 0.3369,
      "step": 12562
    },
    {
      "epoch": 0.05909385965737509,
      "grad_norm": 2.080803155899048,
      "learning_rate": 0.00018862485501711505,
      "loss": 0.2539,
      "step": 12563
    },
    {
      "epoch": 0.059098563458987556,
      "grad_norm": 2.8749306201934814,
      "learning_rate": 0.00018862391203900157,
      "loss": 0.4473,
      "step": 12564
    },
    {
      "epoch": 0.059103267260600015,
      "grad_norm": 3.093416452407837,
      "learning_rate": 0.0001886229690608881,
      "loss": 0.2967,
      "step": 12565
    },
    {
      "epoch": 0.05910797106221248,
      "grad_norm": 1.8459986448287964,
      "learning_rate": 0.00018862202608277464,
      "loss": 0.1704,
      "step": 12566
    },
    {
      "epoch": 0.059112674863824946,
      "grad_norm": 3.5549914836883545,
      "learning_rate": 0.00018862108310466116,
      "loss": 0.352,
      "step": 12567
    },
    {
      "epoch": 0.059117378665437405,
      "grad_norm": 0.9630619883537292,
      "learning_rate": 0.00018862014012654767,
      "loss": 0.0961,
      "step": 12568
    },
    {
      "epoch": 0.05912208246704987,
      "grad_norm": 1.262123942375183,
      "learning_rate": 0.0001886191971484342,
      "loss": 0.1315,
      "step": 12569
    },
    {
      "epoch": 0.059126786268662336,
      "grad_norm": 8.700201988220215,
      "learning_rate": 0.0001886182541703207,
      "loss": 0.6543,
      "step": 12570
    },
    {
      "epoch": 0.059131490070274795,
      "grad_norm": 2.6829864978790283,
      "learning_rate": 0.00018861731119220726,
      "loss": 0.1936,
      "step": 12571
    },
    {
      "epoch": 0.05913619387188726,
      "grad_norm": 2.0672268867492676,
      "learning_rate": 0.00018861636821409378,
      "loss": 0.4109,
      "step": 12572
    },
    {
      "epoch": 0.05914089767349972,
      "grad_norm": 0.5082783699035645,
      "learning_rate": 0.0001886154252359803,
      "loss": 0.0422,
      "step": 12573
    },
    {
      "epoch": 0.059145601475112185,
      "grad_norm": 2.011659622192383,
      "learning_rate": 0.00018861448225786679,
      "loss": 0.0927,
      "step": 12574
    },
    {
      "epoch": 0.05915030527672465,
      "grad_norm": 1.4996154308319092,
      "learning_rate": 0.0001886135392797533,
      "loss": 0.0881,
      "step": 12575
    },
    {
      "epoch": 0.05915500907833711,
      "grad_norm": 0.21833859384059906,
      "learning_rate": 0.00018861259630163985,
      "loss": 0.0177,
      "step": 12576
    },
    {
      "epoch": 0.059159712879949575,
      "grad_norm": 1.8712804317474365,
      "learning_rate": 0.00018861165332352637,
      "loss": 0.2408,
      "step": 12577
    },
    {
      "epoch": 0.05916441668156204,
      "grad_norm": 3.9933149814605713,
      "learning_rate": 0.0001886107103454129,
      "loss": 0.7868,
      "step": 12578
    },
    {
      "epoch": 0.0591691204831745,
      "grad_norm": 1.9030567407608032,
      "learning_rate": 0.0001886097673672994,
      "loss": 0.2273,
      "step": 12579
    },
    {
      "epoch": 0.059173824284786965,
      "grad_norm": 1.420684814453125,
      "learning_rate": 0.00018860882438918595,
      "loss": 0.1482,
      "step": 12580
    },
    {
      "epoch": 0.05917852808639943,
      "grad_norm": 4.22706413269043,
      "learning_rate": 0.00018860788141107247,
      "loss": 0.9448,
      "step": 12581
    },
    {
      "epoch": 0.05918323188801189,
      "grad_norm": 3.2382750511169434,
      "learning_rate": 0.000188606938432959,
      "loss": 0.2423,
      "step": 12582
    },
    {
      "epoch": 0.059187935689624355,
      "grad_norm": 2.713883638381958,
      "learning_rate": 0.0001886059954548455,
      "loss": 0.4191,
      "step": 12583
    },
    {
      "epoch": 0.05919263949123682,
      "grad_norm": 5.125797748565674,
      "learning_rate": 0.00018860505247673203,
      "loss": 0.8802,
      "step": 12584
    },
    {
      "epoch": 0.05919734329284928,
      "grad_norm": 4.324519634246826,
      "learning_rate": 0.00018860410949861855,
      "loss": 0.6014,
      "step": 12585
    },
    {
      "epoch": 0.059202047094461745,
      "grad_norm": 0.6032834649085999,
      "learning_rate": 0.00018860316652050506,
      "loss": 0.0545,
      "step": 12586
    },
    {
      "epoch": 0.05920675089607421,
      "grad_norm": 2.8668081760406494,
      "learning_rate": 0.00018860222354239158,
      "loss": 0.4888,
      "step": 12587
    },
    {
      "epoch": 0.05921145469768667,
      "grad_norm": 0.20782014727592468,
      "learning_rate": 0.0001886012805642781,
      "loss": 0.0155,
      "step": 12588
    },
    {
      "epoch": 0.059216158499299135,
      "grad_norm": 2.4747862815856934,
      "learning_rate": 0.00018860033758616465,
      "loss": 0.831,
      "step": 12589
    },
    {
      "epoch": 0.05922086230091159,
      "grad_norm": 3.9106497764587402,
      "learning_rate": 0.00018859939460805117,
      "loss": 0.6439,
      "step": 12590
    },
    {
      "epoch": 0.05922556610252406,
      "grad_norm": 0.9551227688789368,
      "learning_rate": 0.00018859845162993768,
      "loss": 0.0868,
      "step": 12591
    },
    {
      "epoch": 0.059230269904136525,
      "grad_norm": 3.849203109741211,
      "learning_rate": 0.0001885975086518242,
      "loss": 0.8432,
      "step": 12592
    },
    {
      "epoch": 0.05923497370574898,
      "grad_norm": 0.4130900502204895,
      "learning_rate": 0.00018859656567371072,
      "loss": 0.0349,
      "step": 12593
    },
    {
      "epoch": 0.05923967750736145,
      "grad_norm": 0.5684449076652527,
      "learning_rate": 0.00018859562269559724,
      "loss": 0.0815,
      "step": 12594
    },
    {
      "epoch": 0.059244381308973915,
      "grad_norm": 0.3165578842163086,
      "learning_rate": 0.00018859467971748376,
      "loss": 0.027,
      "step": 12595
    },
    {
      "epoch": 0.05924908511058637,
      "grad_norm": 0.7585532665252686,
      "learning_rate": 0.00018859373673937028,
      "loss": 0.1325,
      "step": 12596
    },
    {
      "epoch": 0.05925378891219884,
      "grad_norm": 0.4964810907840729,
      "learning_rate": 0.0001885927937612568,
      "loss": 0.0571,
      "step": 12597
    },
    {
      "epoch": 0.059258492713811305,
      "grad_norm": 3.620861530303955,
      "learning_rate": 0.00018859185078314334,
      "loss": 0.4042,
      "step": 12598
    },
    {
      "epoch": 0.05926319651542376,
      "grad_norm": 1.0948822498321533,
      "learning_rate": 0.00018859090780502986,
      "loss": 0.1616,
      "step": 12599
    },
    {
      "epoch": 0.05926790031703623,
      "grad_norm": 1.0258431434631348,
      "learning_rate": 0.00018858996482691638,
      "loss": 0.0814,
      "step": 12600
    },
    {
      "epoch": 0.059272604118648695,
      "grad_norm": 1.049389123916626,
      "learning_rate": 0.0001885890218488029,
      "loss": 0.0798,
      "step": 12601
    },
    {
      "epoch": 0.05927730792026115,
      "grad_norm": 0.8800644874572754,
      "learning_rate": 0.00018858807887068942,
      "loss": 0.0856,
      "step": 12602
    },
    {
      "epoch": 0.05928201172187362,
      "grad_norm": 3.766723871231079,
      "learning_rate": 0.00018858713589257596,
      "loss": 0.9622,
      "step": 12603
    },
    {
      "epoch": 0.059286715523486085,
      "grad_norm": 2.0722556114196777,
      "learning_rate": 0.00018858619291446248,
      "loss": 0.1577,
      "step": 12604
    },
    {
      "epoch": 0.05929141932509854,
      "grad_norm": 1.373512625694275,
      "learning_rate": 0.00018858524993634897,
      "loss": 0.1457,
      "step": 12605
    },
    {
      "epoch": 0.05929612312671101,
      "grad_norm": 2.1257355213165283,
      "learning_rate": 0.0001885843069582355,
      "loss": 0.3377,
      "step": 12606
    },
    {
      "epoch": 0.05930082692832347,
      "grad_norm": 2.680877685546875,
      "learning_rate": 0.00018858336398012204,
      "loss": 0.4409,
      "step": 12607
    },
    {
      "epoch": 0.05930553072993593,
      "grad_norm": 1.9887762069702148,
      "learning_rate": 0.00018858242100200856,
      "loss": 0.3532,
      "step": 12608
    },
    {
      "epoch": 0.0593102345315484,
      "grad_norm": 1.2456814050674438,
      "learning_rate": 0.00018858147802389507,
      "loss": 0.1305,
      "step": 12609
    },
    {
      "epoch": 0.05931493833316086,
      "grad_norm": 3.565176248550415,
      "learning_rate": 0.0001885805350457816,
      "loss": 0.5998,
      "step": 12610
    },
    {
      "epoch": 0.05931964213477332,
      "grad_norm": 3.0949480533599854,
      "learning_rate": 0.0001885795920676681,
      "loss": 0.5398,
      "step": 12611
    },
    {
      "epoch": 0.05932434593638579,
      "grad_norm": 2.2770297527313232,
      "learning_rate": 0.00018857864908955466,
      "loss": 0.2549,
      "step": 12612
    },
    {
      "epoch": 0.05932904973799825,
      "grad_norm": 0.40029171109199524,
      "learning_rate": 0.00018857770611144118,
      "loss": 0.0282,
      "step": 12613
    },
    {
      "epoch": 0.05933375353961071,
      "grad_norm": 0.31761661171913147,
      "learning_rate": 0.0001885767631333277,
      "loss": 0.0106,
      "step": 12614
    },
    {
      "epoch": 0.05933845734122318,
      "grad_norm": 3.533921957015991,
      "learning_rate": 0.0001885758201552142,
      "loss": 0.3829,
      "step": 12615
    },
    {
      "epoch": 0.05934316114283564,
      "grad_norm": 2.6176834106445312,
      "learning_rate": 0.00018857487717710073,
      "loss": 0.3051,
      "step": 12616
    },
    {
      "epoch": 0.0593478649444481,
      "grad_norm": 3.095940113067627,
      "learning_rate": 0.00018857393419898725,
      "loss": 0.494,
      "step": 12617
    },
    {
      "epoch": 0.05935256874606057,
      "grad_norm": 2.3678648471832275,
      "learning_rate": 0.00018857299122087377,
      "loss": 0.3361,
      "step": 12618
    },
    {
      "epoch": 0.05935727254767303,
      "grad_norm": 2.9763400554656982,
      "learning_rate": 0.0001885720482427603,
      "loss": 0.8529,
      "step": 12619
    },
    {
      "epoch": 0.05936197634928549,
      "grad_norm": 3.5502872467041016,
      "learning_rate": 0.0001885711052646468,
      "loss": 0.3498,
      "step": 12620
    },
    {
      "epoch": 0.05936668015089796,
      "grad_norm": 3.5426812171936035,
      "learning_rate": 0.00018857016228653335,
      "loss": 0.5714,
      "step": 12621
    },
    {
      "epoch": 0.05937138395251042,
      "grad_norm": 1.4624416828155518,
      "learning_rate": 0.00018856921930841987,
      "loss": 0.2823,
      "step": 12622
    },
    {
      "epoch": 0.05937608775412288,
      "grad_norm": 1.046926498413086,
      "learning_rate": 0.0001885682763303064,
      "loss": 0.1434,
      "step": 12623
    },
    {
      "epoch": 0.05938079155573534,
      "grad_norm": 0.7305793762207031,
      "learning_rate": 0.0001885673333521929,
      "loss": 0.0727,
      "step": 12624
    },
    {
      "epoch": 0.05938549535734781,
      "grad_norm": 4.6949005126953125,
      "learning_rate": 0.00018856639037407943,
      "loss": 0.8484,
      "step": 12625
    },
    {
      "epoch": 0.05939019915896027,
      "grad_norm": 2.3606085777282715,
      "learning_rate": 0.00018856544739596595,
      "loss": 0.5365,
      "step": 12626
    },
    {
      "epoch": 0.05939490296057273,
      "grad_norm": 1.5575499534606934,
      "learning_rate": 0.00018856450441785246,
      "loss": 0.2158,
      "step": 12627
    },
    {
      "epoch": 0.0593996067621852,
      "grad_norm": 1.781020164489746,
      "learning_rate": 0.00018856356143973898,
      "loss": 0.2043,
      "step": 12628
    },
    {
      "epoch": 0.05940431056379766,
      "grad_norm": 1.160098910331726,
      "learning_rate": 0.0001885626184616255,
      "loss": 0.1957,
      "step": 12629
    },
    {
      "epoch": 0.05940901436541012,
      "grad_norm": 1.7195439338684082,
      "learning_rate": 0.00018856167548351205,
      "loss": 0.2626,
      "step": 12630
    },
    {
      "epoch": 0.05941371816702259,
      "grad_norm": 1.808653712272644,
      "learning_rate": 0.00018856073250539857,
      "loss": 0.2318,
      "step": 12631
    },
    {
      "epoch": 0.05941842196863505,
      "grad_norm": 0.6612040996551514,
      "learning_rate": 0.00018855978952728508,
      "loss": 0.1274,
      "step": 12632
    },
    {
      "epoch": 0.05942312577024751,
      "grad_norm": 1.3165048360824585,
      "learning_rate": 0.0001885588465491716,
      "loss": 0.1784,
      "step": 12633
    },
    {
      "epoch": 0.05942782957185998,
      "grad_norm": 1.8289004564285278,
      "learning_rate": 0.00018855790357105812,
      "loss": 0.2748,
      "step": 12634
    },
    {
      "epoch": 0.05943253337347244,
      "grad_norm": 1.5809791088104248,
      "learning_rate": 0.00018855696059294467,
      "loss": 0.2723,
      "step": 12635
    },
    {
      "epoch": 0.0594372371750849,
      "grad_norm": 2.696713924407959,
      "learning_rate": 0.00018855601761483116,
      "loss": 0.331,
      "step": 12636
    },
    {
      "epoch": 0.05944194097669737,
      "grad_norm": 1.148660659790039,
      "learning_rate": 0.00018855507463671768,
      "loss": 0.0961,
      "step": 12637
    },
    {
      "epoch": 0.05944664477830983,
      "grad_norm": 1.051954746246338,
      "learning_rate": 0.0001885541316586042,
      "loss": 0.0813,
      "step": 12638
    },
    {
      "epoch": 0.05945134857992229,
      "grad_norm": 0.9660884737968445,
      "learning_rate": 0.00018855318868049074,
      "loss": 0.0829,
      "step": 12639
    },
    {
      "epoch": 0.05945605238153476,
      "grad_norm": 2.2027101516723633,
      "learning_rate": 0.00018855224570237726,
      "loss": 0.3033,
      "step": 12640
    },
    {
      "epoch": 0.059460756183147216,
      "grad_norm": 0.7184301018714905,
      "learning_rate": 0.00018855130272426378,
      "loss": 0.0599,
      "step": 12641
    },
    {
      "epoch": 0.05946545998475968,
      "grad_norm": 0.7460775375366211,
      "learning_rate": 0.0001885503597461503,
      "loss": 0.0977,
      "step": 12642
    },
    {
      "epoch": 0.05947016378637215,
      "grad_norm": 1.5208920240402222,
      "learning_rate": 0.00018854941676803682,
      "loss": 0.1159,
      "step": 12643
    },
    {
      "epoch": 0.059474867587984606,
      "grad_norm": 1.18972909450531,
      "learning_rate": 0.00018854847378992336,
      "loss": 0.1536,
      "step": 12644
    },
    {
      "epoch": 0.05947957138959707,
      "grad_norm": 1.9011987447738647,
      "learning_rate": 0.00018854753081180988,
      "loss": 0.2863,
      "step": 12645
    },
    {
      "epoch": 0.05948427519120954,
      "grad_norm": 2.7382638454437256,
      "learning_rate": 0.0001885465878336964,
      "loss": 0.5961,
      "step": 12646
    },
    {
      "epoch": 0.059488978992821996,
      "grad_norm": 3.6985459327697754,
      "learning_rate": 0.0001885456448555829,
      "loss": 0.6504,
      "step": 12647
    },
    {
      "epoch": 0.05949368279443446,
      "grad_norm": 2.5055558681488037,
      "learning_rate": 0.00018854470187746944,
      "loss": 0.2796,
      "step": 12648
    },
    {
      "epoch": 0.05949838659604693,
      "grad_norm": 1.970533847808838,
      "learning_rate": 0.00018854375889935596,
      "loss": 0.1755,
      "step": 12649
    },
    {
      "epoch": 0.059503090397659386,
      "grad_norm": 2.4628500938415527,
      "learning_rate": 0.00018854281592124247,
      "loss": 0.4445,
      "step": 12650
    },
    {
      "epoch": 0.05950779419927185,
      "grad_norm": 1.8669129610061646,
      "learning_rate": 0.000188541872943129,
      "loss": 0.16,
      "step": 12651
    },
    {
      "epoch": 0.05951249800088432,
      "grad_norm": 1.761196255683899,
      "learning_rate": 0.0001885409299650155,
      "loss": 0.1033,
      "step": 12652
    },
    {
      "epoch": 0.059517201802496776,
      "grad_norm": 1.7072926759719849,
      "learning_rate": 0.00018853998698690206,
      "loss": 0.1573,
      "step": 12653
    },
    {
      "epoch": 0.05952190560410924,
      "grad_norm": 2.683380603790283,
      "learning_rate": 0.00018853904400878858,
      "loss": 0.2148,
      "step": 12654
    },
    {
      "epoch": 0.05952660940572171,
      "grad_norm": 5.804067611694336,
      "learning_rate": 0.0001885381010306751,
      "loss": 0.575,
      "step": 12655
    },
    {
      "epoch": 0.059531313207334166,
      "grad_norm": 1.330104947090149,
      "learning_rate": 0.0001885371580525616,
      "loss": 0.1063,
      "step": 12656
    },
    {
      "epoch": 0.05953601700894663,
      "grad_norm": 2.8492519855499268,
      "learning_rate": 0.00018853621507444813,
      "loss": 0.4724,
      "step": 12657
    },
    {
      "epoch": 0.05954072081055909,
      "grad_norm": 2.1245083808898926,
      "learning_rate": 0.00018853527209633465,
      "loss": 0.134,
      "step": 12658
    },
    {
      "epoch": 0.059545424612171556,
      "grad_norm": 1.957992672920227,
      "learning_rate": 0.00018853432911822117,
      "loss": 0.2097,
      "step": 12659
    },
    {
      "epoch": 0.05955012841378402,
      "grad_norm": 0.310878723859787,
      "learning_rate": 0.0001885333861401077,
      "loss": 0.0166,
      "step": 12660
    },
    {
      "epoch": 0.05955483221539648,
      "grad_norm": 1.441219449043274,
      "learning_rate": 0.0001885324431619942,
      "loss": 0.1453,
      "step": 12661
    },
    {
      "epoch": 0.059559536017008946,
      "grad_norm": 0.9012735486030579,
      "learning_rate": 0.00018853150018388075,
      "loss": 0.0868,
      "step": 12662
    },
    {
      "epoch": 0.05956423981862141,
      "grad_norm": 1.6762112379074097,
      "learning_rate": 0.00018853055720576727,
      "loss": 0.1374,
      "step": 12663
    },
    {
      "epoch": 0.05956894362023387,
      "grad_norm": 0.9423128366470337,
      "learning_rate": 0.0001885296142276538,
      "loss": 0.0548,
      "step": 12664
    },
    {
      "epoch": 0.059573647421846336,
      "grad_norm": 1.5795010328292847,
      "learning_rate": 0.0001885286712495403,
      "loss": 0.1009,
      "step": 12665
    },
    {
      "epoch": 0.0595783512234588,
      "grad_norm": 2.7777349948883057,
      "learning_rate": 0.00018852772827142685,
      "loss": 0.1877,
      "step": 12666
    },
    {
      "epoch": 0.05958305502507126,
      "grad_norm": 3.7490460872650146,
      "learning_rate": 0.00018852678529331335,
      "loss": 0.817,
      "step": 12667
    },
    {
      "epoch": 0.059587758826683726,
      "grad_norm": 1.9822666645050049,
      "learning_rate": 0.00018852584231519986,
      "loss": 0.1243,
      "step": 12668
    },
    {
      "epoch": 0.05959246262829619,
      "grad_norm": 3.424898147583008,
      "learning_rate": 0.00018852489933708638,
      "loss": 0.5582,
      "step": 12669
    },
    {
      "epoch": 0.05959716642990865,
      "grad_norm": 2.056283712387085,
      "learning_rate": 0.0001885239563589729,
      "loss": 0.1451,
      "step": 12670
    },
    {
      "epoch": 0.059601870231521116,
      "grad_norm": 2.15571665763855,
      "learning_rate": 0.00018852301338085945,
      "loss": 0.2291,
      "step": 12671
    },
    {
      "epoch": 0.05960657403313358,
      "grad_norm": 3.2060911655426025,
      "learning_rate": 0.00018852207040274597,
      "loss": 0.181,
      "step": 12672
    },
    {
      "epoch": 0.05961127783474604,
      "grad_norm": 3.460928440093994,
      "learning_rate": 0.00018852112742463248,
      "loss": 0.277,
      "step": 12673
    },
    {
      "epoch": 0.059615981636358506,
      "grad_norm": 2.623568534851074,
      "learning_rate": 0.000188520184446519,
      "loss": 0.1905,
      "step": 12674
    },
    {
      "epoch": 0.059620685437970965,
      "grad_norm": 2.1448159217834473,
      "learning_rate": 0.00018851924146840552,
      "loss": 0.1148,
      "step": 12675
    },
    {
      "epoch": 0.05962538923958343,
      "grad_norm": 0.5706614255905151,
      "learning_rate": 0.00018851829849029207,
      "loss": 0.0373,
      "step": 12676
    },
    {
      "epoch": 0.059630093041195896,
      "grad_norm": 0.5373802185058594,
      "learning_rate": 0.00018851735551217859,
      "loss": 0.0415,
      "step": 12677
    },
    {
      "epoch": 0.059634796842808355,
      "grad_norm": 2.367171287536621,
      "learning_rate": 0.00018851641253406508,
      "loss": 0.138,
      "step": 12678
    },
    {
      "epoch": 0.05963950064442082,
      "grad_norm": 2.5422890186309814,
      "learning_rate": 0.0001885154695559516,
      "loss": 0.1628,
      "step": 12679
    },
    {
      "epoch": 0.059644204446033286,
      "grad_norm": 2.7298994064331055,
      "learning_rate": 0.00018851452657783814,
      "loss": 0.2084,
      "step": 12680
    },
    {
      "epoch": 0.059648908247645745,
      "grad_norm": 2.295973062515259,
      "learning_rate": 0.00018851358359972466,
      "loss": 0.1106,
      "step": 12681
    },
    {
      "epoch": 0.05965361204925821,
      "grad_norm": 1.6667487621307373,
      "learning_rate": 0.00018851264062161118,
      "loss": 0.1117,
      "step": 12682
    },
    {
      "epoch": 0.059658315850870676,
      "grad_norm": 3.0412609577178955,
      "learning_rate": 0.0001885116976434977,
      "loss": 0.7645,
      "step": 12683
    },
    {
      "epoch": 0.059663019652483135,
      "grad_norm": 3.8240301609039307,
      "learning_rate": 0.00018851075466538422,
      "loss": 0.3724,
      "step": 12684
    },
    {
      "epoch": 0.0596677234540956,
      "grad_norm": 2.471700429916382,
      "learning_rate": 0.00018850981168727076,
      "loss": 0.2953,
      "step": 12685
    },
    {
      "epoch": 0.059672427255708066,
      "grad_norm": 4.0374627113342285,
      "learning_rate": 0.00018850886870915728,
      "loss": 0.3512,
      "step": 12686
    },
    {
      "epoch": 0.059677131057320525,
      "grad_norm": 4.938199520111084,
      "learning_rate": 0.0001885079257310438,
      "loss": 0.832,
      "step": 12687
    },
    {
      "epoch": 0.05968183485893299,
      "grad_norm": 3.448787212371826,
      "learning_rate": 0.00018850698275293032,
      "loss": 0.328,
      "step": 12688
    },
    {
      "epoch": 0.059686538660545456,
      "grad_norm": 1.7930636405944824,
      "learning_rate": 0.00018850603977481684,
      "loss": 0.1227,
      "step": 12689
    },
    {
      "epoch": 0.059691242462157915,
      "grad_norm": 1.8647485971450806,
      "learning_rate": 0.00018850509679670336,
      "loss": 0.1766,
      "step": 12690
    },
    {
      "epoch": 0.05969594626377038,
      "grad_norm": 1.7396824359893799,
      "learning_rate": 0.00018850415381858987,
      "loss": 0.1924,
      "step": 12691
    },
    {
      "epoch": 0.05970065006538284,
      "grad_norm": 2.189333438873291,
      "learning_rate": 0.0001885032108404764,
      "loss": 0.1767,
      "step": 12692
    },
    {
      "epoch": 0.059705353866995305,
      "grad_norm": 6.905590534210205,
      "learning_rate": 0.0001885022678623629,
      "loss": 0.2737,
      "step": 12693
    },
    {
      "epoch": 0.05971005766860777,
      "grad_norm": 0.32911789417266846,
      "learning_rate": 0.00018850132488424946,
      "loss": 0.0309,
      "step": 12694
    },
    {
      "epoch": 0.05971476147022023,
      "grad_norm": 2.334895133972168,
      "learning_rate": 0.00018850038190613598,
      "loss": 0.2927,
      "step": 12695
    },
    {
      "epoch": 0.059719465271832695,
      "grad_norm": 2.439194917678833,
      "learning_rate": 0.0001884994389280225,
      "loss": 0.2891,
      "step": 12696
    },
    {
      "epoch": 0.05972416907344516,
      "grad_norm": 1.6326863765716553,
      "learning_rate": 0.000188498495949909,
      "loss": 0.3633,
      "step": 12697
    },
    {
      "epoch": 0.05972887287505762,
      "grad_norm": 1.2422237396240234,
      "learning_rate": 0.00018849755297179553,
      "loss": 0.1544,
      "step": 12698
    },
    {
      "epoch": 0.059733576676670085,
      "grad_norm": 2.0696938037872314,
      "learning_rate": 0.00018849660999368205,
      "loss": 0.518,
      "step": 12699
    },
    {
      "epoch": 0.05973828047828255,
      "grad_norm": 1.363201379776001,
      "learning_rate": 0.00018849566701556857,
      "loss": 0.1642,
      "step": 12700
    },
    {
      "epoch": 0.05974298427989501,
      "grad_norm": 4.340232849121094,
      "learning_rate": 0.0001884947240374551,
      "loss": 0.3562,
      "step": 12701
    },
    {
      "epoch": 0.059747688081507475,
      "grad_norm": 1.019074559211731,
      "learning_rate": 0.0001884937810593416,
      "loss": 0.0918,
      "step": 12702
    },
    {
      "epoch": 0.05975239188311994,
      "grad_norm": 1.3313225507736206,
      "learning_rate": 0.00018849283808122815,
      "loss": 0.0801,
      "step": 12703
    },
    {
      "epoch": 0.0597570956847324,
      "grad_norm": 1.413644790649414,
      "learning_rate": 0.00018849189510311467,
      "loss": 0.1315,
      "step": 12704
    },
    {
      "epoch": 0.059761799486344865,
      "grad_norm": 2.4939630031585693,
      "learning_rate": 0.0001884909521250012,
      "loss": 0.2077,
      "step": 12705
    },
    {
      "epoch": 0.05976650328795733,
      "grad_norm": 4.585124492645264,
      "learning_rate": 0.0001884900091468877,
      "loss": 0.3818,
      "step": 12706
    },
    {
      "epoch": 0.05977120708956979,
      "grad_norm": 4.319403648376465,
      "learning_rate": 0.00018848906616877425,
      "loss": 0.4512,
      "step": 12707
    },
    {
      "epoch": 0.059775910891182255,
      "grad_norm": 3.184483051300049,
      "learning_rate": 0.00018848812319066077,
      "loss": 0.1801,
      "step": 12708
    },
    {
      "epoch": 0.05978061469279471,
      "grad_norm": 3.515925168991089,
      "learning_rate": 0.00018848718021254726,
      "loss": 0.3096,
      "step": 12709
    },
    {
      "epoch": 0.05978531849440718,
      "grad_norm": 2.6624197959899902,
      "learning_rate": 0.00018848623723443378,
      "loss": 0.2061,
      "step": 12710
    },
    {
      "epoch": 0.059790022296019645,
      "grad_norm": 4.132059574127197,
      "learning_rate": 0.0001884852942563203,
      "loss": 0.3556,
      "step": 12711
    },
    {
      "epoch": 0.0597947260976321,
      "grad_norm": 1.3179376125335693,
      "learning_rate": 0.00018848435127820685,
      "loss": 0.0834,
      "step": 12712
    },
    {
      "epoch": 0.05979942989924457,
      "grad_norm": 3.6420845985412598,
      "learning_rate": 0.00018848340830009337,
      "loss": 0.4413,
      "step": 12713
    },
    {
      "epoch": 0.059804133700857035,
      "grad_norm": 3.770137071609497,
      "learning_rate": 0.00018848246532197988,
      "loss": 0.321,
      "step": 12714
    },
    {
      "epoch": 0.05980883750246949,
      "grad_norm": 3.5210821628570557,
      "learning_rate": 0.0001884815223438664,
      "loss": 0.2861,
      "step": 12715
    },
    {
      "epoch": 0.05981354130408196,
      "grad_norm": 1.982723593711853,
      "learning_rate": 0.00018848057936575295,
      "loss": 0.1784,
      "step": 12716
    },
    {
      "epoch": 0.059818245105694424,
      "grad_norm": 0.7378039360046387,
      "learning_rate": 0.00018847963638763947,
      "loss": 0.0533,
      "step": 12717
    },
    {
      "epoch": 0.05982294890730688,
      "grad_norm": 2.9982330799102783,
      "learning_rate": 0.00018847869340952599,
      "loss": 0.3205,
      "step": 12718
    },
    {
      "epoch": 0.05982765270891935,
      "grad_norm": 2.2415363788604736,
      "learning_rate": 0.0001884777504314125,
      "loss": 0.1677,
      "step": 12719
    },
    {
      "epoch": 0.059832356510531814,
      "grad_norm": 3.584244966506958,
      "learning_rate": 0.000188476807453299,
      "loss": 0.2727,
      "step": 12720
    },
    {
      "epoch": 0.05983706031214427,
      "grad_norm": 3.151668071746826,
      "learning_rate": 0.00018847586447518554,
      "loss": 0.4301,
      "step": 12721
    },
    {
      "epoch": 0.05984176411375674,
      "grad_norm": 6.836867332458496,
      "learning_rate": 0.00018847492149707206,
      "loss": 0.7407,
      "step": 12722
    },
    {
      "epoch": 0.059846467915369204,
      "grad_norm": 3.0124313831329346,
      "learning_rate": 0.00018847397851895858,
      "loss": 0.2986,
      "step": 12723
    },
    {
      "epoch": 0.05985117171698166,
      "grad_norm": 1.2032628059387207,
      "learning_rate": 0.0001884730355408451,
      "loss": 0.1103,
      "step": 12724
    },
    {
      "epoch": 0.05985587551859413,
      "grad_norm": 2.482219696044922,
      "learning_rate": 0.00018847209256273162,
      "loss": 0.3934,
      "step": 12725
    },
    {
      "epoch": 0.05986057932020659,
      "grad_norm": 2.274258613586426,
      "learning_rate": 0.00018847114958461816,
      "loss": 0.2653,
      "step": 12726
    },
    {
      "epoch": 0.05986528312181905,
      "grad_norm": 0.26712629199028015,
      "learning_rate": 0.00018847020660650468,
      "loss": 0.0164,
      "step": 12727
    },
    {
      "epoch": 0.05986998692343152,
      "grad_norm": 3.2466461658477783,
      "learning_rate": 0.0001884692636283912,
      "loss": 0.4269,
      "step": 12728
    },
    {
      "epoch": 0.05987469072504398,
      "grad_norm": 1.9890267848968506,
      "learning_rate": 0.00018846832065027772,
      "loss": 0.1823,
      "step": 12729
    },
    {
      "epoch": 0.05987939452665644,
      "grad_norm": 1.8122873306274414,
      "learning_rate": 0.00018846737767216424,
      "loss": 0.1069,
      "step": 12730
    },
    {
      "epoch": 0.05988409832826891,
      "grad_norm": 3.8237807750701904,
      "learning_rate": 0.00018846643469405076,
      "loss": 0.5925,
      "step": 12731
    },
    {
      "epoch": 0.05988880212988137,
      "grad_norm": 2.231266498565674,
      "learning_rate": 0.00018846549171593727,
      "loss": 0.1207,
      "step": 12732
    },
    {
      "epoch": 0.05989350593149383,
      "grad_norm": 2.168447732925415,
      "learning_rate": 0.0001884645487378238,
      "loss": 0.238,
      "step": 12733
    },
    {
      "epoch": 0.0598982097331063,
      "grad_norm": 3.069570779800415,
      "learning_rate": 0.0001884636057597103,
      "loss": 0.2494,
      "step": 12734
    },
    {
      "epoch": 0.05990291353471876,
      "grad_norm": 0.7121466398239136,
      "learning_rate": 0.00018846266278159686,
      "loss": 0.0741,
      "step": 12735
    },
    {
      "epoch": 0.05990761733633122,
      "grad_norm": 0.2054750919342041,
      "learning_rate": 0.00018846171980348338,
      "loss": 0.0121,
      "step": 12736
    },
    {
      "epoch": 0.05991232113794369,
      "grad_norm": 3.1186299324035645,
      "learning_rate": 0.0001884607768253699,
      "loss": 0.2974,
      "step": 12737
    },
    {
      "epoch": 0.05991702493955615,
      "grad_norm": 0.21832464635372162,
      "learning_rate": 0.0001884598338472564,
      "loss": 0.0142,
      "step": 12738
    },
    {
      "epoch": 0.05992172874116861,
      "grad_norm": 0.5629307627677917,
      "learning_rate": 0.00018845889086914296,
      "loss": 0.0257,
      "step": 12739
    },
    {
      "epoch": 0.05992643254278108,
      "grad_norm": 1.5586789846420288,
      "learning_rate": 0.00018845794789102945,
      "loss": 0.0771,
      "step": 12740
    },
    {
      "epoch": 0.05993113634439354,
      "grad_norm": 2.343583583831787,
      "learning_rate": 0.00018845700491291597,
      "loss": 0.2171,
      "step": 12741
    },
    {
      "epoch": 0.059935840146006,
      "grad_norm": 8.684459686279297,
      "learning_rate": 0.0001884560619348025,
      "loss": 0.8417,
      "step": 12742
    },
    {
      "epoch": 0.05994054394761847,
      "grad_norm": 0.2797758877277374,
      "learning_rate": 0.000188455118956689,
      "loss": 0.0366,
      "step": 12743
    },
    {
      "epoch": 0.05994524774923093,
      "grad_norm": 4.10103178024292,
      "learning_rate": 0.00018845417597857555,
      "loss": 1.1407,
      "step": 12744
    },
    {
      "epoch": 0.05994995155084339,
      "grad_norm": 1.8301678895950317,
      "learning_rate": 0.00018845323300046207,
      "loss": 0.1183,
      "step": 12745
    },
    {
      "epoch": 0.05995465535245585,
      "grad_norm": 0.3904425799846649,
      "learning_rate": 0.0001884522900223486,
      "loss": 0.03,
      "step": 12746
    },
    {
      "epoch": 0.05995935915406832,
      "grad_norm": 2.4174227714538574,
      "learning_rate": 0.0001884513470442351,
      "loss": 0.5109,
      "step": 12747
    },
    {
      "epoch": 0.05996406295568078,
      "grad_norm": 2.1602976322174072,
      "learning_rate": 0.00018845040406612165,
      "loss": 0.3916,
      "step": 12748
    },
    {
      "epoch": 0.05996876675729324,
      "grad_norm": 5.766707420349121,
      "learning_rate": 0.00018844946108800817,
      "loss": 1.3271,
      "step": 12749
    },
    {
      "epoch": 0.05997347055890571,
      "grad_norm": 1.8380622863769531,
      "learning_rate": 0.0001884485181098947,
      "loss": 0.2646,
      "step": 12750
    },
    {
      "epoch": 0.05997817436051817,
      "grad_norm": 1.015558123588562,
      "learning_rate": 0.00018844757513178118,
      "loss": 0.0517,
      "step": 12751
    },
    {
      "epoch": 0.05998287816213063,
      "grad_norm": 1.7007983922958374,
      "learning_rate": 0.0001884466321536677,
      "loss": 0.1094,
      "step": 12752
    },
    {
      "epoch": 0.0599875819637431,
      "grad_norm": 3.3158159255981445,
      "learning_rate": 0.00018844568917555425,
      "loss": 0.2324,
      "step": 12753
    },
    {
      "epoch": 0.05999228576535556,
      "grad_norm": 3.240204334259033,
      "learning_rate": 0.00018844474619744077,
      "loss": 0.5484,
      "step": 12754
    },
    {
      "epoch": 0.05999698956696802,
      "grad_norm": 3.7673373222351074,
      "learning_rate": 0.00018844380321932728,
      "loss": 0.396,
      "step": 12755
    },
    {
      "epoch": 0.06000169336858049,
      "grad_norm": 3.404595136642456,
      "learning_rate": 0.0001884428602412138,
      "loss": 0.5032,
      "step": 12756
    },
    {
      "epoch": 0.06000639717019295,
      "grad_norm": 3.445103883743286,
      "learning_rate": 0.00018844191726310035,
      "loss": 0.3605,
      "step": 12757
    },
    {
      "epoch": 0.06001110097180541,
      "grad_norm": 5.040542125701904,
      "learning_rate": 0.00018844097428498687,
      "loss": 0.8499,
      "step": 12758
    },
    {
      "epoch": 0.06001580477341788,
      "grad_norm": 2.872770309448242,
      "learning_rate": 0.00018844003130687339,
      "loss": 0.2038,
      "step": 12759
    },
    {
      "epoch": 0.06002050857503034,
      "grad_norm": 3.4117271900177,
      "learning_rate": 0.0001884390883287599,
      "loss": 0.5031,
      "step": 12760
    },
    {
      "epoch": 0.0600252123766428,
      "grad_norm": 3.3350002765655518,
      "learning_rate": 0.00018843814535064642,
      "loss": 0.402,
      "step": 12761
    },
    {
      "epoch": 0.06002991617825527,
      "grad_norm": 3.693639039993286,
      "learning_rate": 0.00018843720237253294,
      "loss": 0.3845,
      "step": 12762
    },
    {
      "epoch": 0.060034619979867726,
      "grad_norm": 2.6997973918914795,
      "learning_rate": 0.00018843625939441946,
      "loss": 0.38,
      "step": 12763
    },
    {
      "epoch": 0.06003932378148019,
      "grad_norm": 3.0601136684417725,
      "learning_rate": 0.00018843531641630598,
      "loss": 0.6033,
      "step": 12764
    },
    {
      "epoch": 0.06004402758309266,
      "grad_norm": 2.3138957023620605,
      "learning_rate": 0.0001884343734381925,
      "loss": 0.2422,
      "step": 12765
    },
    {
      "epoch": 0.060048731384705116,
      "grad_norm": 1.371086597442627,
      "learning_rate": 0.00018843343046007904,
      "loss": 0.1838,
      "step": 12766
    },
    {
      "epoch": 0.06005343518631758,
      "grad_norm": 1.730951189994812,
      "learning_rate": 0.00018843248748196556,
      "loss": 0.4051,
      "step": 12767
    },
    {
      "epoch": 0.06005813898793005,
      "grad_norm": 0.5653197169303894,
      "learning_rate": 0.00018843154450385208,
      "loss": 0.0685,
      "step": 12768
    },
    {
      "epoch": 0.060062842789542506,
      "grad_norm": 2.962306022644043,
      "learning_rate": 0.0001884306015257386,
      "loss": 0.3776,
      "step": 12769
    },
    {
      "epoch": 0.06006754659115497,
      "grad_norm": 2.845510959625244,
      "learning_rate": 0.00018842965854762512,
      "loss": 0.3648,
      "step": 12770
    },
    {
      "epoch": 0.06007225039276744,
      "grad_norm": 1.7184168100357056,
      "learning_rate": 0.00018842871556951164,
      "loss": 0.2483,
      "step": 12771
    },
    {
      "epoch": 0.060076954194379896,
      "grad_norm": 0.7512850761413574,
      "learning_rate": 0.00018842777259139816,
      "loss": 0.0916,
      "step": 12772
    },
    {
      "epoch": 0.06008165799599236,
      "grad_norm": 2.066734552383423,
      "learning_rate": 0.00018842682961328467,
      "loss": 0.266,
      "step": 12773
    },
    {
      "epoch": 0.06008636179760483,
      "grad_norm": 2.296236991882324,
      "learning_rate": 0.0001884258866351712,
      "loss": 0.5369,
      "step": 12774
    },
    {
      "epoch": 0.060091065599217286,
      "grad_norm": 0.803181529045105,
      "learning_rate": 0.00018842494365705774,
      "loss": 0.122,
      "step": 12775
    },
    {
      "epoch": 0.06009576940082975,
      "grad_norm": 1.8078362941741943,
      "learning_rate": 0.00018842400067894426,
      "loss": 0.4422,
      "step": 12776
    },
    {
      "epoch": 0.06010047320244222,
      "grad_norm": 1.5028620958328247,
      "learning_rate": 0.00018842305770083078,
      "loss": 0.1778,
      "step": 12777
    },
    {
      "epoch": 0.060105177004054676,
      "grad_norm": 1.9502458572387695,
      "learning_rate": 0.0001884221147227173,
      "loss": 0.4546,
      "step": 12778
    },
    {
      "epoch": 0.06010988080566714,
      "grad_norm": 2.2881760597229004,
      "learning_rate": 0.0001884211717446038,
      "loss": 0.3563,
      "step": 12779
    },
    {
      "epoch": 0.0601145846072796,
      "grad_norm": 1.0667933225631714,
      "learning_rate": 0.00018842022876649036,
      "loss": 0.1872,
      "step": 12780
    },
    {
      "epoch": 0.060119288408892066,
      "grad_norm": 0.8974884152412415,
      "learning_rate": 0.00018841928578837688,
      "loss": 0.101,
      "step": 12781
    },
    {
      "epoch": 0.06012399221050453,
      "grad_norm": 2.3267910480499268,
      "learning_rate": 0.00018841834281026337,
      "loss": 0.3259,
      "step": 12782
    },
    {
      "epoch": 0.06012869601211699,
      "grad_norm": 0.7496662735939026,
      "learning_rate": 0.0001884173998321499,
      "loss": 0.1176,
      "step": 12783
    },
    {
      "epoch": 0.060133399813729456,
      "grad_norm": 1.3439702987670898,
      "learning_rate": 0.0001884164568540364,
      "loss": 0.3284,
      "step": 12784
    },
    {
      "epoch": 0.06013810361534192,
      "grad_norm": 1.6270740032196045,
      "learning_rate": 0.00018841551387592295,
      "loss": 0.3445,
      "step": 12785
    },
    {
      "epoch": 0.06014280741695438,
      "grad_norm": 0.46463924646377563,
      "learning_rate": 0.00018841457089780947,
      "loss": 0.0439,
      "step": 12786
    },
    {
      "epoch": 0.060147511218566846,
      "grad_norm": 1.4842991828918457,
      "learning_rate": 0.000188413627919696,
      "loss": 0.1425,
      "step": 12787
    },
    {
      "epoch": 0.06015221502017931,
      "grad_norm": 1.4603872299194336,
      "learning_rate": 0.0001884126849415825,
      "loss": 0.191,
      "step": 12788
    },
    {
      "epoch": 0.06015691882179177,
      "grad_norm": 0.9253833293914795,
      "learning_rate": 0.00018841174196346905,
      "loss": 0.1599,
      "step": 12789
    },
    {
      "epoch": 0.060161622623404236,
      "grad_norm": 1.6038684844970703,
      "learning_rate": 0.00018841079898535557,
      "loss": 0.1911,
      "step": 12790
    },
    {
      "epoch": 0.0601663264250167,
      "grad_norm": 2.317491292953491,
      "learning_rate": 0.0001884098560072421,
      "loss": 0.2878,
      "step": 12791
    },
    {
      "epoch": 0.06017103022662916,
      "grad_norm": 1.7179391384124756,
      "learning_rate": 0.0001884089130291286,
      "loss": 0.1503,
      "step": 12792
    },
    {
      "epoch": 0.060175734028241626,
      "grad_norm": 1.8335713148117065,
      "learning_rate": 0.00018840797005101513,
      "loss": 0.3208,
      "step": 12793
    },
    {
      "epoch": 0.06018043782985409,
      "grad_norm": 3.955228567123413,
      "learning_rate": 0.00018840702707290165,
      "loss": 0.4312,
      "step": 12794
    },
    {
      "epoch": 0.06018514163146655,
      "grad_norm": 0.4648847281932831,
      "learning_rate": 0.00018840608409478817,
      "loss": 0.0477,
      "step": 12795
    },
    {
      "epoch": 0.060189845433079016,
      "grad_norm": 0.5649154782295227,
      "learning_rate": 0.00018840514111667468,
      "loss": 0.0724,
      "step": 12796
    },
    {
      "epoch": 0.060194549234691475,
      "grad_norm": 2.1188056468963623,
      "learning_rate": 0.0001884041981385612,
      "loss": 0.5685,
      "step": 12797
    },
    {
      "epoch": 0.06019925303630394,
      "grad_norm": 1.5882679224014282,
      "learning_rate": 0.00018840325516044775,
      "loss": 0.3037,
      "step": 12798
    },
    {
      "epoch": 0.060203956837916406,
      "grad_norm": 2.052617311477661,
      "learning_rate": 0.00018840231218233427,
      "loss": 0.3925,
      "step": 12799
    },
    {
      "epoch": 0.060208660639528865,
      "grad_norm": 0.38148170709609985,
      "learning_rate": 0.00018840136920422079,
      "loss": 0.0285,
      "step": 12800
    },
    {
      "epoch": 0.06021336444114133,
      "grad_norm": 1.5599066019058228,
      "learning_rate": 0.0001884004262261073,
      "loss": 0.1464,
      "step": 12801
    },
    {
      "epoch": 0.060218068242753796,
      "grad_norm": 2.8552329540252686,
      "learning_rate": 0.00018839948324799382,
      "loss": 0.5455,
      "step": 12802
    },
    {
      "epoch": 0.060222772044366255,
      "grad_norm": 1.3718817234039307,
      "learning_rate": 0.00018839854026988034,
      "loss": 0.1075,
      "step": 12803
    },
    {
      "epoch": 0.06022747584597872,
      "grad_norm": 2.505622386932373,
      "learning_rate": 0.00018839759729176686,
      "loss": 0.2281,
      "step": 12804
    },
    {
      "epoch": 0.060232179647591186,
      "grad_norm": 2.453205108642578,
      "learning_rate": 0.00018839665431365338,
      "loss": 0.1737,
      "step": 12805
    },
    {
      "epoch": 0.060236883449203645,
      "grad_norm": 2.9828782081604004,
      "learning_rate": 0.0001883957113355399,
      "loss": 0.2716,
      "step": 12806
    },
    {
      "epoch": 0.06024158725081611,
      "grad_norm": 3.4696953296661377,
      "learning_rate": 0.00018839476835742644,
      "loss": 0.3815,
      "step": 12807
    },
    {
      "epoch": 0.060246291052428576,
      "grad_norm": 4.391715049743652,
      "learning_rate": 0.00018839382537931296,
      "loss": 0.4061,
      "step": 12808
    },
    {
      "epoch": 0.060250994854041034,
      "grad_norm": 2.4144647121429443,
      "learning_rate": 0.00018839288240119948,
      "loss": 0.1937,
      "step": 12809
    },
    {
      "epoch": 0.0602556986556535,
      "grad_norm": 2.4493398666381836,
      "learning_rate": 0.000188391939423086,
      "loss": 0.2213,
      "step": 12810
    },
    {
      "epoch": 0.060260402457265966,
      "grad_norm": 3.5534536838531494,
      "learning_rate": 0.00018839099644497252,
      "loss": 0.6793,
      "step": 12811
    },
    {
      "epoch": 0.060265106258878424,
      "grad_norm": 0.248458132147789,
      "learning_rate": 0.00018839005346685906,
      "loss": 0.0167,
      "step": 12812
    },
    {
      "epoch": 0.06026981006049089,
      "grad_norm": 1.866674542427063,
      "learning_rate": 0.00018838911048874556,
      "loss": 0.1343,
      "step": 12813
    },
    {
      "epoch": 0.06027451386210335,
      "grad_norm": 1.7893121242523193,
      "learning_rate": 0.00018838816751063207,
      "loss": 0.2169,
      "step": 12814
    },
    {
      "epoch": 0.060279217663715814,
      "grad_norm": 2.2269153594970703,
      "learning_rate": 0.0001883872245325186,
      "loss": 0.3508,
      "step": 12815
    },
    {
      "epoch": 0.06028392146532828,
      "grad_norm": 1.9318616390228271,
      "learning_rate": 0.00018838628155440514,
      "loss": 0.2678,
      "step": 12816
    },
    {
      "epoch": 0.06028862526694074,
      "grad_norm": 3.645207405090332,
      "learning_rate": 0.00018838533857629166,
      "loss": 0.2399,
      "step": 12817
    },
    {
      "epoch": 0.060293329068553204,
      "grad_norm": 1.0439987182617188,
      "learning_rate": 0.00018838439559817818,
      "loss": 0.0605,
      "step": 12818
    },
    {
      "epoch": 0.06029803287016567,
      "grad_norm": 0.7261905074119568,
      "learning_rate": 0.0001883834526200647,
      "loss": 0.0616,
      "step": 12819
    },
    {
      "epoch": 0.06030273667177813,
      "grad_norm": 0.28330355882644653,
      "learning_rate": 0.0001883825096419512,
      "loss": 0.0202,
      "step": 12820
    },
    {
      "epoch": 0.060307440473390594,
      "grad_norm": 3.2157931327819824,
      "learning_rate": 0.00018838156666383776,
      "loss": 0.3905,
      "step": 12821
    },
    {
      "epoch": 0.06031214427500306,
      "grad_norm": 0.1461688131093979,
      "learning_rate": 0.00018838062368572428,
      "loss": 0.0093,
      "step": 12822
    },
    {
      "epoch": 0.06031684807661552,
      "grad_norm": 0.36417242884635925,
      "learning_rate": 0.0001883796807076108,
      "loss": 0.0247,
      "step": 12823
    },
    {
      "epoch": 0.060321551878227984,
      "grad_norm": 3.663294553756714,
      "learning_rate": 0.00018837873772949731,
      "loss": 0.4288,
      "step": 12824
    },
    {
      "epoch": 0.06032625567984045,
      "grad_norm": 2.714956045150757,
      "learning_rate": 0.00018837779475138383,
      "loss": 0.6312,
      "step": 12825
    },
    {
      "epoch": 0.06033095948145291,
      "grad_norm": 0.8934177160263062,
      "learning_rate": 0.00018837685177327035,
      "loss": 0.0642,
      "step": 12826
    },
    {
      "epoch": 0.060335663283065374,
      "grad_norm": 5.305839538574219,
      "learning_rate": 0.00018837590879515687,
      "loss": 0.6985,
      "step": 12827
    },
    {
      "epoch": 0.06034036708467784,
      "grad_norm": 0.26437535881996155,
      "learning_rate": 0.0001883749658170434,
      "loss": 0.018,
      "step": 12828
    },
    {
      "epoch": 0.0603450708862903,
      "grad_norm": 2.3752994537353516,
      "learning_rate": 0.0001883740228389299,
      "loss": 0.3476,
      "step": 12829
    },
    {
      "epoch": 0.060349774687902764,
      "grad_norm": 0.2934068441390991,
      "learning_rate": 0.00018837307986081645,
      "loss": 0.0314,
      "step": 12830
    },
    {
      "epoch": 0.06035447848951522,
      "grad_norm": 3.517840623855591,
      "learning_rate": 0.00018837213688270297,
      "loss": 0.3069,
      "step": 12831
    },
    {
      "epoch": 0.06035918229112769,
      "grad_norm": 2.0846362113952637,
      "learning_rate": 0.0001883711939045895,
      "loss": 0.2747,
      "step": 12832
    },
    {
      "epoch": 0.060363886092740154,
      "grad_norm": 1.3445547819137573,
      "learning_rate": 0.000188370250926476,
      "loss": 0.1289,
      "step": 12833
    },
    {
      "epoch": 0.06036858989435261,
      "grad_norm": 1.3774974346160889,
      "learning_rate": 0.00018836930794836253,
      "loss": 0.1391,
      "step": 12834
    },
    {
      "epoch": 0.06037329369596508,
      "grad_norm": 2.03518009185791,
      "learning_rate": 0.00018836836497024905,
      "loss": 0.2193,
      "step": 12835
    },
    {
      "epoch": 0.060377997497577544,
      "grad_norm": 4.561068058013916,
      "learning_rate": 0.00018836742199213557,
      "loss": 0.8411,
      "step": 12836
    },
    {
      "epoch": 0.06038270129919,
      "grad_norm": 0.812602698802948,
      "learning_rate": 0.00018836647901402208,
      "loss": 0.0812,
      "step": 12837
    },
    {
      "epoch": 0.06038740510080247,
      "grad_norm": 2.1699044704437256,
      "learning_rate": 0.0001883655360359086,
      "loss": 0.3201,
      "step": 12838
    },
    {
      "epoch": 0.060392108902414934,
      "grad_norm": 2.94317364692688,
      "learning_rate": 0.00018836459305779515,
      "loss": 0.5942,
      "step": 12839
    },
    {
      "epoch": 0.06039681270402739,
      "grad_norm": 2.9989547729492188,
      "learning_rate": 0.00018836365007968167,
      "loss": 0.4373,
      "step": 12840
    },
    {
      "epoch": 0.06040151650563986,
      "grad_norm": 1.556107997894287,
      "learning_rate": 0.00018836270710156819,
      "loss": 0.0872,
      "step": 12841
    },
    {
      "epoch": 0.060406220307252324,
      "grad_norm": 4.00591516494751,
      "learning_rate": 0.0001883617641234547,
      "loss": 0.9365,
      "step": 12842
    },
    {
      "epoch": 0.06041092410886478,
      "grad_norm": 2.0001070499420166,
      "learning_rate": 0.00018836082114534122,
      "loss": 0.206,
      "step": 12843
    },
    {
      "epoch": 0.06041562791047725,
      "grad_norm": 7.8229780197143555,
      "learning_rate": 0.00018835987816722774,
      "loss": 0.8893,
      "step": 12844
    },
    {
      "epoch": 0.060420331712089714,
      "grad_norm": 2.947040319442749,
      "learning_rate": 0.00018835893518911426,
      "loss": 0.3423,
      "step": 12845
    },
    {
      "epoch": 0.06042503551370217,
      "grad_norm": 0.9329938292503357,
      "learning_rate": 0.00018835799221100078,
      "loss": 0.0887,
      "step": 12846
    },
    {
      "epoch": 0.06042973931531464,
      "grad_norm": 1.018934965133667,
      "learning_rate": 0.0001883570492328873,
      "loss": 0.1008,
      "step": 12847
    },
    {
      "epoch": 0.0604344431169271,
      "grad_norm": 2.350389003753662,
      "learning_rate": 0.00018835610625477384,
      "loss": 0.5974,
      "step": 12848
    },
    {
      "epoch": 0.06043914691853956,
      "grad_norm": 0.8292126059532166,
      "learning_rate": 0.00018835516327666036,
      "loss": 0.0625,
      "step": 12849
    },
    {
      "epoch": 0.06044385072015203,
      "grad_norm": 0.6850141286849976,
      "learning_rate": 0.00018835422029854688,
      "loss": 0.0597,
      "step": 12850
    },
    {
      "epoch": 0.06044855452176449,
      "grad_norm": 7.082034111022949,
      "learning_rate": 0.0001883532773204334,
      "loss": 0.5754,
      "step": 12851
    },
    {
      "epoch": 0.06045325832337695,
      "grad_norm": 3.542100191116333,
      "learning_rate": 0.00018835233434231992,
      "loss": 0.3556,
      "step": 12852
    },
    {
      "epoch": 0.06045796212498942,
      "grad_norm": 2.8968613147735596,
      "learning_rate": 0.00018835139136420646,
      "loss": 0.1292,
      "step": 12853
    },
    {
      "epoch": 0.06046266592660188,
      "grad_norm": 1.0777745246887207,
      "learning_rate": 0.00018835044838609298,
      "loss": 0.1315,
      "step": 12854
    },
    {
      "epoch": 0.06046736972821434,
      "grad_norm": 1.7015774250030518,
      "learning_rate": 0.0001883495054079795,
      "loss": 0.1207,
      "step": 12855
    },
    {
      "epoch": 0.06047207352982681,
      "grad_norm": 4.702974796295166,
      "learning_rate": 0.000188348562429866,
      "loss": 0.5964,
      "step": 12856
    },
    {
      "epoch": 0.06047677733143927,
      "grad_norm": 2.752331256866455,
      "learning_rate": 0.00018834761945175254,
      "loss": 0.1946,
      "step": 12857
    },
    {
      "epoch": 0.06048148113305173,
      "grad_norm": 1.409499168395996,
      "learning_rate": 0.00018834667647363906,
      "loss": 0.1169,
      "step": 12858
    },
    {
      "epoch": 0.0604861849346642,
      "grad_norm": 0.5428085923194885,
      "learning_rate": 0.00018834573349552558,
      "loss": 0.0397,
      "step": 12859
    },
    {
      "epoch": 0.06049088873627666,
      "grad_norm": 5.563494682312012,
      "learning_rate": 0.0001883447905174121,
      "loss": 0.4411,
      "step": 12860
    },
    {
      "epoch": 0.06049559253788912,
      "grad_norm": 2.359492301940918,
      "learning_rate": 0.0001883438475392986,
      "loss": 0.2627,
      "step": 12861
    },
    {
      "epoch": 0.06050029633950159,
      "grad_norm": 1.9228925704956055,
      "learning_rate": 0.00018834290456118516,
      "loss": 0.1664,
      "step": 12862
    },
    {
      "epoch": 0.06050500014111405,
      "grad_norm": 3.7007522583007812,
      "learning_rate": 0.00018834196158307168,
      "loss": 0.4239,
      "step": 12863
    },
    {
      "epoch": 0.06050970394272651,
      "grad_norm": 2.6899163722991943,
      "learning_rate": 0.0001883410186049582,
      "loss": 0.2191,
      "step": 12864
    },
    {
      "epoch": 0.06051440774433897,
      "grad_norm": 1.1042578220367432,
      "learning_rate": 0.00018834007562684471,
      "loss": 0.0892,
      "step": 12865
    },
    {
      "epoch": 0.06051911154595144,
      "grad_norm": 2.512214183807373,
      "learning_rate": 0.00018833913264873123,
      "loss": 0.2583,
      "step": 12866
    },
    {
      "epoch": 0.0605238153475639,
      "grad_norm": 2.9037744998931885,
      "learning_rate": 0.00018833818967061775,
      "loss": 0.1566,
      "step": 12867
    },
    {
      "epoch": 0.06052851914917636,
      "grad_norm": 2.879669666290283,
      "learning_rate": 0.00018833724669250427,
      "loss": 0.3613,
      "step": 12868
    },
    {
      "epoch": 0.06053322295078883,
      "grad_norm": 2.261528730392456,
      "learning_rate": 0.0001883363037143908,
      "loss": 0.1809,
      "step": 12869
    },
    {
      "epoch": 0.06053792675240129,
      "grad_norm": 3.570694923400879,
      "learning_rate": 0.0001883353607362773,
      "loss": 0.3629,
      "step": 12870
    },
    {
      "epoch": 0.06054263055401375,
      "grad_norm": 3.8229587078094482,
      "learning_rate": 0.00018833441775816385,
      "loss": 0.2265,
      "step": 12871
    },
    {
      "epoch": 0.06054733435562622,
      "grad_norm": 3.977640151977539,
      "learning_rate": 0.00018833347478005037,
      "loss": 0.8416,
      "step": 12872
    },
    {
      "epoch": 0.06055203815723868,
      "grad_norm": 4.514009952545166,
      "learning_rate": 0.0001883325318019369,
      "loss": 0.7891,
      "step": 12873
    },
    {
      "epoch": 0.06055674195885114,
      "grad_norm": 1.3929036855697632,
      "learning_rate": 0.0001883315888238234,
      "loss": 0.1428,
      "step": 12874
    },
    {
      "epoch": 0.06056144576046361,
      "grad_norm": 0.9758620262145996,
      "learning_rate": 0.00018833064584570993,
      "loss": 0.0488,
      "step": 12875
    },
    {
      "epoch": 0.06056614956207607,
      "grad_norm": 2.7676093578338623,
      "learning_rate": 0.00018832970286759645,
      "loss": 0.3954,
      "step": 12876
    },
    {
      "epoch": 0.06057085336368853,
      "grad_norm": 1.4371505975723267,
      "learning_rate": 0.00018832875988948297,
      "loss": 0.1427,
      "step": 12877
    },
    {
      "epoch": 0.060575557165301,
      "grad_norm": 1.6959123611450195,
      "learning_rate": 0.00018832781691136948,
      "loss": 0.1499,
      "step": 12878
    },
    {
      "epoch": 0.06058026096691346,
      "grad_norm": 0.9375878572463989,
      "learning_rate": 0.000188326873933256,
      "loss": 0.0632,
      "step": 12879
    },
    {
      "epoch": 0.06058496476852592,
      "grad_norm": 4.435857772827148,
      "learning_rate": 0.00018832593095514255,
      "loss": 0.4737,
      "step": 12880
    },
    {
      "epoch": 0.06058966857013839,
      "grad_norm": 0.9932181239128113,
      "learning_rate": 0.00018832498797702907,
      "loss": 0.082,
      "step": 12881
    },
    {
      "epoch": 0.060594372371750846,
      "grad_norm": 4.957139492034912,
      "learning_rate": 0.00018832404499891559,
      "loss": 0.2069,
      "step": 12882
    },
    {
      "epoch": 0.06059907617336331,
      "grad_norm": 0.7308370471000671,
      "learning_rate": 0.0001883231020208021,
      "loss": 0.083,
      "step": 12883
    },
    {
      "epoch": 0.06060377997497578,
      "grad_norm": 1.3555737733840942,
      "learning_rate": 0.00018832215904268862,
      "loss": 0.1497,
      "step": 12884
    },
    {
      "epoch": 0.060608483776588236,
      "grad_norm": 4.127061367034912,
      "learning_rate": 0.00018832121606457517,
      "loss": 0.6287,
      "step": 12885
    },
    {
      "epoch": 0.0606131875782007,
      "grad_norm": 4.121133804321289,
      "learning_rate": 0.0001883202730864617,
      "loss": 0.4598,
      "step": 12886
    },
    {
      "epoch": 0.06061789137981317,
      "grad_norm": 0.594369649887085,
      "learning_rate": 0.00018831933010834818,
      "loss": 0.0558,
      "step": 12887
    },
    {
      "epoch": 0.060622595181425626,
      "grad_norm": 0.5313152074813843,
      "learning_rate": 0.0001883183871302347,
      "loss": 0.0335,
      "step": 12888
    },
    {
      "epoch": 0.06062729898303809,
      "grad_norm": 2.5631754398345947,
      "learning_rate": 0.00018831744415212124,
      "loss": 0.318,
      "step": 12889
    },
    {
      "epoch": 0.06063200278465056,
      "grad_norm": 0.16835543513298035,
      "learning_rate": 0.00018831650117400776,
      "loss": 0.0093,
      "step": 12890
    },
    {
      "epoch": 0.060636706586263016,
      "grad_norm": 1.4845753908157349,
      "learning_rate": 0.00018831555819589428,
      "loss": 0.1043,
      "step": 12891
    },
    {
      "epoch": 0.06064141038787548,
      "grad_norm": 1.63716459274292,
      "learning_rate": 0.0001883146152177808,
      "loss": 0.151,
      "step": 12892
    },
    {
      "epoch": 0.06064611418948795,
      "grad_norm": 2.2870233058929443,
      "learning_rate": 0.00018831367223966732,
      "loss": 0.3358,
      "step": 12893
    },
    {
      "epoch": 0.060650817991100406,
      "grad_norm": 3.412627696990967,
      "learning_rate": 0.00018831272926155386,
      "loss": 0.5204,
      "step": 12894
    },
    {
      "epoch": 0.06065552179271287,
      "grad_norm": 0.5167675614356995,
      "learning_rate": 0.00018831178628344038,
      "loss": 0.0345,
      "step": 12895
    },
    {
      "epoch": 0.06066022559432534,
      "grad_norm": 0.6182553768157959,
      "learning_rate": 0.0001883108433053269,
      "loss": 0.0448,
      "step": 12896
    },
    {
      "epoch": 0.060664929395937796,
      "grad_norm": 4.262756824493408,
      "learning_rate": 0.00018830990032721342,
      "loss": 0.7056,
      "step": 12897
    },
    {
      "epoch": 0.06066963319755026,
      "grad_norm": 0.8344531655311584,
      "learning_rate": 0.00018830895734909994,
      "loss": 0.056,
      "step": 12898
    },
    {
      "epoch": 0.06067433699916272,
      "grad_norm": 4.205057621002197,
      "learning_rate": 0.00018830801437098646,
      "loss": 0.7724,
      "step": 12899
    },
    {
      "epoch": 0.060679040800775186,
      "grad_norm": 2.4834160804748535,
      "learning_rate": 0.00018830707139287298,
      "loss": 0.2506,
      "step": 12900
    },
    {
      "epoch": 0.06068374460238765,
      "grad_norm": 4.198810577392578,
      "learning_rate": 0.0001883061284147595,
      "loss": 0.5876,
      "step": 12901
    },
    {
      "epoch": 0.06068844840400011,
      "grad_norm": 2.51078462600708,
      "learning_rate": 0.000188305185436646,
      "loss": 0.2049,
      "step": 12902
    },
    {
      "epoch": 0.060693152205612576,
      "grad_norm": 1.6654337644577026,
      "learning_rate": 0.00018830424245853256,
      "loss": 0.0879,
      "step": 12903
    },
    {
      "epoch": 0.06069785600722504,
      "grad_norm": 4.325108051300049,
      "learning_rate": 0.00018830329948041908,
      "loss": 0.1713,
      "step": 12904
    },
    {
      "epoch": 0.0607025598088375,
      "grad_norm": 1.069058895111084,
      "learning_rate": 0.0001883023565023056,
      "loss": 0.064,
      "step": 12905
    },
    {
      "epoch": 0.060707263610449966,
      "grad_norm": 3.1362714767456055,
      "learning_rate": 0.00018830141352419211,
      "loss": 0.2642,
      "step": 12906
    },
    {
      "epoch": 0.06071196741206243,
      "grad_norm": 5.2119927406311035,
      "learning_rate": 0.00018830047054607863,
      "loss": 0.8938,
      "step": 12907
    },
    {
      "epoch": 0.06071667121367489,
      "grad_norm": 1.6553984880447388,
      "learning_rate": 0.00018829952756796515,
      "loss": 0.1891,
      "step": 12908
    },
    {
      "epoch": 0.060721375015287356,
      "grad_norm": 1.9164371490478516,
      "learning_rate": 0.00018829858458985167,
      "loss": 0.1648,
      "step": 12909
    },
    {
      "epoch": 0.06072607881689982,
      "grad_norm": 1.356744647026062,
      "learning_rate": 0.0001882976416117382,
      "loss": 0.102,
      "step": 12910
    },
    {
      "epoch": 0.06073078261851228,
      "grad_norm": 3.808565616607666,
      "learning_rate": 0.0001882966986336247,
      "loss": 0.2387,
      "step": 12911
    },
    {
      "epoch": 0.060735486420124746,
      "grad_norm": 3.1060402393341064,
      "learning_rate": 0.00018829575565551125,
      "loss": 0.2722,
      "step": 12912
    },
    {
      "epoch": 0.06074019022173721,
      "grad_norm": 2.613701343536377,
      "learning_rate": 0.00018829481267739777,
      "loss": 0.2542,
      "step": 12913
    },
    {
      "epoch": 0.06074489402334967,
      "grad_norm": 3.0070958137512207,
      "learning_rate": 0.0001882938696992843,
      "loss": 0.4352,
      "step": 12914
    },
    {
      "epoch": 0.060749597824962136,
      "grad_norm": 4.682122707366943,
      "learning_rate": 0.0001882929267211708,
      "loss": 1.1036,
      "step": 12915
    },
    {
      "epoch": 0.060754301626574594,
      "grad_norm": 0.43898260593414307,
      "learning_rate": 0.00018829198374305736,
      "loss": 0.0304,
      "step": 12916
    },
    {
      "epoch": 0.06075900542818706,
      "grad_norm": 2.89284086227417,
      "learning_rate": 0.00018829104076494387,
      "loss": 0.6154,
      "step": 12917
    },
    {
      "epoch": 0.060763709229799526,
      "grad_norm": 0.8512396812438965,
      "learning_rate": 0.00018829009778683037,
      "loss": 0.0608,
      "step": 12918
    },
    {
      "epoch": 0.060768413031411984,
      "grad_norm": 1.5134508609771729,
      "learning_rate": 0.00018828915480871688,
      "loss": 0.1237,
      "step": 12919
    },
    {
      "epoch": 0.06077311683302445,
      "grad_norm": 2.120695114135742,
      "learning_rate": 0.0001882882118306034,
      "loss": 0.1839,
      "step": 12920
    },
    {
      "epoch": 0.060777820634636916,
      "grad_norm": 2.187063455581665,
      "learning_rate": 0.00018828726885248995,
      "loss": 0.2701,
      "step": 12921
    },
    {
      "epoch": 0.060782524436249374,
      "grad_norm": 1.49308443069458,
      "learning_rate": 0.00018828632587437647,
      "loss": 0.1315,
      "step": 12922
    },
    {
      "epoch": 0.06078722823786184,
      "grad_norm": 2.4661121368408203,
      "learning_rate": 0.00018828538289626299,
      "loss": 0.2226,
      "step": 12923
    },
    {
      "epoch": 0.060791932039474306,
      "grad_norm": 2.3814122676849365,
      "learning_rate": 0.0001882844399181495,
      "loss": 0.1831,
      "step": 12924
    },
    {
      "epoch": 0.060796635841086764,
      "grad_norm": 1.2207261323928833,
      "learning_rate": 0.00018828349694003605,
      "loss": 0.142,
      "step": 12925
    },
    {
      "epoch": 0.06080133964269923,
      "grad_norm": 3.7516286373138428,
      "learning_rate": 0.00018828255396192257,
      "loss": 0.6389,
      "step": 12926
    },
    {
      "epoch": 0.060806043444311696,
      "grad_norm": 2.742055892944336,
      "learning_rate": 0.0001882816109838091,
      "loss": 0.2013,
      "step": 12927
    },
    {
      "epoch": 0.060810747245924154,
      "grad_norm": 1.9515069723129272,
      "learning_rate": 0.0001882806680056956,
      "loss": 0.145,
      "step": 12928
    },
    {
      "epoch": 0.06081545104753662,
      "grad_norm": 2.1306257247924805,
      "learning_rate": 0.0001882797250275821,
      "loss": 0.4433,
      "step": 12929
    },
    {
      "epoch": 0.060820154849149086,
      "grad_norm": 2.624080181121826,
      "learning_rate": 0.00018827878204946864,
      "loss": 0.5728,
      "step": 12930
    },
    {
      "epoch": 0.060824858650761544,
      "grad_norm": 1.3832329511642456,
      "learning_rate": 0.00018827783907135516,
      "loss": 0.1581,
      "step": 12931
    },
    {
      "epoch": 0.06082956245237401,
      "grad_norm": 0.2590300142765045,
      "learning_rate": 0.00018827689609324168,
      "loss": 0.0142,
      "step": 12932
    },
    {
      "epoch": 0.06083426625398647,
      "grad_norm": 0.9011699557304382,
      "learning_rate": 0.0001882759531151282,
      "loss": 0.0657,
      "step": 12933
    },
    {
      "epoch": 0.060838970055598934,
      "grad_norm": 1.5340451002120972,
      "learning_rate": 0.00018827501013701472,
      "loss": 0.1903,
      "step": 12934
    },
    {
      "epoch": 0.0608436738572114,
      "grad_norm": 2.508273124694824,
      "learning_rate": 0.00018827406715890126,
      "loss": 0.2939,
      "step": 12935
    },
    {
      "epoch": 0.06084837765882386,
      "grad_norm": 2.665525197982788,
      "learning_rate": 0.00018827312418078778,
      "loss": 0.4232,
      "step": 12936
    },
    {
      "epoch": 0.060853081460436324,
      "grad_norm": 3.056889295578003,
      "learning_rate": 0.0001882721812026743,
      "loss": 0.6938,
      "step": 12937
    },
    {
      "epoch": 0.06085778526204879,
      "grad_norm": 2.761530637741089,
      "learning_rate": 0.00018827123822456082,
      "loss": 0.364,
      "step": 12938
    },
    {
      "epoch": 0.06086248906366125,
      "grad_norm": 2.2034096717834473,
      "learning_rate": 0.00018827029524644734,
      "loss": 0.2522,
      "step": 12939
    },
    {
      "epoch": 0.060867192865273714,
      "grad_norm": 1.6029289960861206,
      "learning_rate": 0.00018826935226833386,
      "loss": 0.2946,
      "step": 12940
    },
    {
      "epoch": 0.06087189666688618,
      "grad_norm": 3.5720276832580566,
      "learning_rate": 0.00018826840929022038,
      "loss": 0.7872,
      "step": 12941
    },
    {
      "epoch": 0.06087660046849864,
      "grad_norm": 1.613085389137268,
      "learning_rate": 0.0001882674663121069,
      "loss": 0.2739,
      "step": 12942
    },
    {
      "epoch": 0.060881304270111104,
      "grad_norm": 3.111520528793335,
      "learning_rate": 0.0001882665233339934,
      "loss": 0.3913,
      "step": 12943
    },
    {
      "epoch": 0.06088600807172357,
      "grad_norm": 2.8919565677642822,
      "learning_rate": 0.00018826558035587996,
      "loss": 0.1542,
      "step": 12944
    },
    {
      "epoch": 0.06089071187333603,
      "grad_norm": 1.0068793296813965,
      "learning_rate": 0.00018826463737776648,
      "loss": 0.1451,
      "step": 12945
    },
    {
      "epoch": 0.060895415674948494,
      "grad_norm": 0.33636730909347534,
      "learning_rate": 0.000188263694399653,
      "loss": 0.0267,
      "step": 12946
    },
    {
      "epoch": 0.06090011947656096,
      "grad_norm": 2.427097797393799,
      "learning_rate": 0.00018826275142153951,
      "loss": 0.3555,
      "step": 12947
    },
    {
      "epoch": 0.06090482327817342,
      "grad_norm": 1.9851455688476562,
      "learning_rate": 0.00018826180844342603,
      "loss": 0.3086,
      "step": 12948
    },
    {
      "epoch": 0.060909527079785884,
      "grad_norm": 3.974294900894165,
      "learning_rate": 0.00018826086546531255,
      "loss": 0.6402,
      "step": 12949
    },
    {
      "epoch": 0.06091423088139834,
      "grad_norm": 0.8748442530632019,
      "learning_rate": 0.00018825992248719907,
      "loss": 0.135,
      "step": 12950
    },
    {
      "epoch": 0.06091893468301081,
      "grad_norm": 0.9449225068092346,
      "learning_rate": 0.0001882589795090856,
      "loss": 0.0631,
      "step": 12951
    },
    {
      "epoch": 0.060923638484623274,
      "grad_norm": 1.0483677387237549,
      "learning_rate": 0.0001882580365309721,
      "loss": 0.0836,
      "step": 12952
    },
    {
      "epoch": 0.06092834228623573,
      "grad_norm": 3.6337850093841553,
      "learning_rate": 0.00018825709355285865,
      "loss": 0.1399,
      "step": 12953
    },
    {
      "epoch": 0.0609330460878482,
      "grad_norm": 4.157952785491943,
      "learning_rate": 0.00018825615057474517,
      "loss": 0.4766,
      "step": 12954
    },
    {
      "epoch": 0.060937749889460664,
      "grad_norm": 1.3364953994750977,
      "learning_rate": 0.0001882552075966317,
      "loss": 0.1225,
      "step": 12955
    },
    {
      "epoch": 0.06094245369107312,
      "grad_norm": 3.0077548027038574,
      "learning_rate": 0.0001882542646185182,
      "loss": 0.305,
      "step": 12956
    },
    {
      "epoch": 0.06094715749268559,
      "grad_norm": 0.5654743909835815,
      "learning_rate": 0.00018825332164040475,
      "loss": 0.0588,
      "step": 12957
    },
    {
      "epoch": 0.060951861294298054,
      "grad_norm": 1.1454851627349854,
      "learning_rate": 0.00018825237866229127,
      "loss": 0.1559,
      "step": 12958
    },
    {
      "epoch": 0.06095656509591051,
      "grad_norm": 2.3173813819885254,
      "learning_rate": 0.0001882514356841778,
      "loss": 0.2456,
      "step": 12959
    },
    {
      "epoch": 0.06096126889752298,
      "grad_norm": 1.4011591672897339,
      "learning_rate": 0.00018825049270606428,
      "loss": 0.1454,
      "step": 12960
    },
    {
      "epoch": 0.060965972699135444,
      "grad_norm": 0.5884371995925903,
      "learning_rate": 0.0001882495497279508,
      "loss": 0.0604,
      "step": 12961
    },
    {
      "epoch": 0.0609706765007479,
      "grad_norm": 3.0010287761688232,
      "learning_rate": 0.00018824860674983735,
      "loss": 0.669,
      "step": 12962
    },
    {
      "epoch": 0.06097538030236037,
      "grad_norm": 1.4393984079360962,
      "learning_rate": 0.00018824766377172387,
      "loss": 0.1184,
      "step": 12963
    },
    {
      "epoch": 0.060980084103972834,
      "grad_norm": 2.667220115661621,
      "learning_rate": 0.00018824672079361039,
      "loss": 0.3503,
      "step": 12964
    },
    {
      "epoch": 0.06098478790558529,
      "grad_norm": 2.469805955886841,
      "learning_rate": 0.0001882457778154969,
      "loss": 0.2759,
      "step": 12965
    },
    {
      "epoch": 0.06098949170719776,
      "grad_norm": 1.7375969886779785,
      "learning_rate": 0.00018824483483738345,
      "loss": 0.1974,
      "step": 12966
    },
    {
      "epoch": 0.06099419550881022,
      "grad_norm": 0.923833429813385,
      "learning_rate": 0.00018824389185926997,
      "loss": 0.0671,
      "step": 12967
    },
    {
      "epoch": 0.06099889931042268,
      "grad_norm": 1.4501639604568481,
      "learning_rate": 0.0001882429488811565,
      "loss": 0.1232,
      "step": 12968
    },
    {
      "epoch": 0.06100360311203515,
      "grad_norm": 2.553368330001831,
      "learning_rate": 0.000188242005903043,
      "loss": 0.3908,
      "step": 12969
    },
    {
      "epoch": 0.06100830691364761,
      "grad_norm": 8.683504104614258,
      "learning_rate": 0.00018824106292492952,
      "loss": 0.5257,
      "step": 12970
    },
    {
      "epoch": 0.06101301071526007,
      "grad_norm": 3.371664047241211,
      "learning_rate": 0.00018824011994681604,
      "loss": 0.8565,
      "step": 12971
    },
    {
      "epoch": 0.06101771451687254,
      "grad_norm": 2.3836629390716553,
      "learning_rate": 0.00018823917696870256,
      "loss": 0.3314,
      "step": 12972
    },
    {
      "epoch": 0.061022418318485,
      "grad_norm": 2.9069526195526123,
      "learning_rate": 0.00018823823399058908,
      "loss": 0.6255,
      "step": 12973
    },
    {
      "epoch": 0.06102712212009746,
      "grad_norm": 4.464652061462402,
      "learning_rate": 0.0001882372910124756,
      "loss": 0.7835,
      "step": 12974
    },
    {
      "epoch": 0.06103182592170993,
      "grad_norm": 0.16633343696594238,
      "learning_rate": 0.00018823634803436214,
      "loss": 0.0109,
      "step": 12975
    },
    {
      "epoch": 0.06103652972332239,
      "grad_norm": 2.0442938804626465,
      "learning_rate": 0.00018823540505624866,
      "loss": 0.2215,
      "step": 12976
    },
    {
      "epoch": 0.06104123352493485,
      "grad_norm": 0.44921043515205383,
      "learning_rate": 0.00018823446207813518,
      "loss": 0.032,
      "step": 12977
    },
    {
      "epoch": 0.06104593732654732,
      "grad_norm": 1.0040138959884644,
      "learning_rate": 0.0001882335191000217,
      "loss": 0.1271,
      "step": 12978
    },
    {
      "epoch": 0.06105064112815978,
      "grad_norm": 1.4106758832931519,
      "learning_rate": 0.00018823257612190822,
      "loss": 0.1826,
      "step": 12979
    },
    {
      "epoch": 0.06105534492977224,
      "grad_norm": 1.1858209371566772,
      "learning_rate": 0.00018823163314379474,
      "loss": 0.1066,
      "step": 12980
    },
    {
      "epoch": 0.06106004873138471,
      "grad_norm": 2.58906888961792,
      "learning_rate": 0.00018823069016568126,
      "loss": 0.5465,
      "step": 12981
    },
    {
      "epoch": 0.06106475253299717,
      "grad_norm": 0.5475791692733765,
      "learning_rate": 0.00018822974718756778,
      "loss": 0.0371,
      "step": 12982
    },
    {
      "epoch": 0.06106945633460963,
      "grad_norm": 1.3243435621261597,
      "learning_rate": 0.0001882288042094543,
      "loss": 0.1441,
      "step": 12983
    },
    {
      "epoch": 0.06107416013622209,
      "grad_norm": 2.4506380558013916,
      "learning_rate": 0.0001882278612313408,
      "loss": 0.3405,
      "step": 12984
    },
    {
      "epoch": 0.06107886393783456,
      "grad_norm": 2.472569704055786,
      "learning_rate": 0.00018822691825322736,
      "loss": 0.3193,
      "step": 12985
    },
    {
      "epoch": 0.06108356773944702,
      "grad_norm": 1.3102376461029053,
      "learning_rate": 0.00018822597527511388,
      "loss": 0.1303,
      "step": 12986
    },
    {
      "epoch": 0.06108827154105948,
      "grad_norm": 2.0017309188842773,
      "learning_rate": 0.0001882250322970004,
      "loss": 0.1295,
      "step": 12987
    },
    {
      "epoch": 0.06109297534267195,
      "grad_norm": 4.681814670562744,
      "learning_rate": 0.00018822408931888691,
      "loss": 0.6738,
      "step": 12988
    },
    {
      "epoch": 0.06109767914428441,
      "grad_norm": 2.525444507598877,
      "learning_rate": 0.00018822314634077346,
      "loss": 0.3044,
      "step": 12989
    },
    {
      "epoch": 0.06110238294589687,
      "grad_norm": 1.6002752780914307,
      "learning_rate": 0.00018822220336265998,
      "loss": 0.1277,
      "step": 12990
    },
    {
      "epoch": 0.06110708674750934,
      "grad_norm": 1.5420235395431519,
      "learning_rate": 0.00018822126038454647,
      "loss": 0.1425,
      "step": 12991
    },
    {
      "epoch": 0.0611117905491218,
      "grad_norm": 0.624369204044342,
      "learning_rate": 0.000188220317406433,
      "loss": 0.064,
      "step": 12992
    },
    {
      "epoch": 0.06111649435073426,
      "grad_norm": 2.3151004314422607,
      "learning_rate": 0.0001882193744283195,
      "loss": 0.3193,
      "step": 12993
    },
    {
      "epoch": 0.06112119815234673,
      "grad_norm": 2.2092528343200684,
      "learning_rate": 0.00018821843145020605,
      "loss": 0.2951,
      "step": 12994
    },
    {
      "epoch": 0.06112590195395919,
      "grad_norm": 1.4715750217437744,
      "learning_rate": 0.00018821748847209257,
      "loss": 0.1281,
      "step": 12995
    },
    {
      "epoch": 0.06113060575557165,
      "grad_norm": 2.183988094329834,
      "learning_rate": 0.0001882165454939791,
      "loss": 0.4382,
      "step": 12996
    },
    {
      "epoch": 0.06113530955718412,
      "grad_norm": 1.997132420539856,
      "learning_rate": 0.0001882156025158656,
      "loss": 0.1845,
      "step": 12997
    },
    {
      "epoch": 0.06114001335879658,
      "grad_norm": 3.3600447177886963,
      "learning_rate": 0.00018821465953775215,
      "loss": 0.5802,
      "step": 12998
    },
    {
      "epoch": 0.06114471716040904,
      "grad_norm": 2.189783811569214,
      "learning_rate": 0.00018821371655963867,
      "loss": 0.1621,
      "step": 12999
    },
    {
      "epoch": 0.06114942096202151,
      "grad_norm": 1.7580500841140747,
      "learning_rate": 0.0001882127735815252,
      "loss": 0.128,
      "step": 13000
    },
    {
      "epoch": 0.061154124763633966,
      "grad_norm": 2.4031198024749756,
      "learning_rate": 0.0001882118306034117,
      "loss": 0.1205,
      "step": 13001
    },
    {
      "epoch": 0.06115882856524643,
      "grad_norm": 5.721625804901123,
      "learning_rate": 0.0001882108876252982,
      "loss": 0.406,
      "step": 13002
    },
    {
      "epoch": 0.0611635323668589,
      "grad_norm": 3.079707622528076,
      "learning_rate": 0.00018820994464718475,
      "loss": 0.2186,
      "step": 13003
    },
    {
      "epoch": 0.061168236168471356,
      "grad_norm": 4.029735565185547,
      "learning_rate": 0.00018820900166907127,
      "loss": 0.2275,
      "step": 13004
    },
    {
      "epoch": 0.06117293997008382,
      "grad_norm": 0.8249629735946655,
      "learning_rate": 0.00018820805869095779,
      "loss": 0.0561,
      "step": 13005
    },
    {
      "epoch": 0.06117764377169629,
      "grad_norm": 1.6219635009765625,
      "learning_rate": 0.0001882071157128443,
      "loss": 0.1146,
      "step": 13006
    },
    {
      "epoch": 0.061182347573308746,
      "grad_norm": 2.092019557952881,
      "learning_rate": 0.00018820617273473085,
      "loss": 0.3182,
      "step": 13007
    },
    {
      "epoch": 0.06118705137492121,
      "grad_norm": 3.9795989990234375,
      "learning_rate": 0.00018820522975661737,
      "loss": 0.4456,
      "step": 13008
    },
    {
      "epoch": 0.06119175517653368,
      "grad_norm": 2.488744020462036,
      "learning_rate": 0.0001882042867785039,
      "loss": 0.3103,
      "step": 13009
    },
    {
      "epoch": 0.061196458978146136,
      "grad_norm": 1.8247759342193604,
      "learning_rate": 0.0001882033438003904,
      "loss": 0.1642,
      "step": 13010
    },
    {
      "epoch": 0.0612011627797586,
      "grad_norm": 3.746274948120117,
      "learning_rate": 0.00018820240082227692,
      "loss": 0.4659,
      "step": 13011
    },
    {
      "epoch": 0.06120586658137107,
      "grad_norm": 0.7660172581672668,
      "learning_rate": 0.00018820145784416344,
      "loss": 0.068,
      "step": 13012
    },
    {
      "epoch": 0.061210570382983526,
      "grad_norm": 0.4210578501224518,
      "learning_rate": 0.00018820051486604996,
      "loss": 0.0375,
      "step": 13013
    },
    {
      "epoch": 0.06121527418459599,
      "grad_norm": 1.761456847190857,
      "learning_rate": 0.00018819957188793648,
      "loss": 0.1586,
      "step": 13014
    },
    {
      "epoch": 0.06121997798620846,
      "grad_norm": 3.363581895828247,
      "learning_rate": 0.000188198628909823,
      "loss": 0.3888,
      "step": 13015
    },
    {
      "epoch": 0.061224681787820916,
      "grad_norm": 3.3634233474731445,
      "learning_rate": 0.00018819768593170954,
      "loss": 0.3668,
      "step": 13016
    },
    {
      "epoch": 0.06122938558943338,
      "grad_norm": 1.913756012916565,
      "learning_rate": 0.00018819674295359606,
      "loss": 0.4159,
      "step": 13017
    },
    {
      "epoch": 0.06123408939104584,
      "grad_norm": 1.66567063331604,
      "learning_rate": 0.00018819579997548258,
      "loss": 0.1715,
      "step": 13018
    },
    {
      "epoch": 0.061238793192658306,
      "grad_norm": 1.0621509552001953,
      "learning_rate": 0.0001881948569973691,
      "loss": 0.0829,
      "step": 13019
    },
    {
      "epoch": 0.06124349699427077,
      "grad_norm": 4.038447856903076,
      "learning_rate": 0.00018819391401925562,
      "loss": 0.7247,
      "step": 13020
    },
    {
      "epoch": 0.06124820079588323,
      "grad_norm": 2.2837698459625244,
      "learning_rate": 0.00018819297104114217,
      "loss": 0.3584,
      "step": 13021
    },
    {
      "epoch": 0.061252904597495696,
      "grad_norm": 2.80570387840271,
      "learning_rate": 0.00018819202806302866,
      "loss": 0.1805,
      "step": 13022
    },
    {
      "epoch": 0.06125760839910816,
      "grad_norm": 0.6339926719665527,
      "learning_rate": 0.00018819108508491518,
      "loss": 0.0483,
      "step": 13023
    },
    {
      "epoch": 0.06126231220072062,
      "grad_norm": 2.025076389312744,
      "learning_rate": 0.0001881901421068017,
      "loss": 0.1595,
      "step": 13024
    },
    {
      "epoch": 0.061267016002333086,
      "grad_norm": 1.3349977731704712,
      "learning_rate": 0.00018818919912868824,
      "loss": 0.1764,
      "step": 13025
    },
    {
      "epoch": 0.06127171980394555,
      "grad_norm": 2.8879082202911377,
      "learning_rate": 0.00018818825615057476,
      "loss": 0.3315,
      "step": 13026
    },
    {
      "epoch": 0.06127642360555801,
      "grad_norm": 2.6252670288085938,
      "learning_rate": 0.00018818731317246128,
      "loss": 0.589,
      "step": 13027
    },
    {
      "epoch": 0.061281127407170476,
      "grad_norm": 1.662917971611023,
      "learning_rate": 0.0001881863701943478,
      "loss": 0.3184,
      "step": 13028
    },
    {
      "epoch": 0.06128583120878294,
      "grad_norm": 0.5895180702209473,
      "learning_rate": 0.00018818542721623431,
      "loss": 0.0477,
      "step": 13029
    },
    {
      "epoch": 0.0612905350103954,
      "grad_norm": 3.6291468143463135,
      "learning_rate": 0.00018818448423812086,
      "loss": 0.5574,
      "step": 13030
    },
    {
      "epoch": 0.061295238812007866,
      "grad_norm": 1.0990325212478638,
      "learning_rate": 0.00018818354126000738,
      "loss": 0.106,
      "step": 13031
    },
    {
      "epoch": 0.06129994261362033,
      "grad_norm": 2.519949197769165,
      "learning_rate": 0.0001881825982818939,
      "loss": 0.501,
      "step": 13032
    },
    {
      "epoch": 0.06130464641523279,
      "grad_norm": 0.3357335329055786,
      "learning_rate": 0.0001881816553037804,
      "loss": 0.0443,
      "step": 13033
    },
    {
      "epoch": 0.061309350216845256,
      "grad_norm": 1.6629410982131958,
      "learning_rate": 0.00018818071232566693,
      "loss": 0.4003,
      "step": 13034
    },
    {
      "epoch": 0.061314054018457714,
      "grad_norm": 0.9115957617759705,
      "learning_rate": 0.00018817976934755345,
      "loss": 0.136,
      "step": 13035
    },
    {
      "epoch": 0.06131875782007018,
      "grad_norm": 4.322314262390137,
      "learning_rate": 0.00018817882636943997,
      "loss": 1.0171,
      "step": 13036
    },
    {
      "epoch": 0.061323461621682646,
      "grad_norm": 1.450486421585083,
      "learning_rate": 0.0001881778833913265,
      "loss": 0.2594,
      "step": 13037
    },
    {
      "epoch": 0.061328165423295104,
      "grad_norm": 0.16570207476615906,
      "learning_rate": 0.000188176940413213,
      "loss": 0.0114,
      "step": 13038
    },
    {
      "epoch": 0.06133286922490757,
      "grad_norm": 3.147597551345825,
      "learning_rate": 0.00018817599743509955,
      "loss": 0.235,
      "step": 13039
    },
    {
      "epoch": 0.061337573026520036,
      "grad_norm": 1.0841137170791626,
      "learning_rate": 0.00018817505445698607,
      "loss": 0.1897,
      "step": 13040
    },
    {
      "epoch": 0.061342276828132494,
      "grad_norm": 0.37781649827957153,
      "learning_rate": 0.0001881741114788726,
      "loss": 0.0518,
      "step": 13041
    },
    {
      "epoch": 0.06134698062974496,
      "grad_norm": 1.0209200382232666,
      "learning_rate": 0.0001881731685007591,
      "loss": 0.1781,
      "step": 13042
    },
    {
      "epoch": 0.061351684431357426,
      "grad_norm": 4.384637832641602,
      "learning_rate": 0.00018817222552264563,
      "loss": 0.2947,
      "step": 13043
    },
    {
      "epoch": 0.061356388232969884,
      "grad_norm": 1.4897199869155884,
      "learning_rate": 0.00018817128254453215,
      "loss": 0.2627,
      "step": 13044
    },
    {
      "epoch": 0.06136109203458235,
      "grad_norm": 2.3290867805480957,
      "learning_rate": 0.00018817033956641867,
      "loss": 0.2432,
      "step": 13045
    },
    {
      "epoch": 0.061365795836194816,
      "grad_norm": 3.1979153156280518,
      "learning_rate": 0.00018816939658830519,
      "loss": 0.6136,
      "step": 13046
    },
    {
      "epoch": 0.061370499637807274,
      "grad_norm": 0.6386911869049072,
      "learning_rate": 0.0001881684536101917,
      "loss": 0.1026,
      "step": 13047
    },
    {
      "epoch": 0.06137520343941974,
      "grad_norm": 2.8328487873077393,
      "learning_rate": 0.00018816751063207825,
      "loss": 0.1887,
      "step": 13048
    },
    {
      "epoch": 0.061379907241032206,
      "grad_norm": 1.6021349430084229,
      "learning_rate": 0.00018816656765396477,
      "loss": 0.2945,
      "step": 13049
    },
    {
      "epoch": 0.061384611042644664,
      "grad_norm": 1.6634567975997925,
      "learning_rate": 0.0001881656246758513,
      "loss": 0.1237,
      "step": 13050
    },
    {
      "epoch": 0.06138931484425713,
      "grad_norm": 0.9441983699798584,
      "learning_rate": 0.0001881646816977378,
      "loss": 0.0439,
      "step": 13051
    },
    {
      "epoch": 0.06139401864586959,
      "grad_norm": 3.397756338119507,
      "learning_rate": 0.00018816373871962432,
      "loss": 0.2114,
      "step": 13052
    },
    {
      "epoch": 0.061398722447482054,
      "grad_norm": 2.9425013065338135,
      "learning_rate": 0.00018816279574151084,
      "loss": 0.398,
      "step": 13053
    },
    {
      "epoch": 0.06140342624909452,
      "grad_norm": 8.93230152130127,
      "learning_rate": 0.00018816185276339736,
      "loss": 0.933,
      "step": 13054
    },
    {
      "epoch": 0.06140813005070698,
      "grad_norm": 2.393900156021118,
      "learning_rate": 0.00018816090978528388,
      "loss": 0.2177,
      "step": 13055
    },
    {
      "epoch": 0.061412833852319444,
      "grad_norm": 5.261373043060303,
      "learning_rate": 0.0001881599668071704,
      "loss": 0.5524,
      "step": 13056
    },
    {
      "epoch": 0.06141753765393191,
      "grad_norm": 10.869593620300293,
      "learning_rate": 0.00018815902382905694,
      "loss": 0.1313,
      "step": 13057
    },
    {
      "epoch": 0.06142224145554437,
      "grad_norm": 2.5146656036376953,
      "learning_rate": 0.00018815808085094346,
      "loss": 0.1837,
      "step": 13058
    },
    {
      "epoch": 0.061426945257156834,
      "grad_norm": 3.1212356090545654,
      "learning_rate": 0.00018815713787282998,
      "loss": 0.3317,
      "step": 13059
    },
    {
      "epoch": 0.0614316490587693,
      "grad_norm": 3.471693277359009,
      "learning_rate": 0.0001881561948947165,
      "loss": 0.3493,
      "step": 13060
    },
    {
      "epoch": 0.06143635286038176,
      "grad_norm": 3.2740607261657715,
      "learning_rate": 0.00018815525191660302,
      "loss": 0.3329,
      "step": 13061
    },
    {
      "epoch": 0.061441056661994224,
      "grad_norm": 7.101953029632568,
      "learning_rate": 0.00018815430893848957,
      "loss": 0.5923,
      "step": 13062
    },
    {
      "epoch": 0.06144576046360669,
      "grad_norm": 1.4447107315063477,
      "learning_rate": 0.00018815336596037608,
      "loss": 0.1418,
      "step": 13063
    },
    {
      "epoch": 0.06145046426521915,
      "grad_norm": 2.329559803009033,
      "learning_rate": 0.00018815242298226258,
      "loss": 0.3961,
      "step": 13064
    },
    {
      "epoch": 0.061455168066831614,
      "grad_norm": 2.8569676876068115,
      "learning_rate": 0.0001881514800041491,
      "loss": 0.2163,
      "step": 13065
    },
    {
      "epoch": 0.06145987186844408,
      "grad_norm": 3.5522000789642334,
      "learning_rate": 0.00018815053702603564,
      "loss": 0.5237,
      "step": 13066
    },
    {
      "epoch": 0.06146457567005654,
      "grad_norm": 1.2316192388534546,
      "learning_rate": 0.00018814959404792216,
      "loss": 0.118,
      "step": 13067
    },
    {
      "epoch": 0.061469279471669004,
      "grad_norm": 0.4438278079032898,
      "learning_rate": 0.00018814865106980868,
      "loss": 0.0461,
      "step": 13068
    },
    {
      "epoch": 0.06147398327328146,
      "grad_norm": 2.571603298187256,
      "learning_rate": 0.0001881477080916952,
      "loss": 0.2599,
      "step": 13069
    },
    {
      "epoch": 0.06147868707489393,
      "grad_norm": 3.3568971157073975,
      "learning_rate": 0.00018814676511358171,
      "loss": 1.0011,
      "step": 13070
    },
    {
      "epoch": 0.061483390876506394,
      "grad_norm": 2.9103598594665527,
      "learning_rate": 0.00018814582213546826,
      "loss": 0.3317,
      "step": 13071
    },
    {
      "epoch": 0.06148809467811885,
      "grad_norm": 0.3528880178928375,
      "learning_rate": 0.00018814487915735478,
      "loss": 0.0352,
      "step": 13072
    },
    {
      "epoch": 0.06149279847973132,
      "grad_norm": 3.100569486618042,
      "learning_rate": 0.0001881439361792413,
      "loss": 0.4128,
      "step": 13073
    },
    {
      "epoch": 0.061497502281343784,
      "grad_norm": 5.031103610992432,
      "learning_rate": 0.00018814299320112782,
      "loss": 0.4144,
      "step": 13074
    },
    {
      "epoch": 0.06150220608295624,
      "grad_norm": 2.8528146743774414,
      "learning_rate": 0.00018814205022301433,
      "loss": 0.4347,
      "step": 13075
    },
    {
      "epoch": 0.06150690988456871,
      "grad_norm": 2.5311100482940674,
      "learning_rate": 0.00018814110724490085,
      "loss": 0.237,
      "step": 13076
    },
    {
      "epoch": 0.061511613686181174,
      "grad_norm": 2.764683723449707,
      "learning_rate": 0.00018814016426678737,
      "loss": 0.288,
      "step": 13077
    },
    {
      "epoch": 0.06151631748779363,
      "grad_norm": 0.4708845317363739,
      "learning_rate": 0.0001881392212886739,
      "loss": 0.0381,
      "step": 13078
    },
    {
      "epoch": 0.0615210212894061,
      "grad_norm": 1.6103355884552002,
      "learning_rate": 0.0001881382783105604,
      "loss": 0.3198,
      "step": 13079
    },
    {
      "epoch": 0.061525725091018564,
      "grad_norm": 1.9539494514465332,
      "learning_rate": 0.00018813733533244695,
      "loss": 0.3616,
      "step": 13080
    },
    {
      "epoch": 0.06153042889263102,
      "grad_norm": 1.3317962884902954,
      "learning_rate": 0.00018813639235433347,
      "loss": 0.2137,
      "step": 13081
    },
    {
      "epoch": 0.06153513269424349,
      "grad_norm": 2.3039610385894775,
      "learning_rate": 0.00018813544937622,
      "loss": 0.166,
      "step": 13082
    },
    {
      "epoch": 0.061539836495855954,
      "grad_norm": 1.8935397863388062,
      "learning_rate": 0.0001881345063981065,
      "loss": 0.4696,
      "step": 13083
    },
    {
      "epoch": 0.06154454029746841,
      "grad_norm": 0.7866608500480652,
      "learning_rate": 0.00018813356341999303,
      "loss": 0.0685,
      "step": 13084
    },
    {
      "epoch": 0.06154924409908088,
      "grad_norm": 1.7234106063842773,
      "learning_rate": 0.00018813262044187955,
      "loss": 0.2147,
      "step": 13085
    },
    {
      "epoch": 0.06155394790069334,
      "grad_norm": 2.0936717987060547,
      "learning_rate": 0.00018813167746376607,
      "loss": 0.5002,
      "step": 13086
    },
    {
      "epoch": 0.0615586517023058,
      "grad_norm": 1.1212126016616821,
      "learning_rate": 0.00018813073448565259,
      "loss": 0.1696,
      "step": 13087
    },
    {
      "epoch": 0.06156335550391827,
      "grad_norm": 1.8819278478622437,
      "learning_rate": 0.0001881297915075391,
      "loss": 0.2664,
      "step": 13088
    },
    {
      "epoch": 0.06156805930553073,
      "grad_norm": 1.0786761045455933,
      "learning_rate": 0.00018812884852942565,
      "loss": 0.2046,
      "step": 13089
    },
    {
      "epoch": 0.06157276310714319,
      "grad_norm": 1.4821115732192993,
      "learning_rate": 0.00018812790555131217,
      "loss": 0.1814,
      "step": 13090
    },
    {
      "epoch": 0.06157746690875566,
      "grad_norm": 0.6133660674095154,
      "learning_rate": 0.0001881269625731987,
      "loss": 0.0894,
      "step": 13091
    },
    {
      "epoch": 0.06158217071036812,
      "grad_norm": 2.246838092803955,
      "learning_rate": 0.0001881260195950852,
      "loss": 0.5431,
      "step": 13092
    },
    {
      "epoch": 0.06158687451198058,
      "grad_norm": 2.749768018722534,
      "learning_rate": 0.00018812507661697172,
      "loss": 0.5797,
      "step": 13093
    },
    {
      "epoch": 0.06159157831359305,
      "grad_norm": 2.3770735263824463,
      "learning_rate": 0.00018812413363885827,
      "loss": 0.1798,
      "step": 13094
    },
    {
      "epoch": 0.06159628211520551,
      "grad_norm": 2.784430742263794,
      "learning_rate": 0.00018812319066074476,
      "loss": 0.4195,
      "step": 13095
    },
    {
      "epoch": 0.06160098591681797,
      "grad_norm": 1.2991251945495605,
      "learning_rate": 0.00018812224768263128,
      "loss": 0.2183,
      "step": 13096
    },
    {
      "epoch": 0.06160568971843044,
      "grad_norm": 0.6466023921966553,
      "learning_rate": 0.0001881213047045178,
      "loss": 0.0894,
      "step": 13097
    },
    {
      "epoch": 0.0616103935200429,
      "grad_norm": 1.136609673500061,
      "learning_rate": 0.00018812036172640434,
      "loss": 0.1624,
      "step": 13098
    },
    {
      "epoch": 0.06161509732165536,
      "grad_norm": 2.9150164127349854,
      "learning_rate": 0.00018811941874829086,
      "loss": 0.6289,
      "step": 13099
    },
    {
      "epoch": 0.06161980112326783,
      "grad_norm": 2.056485414505005,
      "learning_rate": 0.00018811847577017738,
      "loss": 0.3518,
      "step": 13100
    },
    {
      "epoch": 0.06162450492488029,
      "grad_norm": 3.509082555770874,
      "learning_rate": 0.0001881175327920639,
      "loss": 0.4067,
      "step": 13101
    },
    {
      "epoch": 0.06162920872649275,
      "grad_norm": 1.0619007349014282,
      "learning_rate": 0.00018811658981395042,
      "loss": 0.1305,
      "step": 13102
    },
    {
      "epoch": 0.06163391252810521,
      "grad_norm": 4.156356334686279,
      "learning_rate": 0.00018811564683583697,
      "loss": 0.525,
      "step": 13103
    },
    {
      "epoch": 0.06163861632971768,
      "grad_norm": 1.8892627954483032,
      "learning_rate": 0.00018811470385772348,
      "loss": 0.2301,
      "step": 13104
    },
    {
      "epoch": 0.06164332013133014,
      "grad_norm": 3.122549533843994,
      "learning_rate": 0.00018811376087961,
      "loss": 0.4686,
      "step": 13105
    },
    {
      "epoch": 0.0616480239329426,
      "grad_norm": 2.022311210632324,
      "learning_rate": 0.00018811281790149652,
      "loss": 0.2353,
      "step": 13106
    },
    {
      "epoch": 0.06165272773455507,
      "grad_norm": 2.4627275466918945,
      "learning_rate": 0.00018811187492338304,
      "loss": 0.3906,
      "step": 13107
    },
    {
      "epoch": 0.06165743153616753,
      "grad_norm": 3.1612706184387207,
      "learning_rate": 0.00018811093194526956,
      "loss": 0.3603,
      "step": 13108
    },
    {
      "epoch": 0.06166213533777999,
      "grad_norm": 1.8800171613693237,
      "learning_rate": 0.00018810998896715608,
      "loss": 0.2632,
      "step": 13109
    },
    {
      "epoch": 0.06166683913939246,
      "grad_norm": 2.4958817958831787,
      "learning_rate": 0.0001881090459890426,
      "loss": 0.1518,
      "step": 13110
    },
    {
      "epoch": 0.06167154294100492,
      "grad_norm": 1.3600883483886719,
      "learning_rate": 0.00018810810301092911,
      "loss": 0.1295,
      "step": 13111
    },
    {
      "epoch": 0.06167624674261738,
      "grad_norm": 1.3727108240127563,
      "learning_rate": 0.00018810716003281566,
      "loss": 0.2706,
      "step": 13112
    },
    {
      "epoch": 0.06168095054422985,
      "grad_norm": 10.975058555603027,
      "learning_rate": 0.00018810621705470218,
      "loss": 0.4623,
      "step": 13113
    },
    {
      "epoch": 0.06168565434584231,
      "grad_norm": 4.309391021728516,
      "learning_rate": 0.0001881052740765887,
      "loss": 0.7701,
      "step": 13114
    },
    {
      "epoch": 0.06169035814745477,
      "grad_norm": 1.2464345693588257,
      "learning_rate": 0.00018810433109847522,
      "loss": 0.1458,
      "step": 13115
    },
    {
      "epoch": 0.06169506194906724,
      "grad_norm": 1.818484902381897,
      "learning_rate": 0.00018810338812036173,
      "loss": 0.4124,
      "step": 13116
    },
    {
      "epoch": 0.0616997657506797,
      "grad_norm": 1.213060975074768,
      "learning_rate": 0.00018810244514224825,
      "loss": 0.1435,
      "step": 13117
    },
    {
      "epoch": 0.06170446955229216,
      "grad_norm": 2.1317172050476074,
      "learning_rate": 0.00018810150216413477,
      "loss": 0.2992,
      "step": 13118
    },
    {
      "epoch": 0.06170917335390463,
      "grad_norm": 2.0547425746917725,
      "learning_rate": 0.0001881005591860213,
      "loss": 0.2556,
      "step": 13119
    },
    {
      "epoch": 0.061713877155517086,
      "grad_norm": 0.5255469083786011,
      "learning_rate": 0.0001880996162079078,
      "loss": 0.0428,
      "step": 13120
    },
    {
      "epoch": 0.06171858095712955,
      "grad_norm": 0.5581905245780945,
      "learning_rate": 0.00018809867322979435,
      "loss": 0.0565,
      "step": 13121
    },
    {
      "epoch": 0.06172328475874202,
      "grad_norm": 1.3800675868988037,
      "learning_rate": 0.00018809773025168087,
      "loss": 0.1476,
      "step": 13122
    },
    {
      "epoch": 0.061727988560354476,
      "grad_norm": 5.558424472808838,
      "learning_rate": 0.0001880967872735674,
      "loss": 0.4708,
      "step": 13123
    },
    {
      "epoch": 0.06173269236196694,
      "grad_norm": 0.9233400225639343,
      "learning_rate": 0.0001880958442954539,
      "loss": 0.1095,
      "step": 13124
    },
    {
      "epoch": 0.06173739616357941,
      "grad_norm": 2.755631446838379,
      "learning_rate": 0.00018809490131734046,
      "loss": 0.6636,
      "step": 13125
    },
    {
      "epoch": 0.061742099965191866,
      "grad_norm": 2.478031873703003,
      "learning_rate": 0.00018809395833922695,
      "loss": 0.4011,
      "step": 13126
    },
    {
      "epoch": 0.06174680376680433,
      "grad_norm": 2.5325772762298584,
      "learning_rate": 0.00018809301536111347,
      "loss": 0.6057,
      "step": 13127
    },
    {
      "epoch": 0.0617515075684168,
      "grad_norm": 0.6077989339828491,
      "learning_rate": 0.00018809207238299999,
      "loss": 0.07,
      "step": 13128
    },
    {
      "epoch": 0.061756211370029256,
      "grad_norm": 0.541723370552063,
      "learning_rate": 0.0001880911294048865,
      "loss": 0.0423,
      "step": 13129
    },
    {
      "epoch": 0.06176091517164172,
      "grad_norm": 0.23662123084068298,
      "learning_rate": 0.00018809018642677305,
      "loss": 0.0164,
      "step": 13130
    },
    {
      "epoch": 0.06176561897325419,
      "grad_norm": 1.634925127029419,
      "learning_rate": 0.00018808924344865957,
      "loss": 0.3705,
      "step": 13131
    },
    {
      "epoch": 0.061770322774866646,
      "grad_norm": 0.7498626708984375,
      "learning_rate": 0.0001880883004705461,
      "loss": 0.0759,
      "step": 13132
    },
    {
      "epoch": 0.06177502657647911,
      "grad_norm": 1.4635155200958252,
      "learning_rate": 0.0001880873574924326,
      "loss": 0.1304,
      "step": 13133
    },
    {
      "epoch": 0.06177973037809158,
      "grad_norm": 1.3636201620101929,
      "learning_rate": 0.00018808641451431915,
      "loss": 0.0704,
      "step": 13134
    },
    {
      "epoch": 0.061784434179704036,
      "grad_norm": 0.5123246312141418,
      "learning_rate": 0.00018808547153620567,
      "loss": 0.0129,
      "step": 13135
    },
    {
      "epoch": 0.0617891379813165,
      "grad_norm": 4.708046913146973,
      "learning_rate": 0.0001880845285580922,
      "loss": 0.5487,
      "step": 13136
    },
    {
      "epoch": 0.06179384178292896,
      "grad_norm": 0.3956596851348877,
      "learning_rate": 0.0001880835855799787,
      "loss": 0.0289,
      "step": 13137
    },
    {
      "epoch": 0.061798545584541426,
      "grad_norm": 2.447855234146118,
      "learning_rate": 0.0001880826426018652,
      "loss": 0.2348,
      "step": 13138
    },
    {
      "epoch": 0.06180324938615389,
      "grad_norm": 1.3627082109451294,
      "learning_rate": 0.00018808169962375174,
      "loss": 0.0907,
      "step": 13139
    },
    {
      "epoch": 0.06180795318776635,
      "grad_norm": 3.0081968307495117,
      "learning_rate": 0.00018808075664563826,
      "loss": 0.3436,
      "step": 13140
    },
    {
      "epoch": 0.061812656989378816,
      "grad_norm": 3.0302517414093018,
      "learning_rate": 0.00018807981366752478,
      "loss": 0.2869,
      "step": 13141
    },
    {
      "epoch": 0.06181736079099128,
      "grad_norm": 1.7225573062896729,
      "learning_rate": 0.0001880788706894113,
      "loss": 0.1302,
      "step": 13142
    },
    {
      "epoch": 0.06182206459260374,
      "grad_norm": 0.4908665120601654,
      "learning_rate": 0.00018807792771129782,
      "loss": 0.0512,
      "step": 13143
    },
    {
      "epoch": 0.061826768394216206,
      "grad_norm": 0.8045698404312134,
      "learning_rate": 0.00018807698473318437,
      "loss": 0.0791,
      "step": 13144
    },
    {
      "epoch": 0.06183147219582867,
      "grad_norm": 1.6013975143432617,
      "learning_rate": 0.00018807604175507088,
      "loss": 0.1517,
      "step": 13145
    },
    {
      "epoch": 0.06183617599744113,
      "grad_norm": 0.26507803797721863,
      "learning_rate": 0.0001880750987769574,
      "loss": 0.0284,
      "step": 13146
    },
    {
      "epoch": 0.061840879799053596,
      "grad_norm": 1.3960025310516357,
      "learning_rate": 0.00018807415579884392,
      "loss": 0.1486,
      "step": 13147
    },
    {
      "epoch": 0.06184558360066606,
      "grad_norm": 1.0930233001708984,
      "learning_rate": 0.00018807321282073044,
      "loss": 0.0625,
      "step": 13148
    },
    {
      "epoch": 0.06185028740227852,
      "grad_norm": 3.1226210594177246,
      "learning_rate": 0.00018807226984261696,
      "loss": 0.4207,
      "step": 13149
    },
    {
      "epoch": 0.061854991203890985,
      "grad_norm": 2.543909788131714,
      "learning_rate": 0.00018807132686450348,
      "loss": 0.2539,
      "step": 13150
    },
    {
      "epoch": 0.06185969500550345,
      "grad_norm": 6.580615997314453,
      "learning_rate": 0.00018807038388639,
      "loss": 0.4551,
      "step": 13151
    },
    {
      "epoch": 0.06186439880711591,
      "grad_norm": 4.247712135314941,
      "learning_rate": 0.00018806944090827651,
      "loss": 0.3331,
      "step": 13152
    },
    {
      "epoch": 0.061869102608728375,
      "grad_norm": 4.275554180145264,
      "learning_rate": 0.00018806849793016306,
      "loss": 0.5677,
      "step": 13153
    },
    {
      "epoch": 0.061873806410340834,
      "grad_norm": 7.4501471519470215,
      "learning_rate": 0.00018806755495204958,
      "loss": 0.5331,
      "step": 13154
    },
    {
      "epoch": 0.0618785102119533,
      "grad_norm": 6.357449531555176,
      "learning_rate": 0.0001880666119739361,
      "loss": 0.6806,
      "step": 13155
    },
    {
      "epoch": 0.061883214013565765,
      "grad_norm": 3.0186195373535156,
      "learning_rate": 0.00018806566899582262,
      "loss": 0.1564,
      "step": 13156
    },
    {
      "epoch": 0.061887917815178224,
      "grad_norm": 0.07517120987176895,
      "learning_rate": 0.00018806472601770913,
      "loss": 0.0036,
      "step": 13157
    },
    {
      "epoch": 0.06189262161679069,
      "grad_norm": 2.244584560394287,
      "learning_rate": 0.00018806378303959565,
      "loss": 0.1648,
      "step": 13158
    },
    {
      "epoch": 0.061897325418403155,
      "grad_norm": 5.9877424240112305,
      "learning_rate": 0.00018806284006148217,
      "loss": 0.3269,
      "step": 13159
    },
    {
      "epoch": 0.061902029220015614,
      "grad_norm": 3.7019617557525635,
      "learning_rate": 0.0001880618970833687,
      "loss": 0.4503,
      "step": 13160
    },
    {
      "epoch": 0.06190673302162808,
      "grad_norm": 2.8794522285461426,
      "learning_rate": 0.0001880609541052552,
      "loss": 0.1689,
      "step": 13161
    },
    {
      "epoch": 0.061911436823240545,
      "grad_norm": 0.8691514134407043,
      "learning_rate": 0.00018806001112714175,
      "loss": 0.0385,
      "step": 13162
    },
    {
      "epoch": 0.061916140624853004,
      "grad_norm": 2.975532293319702,
      "learning_rate": 0.00018805906814902827,
      "loss": 0.3348,
      "step": 13163
    },
    {
      "epoch": 0.06192084442646547,
      "grad_norm": 1.4889461994171143,
      "learning_rate": 0.0001880581251709148,
      "loss": 0.0513,
      "step": 13164
    },
    {
      "epoch": 0.061925548228077935,
      "grad_norm": 3.7073311805725098,
      "learning_rate": 0.0001880571821928013,
      "loss": 0.4473,
      "step": 13165
    },
    {
      "epoch": 0.061930252029690394,
      "grad_norm": 4.419768333435059,
      "learning_rate": 0.00018805623921468786,
      "loss": 0.3799,
      "step": 13166
    },
    {
      "epoch": 0.06193495583130286,
      "grad_norm": 4.614716053009033,
      "learning_rate": 0.00018805529623657438,
      "loss": 0.7416,
      "step": 13167
    },
    {
      "epoch": 0.061939659632915325,
      "grad_norm": 4.734128475189209,
      "learning_rate": 0.0001880543532584609,
      "loss": 0.8035,
      "step": 13168
    },
    {
      "epoch": 0.061944363434527784,
      "grad_norm": 2.1357429027557373,
      "learning_rate": 0.00018805341028034739,
      "loss": 0.105,
      "step": 13169
    },
    {
      "epoch": 0.06194906723614025,
      "grad_norm": 2.768798351287842,
      "learning_rate": 0.0001880524673022339,
      "loss": 0.4766,
      "step": 13170
    },
    {
      "epoch": 0.06195377103775271,
      "grad_norm": 1.7721257209777832,
      "learning_rate": 0.00018805152432412045,
      "loss": 0.134,
      "step": 13171
    },
    {
      "epoch": 0.061958474839365174,
      "grad_norm": 1.4800359010696411,
      "learning_rate": 0.00018805058134600697,
      "loss": 0.1233,
      "step": 13172
    },
    {
      "epoch": 0.06196317864097764,
      "grad_norm": 1.731092095375061,
      "learning_rate": 0.0001880496383678935,
      "loss": 0.1371,
      "step": 13173
    },
    {
      "epoch": 0.0619678824425901,
      "grad_norm": 3.810352087020874,
      "learning_rate": 0.00018804869538978,
      "loss": 0.2891,
      "step": 13174
    },
    {
      "epoch": 0.061972586244202564,
      "grad_norm": 0.5116921067237854,
      "learning_rate": 0.00018804775241166655,
      "loss": 0.0293,
      "step": 13175
    },
    {
      "epoch": 0.06197729004581503,
      "grad_norm": 0.7308371663093567,
      "learning_rate": 0.00018804680943355307,
      "loss": 0.0847,
      "step": 13176
    },
    {
      "epoch": 0.06198199384742749,
      "grad_norm": 3.878749370574951,
      "learning_rate": 0.0001880458664554396,
      "loss": 0.4573,
      "step": 13177
    },
    {
      "epoch": 0.061986697649039954,
      "grad_norm": 1.1787446737289429,
      "learning_rate": 0.0001880449234773261,
      "loss": 0.029,
      "step": 13178
    },
    {
      "epoch": 0.06199140145065242,
      "grad_norm": 0.9862165451049805,
      "learning_rate": 0.00018804398049921263,
      "loss": 0.065,
      "step": 13179
    },
    {
      "epoch": 0.06199610525226488,
      "grad_norm": 2.4359848499298096,
      "learning_rate": 0.00018804303752109914,
      "loss": 0.2109,
      "step": 13180
    },
    {
      "epoch": 0.062000809053877344,
      "grad_norm": 1.5080714225769043,
      "learning_rate": 0.00018804209454298566,
      "loss": 0.1279,
      "step": 13181
    },
    {
      "epoch": 0.06200551285548981,
      "grad_norm": 4.346567630767822,
      "learning_rate": 0.00018804115156487218,
      "loss": 0.7342,
      "step": 13182
    },
    {
      "epoch": 0.06201021665710227,
      "grad_norm": 3.106739044189453,
      "learning_rate": 0.0001880402085867587,
      "loss": 0.1981,
      "step": 13183
    },
    {
      "epoch": 0.062014920458714734,
      "grad_norm": 2.7452332973480225,
      "learning_rate": 0.00018803926560864525,
      "loss": 0.2758,
      "step": 13184
    },
    {
      "epoch": 0.0620196242603272,
      "grad_norm": 2.6362924575805664,
      "learning_rate": 0.00018803832263053176,
      "loss": 0.2594,
      "step": 13185
    },
    {
      "epoch": 0.06202432806193966,
      "grad_norm": 2.1687307357788086,
      "learning_rate": 0.00018803737965241828,
      "loss": 0.3432,
      "step": 13186
    },
    {
      "epoch": 0.062029031863552124,
      "grad_norm": 2.8801231384277344,
      "learning_rate": 0.0001880364366743048,
      "loss": 0.2968,
      "step": 13187
    },
    {
      "epoch": 0.06203373566516458,
      "grad_norm": 1.1129013299942017,
      "learning_rate": 0.00018803549369619132,
      "loss": 0.0848,
      "step": 13188
    },
    {
      "epoch": 0.06203843946677705,
      "grad_norm": 1.7580056190490723,
      "learning_rate": 0.00018803455071807784,
      "loss": 0.122,
      "step": 13189
    },
    {
      "epoch": 0.062043143268389514,
      "grad_norm": 1.105006217956543,
      "learning_rate": 0.00018803360773996436,
      "loss": 0.0733,
      "step": 13190
    },
    {
      "epoch": 0.06204784707000197,
      "grad_norm": 0.1127687469124794,
      "learning_rate": 0.00018803266476185088,
      "loss": 0.0055,
      "step": 13191
    },
    {
      "epoch": 0.06205255087161444,
      "grad_norm": 7.582452297210693,
      "learning_rate": 0.0001880317217837374,
      "loss": 0.6694,
      "step": 13192
    },
    {
      "epoch": 0.062057254673226904,
      "grad_norm": 1.1786155700683594,
      "learning_rate": 0.00018803077880562391,
      "loss": 0.0852,
      "step": 13193
    },
    {
      "epoch": 0.06206195847483936,
      "grad_norm": 0.5349932909011841,
      "learning_rate": 0.00018802983582751046,
      "loss": 0.0314,
      "step": 13194
    },
    {
      "epoch": 0.06206666227645183,
      "grad_norm": 5.062808036804199,
      "learning_rate": 0.00018802889284939698,
      "loss": 0.557,
      "step": 13195
    },
    {
      "epoch": 0.062071366078064294,
      "grad_norm": 0.35227859020233154,
      "learning_rate": 0.0001880279498712835,
      "loss": 0.0232,
      "step": 13196
    },
    {
      "epoch": 0.06207606987967675,
      "grad_norm": 0.2053607702255249,
      "learning_rate": 0.00018802700689317002,
      "loss": 0.0111,
      "step": 13197
    },
    {
      "epoch": 0.06208077368128922,
      "grad_norm": 0.14022211730480194,
      "learning_rate": 0.00018802606391505656,
      "loss": 0.0061,
      "step": 13198
    },
    {
      "epoch": 0.062085477482901684,
      "grad_norm": 2.996659278869629,
      "learning_rate": 0.00018802512093694305,
      "loss": 0.3788,
      "step": 13199
    },
    {
      "epoch": 0.06209018128451414,
      "grad_norm": 4.188478469848633,
      "learning_rate": 0.00018802417795882957,
      "loss": 1.1908,
      "step": 13200
    },
    {
      "epoch": 0.06209488508612661,
      "grad_norm": 1.2954069375991821,
      "learning_rate": 0.0001880232349807161,
      "loss": 0.0666,
      "step": 13201
    },
    {
      "epoch": 0.062099588887739074,
      "grad_norm": 8.046782493591309,
      "learning_rate": 0.0001880222920026026,
      "loss": 0.7003,
      "step": 13202
    },
    {
      "epoch": 0.06210429268935153,
      "grad_norm": 2.9783005714416504,
      "learning_rate": 0.00018802134902448915,
      "loss": 0.5129,
      "step": 13203
    },
    {
      "epoch": 0.062108996490964,
      "grad_norm": 1.2712318897247314,
      "learning_rate": 0.00018802040604637567,
      "loss": 0.0958,
      "step": 13204
    },
    {
      "epoch": 0.06211370029257646,
      "grad_norm": 5.278674125671387,
      "learning_rate": 0.0001880194630682622,
      "loss": 0.66,
      "step": 13205
    },
    {
      "epoch": 0.06211840409418892,
      "grad_norm": 4.660032272338867,
      "learning_rate": 0.0001880185200901487,
      "loss": 0.3695,
      "step": 13206
    },
    {
      "epoch": 0.06212310789580139,
      "grad_norm": 3.8242321014404297,
      "learning_rate": 0.00018801757711203526,
      "loss": 0.5486,
      "step": 13207
    },
    {
      "epoch": 0.06212781169741385,
      "grad_norm": 1.8216044902801514,
      "learning_rate": 0.00018801663413392178,
      "loss": 0.3306,
      "step": 13208
    },
    {
      "epoch": 0.06213251549902631,
      "grad_norm": 0.03835025802254677,
      "learning_rate": 0.0001880156911558083,
      "loss": 0.0015,
      "step": 13209
    },
    {
      "epoch": 0.06213721930063878,
      "grad_norm": 0.9226983189582825,
      "learning_rate": 0.0001880147481776948,
      "loss": 0.0556,
      "step": 13210
    },
    {
      "epoch": 0.06214192310225124,
      "grad_norm": 3.3705368041992188,
      "learning_rate": 0.0001880138051995813,
      "loss": 0.7771,
      "step": 13211
    },
    {
      "epoch": 0.0621466269038637,
      "grad_norm": 1.2598259449005127,
      "learning_rate": 0.00018801286222146785,
      "loss": 0.081,
      "step": 13212
    },
    {
      "epoch": 0.06215133070547617,
      "grad_norm": 3.001323938369751,
      "learning_rate": 0.00018801191924335437,
      "loss": 0.4134,
      "step": 13213
    },
    {
      "epoch": 0.06215603450708863,
      "grad_norm": 3.1726725101470947,
      "learning_rate": 0.0001880109762652409,
      "loss": 0.4045,
      "step": 13214
    },
    {
      "epoch": 0.06216073830870109,
      "grad_norm": 1.0287693738937378,
      "learning_rate": 0.0001880100332871274,
      "loss": 0.1038,
      "step": 13215
    },
    {
      "epoch": 0.06216544211031356,
      "grad_norm": 0.3168981075286865,
      "learning_rate": 0.00018800909030901395,
      "loss": 0.025,
      "step": 13216
    },
    {
      "epoch": 0.06217014591192602,
      "grad_norm": 2.1536643505096436,
      "learning_rate": 0.00018800814733090047,
      "loss": 0.2314,
      "step": 13217
    },
    {
      "epoch": 0.06217484971353848,
      "grad_norm": 0.4358842074871063,
      "learning_rate": 0.000188007204352787,
      "loss": 0.0469,
      "step": 13218
    },
    {
      "epoch": 0.06217955351515095,
      "grad_norm": 2.0266501903533936,
      "learning_rate": 0.0001880062613746735,
      "loss": 0.2103,
      "step": 13219
    },
    {
      "epoch": 0.06218425731676341,
      "grad_norm": 5.421212673187256,
      "learning_rate": 0.00018800531839656003,
      "loss": 0.909,
      "step": 13220
    },
    {
      "epoch": 0.06218896111837587,
      "grad_norm": 2.9097537994384766,
      "learning_rate": 0.00018800437541844654,
      "loss": 0.4737,
      "step": 13221
    },
    {
      "epoch": 0.06219366491998833,
      "grad_norm": 7.837690353393555,
      "learning_rate": 0.00018800343244033306,
      "loss": 0.0855,
      "step": 13222
    },
    {
      "epoch": 0.0621983687216008,
      "grad_norm": 0.4310022294521332,
      "learning_rate": 0.00018800248946221958,
      "loss": 0.0654,
      "step": 13223
    },
    {
      "epoch": 0.06220307252321326,
      "grad_norm": 2.391038417816162,
      "learning_rate": 0.0001880015464841061,
      "loss": 0.2606,
      "step": 13224
    },
    {
      "epoch": 0.06220777632482572,
      "grad_norm": 1.9303511381149292,
      "learning_rate": 0.00018800060350599265,
      "loss": 0.291,
      "step": 13225
    },
    {
      "epoch": 0.06221248012643819,
      "grad_norm": 1.699984073638916,
      "learning_rate": 0.00018799966052787916,
      "loss": 0.121,
      "step": 13226
    },
    {
      "epoch": 0.06221718392805065,
      "grad_norm": 1.8048789501190186,
      "learning_rate": 0.00018799871754976568,
      "loss": 0.2208,
      "step": 13227
    },
    {
      "epoch": 0.06222188772966311,
      "grad_norm": 1.4298032522201538,
      "learning_rate": 0.0001879977745716522,
      "loss": 0.2214,
      "step": 13228
    },
    {
      "epoch": 0.06222659153127558,
      "grad_norm": 2.784383773803711,
      "learning_rate": 0.00018799683159353872,
      "loss": 0.4719,
      "step": 13229
    },
    {
      "epoch": 0.06223129533288804,
      "grad_norm": 2.435039758682251,
      "learning_rate": 0.00018799588861542524,
      "loss": 0.328,
      "step": 13230
    },
    {
      "epoch": 0.0622359991345005,
      "grad_norm": 1.1962058544158936,
      "learning_rate": 0.00018799494563731176,
      "loss": 0.1241,
      "step": 13231
    },
    {
      "epoch": 0.06224070293611297,
      "grad_norm": 3.0233962535858154,
      "learning_rate": 0.00018799400265919828,
      "loss": 0.8369,
      "step": 13232
    },
    {
      "epoch": 0.06224540673772543,
      "grad_norm": 1.1285114288330078,
      "learning_rate": 0.0001879930596810848,
      "loss": 0.1015,
      "step": 13233
    },
    {
      "epoch": 0.06225011053933789,
      "grad_norm": 2.2293972969055176,
      "learning_rate": 0.00018799211670297134,
      "loss": 0.5123,
      "step": 13234
    },
    {
      "epoch": 0.06225481434095036,
      "grad_norm": 1.0612112283706665,
      "learning_rate": 0.00018799117372485786,
      "loss": 0.1064,
      "step": 13235
    },
    {
      "epoch": 0.06225951814256282,
      "grad_norm": 0.5340994596481323,
      "learning_rate": 0.00018799023074674438,
      "loss": 0.0377,
      "step": 13236
    },
    {
      "epoch": 0.06226422194417528,
      "grad_norm": 0.9916577339172363,
      "learning_rate": 0.0001879892877686309,
      "loss": 0.0929,
      "step": 13237
    },
    {
      "epoch": 0.06226892574578775,
      "grad_norm": 1.6324646472930908,
      "learning_rate": 0.00018798834479051742,
      "loss": 0.2062,
      "step": 13238
    },
    {
      "epoch": 0.062273629547400206,
      "grad_norm": 1.4672012329101562,
      "learning_rate": 0.00018798740181240396,
      "loss": 0.1284,
      "step": 13239
    },
    {
      "epoch": 0.06227833334901267,
      "grad_norm": 2.5512313842773438,
      "learning_rate": 0.00018798645883429048,
      "loss": 0.6957,
      "step": 13240
    },
    {
      "epoch": 0.06228303715062514,
      "grad_norm": 0.7323530912399292,
      "learning_rate": 0.000187985515856177,
      "loss": 0.0478,
      "step": 13241
    },
    {
      "epoch": 0.062287740952237595,
      "grad_norm": 0.5211160778999329,
      "learning_rate": 0.0001879845728780635,
      "loss": 0.0403,
      "step": 13242
    },
    {
      "epoch": 0.06229244475385006,
      "grad_norm": 1.6117757558822632,
      "learning_rate": 0.00018798362989995004,
      "loss": 0.1537,
      "step": 13243
    },
    {
      "epoch": 0.06229714855546253,
      "grad_norm": 1.9260125160217285,
      "learning_rate": 0.00018798268692183655,
      "loss": 0.3134,
      "step": 13244
    },
    {
      "epoch": 0.062301852357074985,
      "grad_norm": 0.7820233106613159,
      "learning_rate": 0.00018798174394372307,
      "loss": 0.0683,
      "step": 13245
    },
    {
      "epoch": 0.06230655615868745,
      "grad_norm": 1.315730333328247,
      "learning_rate": 0.0001879808009656096,
      "loss": 0.0827,
      "step": 13246
    },
    {
      "epoch": 0.06231125996029992,
      "grad_norm": 2.7348380088806152,
      "learning_rate": 0.0001879798579874961,
      "loss": 0.6848,
      "step": 13247
    },
    {
      "epoch": 0.062315963761912375,
      "grad_norm": 1.4650198221206665,
      "learning_rate": 0.00018797891500938266,
      "loss": 0.1275,
      "step": 13248
    },
    {
      "epoch": 0.06232066756352484,
      "grad_norm": 4.145851135253906,
      "learning_rate": 0.00018797797203126918,
      "loss": 1.1392,
      "step": 13249
    },
    {
      "epoch": 0.06232537136513731,
      "grad_norm": 1.7964978218078613,
      "learning_rate": 0.0001879770290531557,
      "loss": 0.1252,
      "step": 13250
    },
    {
      "epoch": 0.062330075166749765,
      "grad_norm": 1.0600467920303345,
      "learning_rate": 0.0001879760860750422,
      "loss": 0.0453,
      "step": 13251
    },
    {
      "epoch": 0.06233477896836223,
      "grad_norm": 1.2900532484054565,
      "learning_rate": 0.00018797514309692873,
      "loss": 0.1783,
      "step": 13252
    },
    {
      "epoch": 0.0623394827699747,
      "grad_norm": 1.7073262929916382,
      "learning_rate": 0.00018797420011881525,
      "loss": 0.2321,
      "step": 13253
    },
    {
      "epoch": 0.062344186571587155,
      "grad_norm": 2.8185348510742188,
      "learning_rate": 0.00018797325714070177,
      "loss": 0.6562,
      "step": 13254
    },
    {
      "epoch": 0.06234889037319962,
      "grad_norm": 2.688180446624756,
      "learning_rate": 0.0001879723141625883,
      "loss": 0.3051,
      "step": 13255
    },
    {
      "epoch": 0.06235359417481208,
      "grad_norm": 2.990643262863159,
      "learning_rate": 0.0001879713711844748,
      "loss": 0.3113,
      "step": 13256
    },
    {
      "epoch": 0.062358297976424545,
      "grad_norm": 1.9857138395309448,
      "learning_rate": 0.00018797042820636135,
      "loss": 0.1316,
      "step": 13257
    },
    {
      "epoch": 0.06236300177803701,
      "grad_norm": 1.2827733755111694,
      "learning_rate": 0.00018796948522824787,
      "loss": 0.2178,
      "step": 13258
    },
    {
      "epoch": 0.06236770557964947,
      "grad_norm": 3.145993709564209,
      "learning_rate": 0.0001879685422501344,
      "loss": 0.7879,
      "step": 13259
    },
    {
      "epoch": 0.062372409381261935,
      "grad_norm": 3.39151668548584,
      "learning_rate": 0.0001879675992720209,
      "loss": 0.4492,
      "step": 13260
    },
    {
      "epoch": 0.0623771131828744,
      "grad_norm": 1.373016595840454,
      "learning_rate": 0.00018796665629390743,
      "loss": 0.1594,
      "step": 13261
    },
    {
      "epoch": 0.06238181698448686,
      "grad_norm": 1.442185878753662,
      "learning_rate": 0.00018796571331579394,
      "loss": 0.3741,
      "step": 13262
    },
    {
      "epoch": 0.062386520786099325,
      "grad_norm": 1.935217022895813,
      "learning_rate": 0.00018796477033768046,
      "loss": 0.2354,
      "step": 13263
    },
    {
      "epoch": 0.06239122458771179,
      "grad_norm": 2.2865889072418213,
      "learning_rate": 0.00018796382735956698,
      "loss": 0.282,
      "step": 13264
    },
    {
      "epoch": 0.06239592838932425,
      "grad_norm": 2.339015007019043,
      "learning_rate": 0.0001879628843814535,
      "loss": 0.1824,
      "step": 13265
    },
    {
      "epoch": 0.062400632190936715,
      "grad_norm": 4.624664306640625,
      "learning_rate": 0.00018796194140334005,
      "loss": 0.3173,
      "step": 13266
    },
    {
      "epoch": 0.06240533599254918,
      "grad_norm": 1.8788409233093262,
      "learning_rate": 0.00018796099842522656,
      "loss": 0.3545,
      "step": 13267
    },
    {
      "epoch": 0.06241003979416164,
      "grad_norm": 1.3430193662643433,
      "learning_rate": 0.00018796005544711308,
      "loss": 0.3201,
      "step": 13268
    },
    {
      "epoch": 0.062414743595774105,
      "grad_norm": 1.369575023651123,
      "learning_rate": 0.0001879591124689996,
      "loss": 0.3165,
      "step": 13269
    },
    {
      "epoch": 0.06241944739738657,
      "grad_norm": 1.136034607887268,
      "learning_rate": 0.00018795816949088612,
      "loss": 0.2301,
      "step": 13270
    },
    {
      "epoch": 0.06242415119899903,
      "grad_norm": 1.009637475013733,
      "learning_rate": 0.00018795722651277267,
      "loss": 0.2348,
      "step": 13271
    },
    {
      "epoch": 0.062428855000611495,
      "grad_norm": 1.1241190433502197,
      "learning_rate": 0.00018795628353465919,
      "loss": 0.2326,
      "step": 13272
    },
    {
      "epoch": 0.062433558802223954,
      "grad_norm": 1.4218538999557495,
      "learning_rate": 0.00018795534055654568,
      "loss": 0.1612,
      "step": 13273
    },
    {
      "epoch": 0.06243826260383642,
      "grad_norm": 2.198838710784912,
      "learning_rate": 0.0001879543975784322,
      "loss": 0.3367,
      "step": 13274
    },
    {
      "epoch": 0.062442966405448885,
      "grad_norm": 1.124297022819519,
      "learning_rate": 0.00018795345460031874,
      "loss": 0.1166,
      "step": 13275
    },
    {
      "epoch": 0.062447670207061344,
      "grad_norm": 0.4178374111652374,
      "learning_rate": 0.00018795251162220526,
      "loss": 0.0336,
      "step": 13276
    },
    {
      "epoch": 0.06245237400867381,
      "grad_norm": 1.3126250505447388,
      "learning_rate": 0.00018795156864409178,
      "loss": 0.4213,
      "step": 13277
    },
    {
      "epoch": 0.062457077810286275,
      "grad_norm": 1.5743452310562134,
      "learning_rate": 0.0001879506256659783,
      "loss": 0.2028,
      "step": 13278
    },
    {
      "epoch": 0.062461781611898734,
      "grad_norm": 3.306225061416626,
      "learning_rate": 0.00018794968268786482,
      "loss": 0.5842,
      "step": 13279
    },
    {
      "epoch": 0.0624664854135112,
      "grad_norm": 0.49004632234573364,
      "learning_rate": 0.00018794873970975136,
      "loss": 0.0478,
      "step": 13280
    },
    {
      "epoch": 0.062471189215123665,
      "grad_norm": 1.3757392168045044,
      "learning_rate": 0.00018794779673163788,
      "loss": 0.2588,
      "step": 13281
    },
    {
      "epoch": 0.062475893016736124,
      "grad_norm": 0.8810733556747437,
      "learning_rate": 0.0001879468537535244,
      "loss": 0.163,
      "step": 13282
    },
    {
      "epoch": 0.06248059681834859,
      "grad_norm": 0.5375415086746216,
      "learning_rate": 0.00018794591077541092,
      "loss": 0.056,
      "step": 13283
    },
    {
      "epoch": 0.062485300619961055,
      "grad_norm": 1.6122804880142212,
      "learning_rate": 0.00018794496779729744,
      "loss": 0.1026,
      "step": 13284
    },
    {
      "epoch": 0.062490004421573514,
      "grad_norm": 1.8579859733581543,
      "learning_rate": 0.00018794402481918395,
      "loss": 0.1828,
      "step": 13285
    },
    {
      "epoch": 0.06249470822318598,
      "grad_norm": 1.3537113666534424,
      "learning_rate": 0.00018794308184107047,
      "loss": 0.2617,
      "step": 13286
    },
    {
      "epoch": 0.062499412024798445,
      "grad_norm": 0.8178966641426086,
      "learning_rate": 0.000187942138862957,
      "loss": 0.0963,
      "step": 13287
    },
    {
      "epoch": 0.06250411582641091,
      "grad_norm": 1.6404849290847778,
      "learning_rate": 0.0001879411958848435,
      "loss": 0.1593,
      "step": 13288
    },
    {
      "epoch": 0.06250881962802336,
      "grad_norm": 0.5527985095977783,
      "learning_rate": 0.00018794025290673006,
      "loss": 0.0326,
      "step": 13289
    },
    {
      "epoch": 0.06251352342963583,
      "grad_norm": 0.9518007636070251,
      "learning_rate": 0.00018793930992861658,
      "loss": 0.1975,
      "step": 13290
    },
    {
      "epoch": 0.0625182272312483,
      "grad_norm": 0.59272301197052,
      "learning_rate": 0.0001879383669505031,
      "loss": 0.0952,
      "step": 13291
    },
    {
      "epoch": 0.06252293103286076,
      "grad_norm": 1.4076826572418213,
      "learning_rate": 0.0001879374239723896,
      "loss": 0.3364,
      "step": 13292
    },
    {
      "epoch": 0.06252763483447323,
      "grad_norm": 3.845921516418457,
      "learning_rate": 0.00018793648099427613,
      "loss": 0.8808,
      "step": 13293
    },
    {
      "epoch": 0.06253233863608569,
      "grad_norm": 0.508158802986145,
      "learning_rate": 0.00018793553801616265,
      "loss": 0.0403,
      "step": 13294
    },
    {
      "epoch": 0.06253704243769814,
      "grad_norm": 0.3623955249786377,
      "learning_rate": 0.00018793459503804917,
      "loss": 0.0274,
      "step": 13295
    },
    {
      "epoch": 0.06254174623931061,
      "grad_norm": 1.392008662223816,
      "learning_rate": 0.0001879336520599357,
      "loss": 0.1813,
      "step": 13296
    },
    {
      "epoch": 0.06254645004092307,
      "grad_norm": 2.6877682209014893,
      "learning_rate": 0.0001879327090818222,
      "loss": 0.75,
      "step": 13297
    },
    {
      "epoch": 0.06255115384253554,
      "grad_norm": 0.5204720497131348,
      "learning_rate": 0.00018793176610370875,
      "loss": 0.0604,
      "step": 13298
    },
    {
      "epoch": 0.062555857644148,
      "grad_norm": 1.2006229162216187,
      "learning_rate": 0.00018793082312559527,
      "loss": 0.335,
      "step": 13299
    },
    {
      "epoch": 0.06256056144576046,
      "grad_norm": 0.7680348753929138,
      "learning_rate": 0.0001879298801474818,
      "loss": 0.0579,
      "step": 13300
    },
    {
      "epoch": 0.06256526524737292,
      "grad_norm": 3.4921681880950928,
      "learning_rate": 0.0001879289371693683,
      "loss": 0.4045,
      "step": 13301
    },
    {
      "epoch": 0.06256996904898539,
      "grad_norm": 1.045055866241455,
      "learning_rate": 0.00018792799419125483,
      "loss": 0.1032,
      "step": 13302
    },
    {
      "epoch": 0.06257467285059785,
      "grad_norm": 1.3609055280685425,
      "learning_rate": 0.00018792705121314137,
      "loss": 0.142,
      "step": 13303
    },
    {
      "epoch": 0.06257937665221032,
      "grad_norm": 2.8083648681640625,
      "learning_rate": 0.00018792610823502786,
      "loss": 0.5327,
      "step": 13304
    },
    {
      "epoch": 0.06258408045382279,
      "grad_norm": 2.477815628051758,
      "learning_rate": 0.00018792516525691438,
      "loss": 0.3966,
      "step": 13305
    },
    {
      "epoch": 0.06258878425543524,
      "grad_norm": 3.1267223358154297,
      "learning_rate": 0.0001879242222788009,
      "loss": 0.2799,
      "step": 13306
    },
    {
      "epoch": 0.0625934880570477,
      "grad_norm": 2.0437960624694824,
      "learning_rate": 0.00018792327930068745,
      "loss": 0.2666,
      "step": 13307
    },
    {
      "epoch": 0.06259819185866017,
      "grad_norm": 4.491281986236572,
      "learning_rate": 0.00018792233632257396,
      "loss": 0.5547,
      "step": 13308
    },
    {
      "epoch": 0.06260289566027263,
      "grad_norm": 2.229936361312866,
      "learning_rate": 0.00018792139334446048,
      "loss": 0.3732,
      "step": 13309
    },
    {
      "epoch": 0.0626075994618851,
      "grad_norm": 3.4413483142852783,
      "learning_rate": 0.000187920450366347,
      "loss": 0.3012,
      "step": 13310
    },
    {
      "epoch": 0.06261230326349757,
      "grad_norm": 5.512845516204834,
      "learning_rate": 0.00018791950738823352,
      "loss": 0.5647,
      "step": 13311
    },
    {
      "epoch": 0.06261700706511002,
      "grad_norm": 0.7255896925926208,
      "learning_rate": 0.00018791856441012007,
      "loss": 0.0508,
      "step": 13312
    },
    {
      "epoch": 0.06262171086672248,
      "grad_norm": 2.877901792526245,
      "learning_rate": 0.00018791762143200659,
      "loss": 0.218,
      "step": 13313
    },
    {
      "epoch": 0.06262641466833495,
      "grad_norm": 3.3252837657928467,
      "learning_rate": 0.0001879166784538931,
      "loss": 0.5106,
      "step": 13314
    },
    {
      "epoch": 0.06263111846994741,
      "grad_norm": 0.8801501393318176,
      "learning_rate": 0.0001879157354757796,
      "loss": 0.0522,
      "step": 13315
    },
    {
      "epoch": 0.06263582227155988,
      "grad_norm": 3.4261932373046875,
      "learning_rate": 0.00018791479249766614,
      "loss": 0.4602,
      "step": 13316
    },
    {
      "epoch": 0.06264052607317233,
      "grad_norm": 1.2623353004455566,
      "learning_rate": 0.00018791384951955266,
      "loss": 0.11,
      "step": 13317
    },
    {
      "epoch": 0.0626452298747848,
      "grad_norm": 0.8452192544937134,
      "learning_rate": 0.00018791290654143918,
      "loss": 0.0898,
      "step": 13318
    },
    {
      "epoch": 0.06264993367639726,
      "grad_norm": 0.5123200416564941,
      "learning_rate": 0.0001879119635633257,
      "loss": 0.0366,
      "step": 13319
    },
    {
      "epoch": 0.06265463747800973,
      "grad_norm": 1.8699963092803955,
      "learning_rate": 0.00018791102058521222,
      "loss": 0.2985,
      "step": 13320
    },
    {
      "epoch": 0.0626593412796222,
      "grad_norm": 0.702753484249115,
      "learning_rate": 0.00018791007760709876,
      "loss": 0.0751,
      "step": 13321
    },
    {
      "epoch": 0.06266404508123466,
      "grad_norm": 3.783543348312378,
      "learning_rate": 0.00018790913462898528,
      "loss": 0.312,
      "step": 13322
    },
    {
      "epoch": 0.06266874888284711,
      "grad_norm": 0.6205925941467285,
      "learning_rate": 0.0001879081916508718,
      "loss": 0.0815,
      "step": 13323
    },
    {
      "epoch": 0.06267345268445958,
      "grad_norm": 3.8178646564483643,
      "learning_rate": 0.00018790724867275832,
      "loss": 0.5349,
      "step": 13324
    },
    {
      "epoch": 0.06267815648607204,
      "grad_norm": 0.22176410257816315,
      "learning_rate": 0.00018790630569464484,
      "loss": 0.0224,
      "step": 13325
    },
    {
      "epoch": 0.06268286028768451,
      "grad_norm": 0.48044946789741516,
      "learning_rate": 0.00018790536271653135,
      "loss": 0.037,
      "step": 13326
    },
    {
      "epoch": 0.06268756408929697,
      "grad_norm": 2.1902267932891846,
      "learning_rate": 0.00018790441973841787,
      "loss": 0.2049,
      "step": 13327
    },
    {
      "epoch": 0.06269226789090944,
      "grad_norm": 0.22614823281764984,
      "learning_rate": 0.0001879034767603044,
      "loss": 0.0111,
      "step": 13328
    },
    {
      "epoch": 0.06269697169252189,
      "grad_norm": 0.656477153301239,
      "learning_rate": 0.0001879025337821909,
      "loss": 0.0763,
      "step": 13329
    },
    {
      "epoch": 0.06270167549413436,
      "grad_norm": 2.6240668296813965,
      "learning_rate": 0.00018790159080407746,
      "loss": 0.1687,
      "step": 13330
    },
    {
      "epoch": 0.06270637929574682,
      "grad_norm": 0.9871840476989746,
      "learning_rate": 0.00018790064782596398,
      "loss": 0.0695,
      "step": 13331
    },
    {
      "epoch": 0.06271108309735929,
      "grad_norm": 7.3229289054870605,
      "learning_rate": 0.0001878997048478505,
      "loss": 0.4672,
      "step": 13332
    },
    {
      "epoch": 0.06271578689897175,
      "grad_norm": 2.3305370807647705,
      "learning_rate": 0.000187898761869737,
      "loss": 0.2053,
      "step": 13333
    },
    {
      "epoch": 0.0627204907005842,
      "grad_norm": 4.14833927154541,
      "learning_rate": 0.00018789781889162356,
      "loss": 0.534,
      "step": 13334
    },
    {
      "epoch": 0.06272519450219667,
      "grad_norm": 2.3850836753845215,
      "learning_rate": 0.00018789687591351005,
      "loss": 0.1835,
      "step": 13335
    },
    {
      "epoch": 0.06272989830380914,
      "grad_norm": 3.053372621536255,
      "learning_rate": 0.00018789593293539657,
      "loss": 0.3352,
      "step": 13336
    },
    {
      "epoch": 0.0627346021054216,
      "grad_norm": 1.0631227493286133,
      "learning_rate": 0.0001878949899572831,
      "loss": 0.0738,
      "step": 13337
    },
    {
      "epoch": 0.06273930590703407,
      "grad_norm": 2.443066120147705,
      "learning_rate": 0.0001878940469791696,
      "loss": 0.2213,
      "step": 13338
    },
    {
      "epoch": 0.06274400970864653,
      "grad_norm": 3.676548957824707,
      "learning_rate": 0.00018789310400105615,
      "loss": 0.5627,
      "step": 13339
    },
    {
      "epoch": 0.06274871351025899,
      "grad_norm": 1.9111337661743164,
      "learning_rate": 0.00018789216102294267,
      "loss": 0.2642,
      "step": 13340
    },
    {
      "epoch": 0.06275341731187145,
      "grad_norm": 0.6600918173789978,
      "learning_rate": 0.0001878912180448292,
      "loss": 0.0235,
      "step": 13341
    },
    {
      "epoch": 0.06275812111348392,
      "grad_norm": 5.871689319610596,
      "learning_rate": 0.0001878902750667157,
      "loss": 0.4543,
      "step": 13342
    },
    {
      "epoch": 0.06276282491509638,
      "grad_norm": 3.0444231033325195,
      "learning_rate": 0.00018788933208860225,
      "loss": 0.4878,
      "step": 13343
    },
    {
      "epoch": 0.06276752871670885,
      "grad_norm": 4.284313678741455,
      "learning_rate": 0.00018788838911048877,
      "loss": 0.864,
      "step": 13344
    },
    {
      "epoch": 0.06277223251832131,
      "grad_norm": 4.843899726867676,
      "learning_rate": 0.0001878874461323753,
      "loss": 1.0012,
      "step": 13345
    },
    {
      "epoch": 0.06277693631993377,
      "grad_norm": 2.9412763118743896,
      "learning_rate": 0.00018788650315426178,
      "loss": 0.3368,
      "step": 13346
    },
    {
      "epoch": 0.06278164012154623,
      "grad_norm": 3.5372660160064697,
      "learning_rate": 0.0001878855601761483,
      "loss": 0.7574,
      "step": 13347
    },
    {
      "epoch": 0.0627863439231587,
      "grad_norm": 2.2422704696655273,
      "learning_rate": 0.00018788461719803485,
      "loss": 0.3753,
      "step": 13348
    },
    {
      "epoch": 0.06279104772477116,
      "grad_norm": 3.889753580093384,
      "learning_rate": 0.00018788367421992136,
      "loss": 0.8507,
      "step": 13349
    },
    {
      "epoch": 0.06279575152638363,
      "grad_norm": 1.1877282857894897,
      "learning_rate": 0.00018788273124180788,
      "loss": 0.1041,
      "step": 13350
    },
    {
      "epoch": 0.06280045532799608,
      "grad_norm": 13.30713939666748,
      "learning_rate": 0.0001878817882636944,
      "loss": 0.7821,
      "step": 13351
    },
    {
      "epoch": 0.06280515912960855,
      "grad_norm": 1.7766108512878418,
      "learning_rate": 0.00018788084528558092,
      "loss": 0.1501,
      "step": 13352
    },
    {
      "epoch": 0.06280986293122101,
      "grad_norm": 3.053811550140381,
      "learning_rate": 0.00018787990230746747,
      "loss": 0.4723,
      "step": 13353
    },
    {
      "epoch": 0.06281456673283348,
      "grad_norm": 0.9239107966423035,
      "learning_rate": 0.00018787895932935399,
      "loss": 0.0704,
      "step": 13354
    },
    {
      "epoch": 0.06281927053444594,
      "grad_norm": 2.995932102203369,
      "learning_rate": 0.0001878780163512405,
      "loss": 0.5201,
      "step": 13355
    },
    {
      "epoch": 0.06282397433605841,
      "grad_norm": 1.7439237833023071,
      "learning_rate": 0.00018787707337312702,
      "loss": 0.1457,
      "step": 13356
    },
    {
      "epoch": 0.06282867813767086,
      "grad_norm": 2.0575006008148193,
      "learning_rate": 0.00018787613039501354,
      "loss": 0.1796,
      "step": 13357
    },
    {
      "epoch": 0.06283338193928333,
      "grad_norm": 2.3129560947418213,
      "learning_rate": 0.00018787518741690006,
      "loss": 0.289,
      "step": 13358
    },
    {
      "epoch": 0.06283808574089579,
      "grad_norm": 2.694780111312866,
      "learning_rate": 0.00018787424443878658,
      "loss": 0.3179,
      "step": 13359
    },
    {
      "epoch": 0.06284278954250826,
      "grad_norm": 0.5821363925933838,
      "learning_rate": 0.0001878733014606731,
      "loss": 0.048,
      "step": 13360
    },
    {
      "epoch": 0.06284749334412072,
      "grad_norm": 1.131637692451477,
      "learning_rate": 0.00018787235848255962,
      "loss": 0.11,
      "step": 13361
    },
    {
      "epoch": 0.06285219714573319,
      "grad_norm": 5.850535869598389,
      "learning_rate": 0.00018787141550444616,
      "loss": 0.2029,
      "step": 13362
    },
    {
      "epoch": 0.06285690094734564,
      "grad_norm": 3.983060121536255,
      "learning_rate": 0.00018787047252633268,
      "loss": 0.3039,
      "step": 13363
    },
    {
      "epoch": 0.0628616047489581,
      "grad_norm": 0.8325002193450928,
      "learning_rate": 0.0001878695295482192,
      "loss": 0.0773,
      "step": 13364
    },
    {
      "epoch": 0.06286630855057057,
      "grad_norm": 1.1432714462280273,
      "learning_rate": 0.00018786858657010572,
      "loss": 0.1767,
      "step": 13365
    },
    {
      "epoch": 0.06287101235218304,
      "grad_norm": 1.7762151956558228,
      "learning_rate": 0.00018786764359199224,
      "loss": 0.3153,
      "step": 13366
    },
    {
      "epoch": 0.0628757161537955,
      "grad_norm": 2.7146406173706055,
      "learning_rate": 0.00018786670061387875,
      "loss": 0.7026,
      "step": 13367
    },
    {
      "epoch": 0.06288041995540795,
      "grad_norm": 0.25118324160575867,
      "learning_rate": 0.00018786575763576527,
      "loss": 0.0229,
      "step": 13368
    },
    {
      "epoch": 0.06288512375702042,
      "grad_norm": 12.493380546569824,
      "learning_rate": 0.0001878648146576518,
      "loss": 0.3226,
      "step": 13369
    },
    {
      "epoch": 0.06288982755863289,
      "grad_norm": 0.42175814509391785,
      "learning_rate": 0.0001878638716795383,
      "loss": 0.0485,
      "step": 13370
    },
    {
      "epoch": 0.06289453136024535,
      "grad_norm": 3.0956428050994873,
      "learning_rate": 0.00018786292870142486,
      "loss": 0.4729,
      "step": 13371
    },
    {
      "epoch": 0.06289923516185782,
      "grad_norm": 1.9174292087554932,
      "learning_rate": 0.00018786198572331137,
      "loss": 0.3483,
      "step": 13372
    },
    {
      "epoch": 0.06290393896347028,
      "grad_norm": 2.006769895553589,
      "learning_rate": 0.0001878610427451979,
      "loss": 0.1376,
      "step": 13373
    },
    {
      "epoch": 0.06290864276508273,
      "grad_norm": 1.2444000244140625,
      "learning_rate": 0.0001878600997670844,
      "loss": 0.1328,
      "step": 13374
    },
    {
      "epoch": 0.0629133465666952,
      "grad_norm": 0.5583743453025818,
      "learning_rate": 0.00018785915678897096,
      "loss": 0.0328,
      "step": 13375
    },
    {
      "epoch": 0.06291805036830767,
      "grad_norm": 2.451239824295044,
      "learning_rate": 0.00018785821381085748,
      "loss": 0.2506,
      "step": 13376
    },
    {
      "epoch": 0.06292275416992013,
      "grad_norm": 2.7689318656921387,
      "learning_rate": 0.00018785727083274397,
      "loss": 0.2118,
      "step": 13377
    },
    {
      "epoch": 0.0629274579715326,
      "grad_norm": 4.536745071411133,
      "learning_rate": 0.0001878563278546305,
      "loss": 0.843,
      "step": 13378
    },
    {
      "epoch": 0.06293216177314506,
      "grad_norm": 2.7865803241729736,
      "learning_rate": 0.000187855384876517,
      "loss": 0.2132,
      "step": 13379
    },
    {
      "epoch": 0.06293686557475751,
      "grad_norm": 1.1441608667373657,
      "learning_rate": 0.00018785444189840355,
      "loss": 0.1372,
      "step": 13380
    },
    {
      "epoch": 0.06294156937636998,
      "grad_norm": 1.5470008850097656,
      "learning_rate": 0.00018785349892029007,
      "loss": 0.119,
      "step": 13381
    },
    {
      "epoch": 0.06294627317798245,
      "grad_norm": 0.19744522869586945,
      "learning_rate": 0.0001878525559421766,
      "loss": 0.013,
      "step": 13382
    },
    {
      "epoch": 0.06295097697959491,
      "grad_norm": 1.5886139869689941,
      "learning_rate": 0.0001878516129640631,
      "loss": 0.1548,
      "step": 13383
    },
    {
      "epoch": 0.06295568078120738,
      "grad_norm": 0.24842239916324615,
      "learning_rate": 0.00018785066998594965,
      "loss": 0.0174,
      "step": 13384
    },
    {
      "epoch": 0.06296038458281983,
      "grad_norm": 1.9924274682998657,
      "learning_rate": 0.00018784972700783617,
      "loss": 0.6365,
      "step": 13385
    },
    {
      "epoch": 0.0629650883844323,
      "grad_norm": 1.2843666076660156,
      "learning_rate": 0.0001878487840297227,
      "loss": 0.179,
      "step": 13386
    },
    {
      "epoch": 0.06296979218604476,
      "grad_norm": 1.2339967489242554,
      "learning_rate": 0.0001878478410516092,
      "loss": 0.0863,
      "step": 13387
    },
    {
      "epoch": 0.06297449598765723,
      "grad_norm": 0.2984849810600281,
      "learning_rate": 0.00018784689807349573,
      "loss": 0.0235,
      "step": 13388
    },
    {
      "epoch": 0.06297919978926969,
      "grad_norm": 0.7028263211250305,
      "learning_rate": 0.00018784595509538225,
      "loss": 0.0641,
      "step": 13389
    },
    {
      "epoch": 0.06298390359088216,
      "grad_norm": 1.9955402612686157,
      "learning_rate": 0.00018784501211726876,
      "loss": 0.3769,
      "step": 13390
    },
    {
      "epoch": 0.06298860739249461,
      "grad_norm": 1.0709871053695679,
      "learning_rate": 0.00018784406913915528,
      "loss": 0.1352,
      "step": 13391
    },
    {
      "epoch": 0.06299331119410707,
      "grad_norm": 1.9493049383163452,
      "learning_rate": 0.0001878431261610418,
      "loss": 0.5239,
      "step": 13392
    },
    {
      "epoch": 0.06299801499571954,
      "grad_norm": 3.4932024478912354,
      "learning_rate": 0.00018784218318292835,
      "loss": 0.4274,
      "step": 13393
    },
    {
      "epoch": 0.063002718797332,
      "grad_norm": 1.9874759912490845,
      "learning_rate": 0.00018784124020481487,
      "loss": 0.4286,
      "step": 13394
    },
    {
      "epoch": 0.06300742259894447,
      "grad_norm": 0.1719951182603836,
      "learning_rate": 0.00018784029722670139,
      "loss": 0.0127,
      "step": 13395
    },
    {
      "epoch": 0.06301212640055694,
      "grad_norm": 1.3075776100158691,
      "learning_rate": 0.0001878393542485879,
      "loss": 0.1616,
      "step": 13396
    },
    {
      "epoch": 0.06301683020216939,
      "grad_norm": 1.2326200008392334,
      "learning_rate": 0.00018783841127047442,
      "loss": 0.0985,
      "step": 13397
    },
    {
      "epoch": 0.06302153400378185,
      "grad_norm": 2.587750196456909,
      "learning_rate": 0.00018783746829236094,
      "loss": 0.5744,
      "step": 13398
    },
    {
      "epoch": 0.06302623780539432,
      "grad_norm": 2.1881909370422363,
      "learning_rate": 0.00018783652531424746,
      "loss": 0.3685,
      "step": 13399
    },
    {
      "epoch": 0.06303094160700679,
      "grad_norm": 0.22094997763633728,
      "learning_rate": 0.00018783558233613398,
      "loss": 0.0169,
      "step": 13400
    },
    {
      "epoch": 0.06303564540861925,
      "grad_norm": 0.413353830575943,
      "learning_rate": 0.0001878346393580205,
      "loss": 0.0376,
      "step": 13401
    },
    {
      "epoch": 0.0630403492102317,
      "grad_norm": 4.185734272003174,
      "learning_rate": 0.00018783369637990702,
      "loss": 0.7251,
      "step": 13402
    },
    {
      "epoch": 0.06304505301184417,
      "grad_norm": 2.3895716667175293,
      "learning_rate": 0.00018783275340179356,
      "loss": 0.3221,
      "step": 13403
    },
    {
      "epoch": 0.06304975681345663,
      "grad_norm": 6.915262699127197,
      "learning_rate": 0.00018783181042368008,
      "loss": 0.4276,
      "step": 13404
    },
    {
      "epoch": 0.0630544606150691,
      "grad_norm": 3.044846534729004,
      "learning_rate": 0.0001878308674455666,
      "loss": 0.4595,
      "step": 13405
    },
    {
      "epoch": 0.06305916441668157,
      "grad_norm": 1.4728360176086426,
      "learning_rate": 0.00018782992446745312,
      "loss": 0.1588,
      "step": 13406
    },
    {
      "epoch": 0.06306386821829403,
      "grad_norm": 1.3026440143585205,
      "learning_rate": 0.00018782898148933966,
      "loss": 0.1219,
      "step": 13407
    },
    {
      "epoch": 0.06306857201990648,
      "grad_norm": 2.2523415088653564,
      "learning_rate": 0.00018782803851122615,
      "loss": 0.2292,
      "step": 13408
    },
    {
      "epoch": 0.06307327582151895,
      "grad_norm": 4.123835563659668,
      "learning_rate": 0.00018782709553311267,
      "loss": 0.9185,
      "step": 13409
    },
    {
      "epoch": 0.06307797962313141,
      "grad_norm": 1.5459692478179932,
      "learning_rate": 0.0001878261525549992,
      "loss": 0.1544,
      "step": 13410
    },
    {
      "epoch": 0.06308268342474388,
      "grad_norm": 3.409109354019165,
      "learning_rate": 0.0001878252095768857,
      "loss": 0.3645,
      "step": 13411
    },
    {
      "epoch": 0.06308738722635635,
      "grad_norm": 1.7234084606170654,
      "learning_rate": 0.00018782426659877226,
      "loss": 0.1504,
      "step": 13412
    },
    {
      "epoch": 0.06309209102796881,
      "grad_norm": 0.7187220454216003,
      "learning_rate": 0.00018782332362065877,
      "loss": 0.0352,
      "step": 13413
    },
    {
      "epoch": 0.06309679482958126,
      "grad_norm": 3.5539910793304443,
      "learning_rate": 0.0001878223806425453,
      "loss": 0.3978,
      "step": 13414
    },
    {
      "epoch": 0.06310149863119373,
      "grad_norm": 1.6426351070404053,
      "learning_rate": 0.0001878214376644318,
      "loss": 0.1996,
      "step": 13415
    },
    {
      "epoch": 0.0631062024328062,
      "grad_norm": 3.1926419734954834,
      "learning_rate": 0.00018782049468631836,
      "loss": 0.2631,
      "step": 13416
    },
    {
      "epoch": 0.06311090623441866,
      "grad_norm": 1.906286358833313,
      "learning_rate": 0.00018781955170820488,
      "loss": 0.203,
      "step": 13417
    },
    {
      "epoch": 0.06311561003603113,
      "grad_norm": 0.7257372736930847,
      "learning_rate": 0.0001878186087300914,
      "loss": 0.0831,
      "step": 13418
    },
    {
      "epoch": 0.06312031383764358,
      "grad_norm": 0.4837929904460907,
      "learning_rate": 0.00018781766575197791,
      "loss": 0.0428,
      "step": 13419
    },
    {
      "epoch": 0.06312501763925604,
      "grad_norm": 2.9712107181549072,
      "learning_rate": 0.0001878167227738644,
      "loss": 0.3984,
      "step": 13420
    },
    {
      "epoch": 0.06312972144086851,
      "grad_norm": 1.9703130722045898,
      "learning_rate": 0.00018781577979575095,
      "loss": 0.2238,
      "step": 13421
    },
    {
      "epoch": 0.06313442524248097,
      "grad_norm": 0.7369617819786072,
      "learning_rate": 0.00018781483681763747,
      "loss": 0.0948,
      "step": 13422
    },
    {
      "epoch": 0.06313912904409344,
      "grad_norm": 0.3302924335002899,
      "learning_rate": 0.000187813893839524,
      "loss": 0.0255,
      "step": 13423
    },
    {
      "epoch": 0.0631438328457059,
      "grad_norm": 1.2784050703048706,
      "learning_rate": 0.0001878129508614105,
      "loss": 0.1513,
      "step": 13424
    },
    {
      "epoch": 0.06314853664731836,
      "grad_norm": 2.0258290767669678,
      "learning_rate": 0.00018781200788329705,
      "loss": 0.2351,
      "step": 13425
    },
    {
      "epoch": 0.06315324044893082,
      "grad_norm": 0.3395411968231201,
      "learning_rate": 0.00018781106490518357,
      "loss": 0.0311,
      "step": 13426
    },
    {
      "epoch": 0.06315794425054329,
      "grad_norm": 17.303237915039062,
      "learning_rate": 0.0001878101219270701,
      "loss": 0.7631,
      "step": 13427
    },
    {
      "epoch": 0.06316264805215575,
      "grad_norm": 2.5241611003875732,
      "learning_rate": 0.0001878091789489566,
      "loss": 0.3758,
      "step": 13428
    },
    {
      "epoch": 0.06316735185376822,
      "grad_norm": 0.42055895924568176,
      "learning_rate": 0.00018780823597084313,
      "loss": 0.0295,
      "step": 13429
    },
    {
      "epoch": 0.06317205565538069,
      "grad_norm": 6.363551139831543,
      "learning_rate": 0.00018780729299272965,
      "loss": 0.8148,
      "step": 13430
    },
    {
      "epoch": 0.06317675945699314,
      "grad_norm": 1.6913431882858276,
      "learning_rate": 0.00018780635001461616,
      "loss": 0.1487,
      "step": 13431
    },
    {
      "epoch": 0.0631814632586056,
      "grad_norm": 1.2140285968780518,
      "learning_rate": 0.00018780540703650268,
      "loss": 0.1417,
      "step": 13432
    },
    {
      "epoch": 0.06318616706021807,
      "grad_norm": 3.299736261367798,
      "learning_rate": 0.0001878044640583892,
      "loss": 0.365,
      "step": 13433
    },
    {
      "epoch": 0.06319087086183053,
      "grad_norm": 0.07392282038927078,
      "learning_rate": 0.00018780352108027575,
      "loss": 0.0045,
      "step": 13434
    },
    {
      "epoch": 0.063195574663443,
      "grad_norm": 3.0679030418395996,
      "learning_rate": 0.00018780257810216227,
      "loss": 0.3249,
      "step": 13435
    },
    {
      "epoch": 0.06320027846505545,
      "grad_norm": 2.2063379287719727,
      "learning_rate": 0.00018780163512404879,
      "loss": 0.2452,
      "step": 13436
    },
    {
      "epoch": 0.06320498226666792,
      "grad_norm": 0.8650062084197998,
      "learning_rate": 0.0001878006921459353,
      "loss": 0.0795,
      "step": 13437
    },
    {
      "epoch": 0.06320968606828038,
      "grad_norm": 2.6837656497955322,
      "learning_rate": 0.00018779974916782182,
      "loss": 0.4574,
      "step": 13438
    },
    {
      "epoch": 0.06321438986989285,
      "grad_norm": 1.9866619110107422,
      "learning_rate": 0.00018779880618970834,
      "loss": 0.4082,
      "step": 13439
    },
    {
      "epoch": 0.06321909367150531,
      "grad_norm": 3.1770830154418945,
      "learning_rate": 0.00018779786321159486,
      "loss": 0.5185,
      "step": 13440
    },
    {
      "epoch": 0.06322379747311778,
      "grad_norm": 2.5740621089935303,
      "learning_rate": 0.00018779692023348138,
      "loss": 0.7068,
      "step": 13441
    },
    {
      "epoch": 0.06322850127473023,
      "grad_norm": 1.4784505367279053,
      "learning_rate": 0.0001877959772553679,
      "loss": 0.1514,
      "step": 13442
    },
    {
      "epoch": 0.0632332050763427,
      "grad_norm": 3.8860983848571777,
      "learning_rate": 0.00018779503427725444,
      "loss": 0.663,
      "step": 13443
    },
    {
      "epoch": 0.06323790887795516,
      "grad_norm": 1.5059118270874023,
      "learning_rate": 0.00018779409129914096,
      "loss": 0.1286,
      "step": 13444
    },
    {
      "epoch": 0.06324261267956763,
      "grad_norm": 1.3060694932937622,
      "learning_rate": 0.00018779314832102748,
      "loss": 0.1327,
      "step": 13445
    },
    {
      "epoch": 0.0632473164811801,
      "grad_norm": 2.038048028945923,
      "learning_rate": 0.000187792205342914,
      "loss": 0.2984,
      "step": 13446
    },
    {
      "epoch": 0.06325202028279256,
      "grad_norm": 1.1782840490341187,
      "learning_rate": 0.00018779126236480052,
      "loss": 0.1602,
      "step": 13447
    },
    {
      "epoch": 0.06325672408440501,
      "grad_norm": 1.3771946430206299,
      "learning_rate": 0.00018779031938668706,
      "loss": 0.1366,
      "step": 13448
    },
    {
      "epoch": 0.06326142788601748,
      "grad_norm": 1.1146984100341797,
      "learning_rate": 0.00018778937640857358,
      "loss": 0.0985,
      "step": 13449
    },
    {
      "epoch": 0.06326613168762994,
      "grad_norm": 3.0685296058654785,
      "learning_rate": 0.00018778843343046007,
      "loss": 0.4574,
      "step": 13450
    },
    {
      "epoch": 0.06327083548924241,
      "grad_norm": 0.757070004940033,
      "learning_rate": 0.0001877874904523466,
      "loss": 0.0548,
      "step": 13451
    },
    {
      "epoch": 0.06327553929085487,
      "grad_norm": 3.899509906768799,
      "learning_rate": 0.0001877865474742331,
      "loss": 0.6785,
      "step": 13452
    },
    {
      "epoch": 0.06328024309246733,
      "grad_norm": 5.113229751586914,
      "learning_rate": 0.00018778560449611966,
      "loss": 0.2565,
      "step": 13453
    },
    {
      "epoch": 0.06328494689407979,
      "grad_norm": 2.3230433464050293,
      "learning_rate": 0.00018778466151800617,
      "loss": 0.306,
      "step": 13454
    },
    {
      "epoch": 0.06328965069569226,
      "grad_norm": 3.2893943786621094,
      "learning_rate": 0.0001877837185398927,
      "loss": 0.4482,
      "step": 13455
    },
    {
      "epoch": 0.06329435449730472,
      "grad_norm": 0.9712239503860474,
      "learning_rate": 0.0001877827755617792,
      "loss": 0.1077,
      "step": 13456
    },
    {
      "epoch": 0.06329905829891719,
      "grad_norm": 4.123353004455566,
      "learning_rate": 0.00018778183258366576,
      "loss": 0.4325,
      "step": 13457
    },
    {
      "epoch": 0.06330376210052965,
      "grad_norm": 1.4260536432266235,
      "learning_rate": 0.00018778088960555228,
      "loss": 0.0796,
      "step": 13458
    },
    {
      "epoch": 0.0633084659021421,
      "grad_norm": 3.137951374053955,
      "learning_rate": 0.0001877799466274388,
      "loss": 0.3211,
      "step": 13459
    },
    {
      "epoch": 0.06331316970375457,
      "grad_norm": 1.2110472917556763,
      "learning_rate": 0.00018777900364932531,
      "loss": 0.1439,
      "step": 13460
    },
    {
      "epoch": 0.06331787350536704,
      "grad_norm": 4.468946933746338,
      "learning_rate": 0.00018777806067121183,
      "loss": 0.5942,
      "step": 13461
    },
    {
      "epoch": 0.0633225773069795,
      "grad_norm": 2.8085734844207764,
      "learning_rate": 0.00018777711769309835,
      "loss": 0.2436,
      "step": 13462
    },
    {
      "epoch": 0.06332728110859197,
      "grad_norm": 0.6724941730499268,
      "learning_rate": 0.00018777617471498487,
      "loss": 0.0886,
      "step": 13463
    },
    {
      "epoch": 0.06333198491020443,
      "grad_norm": 0.9537978768348694,
      "learning_rate": 0.0001877752317368714,
      "loss": 0.0595,
      "step": 13464
    },
    {
      "epoch": 0.06333668871181689,
      "grad_norm": 2.801645040512085,
      "learning_rate": 0.0001877742887587579,
      "loss": 0.4345,
      "step": 13465
    },
    {
      "epoch": 0.06334139251342935,
      "grad_norm": 4.130584239959717,
      "learning_rate": 0.00018777334578064445,
      "loss": 0.5221,
      "step": 13466
    },
    {
      "epoch": 0.06334609631504182,
      "grad_norm": 1.2491326332092285,
      "learning_rate": 0.00018777240280253097,
      "loss": 0.097,
      "step": 13467
    },
    {
      "epoch": 0.06335080011665428,
      "grad_norm": 0.1365920752286911,
      "learning_rate": 0.0001877714598244175,
      "loss": 0.0139,
      "step": 13468
    },
    {
      "epoch": 0.06335550391826675,
      "grad_norm": 2.80792498588562,
      "learning_rate": 0.000187770516846304,
      "loss": 0.6327,
      "step": 13469
    },
    {
      "epoch": 0.0633602077198792,
      "grad_norm": 2.0102412700653076,
      "learning_rate": 0.00018776957386819053,
      "loss": 0.2682,
      "step": 13470
    },
    {
      "epoch": 0.06336491152149167,
      "grad_norm": 1.3810311555862427,
      "learning_rate": 0.00018776863089007705,
      "loss": 0.276,
      "step": 13471
    },
    {
      "epoch": 0.06336961532310413,
      "grad_norm": 2.2642767429351807,
      "learning_rate": 0.00018776768791196356,
      "loss": 0.5002,
      "step": 13472
    },
    {
      "epoch": 0.0633743191247166,
      "grad_norm": 2.0948610305786133,
      "learning_rate": 0.00018776674493385008,
      "loss": 0.3277,
      "step": 13473
    },
    {
      "epoch": 0.06337902292632906,
      "grad_norm": 2.3992321491241455,
      "learning_rate": 0.0001877658019557366,
      "loss": 0.3054,
      "step": 13474
    },
    {
      "epoch": 0.06338372672794153,
      "grad_norm": 0.36675670742988586,
      "learning_rate": 0.00018776485897762315,
      "loss": 0.0208,
      "step": 13475
    },
    {
      "epoch": 0.06338843052955398,
      "grad_norm": 3.234778881072998,
      "learning_rate": 0.00018776391599950967,
      "loss": 0.7142,
      "step": 13476
    },
    {
      "epoch": 0.06339313433116645,
      "grad_norm": 1.3288949728012085,
      "learning_rate": 0.00018776297302139619,
      "loss": 0.1974,
      "step": 13477
    },
    {
      "epoch": 0.06339783813277891,
      "grad_norm": 1.4084643125534058,
      "learning_rate": 0.0001877620300432827,
      "loss": 0.1574,
      "step": 13478
    },
    {
      "epoch": 0.06340254193439138,
      "grad_norm": 0.38310912251472473,
      "learning_rate": 0.00018776108706516922,
      "loss": 0.0399,
      "step": 13479
    },
    {
      "epoch": 0.06340724573600384,
      "grad_norm": 3.3372457027435303,
      "learning_rate": 0.00018776014408705577,
      "loss": 0.9842,
      "step": 13480
    },
    {
      "epoch": 0.06341194953761631,
      "grad_norm": 1.2662452459335327,
      "learning_rate": 0.00018775920110894226,
      "loss": 0.1671,
      "step": 13481
    },
    {
      "epoch": 0.06341665333922876,
      "grad_norm": 2.350984573364258,
      "learning_rate": 0.00018775825813082878,
      "loss": 0.4324,
      "step": 13482
    },
    {
      "epoch": 0.06342135714084123,
      "grad_norm": 4.3989033699035645,
      "learning_rate": 0.0001877573151527153,
      "loss": 1.2582,
      "step": 13483
    },
    {
      "epoch": 0.06342606094245369,
      "grad_norm": 0.4898398816585541,
      "learning_rate": 0.00018775637217460184,
      "loss": 0.0515,
      "step": 13484
    },
    {
      "epoch": 0.06343076474406616,
      "grad_norm": 1.5103710889816284,
      "learning_rate": 0.00018775542919648836,
      "loss": 0.2526,
      "step": 13485
    },
    {
      "epoch": 0.06343546854567862,
      "grad_norm": 2.314237356185913,
      "learning_rate": 0.00018775448621837488,
      "loss": 0.2379,
      "step": 13486
    },
    {
      "epoch": 0.06344017234729107,
      "grad_norm": 3.1021950244903564,
      "learning_rate": 0.0001877535432402614,
      "loss": 0.5756,
      "step": 13487
    },
    {
      "epoch": 0.06344487614890354,
      "grad_norm": 3.1813433170318604,
      "learning_rate": 0.00018775260026214792,
      "loss": 0.6393,
      "step": 13488
    },
    {
      "epoch": 0.063449579950516,
      "grad_norm": 1.9914542436599731,
      "learning_rate": 0.00018775165728403446,
      "loss": 0.5034,
      "step": 13489
    },
    {
      "epoch": 0.06345428375212847,
      "grad_norm": 3.6427836418151855,
      "learning_rate": 0.00018775071430592098,
      "loss": 0.2733,
      "step": 13490
    },
    {
      "epoch": 0.06345898755374094,
      "grad_norm": 1.4386286735534668,
      "learning_rate": 0.0001877497713278075,
      "loss": 0.2245,
      "step": 13491
    },
    {
      "epoch": 0.0634636913553534,
      "grad_norm": 2.0091991424560547,
      "learning_rate": 0.00018774882834969402,
      "loss": 0.2091,
      "step": 13492
    },
    {
      "epoch": 0.06346839515696585,
      "grad_norm": 1.1606441736221313,
      "learning_rate": 0.00018774788537158054,
      "loss": 0.1661,
      "step": 13493
    },
    {
      "epoch": 0.06347309895857832,
      "grad_norm": 1.381624698638916,
      "learning_rate": 0.00018774694239346706,
      "loss": 0.2399,
      "step": 13494
    },
    {
      "epoch": 0.06347780276019079,
      "grad_norm": 2.361051082611084,
      "learning_rate": 0.00018774599941535357,
      "loss": 0.6158,
      "step": 13495
    },
    {
      "epoch": 0.06348250656180325,
      "grad_norm": 1.2954918146133423,
      "learning_rate": 0.0001877450564372401,
      "loss": 0.2446,
      "step": 13496
    },
    {
      "epoch": 0.06348721036341572,
      "grad_norm": 0.8703485131263733,
      "learning_rate": 0.0001877441134591266,
      "loss": 0.1856,
      "step": 13497
    },
    {
      "epoch": 0.06349191416502818,
      "grad_norm": 2.927213430404663,
      "learning_rate": 0.00018774317048101316,
      "loss": 0.3401,
      "step": 13498
    },
    {
      "epoch": 0.06349661796664063,
      "grad_norm": 1.1703100204467773,
      "learning_rate": 0.00018774222750289968,
      "loss": 0.1173,
      "step": 13499
    },
    {
      "epoch": 0.0635013217682531,
      "grad_norm": 1.5541616678237915,
      "learning_rate": 0.0001877412845247862,
      "loss": 0.3036,
      "step": 13500
    },
    {
      "epoch": 0.06350602556986557,
      "grad_norm": 2.013216495513916,
      "learning_rate": 0.00018774034154667271,
      "loss": 0.3396,
      "step": 13501
    },
    {
      "epoch": 0.06351072937147803,
      "grad_norm": 0.4848273694515228,
      "learning_rate": 0.00018773939856855923,
      "loss": 0.0459,
      "step": 13502
    },
    {
      "epoch": 0.0635154331730905,
      "grad_norm": 1.5744471549987793,
      "learning_rate": 0.00018773845559044575,
      "loss": 0.1698,
      "step": 13503
    },
    {
      "epoch": 0.06352013697470295,
      "grad_norm": 2.0553832054138184,
      "learning_rate": 0.00018773751261233227,
      "loss": 0.2302,
      "step": 13504
    },
    {
      "epoch": 0.06352484077631541,
      "grad_norm": 4.025034427642822,
      "learning_rate": 0.0001877365696342188,
      "loss": 0.3407,
      "step": 13505
    },
    {
      "epoch": 0.06352954457792788,
      "grad_norm": 3.7110161781311035,
      "learning_rate": 0.0001877356266561053,
      "loss": 1.0141,
      "step": 13506
    },
    {
      "epoch": 0.06353424837954035,
      "grad_norm": 2.4779269695281982,
      "learning_rate": 0.00018773468367799185,
      "loss": 0.3527,
      "step": 13507
    },
    {
      "epoch": 0.06353895218115281,
      "grad_norm": 1.0127217769622803,
      "learning_rate": 0.00018773374069987837,
      "loss": 0.1003,
      "step": 13508
    },
    {
      "epoch": 0.06354365598276528,
      "grad_norm": 3.908961772918701,
      "learning_rate": 0.0001877327977217649,
      "loss": 0.188,
      "step": 13509
    },
    {
      "epoch": 0.06354835978437773,
      "grad_norm": 0.7349862456321716,
      "learning_rate": 0.0001877318547436514,
      "loss": 0.0685,
      "step": 13510
    },
    {
      "epoch": 0.0635530635859902,
      "grad_norm": 1.1925773620605469,
      "learning_rate": 0.00018773091176553793,
      "loss": 0.143,
      "step": 13511
    },
    {
      "epoch": 0.06355776738760266,
      "grad_norm": 1.7207611799240112,
      "learning_rate": 0.00018772996878742445,
      "loss": 0.2634,
      "step": 13512
    },
    {
      "epoch": 0.06356247118921513,
      "grad_norm": 0.9119068384170532,
      "learning_rate": 0.00018772902580931096,
      "loss": 0.1028,
      "step": 13513
    },
    {
      "epoch": 0.06356717499082759,
      "grad_norm": 2.9677672386169434,
      "learning_rate": 0.00018772808283119748,
      "loss": 0.3341,
      "step": 13514
    },
    {
      "epoch": 0.06357187879244006,
      "grad_norm": 1.1473685503005981,
      "learning_rate": 0.000187727139853084,
      "loss": 0.1251,
      "step": 13515
    },
    {
      "epoch": 0.06357658259405251,
      "grad_norm": 2.6307952404022217,
      "learning_rate": 0.00018772619687497055,
      "loss": 0.2163,
      "step": 13516
    },
    {
      "epoch": 0.06358128639566497,
      "grad_norm": 3.3950693607330322,
      "learning_rate": 0.00018772525389685707,
      "loss": 0.3505,
      "step": 13517
    },
    {
      "epoch": 0.06358599019727744,
      "grad_norm": 1.6734027862548828,
      "learning_rate": 0.00018772431091874359,
      "loss": 0.1889,
      "step": 13518
    },
    {
      "epoch": 0.0635906939988899,
      "grad_norm": 8.119866371154785,
      "learning_rate": 0.0001877233679406301,
      "loss": 0.8737,
      "step": 13519
    },
    {
      "epoch": 0.06359539780050237,
      "grad_norm": 1.3878291845321655,
      "learning_rate": 0.00018772242496251662,
      "loss": 0.1476,
      "step": 13520
    },
    {
      "epoch": 0.06360010160211482,
      "grad_norm": 1.9487121105194092,
      "learning_rate": 0.00018772148198440317,
      "loss": 0.2503,
      "step": 13521
    },
    {
      "epoch": 0.06360480540372729,
      "grad_norm": 0.9195855259895325,
      "learning_rate": 0.0001877205390062897,
      "loss": 0.0658,
      "step": 13522
    },
    {
      "epoch": 0.06360950920533975,
      "grad_norm": 1.6183784008026123,
      "learning_rate": 0.0001877195960281762,
      "loss": 0.1561,
      "step": 13523
    },
    {
      "epoch": 0.06361421300695222,
      "grad_norm": 1.0500051975250244,
      "learning_rate": 0.0001877186530500627,
      "loss": 0.073,
      "step": 13524
    },
    {
      "epoch": 0.06361891680856469,
      "grad_norm": 1.117496132850647,
      "learning_rate": 0.00018771771007194924,
      "loss": 0.1596,
      "step": 13525
    },
    {
      "epoch": 0.06362362061017715,
      "grad_norm": 0.748271644115448,
      "learning_rate": 0.00018771676709383576,
      "loss": 0.054,
      "step": 13526
    },
    {
      "epoch": 0.0636283244117896,
      "grad_norm": 2.464482545852661,
      "learning_rate": 0.00018771582411572228,
      "loss": 0.483,
      "step": 13527
    },
    {
      "epoch": 0.06363302821340207,
      "grad_norm": 3.9390814304351807,
      "learning_rate": 0.0001877148811376088,
      "loss": 0.1115,
      "step": 13528
    },
    {
      "epoch": 0.06363773201501453,
      "grad_norm": 3.3803224563598633,
      "learning_rate": 0.00018771393815949532,
      "loss": 0.4424,
      "step": 13529
    },
    {
      "epoch": 0.063642435816627,
      "grad_norm": 2.183227777481079,
      "learning_rate": 0.00018771299518138186,
      "loss": 0.3151,
      "step": 13530
    },
    {
      "epoch": 0.06364713961823946,
      "grad_norm": 1.0282704830169678,
      "learning_rate": 0.00018771205220326838,
      "loss": 0.0678,
      "step": 13531
    },
    {
      "epoch": 0.06365184341985193,
      "grad_norm": 0.5009092092514038,
      "learning_rate": 0.0001877111092251549,
      "loss": 0.0331,
      "step": 13532
    },
    {
      "epoch": 0.06365654722146438,
      "grad_norm": 0.2898038327693939,
      "learning_rate": 0.00018771016624704142,
      "loss": 0.0131,
      "step": 13533
    },
    {
      "epoch": 0.06366125102307685,
      "grad_norm": 4.148658275604248,
      "learning_rate": 0.00018770922326892794,
      "loss": 0.0631,
      "step": 13534
    },
    {
      "epoch": 0.06366595482468931,
      "grad_norm": 3.9217681884765625,
      "learning_rate": 0.00018770828029081446,
      "loss": 0.5824,
      "step": 13535
    },
    {
      "epoch": 0.06367065862630178,
      "grad_norm": 0.6527911424636841,
      "learning_rate": 0.00018770733731270097,
      "loss": 0.0555,
      "step": 13536
    },
    {
      "epoch": 0.06367536242791424,
      "grad_norm": 0.30559781193733215,
      "learning_rate": 0.0001877063943345875,
      "loss": 0.0179,
      "step": 13537
    },
    {
      "epoch": 0.0636800662295267,
      "grad_norm": 3.3592562675476074,
      "learning_rate": 0.000187705451356474,
      "loss": 0.6207,
      "step": 13538
    },
    {
      "epoch": 0.06368477003113916,
      "grad_norm": 2.330416440963745,
      "learning_rate": 0.00018770450837836056,
      "loss": 0.1623,
      "step": 13539
    },
    {
      "epoch": 0.06368947383275163,
      "grad_norm": 3.085705041885376,
      "learning_rate": 0.00018770356540024708,
      "loss": 0.1609,
      "step": 13540
    },
    {
      "epoch": 0.0636941776343641,
      "grad_norm": 3.5565545558929443,
      "learning_rate": 0.0001877026224221336,
      "loss": 0.3662,
      "step": 13541
    },
    {
      "epoch": 0.06369888143597656,
      "grad_norm": 2.822373151779175,
      "learning_rate": 0.00018770167944402011,
      "loss": 0.7059,
      "step": 13542
    },
    {
      "epoch": 0.06370358523758902,
      "grad_norm": 2.118654489517212,
      "learning_rate": 0.00018770073646590663,
      "loss": 0.2383,
      "step": 13543
    },
    {
      "epoch": 0.06370828903920148,
      "grad_norm": 3.214247465133667,
      "learning_rate": 0.00018769979348779315,
      "loss": 0.2318,
      "step": 13544
    },
    {
      "epoch": 0.06371299284081394,
      "grad_norm": 1.6647214889526367,
      "learning_rate": 0.00018769885050967967,
      "loss": 0.1375,
      "step": 13545
    },
    {
      "epoch": 0.06371769664242641,
      "grad_norm": 1.9967840909957886,
      "learning_rate": 0.0001876979075315662,
      "loss": 0.4199,
      "step": 13546
    },
    {
      "epoch": 0.06372240044403887,
      "grad_norm": 0.578108012676239,
      "learning_rate": 0.0001876969645534527,
      "loss": 0.0414,
      "step": 13547
    },
    {
      "epoch": 0.06372710424565134,
      "grad_norm": 3.6933438777923584,
      "learning_rate": 0.00018769602157533925,
      "loss": 0.5226,
      "step": 13548
    },
    {
      "epoch": 0.0637318080472638,
      "grad_norm": 1.78750479221344,
      "learning_rate": 0.00018769507859722577,
      "loss": 0.1966,
      "step": 13549
    },
    {
      "epoch": 0.06373651184887626,
      "grad_norm": 4.223665714263916,
      "learning_rate": 0.0001876941356191123,
      "loss": 0.8767,
      "step": 13550
    },
    {
      "epoch": 0.06374121565048872,
      "grad_norm": 0.2184150069952011,
      "learning_rate": 0.0001876931926409988,
      "loss": 0.0146,
      "step": 13551
    },
    {
      "epoch": 0.06374591945210119,
      "grad_norm": 3.307532548904419,
      "learning_rate": 0.00018769224966288535,
      "loss": 0.2359,
      "step": 13552
    },
    {
      "epoch": 0.06375062325371365,
      "grad_norm": 10.726784706115723,
      "learning_rate": 0.00018769130668477187,
      "loss": 0.1961,
      "step": 13553
    },
    {
      "epoch": 0.06375532705532612,
      "grad_norm": 2.663827419281006,
      "learning_rate": 0.0001876903637066584,
      "loss": 0.1978,
      "step": 13554
    },
    {
      "epoch": 0.06376003085693857,
      "grad_norm": 5.7713141441345215,
      "learning_rate": 0.00018768942072854488,
      "loss": 0.6796,
      "step": 13555
    },
    {
      "epoch": 0.06376473465855104,
      "grad_norm": 5.190673828125,
      "learning_rate": 0.0001876884777504314,
      "loss": 0.7075,
      "step": 13556
    },
    {
      "epoch": 0.0637694384601635,
      "grad_norm": 2.8897817134857178,
      "learning_rate": 0.00018768753477231795,
      "loss": 0.2541,
      "step": 13557
    },
    {
      "epoch": 0.06377414226177597,
      "grad_norm": 2.4120538234710693,
      "learning_rate": 0.00018768659179420447,
      "loss": 0.3201,
      "step": 13558
    },
    {
      "epoch": 0.06377884606338843,
      "grad_norm": 0.6982941031455994,
      "learning_rate": 0.00018768564881609099,
      "loss": 0.0446,
      "step": 13559
    },
    {
      "epoch": 0.0637835498650009,
      "grad_norm": 2.4295148849487305,
      "learning_rate": 0.0001876847058379775,
      "loss": 0.1754,
      "step": 13560
    },
    {
      "epoch": 0.06378825366661335,
      "grad_norm": 1.8193626403808594,
      "learning_rate": 0.00018768376285986402,
      "loss": 0.1132,
      "step": 13561
    },
    {
      "epoch": 0.06379295746822582,
      "grad_norm": 2.602102279663086,
      "learning_rate": 0.00018768281988175057,
      "loss": 0.2293,
      "step": 13562
    },
    {
      "epoch": 0.06379766126983828,
      "grad_norm": 0.6022487282752991,
      "learning_rate": 0.0001876818769036371,
      "loss": 0.0389,
      "step": 13563
    },
    {
      "epoch": 0.06380236507145075,
      "grad_norm": 3.3425045013427734,
      "learning_rate": 0.0001876809339255236,
      "loss": 0.5859,
      "step": 13564
    },
    {
      "epoch": 0.06380706887306321,
      "grad_norm": 4.244608402252197,
      "learning_rate": 0.00018767999094741012,
      "loss": 0.9329,
      "step": 13565
    },
    {
      "epoch": 0.06381177267467568,
      "grad_norm": 1.7018418312072754,
      "learning_rate": 0.00018767904796929664,
      "loss": 0.1424,
      "step": 13566
    },
    {
      "epoch": 0.06381647647628813,
      "grad_norm": 1.3633334636688232,
      "learning_rate": 0.00018767810499118316,
      "loss": 0.1149,
      "step": 13567
    },
    {
      "epoch": 0.0638211802779006,
      "grad_norm": 1.725322961807251,
      "learning_rate": 0.00018767716201306968,
      "loss": 0.2513,
      "step": 13568
    },
    {
      "epoch": 0.06382588407951306,
      "grad_norm": 1.1265045404434204,
      "learning_rate": 0.0001876762190349562,
      "loss": 0.1002,
      "step": 13569
    },
    {
      "epoch": 0.06383058788112553,
      "grad_norm": 1.4010615348815918,
      "learning_rate": 0.00018767527605684272,
      "loss": 0.0847,
      "step": 13570
    },
    {
      "epoch": 0.063835291682738,
      "grad_norm": 3.1941847801208496,
      "learning_rate": 0.00018767433307872926,
      "loss": 0.3458,
      "step": 13571
    },
    {
      "epoch": 0.06383999548435045,
      "grad_norm": 1.2955979108810425,
      "learning_rate": 0.00018767339010061578,
      "loss": 0.0843,
      "step": 13572
    },
    {
      "epoch": 0.06384469928596291,
      "grad_norm": 0.28351494669914246,
      "learning_rate": 0.0001876724471225023,
      "loss": 0.0311,
      "step": 13573
    },
    {
      "epoch": 0.06384940308757538,
      "grad_norm": 3.635847568511963,
      "learning_rate": 0.00018767150414438882,
      "loss": 0.2396,
      "step": 13574
    },
    {
      "epoch": 0.06385410688918784,
      "grad_norm": 2.03204345703125,
      "learning_rate": 0.00018767056116627534,
      "loss": 0.3503,
      "step": 13575
    },
    {
      "epoch": 0.06385881069080031,
      "grad_norm": 2.720423460006714,
      "learning_rate": 0.00018766961818816186,
      "loss": 0.5047,
      "step": 13576
    },
    {
      "epoch": 0.06386351449241277,
      "grad_norm": 4.2849249839782715,
      "learning_rate": 0.00018766867521004837,
      "loss": 0.9562,
      "step": 13577
    },
    {
      "epoch": 0.06386821829402523,
      "grad_norm": 1.256145715713501,
      "learning_rate": 0.0001876677322319349,
      "loss": 0.0994,
      "step": 13578
    },
    {
      "epoch": 0.06387292209563769,
      "grad_norm": 1.4154046773910522,
      "learning_rate": 0.0001876667892538214,
      "loss": 0.1143,
      "step": 13579
    },
    {
      "epoch": 0.06387762589725016,
      "grad_norm": 0.15764285624027252,
      "learning_rate": 0.00018766584627570796,
      "loss": 0.0162,
      "step": 13580
    },
    {
      "epoch": 0.06388232969886262,
      "grad_norm": 0.37347421050071716,
      "learning_rate": 0.00018766490329759448,
      "loss": 0.0368,
      "step": 13581
    },
    {
      "epoch": 0.06388703350047509,
      "grad_norm": 0.7961229681968689,
      "learning_rate": 0.000187663960319481,
      "loss": 0.1051,
      "step": 13582
    },
    {
      "epoch": 0.06389173730208755,
      "grad_norm": 2.777310371398926,
      "learning_rate": 0.00018766301734136751,
      "loss": 0.4385,
      "step": 13583
    },
    {
      "epoch": 0.0638964411037,
      "grad_norm": 3.1198251247406006,
      "learning_rate": 0.00018766207436325406,
      "loss": 0.5117,
      "step": 13584
    },
    {
      "epoch": 0.06390114490531247,
      "grad_norm": 2.5209686756134033,
      "learning_rate": 0.00018766113138514058,
      "loss": 0.3664,
      "step": 13585
    },
    {
      "epoch": 0.06390584870692494,
      "grad_norm": 1.4031206369400024,
      "learning_rate": 0.00018766018840702707,
      "loss": 0.1751,
      "step": 13586
    },
    {
      "epoch": 0.0639105525085374,
      "grad_norm": 0.8259081244468689,
      "learning_rate": 0.0001876592454289136,
      "loss": 0.089,
      "step": 13587
    },
    {
      "epoch": 0.06391525631014987,
      "grad_norm": 2.664207696914673,
      "learning_rate": 0.0001876583024508001,
      "loss": 0.544,
      "step": 13588
    },
    {
      "epoch": 0.06391996011176232,
      "grad_norm": 4.412260055541992,
      "learning_rate": 0.00018765735947268665,
      "loss": 0.5402,
      "step": 13589
    },
    {
      "epoch": 0.06392466391337479,
      "grad_norm": 2.709561586380005,
      "learning_rate": 0.00018765641649457317,
      "loss": 0.3696,
      "step": 13590
    },
    {
      "epoch": 0.06392936771498725,
      "grad_norm": 1.0189456939697266,
      "learning_rate": 0.0001876554735164597,
      "loss": 0.1299,
      "step": 13591
    },
    {
      "epoch": 0.06393407151659972,
      "grad_norm": 1.327081322669983,
      "learning_rate": 0.0001876545305383462,
      "loss": 0.1993,
      "step": 13592
    },
    {
      "epoch": 0.06393877531821218,
      "grad_norm": 0.2131727933883667,
      "learning_rate": 0.00018765358756023275,
      "loss": 0.026,
      "step": 13593
    },
    {
      "epoch": 0.06394347911982465,
      "grad_norm": 1.7653870582580566,
      "learning_rate": 0.00018765264458211927,
      "loss": 0.2434,
      "step": 13594
    },
    {
      "epoch": 0.0639481829214371,
      "grad_norm": 1.7631710767745972,
      "learning_rate": 0.0001876517016040058,
      "loss": 0.2132,
      "step": 13595
    },
    {
      "epoch": 0.06395288672304957,
      "grad_norm": 3.8382740020751953,
      "learning_rate": 0.0001876507586258923,
      "loss": 0.6329,
      "step": 13596
    },
    {
      "epoch": 0.06395759052466203,
      "grad_norm": 4.010382175445557,
      "learning_rate": 0.0001876498156477788,
      "loss": 0.4038,
      "step": 13597
    },
    {
      "epoch": 0.0639622943262745,
      "grad_norm": 1.665486216545105,
      "learning_rate": 0.00018764887266966535,
      "loss": 0.3145,
      "step": 13598
    },
    {
      "epoch": 0.06396699812788696,
      "grad_norm": 1.7833296060562134,
      "learning_rate": 0.00018764792969155187,
      "loss": 0.3251,
      "step": 13599
    },
    {
      "epoch": 0.06397170192949943,
      "grad_norm": 2.981234312057495,
      "learning_rate": 0.00018764698671343838,
      "loss": 0.3785,
      "step": 13600
    },
    {
      "epoch": 0.06397640573111188,
      "grad_norm": 0.8620110154151917,
      "learning_rate": 0.0001876460437353249,
      "loss": 0.0583,
      "step": 13601
    },
    {
      "epoch": 0.06398110953272435,
      "grad_norm": 2.5822746753692627,
      "learning_rate": 0.00018764510075721145,
      "loss": 0.2155,
      "step": 13602
    },
    {
      "epoch": 0.06398581333433681,
      "grad_norm": 2.4054691791534424,
      "learning_rate": 0.00018764415777909797,
      "loss": 0.2086,
      "step": 13603
    },
    {
      "epoch": 0.06399051713594928,
      "grad_norm": 3.3852741718292236,
      "learning_rate": 0.0001876432148009845,
      "loss": 0.3326,
      "step": 13604
    },
    {
      "epoch": 0.06399522093756174,
      "grad_norm": 2.4895105361938477,
      "learning_rate": 0.000187642271822871,
      "loss": 0.4936,
      "step": 13605
    },
    {
      "epoch": 0.0639999247391742,
      "grad_norm": 2.2759053707122803,
      "learning_rate": 0.00018764132884475752,
      "loss": 0.2499,
      "step": 13606
    },
    {
      "epoch": 0.06400462854078666,
      "grad_norm": 1.5890628099441528,
      "learning_rate": 0.00018764038586664404,
      "loss": 0.1511,
      "step": 13607
    },
    {
      "epoch": 0.06400933234239913,
      "grad_norm": 1.6211899518966675,
      "learning_rate": 0.00018763944288853056,
      "loss": 0.2059,
      "step": 13608
    },
    {
      "epoch": 0.06401403614401159,
      "grad_norm": 2.3716139793395996,
      "learning_rate": 0.00018763849991041708,
      "loss": 0.226,
      "step": 13609
    },
    {
      "epoch": 0.06401873994562406,
      "grad_norm": 2.9288599491119385,
      "learning_rate": 0.0001876375569323036,
      "loss": 0.3162,
      "step": 13610
    },
    {
      "epoch": 0.06402344374723652,
      "grad_norm": 2.7682735919952393,
      "learning_rate": 0.00018763661395419012,
      "loss": 0.6313,
      "step": 13611
    },
    {
      "epoch": 0.06402814754884897,
      "grad_norm": 2.945093870162964,
      "learning_rate": 0.00018763567097607666,
      "loss": 0.3518,
      "step": 13612
    },
    {
      "epoch": 0.06403285135046144,
      "grad_norm": 3.2221896648406982,
      "learning_rate": 0.00018763472799796318,
      "loss": 0.2419,
      "step": 13613
    },
    {
      "epoch": 0.0640375551520739,
      "grad_norm": 1.2035841941833496,
      "learning_rate": 0.0001876337850198497,
      "loss": 0.1192,
      "step": 13614
    },
    {
      "epoch": 0.06404225895368637,
      "grad_norm": 0.30302828550338745,
      "learning_rate": 0.00018763284204173622,
      "loss": 0.0331,
      "step": 13615
    },
    {
      "epoch": 0.06404696275529884,
      "grad_norm": 1.8813369274139404,
      "learning_rate": 0.00018763189906362276,
      "loss": 0.2183,
      "step": 13616
    },
    {
      "epoch": 0.0640516665569113,
      "grad_norm": 1.838396430015564,
      "learning_rate": 0.00018763095608550926,
      "loss": 0.249,
      "step": 13617
    },
    {
      "epoch": 0.06405637035852375,
      "grad_norm": 1.6455254554748535,
      "learning_rate": 0.00018763001310739577,
      "loss": 0.2385,
      "step": 13618
    },
    {
      "epoch": 0.06406107416013622,
      "grad_norm": 0.9289897084236145,
      "learning_rate": 0.0001876290701292823,
      "loss": 0.0922,
      "step": 13619
    },
    {
      "epoch": 0.06406577796174868,
      "grad_norm": 2.577469825744629,
      "learning_rate": 0.0001876281271511688,
      "loss": 0.2985,
      "step": 13620
    },
    {
      "epoch": 0.06407048176336115,
      "grad_norm": 0.6676471829414368,
      "learning_rate": 0.00018762718417305536,
      "loss": 0.0615,
      "step": 13621
    },
    {
      "epoch": 0.06407518556497362,
      "grad_norm": 2.8497800827026367,
      "learning_rate": 0.00018762624119494188,
      "loss": 0.4912,
      "step": 13622
    },
    {
      "epoch": 0.06407988936658607,
      "grad_norm": 1.354888916015625,
      "learning_rate": 0.0001876252982168284,
      "loss": 0.198,
      "step": 13623
    },
    {
      "epoch": 0.06408459316819853,
      "grad_norm": 1.7398083209991455,
      "learning_rate": 0.00018762435523871491,
      "loss": 0.2199,
      "step": 13624
    },
    {
      "epoch": 0.064089296969811,
      "grad_norm": 1.860107183456421,
      "learning_rate": 0.00018762341226060146,
      "loss": 0.2796,
      "step": 13625
    },
    {
      "epoch": 0.06409400077142346,
      "grad_norm": 0.45329099893569946,
      "learning_rate": 0.00018762246928248798,
      "loss": 0.0491,
      "step": 13626
    },
    {
      "epoch": 0.06409870457303593,
      "grad_norm": 3.3934106826782227,
      "learning_rate": 0.0001876215263043745,
      "loss": 0.498,
      "step": 13627
    },
    {
      "epoch": 0.0641034083746484,
      "grad_norm": 3.825829029083252,
      "learning_rate": 0.000187620583326261,
      "loss": 0.4682,
      "step": 13628
    },
    {
      "epoch": 0.06410811217626085,
      "grad_norm": 5.1954665184021,
      "learning_rate": 0.0001876196403481475,
      "loss": 0.9158,
      "step": 13629
    },
    {
      "epoch": 0.06411281597787331,
      "grad_norm": 2.0944769382476807,
      "learning_rate": 0.00018761869737003405,
      "loss": 0.2323,
      "step": 13630
    },
    {
      "epoch": 0.06411751977948578,
      "grad_norm": 0.5635970234870911,
      "learning_rate": 0.00018761775439192057,
      "loss": 0.0402,
      "step": 13631
    },
    {
      "epoch": 0.06412222358109824,
      "grad_norm": 4.145688533782959,
      "learning_rate": 0.0001876168114138071,
      "loss": 0.5277,
      "step": 13632
    },
    {
      "epoch": 0.06412692738271071,
      "grad_norm": 2.193812131881714,
      "learning_rate": 0.0001876158684356936,
      "loss": 0.3385,
      "step": 13633
    },
    {
      "epoch": 0.06413163118432318,
      "grad_norm": 0.42645329236984253,
      "learning_rate": 0.00018761492545758015,
      "loss": 0.014,
      "step": 13634
    },
    {
      "epoch": 0.06413633498593563,
      "grad_norm": 2.244736909866333,
      "learning_rate": 0.00018761398247946667,
      "loss": 0.3042,
      "step": 13635
    },
    {
      "epoch": 0.0641410387875481,
      "grad_norm": 1.3841962814331055,
      "learning_rate": 0.0001876130395013532,
      "loss": 0.213,
      "step": 13636
    },
    {
      "epoch": 0.06414574258916056,
      "grad_norm": 1.9117828607559204,
      "learning_rate": 0.0001876120965232397,
      "loss": 0.1922,
      "step": 13637
    },
    {
      "epoch": 0.06415044639077302,
      "grad_norm": 1.5440300703048706,
      "learning_rate": 0.00018761115354512623,
      "loss": 0.104,
      "step": 13638
    },
    {
      "epoch": 0.06415515019238549,
      "grad_norm": 3.5262415409088135,
      "learning_rate": 0.00018761021056701275,
      "loss": 0.7361,
      "step": 13639
    },
    {
      "epoch": 0.06415985399399794,
      "grad_norm": 0.6294836401939392,
      "learning_rate": 0.00018760926758889927,
      "loss": 0.0725,
      "step": 13640
    },
    {
      "epoch": 0.06416455779561041,
      "grad_norm": 2.144955635070801,
      "learning_rate": 0.00018760832461078578,
      "loss": 0.3298,
      "step": 13641
    },
    {
      "epoch": 0.06416926159722287,
      "grad_norm": 0.5807859897613525,
      "learning_rate": 0.0001876073816326723,
      "loss": 0.0546,
      "step": 13642
    },
    {
      "epoch": 0.06417396539883534,
      "grad_norm": 1.7353745698928833,
      "learning_rate": 0.00018760643865455885,
      "loss": 0.2172,
      "step": 13643
    },
    {
      "epoch": 0.0641786692004478,
      "grad_norm": 1.2858737707138062,
      "learning_rate": 0.00018760549567644537,
      "loss": 0.1087,
      "step": 13644
    },
    {
      "epoch": 0.06418337300206027,
      "grad_norm": 2.1493148803710938,
      "learning_rate": 0.0001876045526983319,
      "loss": 0.3089,
      "step": 13645
    },
    {
      "epoch": 0.06418807680367272,
      "grad_norm": 1.8733443021774292,
      "learning_rate": 0.0001876036097202184,
      "loss": 0.191,
      "step": 13646
    },
    {
      "epoch": 0.06419278060528519,
      "grad_norm": 1.2014044523239136,
      "learning_rate": 0.00018760266674210492,
      "loss": 0.1307,
      "step": 13647
    },
    {
      "epoch": 0.06419748440689765,
      "grad_norm": 0.7092190980911255,
      "learning_rate": 0.00018760172376399144,
      "loss": 0.0542,
      "step": 13648
    },
    {
      "epoch": 0.06420218820851012,
      "grad_norm": 3.3819475173950195,
      "learning_rate": 0.00018760078078587796,
      "loss": 0.8944,
      "step": 13649
    },
    {
      "epoch": 0.06420689201012258,
      "grad_norm": 1.1618058681488037,
      "learning_rate": 0.00018759983780776448,
      "loss": 0.1546,
      "step": 13650
    },
    {
      "epoch": 0.06421159581173505,
      "grad_norm": 3.861201286315918,
      "learning_rate": 0.000187598894829651,
      "loss": 0.4794,
      "step": 13651
    },
    {
      "epoch": 0.0642162996133475,
      "grad_norm": 1.2346298694610596,
      "learning_rate": 0.00018759795185153754,
      "loss": 0.0929,
      "step": 13652
    },
    {
      "epoch": 0.06422100341495997,
      "grad_norm": 1.0790042877197266,
      "learning_rate": 0.00018759700887342406,
      "loss": 0.0641,
      "step": 13653
    },
    {
      "epoch": 0.06422570721657243,
      "grad_norm": 2.60880708694458,
      "learning_rate": 0.00018759606589531058,
      "loss": 0.2428,
      "step": 13654
    },
    {
      "epoch": 0.0642304110181849,
      "grad_norm": 1.8896585702896118,
      "learning_rate": 0.0001875951229171971,
      "loss": 0.1435,
      "step": 13655
    },
    {
      "epoch": 0.06423511481979736,
      "grad_norm": 2.165473461151123,
      "learning_rate": 0.00018759417993908362,
      "loss": 0.1853,
      "step": 13656
    },
    {
      "epoch": 0.06423981862140982,
      "grad_norm": 1.3570971488952637,
      "learning_rate": 0.00018759323696097016,
      "loss": 0.1366,
      "step": 13657
    },
    {
      "epoch": 0.06424452242302228,
      "grad_norm": 2.9605600833892822,
      "learning_rate": 0.00018759229398285668,
      "loss": 0.5776,
      "step": 13658
    },
    {
      "epoch": 0.06424922622463475,
      "grad_norm": 2.884791135787964,
      "learning_rate": 0.00018759135100474317,
      "loss": 0.233,
      "step": 13659
    },
    {
      "epoch": 0.06425393002624721,
      "grad_norm": 3.1513547897338867,
      "learning_rate": 0.0001875904080266297,
      "loss": 0.4577,
      "step": 13660
    },
    {
      "epoch": 0.06425863382785968,
      "grad_norm": 2.741330862045288,
      "learning_rate": 0.0001875894650485162,
      "loss": 0.4145,
      "step": 13661
    },
    {
      "epoch": 0.06426333762947214,
      "grad_norm": 3.2383713722229004,
      "learning_rate": 0.00018758852207040276,
      "loss": 0.4302,
      "step": 13662
    },
    {
      "epoch": 0.0642680414310846,
      "grad_norm": 2.546347141265869,
      "learning_rate": 0.00018758757909228928,
      "loss": 0.4364,
      "step": 13663
    },
    {
      "epoch": 0.06427274523269706,
      "grad_norm": 2.368569850921631,
      "learning_rate": 0.0001875866361141758,
      "loss": 0.4042,
      "step": 13664
    },
    {
      "epoch": 0.06427744903430953,
      "grad_norm": 1.8379321098327637,
      "learning_rate": 0.00018758569313606231,
      "loss": 0.1384,
      "step": 13665
    },
    {
      "epoch": 0.064282152835922,
      "grad_norm": 2.0304176807403564,
      "learning_rate": 0.00018758475015794886,
      "loss": 0.1957,
      "step": 13666
    },
    {
      "epoch": 0.06428685663753446,
      "grad_norm": 0.7624027729034424,
      "learning_rate": 0.00018758380717983538,
      "loss": 0.0747,
      "step": 13667
    },
    {
      "epoch": 0.06429156043914692,
      "grad_norm": 3.214597702026367,
      "learning_rate": 0.0001875828642017219,
      "loss": 0.325,
      "step": 13668
    },
    {
      "epoch": 0.06429626424075938,
      "grad_norm": 1.8451427221298218,
      "learning_rate": 0.00018758192122360842,
      "loss": 0.3595,
      "step": 13669
    },
    {
      "epoch": 0.06430096804237184,
      "grad_norm": 2.296311378479004,
      "learning_rate": 0.0001875809782454949,
      "loss": 0.2687,
      "step": 13670
    },
    {
      "epoch": 0.06430567184398431,
      "grad_norm": 2.370448112487793,
      "learning_rate": 0.00018758003526738145,
      "loss": 0.3154,
      "step": 13671
    },
    {
      "epoch": 0.06431037564559677,
      "grad_norm": 3.337860107421875,
      "learning_rate": 0.00018757909228926797,
      "loss": 0.478,
      "step": 13672
    },
    {
      "epoch": 0.06431507944720924,
      "grad_norm": 1.8053795099258423,
      "learning_rate": 0.0001875781493111545,
      "loss": 0.2179,
      "step": 13673
    },
    {
      "epoch": 0.06431978324882169,
      "grad_norm": 0.9024985432624817,
      "learning_rate": 0.000187577206333041,
      "loss": 0.0747,
      "step": 13674
    },
    {
      "epoch": 0.06432448705043416,
      "grad_norm": 2.0645320415496826,
      "learning_rate": 0.00018757626335492755,
      "loss": 0.1793,
      "step": 13675
    },
    {
      "epoch": 0.06432919085204662,
      "grad_norm": 2.1208672523498535,
      "learning_rate": 0.00018757532037681407,
      "loss": 0.2993,
      "step": 13676
    },
    {
      "epoch": 0.06433389465365909,
      "grad_norm": 2.1411659717559814,
      "learning_rate": 0.0001875743773987006,
      "loss": 0.3325,
      "step": 13677
    },
    {
      "epoch": 0.06433859845527155,
      "grad_norm": 0.4246637225151062,
      "learning_rate": 0.0001875734344205871,
      "loss": 0.0431,
      "step": 13678
    },
    {
      "epoch": 0.06434330225688402,
      "grad_norm": 0.7096578478813171,
      "learning_rate": 0.00018757249144247363,
      "loss": 0.0935,
      "step": 13679
    },
    {
      "epoch": 0.06434800605849647,
      "grad_norm": 1.8360542058944702,
      "learning_rate": 0.00018757154846436015,
      "loss": 0.2109,
      "step": 13680
    },
    {
      "epoch": 0.06435270986010894,
      "grad_norm": 0.33331993222236633,
      "learning_rate": 0.00018757060548624667,
      "loss": 0.0289,
      "step": 13681
    },
    {
      "epoch": 0.0643574136617214,
      "grad_norm": 2.406198263168335,
      "learning_rate": 0.00018756966250813318,
      "loss": 0.2478,
      "step": 13682
    },
    {
      "epoch": 0.06436211746333387,
      "grad_norm": 2.034778594970703,
      "learning_rate": 0.0001875687195300197,
      "loss": 0.5522,
      "step": 13683
    },
    {
      "epoch": 0.06436682126494633,
      "grad_norm": 2.2264442443847656,
      "learning_rate": 0.00018756777655190625,
      "loss": 0.4461,
      "step": 13684
    },
    {
      "epoch": 0.0643715250665588,
      "grad_norm": 0.9956117272377014,
      "learning_rate": 0.00018756683357379277,
      "loss": 0.1477,
      "step": 13685
    },
    {
      "epoch": 0.06437622886817125,
      "grad_norm": 0.8811957240104675,
      "learning_rate": 0.0001875658905956793,
      "loss": 0.1248,
      "step": 13686
    },
    {
      "epoch": 0.06438093266978372,
      "grad_norm": 1.3805742263793945,
      "learning_rate": 0.0001875649476175658,
      "loss": 0.1126,
      "step": 13687
    },
    {
      "epoch": 0.06438563647139618,
      "grad_norm": 2.6607277393341064,
      "learning_rate": 0.00018756400463945232,
      "loss": 0.3057,
      "step": 13688
    },
    {
      "epoch": 0.06439034027300865,
      "grad_norm": 1.9588793516159058,
      "learning_rate": 0.00018756306166133887,
      "loss": 0.2159,
      "step": 13689
    },
    {
      "epoch": 0.06439504407462111,
      "grad_norm": 1.8167213201522827,
      "learning_rate": 0.00018756211868322536,
      "loss": 0.1534,
      "step": 13690
    },
    {
      "epoch": 0.06439974787623357,
      "grad_norm": 0.5006365776062012,
      "learning_rate": 0.00018756117570511188,
      "loss": 0.0399,
      "step": 13691
    },
    {
      "epoch": 0.06440445167784603,
      "grad_norm": 3.8071911334991455,
      "learning_rate": 0.0001875602327269984,
      "loss": 0.6493,
      "step": 13692
    },
    {
      "epoch": 0.0644091554794585,
      "grad_norm": 3.740741014480591,
      "learning_rate": 0.00018755928974888494,
      "loss": 0.7703,
      "step": 13693
    },
    {
      "epoch": 0.06441385928107096,
      "grad_norm": 0.6947550773620605,
      "learning_rate": 0.00018755834677077146,
      "loss": 0.0834,
      "step": 13694
    },
    {
      "epoch": 0.06441856308268343,
      "grad_norm": 2.6956350803375244,
      "learning_rate": 0.00018755740379265798,
      "loss": 0.2926,
      "step": 13695
    },
    {
      "epoch": 0.0644232668842959,
      "grad_norm": 3.127929210662842,
      "learning_rate": 0.0001875564608145445,
      "loss": 0.4371,
      "step": 13696
    },
    {
      "epoch": 0.06442797068590835,
      "grad_norm": 2.7517223358154297,
      "learning_rate": 0.00018755551783643102,
      "loss": 0.344,
      "step": 13697
    },
    {
      "epoch": 0.06443267448752081,
      "grad_norm": 1.8488818407058716,
      "learning_rate": 0.00018755457485831756,
      "loss": 0.1042,
      "step": 13698
    },
    {
      "epoch": 0.06443737828913328,
      "grad_norm": 0.33512166142463684,
      "learning_rate": 0.00018755363188020408,
      "loss": 0.0254,
      "step": 13699
    },
    {
      "epoch": 0.06444208209074574,
      "grad_norm": 2.2275302410125732,
      "learning_rate": 0.0001875526889020906,
      "loss": 0.3212,
      "step": 13700
    },
    {
      "epoch": 0.06444678589235821,
      "grad_norm": 8.492218017578125,
      "learning_rate": 0.0001875517459239771,
      "loss": 0.1643,
      "step": 13701
    },
    {
      "epoch": 0.06445148969397067,
      "grad_norm": 2.679441452026367,
      "learning_rate": 0.00018755080294586364,
      "loss": 0.3194,
      "step": 13702
    },
    {
      "epoch": 0.06445619349558313,
      "grad_norm": 3.4658920764923096,
      "learning_rate": 0.00018754985996775016,
      "loss": 0.6254,
      "step": 13703
    },
    {
      "epoch": 0.06446089729719559,
      "grad_norm": 1.4809339046478271,
      "learning_rate": 0.00018754891698963668,
      "loss": 0.104,
      "step": 13704
    },
    {
      "epoch": 0.06446560109880806,
      "grad_norm": 4.796542644500732,
      "learning_rate": 0.0001875479740115232,
      "loss": 0.4523,
      "step": 13705
    },
    {
      "epoch": 0.06447030490042052,
      "grad_norm": 1.4926730394363403,
      "learning_rate": 0.00018754703103340971,
      "loss": 0.1172,
      "step": 13706
    },
    {
      "epoch": 0.06447500870203299,
      "grad_norm": 2.6782608032226562,
      "learning_rate": 0.00018754608805529626,
      "loss": 0.1268,
      "step": 13707
    },
    {
      "epoch": 0.06447971250364544,
      "grad_norm": 3.128758668899536,
      "learning_rate": 0.00018754514507718278,
      "loss": 0.2779,
      "step": 13708
    },
    {
      "epoch": 0.0644844163052579,
      "grad_norm": 0.7387405037879944,
      "learning_rate": 0.0001875442020990693,
      "loss": 0.0447,
      "step": 13709
    },
    {
      "epoch": 0.06448912010687037,
      "grad_norm": 3.1679656505584717,
      "learning_rate": 0.00018754325912095582,
      "loss": 0.222,
      "step": 13710
    },
    {
      "epoch": 0.06449382390848284,
      "grad_norm": 4.4685282707214355,
      "learning_rate": 0.00018754231614284233,
      "loss": 0.8506,
      "step": 13711
    },
    {
      "epoch": 0.0644985277100953,
      "grad_norm": 4.8758463859558105,
      "learning_rate": 0.00018754137316472885,
      "loss": 0.7081,
      "step": 13712
    },
    {
      "epoch": 0.06450323151170777,
      "grad_norm": 2.7391016483306885,
      "learning_rate": 0.00018754043018661537,
      "loss": 0.3176,
      "step": 13713
    },
    {
      "epoch": 0.06450793531332022,
      "grad_norm": 1.0074247121810913,
      "learning_rate": 0.0001875394872085019,
      "loss": 0.123,
      "step": 13714
    },
    {
      "epoch": 0.06451263911493268,
      "grad_norm": 4.064918518066406,
      "learning_rate": 0.0001875385442303884,
      "loss": 0.6127,
      "step": 13715
    },
    {
      "epoch": 0.06451734291654515,
      "grad_norm": 2.288015604019165,
      "learning_rate": 0.00018753760125227495,
      "loss": 0.1409,
      "step": 13716
    },
    {
      "epoch": 0.06452204671815762,
      "grad_norm": 2.2097327709198,
      "learning_rate": 0.00018753665827416147,
      "loss": 0.2394,
      "step": 13717
    },
    {
      "epoch": 0.06452675051977008,
      "grad_norm": 2.615997314453125,
      "learning_rate": 0.000187535715296048,
      "loss": 0.7808,
      "step": 13718
    },
    {
      "epoch": 0.06453145432138255,
      "grad_norm": 3.849224805831909,
      "learning_rate": 0.0001875347723179345,
      "loss": 0.4758,
      "step": 13719
    },
    {
      "epoch": 0.064536158122995,
      "grad_norm": 1.806216835975647,
      "learning_rate": 0.00018753382933982103,
      "loss": 0.1612,
      "step": 13720
    },
    {
      "epoch": 0.06454086192460746,
      "grad_norm": 4.519412040710449,
      "learning_rate": 0.00018753288636170755,
      "loss": 0.9561,
      "step": 13721
    },
    {
      "epoch": 0.06454556572621993,
      "grad_norm": 1.0926103591918945,
      "learning_rate": 0.00018753194338359407,
      "loss": 0.0985,
      "step": 13722
    },
    {
      "epoch": 0.0645502695278324,
      "grad_norm": 1.7754626274108887,
      "learning_rate": 0.00018753100040548058,
      "loss": 0.1907,
      "step": 13723
    },
    {
      "epoch": 0.06455497332944486,
      "grad_norm": 0.8389647603034973,
      "learning_rate": 0.0001875300574273671,
      "loss": 0.0691,
      "step": 13724
    },
    {
      "epoch": 0.06455967713105731,
      "grad_norm": 1.0179229974746704,
      "learning_rate": 0.00018752911444925365,
      "loss": 0.1469,
      "step": 13725
    },
    {
      "epoch": 0.06456438093266978,
      "grad_norm": 1.7857078313827515,
      "learning_rate": 0.00018752817147114017,
      "loss": 0.4382,
      "step": 13726
    },
    {
      "epoch": 0.06456908473428224,
      "grad_norm": 2.3699748516082764,
      "learning_rate": 0.0001875272284930267,
      "loss": 0.2081,
      "step": 13727
    },
    {
      "epoch": 0.06457378853589471,
      "grad_norm": 0.8157716393470764,
      "learning_rate": 0.0001875262855149132,
      "loss": 0.1032,
      "step": 13728
    },
    {
      "epoch": 0.06457849233750718,
      "grad_norm": 2.2116034030914307,
      "learning_rate": 0.00018752534253679972,
      "loss": 0.2281,
      "step": 13729
    },
    {
      "epoch": 0.06458319613911964,
      "grad_norm": 2.1423451900482178,
      "learning_rate": 0.00018752439955868627,
      "loss": 0.2415,
      "step": 13730
    },
    {
      "epoch": 0.0645878999407321,
      "grad_norm": 1.0239155292510986,
      "learning_rate": 0.0001875234565805728,
      "loss": 0.2251,
      "step": 13731
    },
    {
      "epoch": 0.06459260374234456,
      "grad_norm": 3.2280306816101074,
      "learning_rate": 0.00018752251360245928,
      "loss": 0.295,
      "step": 13732
    },
    {
      "epoch": 0.06459730754395702,
      "grad_norm": 1.1980409622192383,
      "learning_rate": 0.0001875215706243458,
      "loss": 0.1196,
      "step": 13733
    },
    {
      "epoch": 0.06460201134556949,
      "grad_norm": 2.555076837539673,
      "learning_rate": 0.00018752062764623234,
      "loss": 0.4058,
      "step": 13734
    },
    {
      "epoch": 0.06460671514718196,
      "grad_norm": 0.7882010340690613,
      "learning_rate": 0.00018751968466811886,
      "loss": 0.1109,
      "step": 13735
    },
    {
      "epoch": 0.06461141894879442,
      "grad_norm": 1.446649432182312,
      "learning_rate": 0.00018751874169000538,
      "loss": 0.2242,
      "step": 13736
    },
    {
      "epoch": 0.06461612275040687,
      "grad_norm": 1.0986285209655762,
      "learning_rate": 0.0001875177987118919,
      "loss": 0.0999,
      "step": 13737
    },
    {
      "epoch": 0.06462082655201934,
      "grad_norm": 3.510680913925171,
      "learning_rate": 0.00018751685573377842,
      "loss": 0.6262,
      "step": 13738
    },
    {
      "epoch": 0.0646255303536318,
      "grad_norm": 1.9258562326431274,
      "learning_rate": 0.00018751591275566496,
      "loss": 0.4657,
      "step": 13739
    },
    {
      "epoch": 0.06463023415524427,
      "grad_norm": 2.846545457839966,
      "learning_rate": 0.00018751496977755148,
      "loss": 0.5128,
      "step": 13740
    },
    {
      "epoch": 0.06463493795685674,
      "grad_norm": 2.010545492172241,
      "learning_rate": 0.000187514026799438,
      "loss": 0.4121,
      "step": 13741
    },
    {
      "epoch": 0.06463964175846919,
      "grad_norm": 1.097819209098816,
      "learning_rate": 0.00018751308382132452,
      "loss": 0.3153,
      "step": 13742
    },
    {
      "epoch": 0.06464434556008165,
      "grad_norm": 1.14695405960083,
      "learning_rate": 0.00018751214084321104,
      "loss": 0.2349,
      "step": 13743
    },
    {
      "epoch": 0.06464904936169412,
      "grad_norm": 0.556404173374176,
      "learning_rate": 0.00018751119786509756,
      "loss": 0.0773,
      "step": 13744
    },
    {
      "epoch": 0.06465375316330658,
      "grad_norm": 1.2883274555206299,
      "learning_rate": 0.00018751025488698408,
      "loss": 0.138,
      "step": 13745
    },
    {
      "epoch": 0.06465845696491905,
      "grad_norm": 7.835623264312744,
      "learning_rate": 0.0001875093119088706,
      "loss": 0.7225,
      "step": 13746
    },
    {
      "epoch": 0.06466316076653152,
      "grad_norm": 3.2716567516326904,
      "learning_rate": 0.00018750836893075711,
      "loss": 0.6589,
      "step": 13747
    },
    {
      "epoch": 0.06466786456814397,
      "grad_norm": 1.7704755067825317,
      "learning_rate": 0.00018750742595264366,
      "loss": 0.4646,
      "step": 13748
    },
    {
      "epoch": 0.06467256836975643,
      "grad_norm": 0.899181067943573,
      "learning_rate": 0.00018750648297453018,
      "loss": 0.2416,
      "step": 13749
    },
    {
      "epoch": 0.0646772721713689,
      "grad_norm": 1.3107830286026,
      "learning_rate": 0.0001875055399964167,
      "loss": 0.2763,
      "step": 13750
    },
    {
      "epoch": 0.06468197597298136,
      "grad_norm": 3.2000620365142822,
      "learning_rate": 0.00018750459701830322,
      "loss": 0.326,
      "step": 13751
    },
    {
      "epoch": 0.06468667977459383,
      "grad_norm": 1.241081953048706,
      "learning_rate": 0.00018750365404018973,
      "loss": 0.1034,
      "step": 13752
    },
    {
      "epoch": 0.0646913835762063,
      "grad_norm": 4.259029865264893,
      "learning_rate": 0.00018750271106207625,
      "loss": 0.2483,
      "step": 13753
    },
    {
      "epoch": 0.06469608737781875,
      "grad_norm": 3.2667644023895264,
      "learning_rate": 0.00018750176808396277,
      "loss": 0.6606,
      "step": 13754
    },
    {
      "epoch": 0.06470079117943121,
      "grad_norm": 3.0436878204345703,
      "learning_rate": 0.0001875008251058493,
      "loss": 0.297,
      "step": 13755
    },
    {
      "epoch": 0.06470549498104368,
      "grad_norm": 1.179352879524231,
      "learning_rate": 0.0001874998821277358,
      "loss": 0.0802,
      "step": 13756
    },
    {
      "epoch": 0.06471019878265614,
      "grad_norm": 0.7050751447677612,
      "learning_rate": 0.00018749893914962235,
      "loss": 0.0919,
      "step": 13757
    },
    {
      "epoch": 0.06471490258426861,
      "grad_norm": 2.56533145904541,
      "learning_rate": 0.00018749799617150887,
      "loss": 0.2557,
      "step": 13758
    },
    {
      "epoch": 0.06471960638588106,
      "grad_norm": 2.662846565246582,
      "learning_rate": 0.0001874970531933954,
      "loss": 0.6289,
      "step": 13759
    },
    {
      "epoch": 0.06472431018749353,
      "grad_norm": 5.510548114776611,
      "learning_rate": 0.0001874961102152819,
      "loss": 0.2819,
      "step": 13760
    },
    {
      "epoch": 0.064729013989106,
      "grad_norm": 3.236729145050049,
      "learning_rate": 0.00018749516723716846,
      "loss": 0.3873,
      "step": 13761
    },
    {
      "epoch": 0.06473371779071846,
      "grad_norm": 1.095888614654541,
      "learning_rate": 0.00018749422425905497,
      "loss": 0.1336,
      "step": 13762
    },
    {
      "epoch": 0.06473842159233092,
      "grad_norm": 2.2583179473876953,
      "learning_rate": 0.00018749328128094147,
      "loss": 0.2939,
      "step": 13763
    },
    {
      "epoch": 0.06474312539394339,
      "grad_norm": 3.1521899700164795,
      "learning_rate": 0.00018749233830282798,
      "loss": 0.7995,
      "step": 13764
    },
    {
      "epoch": 0.06474782919555584,
      "grad_norm": 3.609854221343994,
      "learning_rate": 0.0001874913953247145,
      "loss": 0.4842,
      "step": 13765
    },
    {
      "epoch": 0.06475253299716831,
      "grad_norm": 2.136256456375122,
      "learning_rate": 0.00018749045234660105,
      "loss": 0.4387,
      "step": 13766
    },
    {
      "epoch": 0.06475723679878077,
      "grad_norm": 3.330203056335449,
      "learning_rate": 0.00018748950936848757,
      "loss": 0.6364,
      "step": 13767
    },
    {
      "epoch": 0.06476194060039324,
      "grad_norm": 0.928032398223877,
      "learning_rate": 0.0001874885663903741,
      "loss": 0.1329,
      "step": 13768
    },
    {
      "epoch": 0.0647666444020057,
      "grad_norm": 1.9121755361557007,
      "learning_rate": 0.0001874876234122606,
      "loss": 0.3573,
      "step": 13769
    },
    {
      "epoch": 0.06477134820361817,
      "grad_norm": 2.3253564834594727,
      "learning_rate": 0.00018748668043414712,
      "loss": 0.2325,
      "step": 13770
    },
    {
      "epoch": 0.06477605200523062,
      "grad_norm": 1.132265567779541,
      "learning_rate": 0.00018748573745603367,
      "loss": 0.2067,
      "step": 13771
    },
    {
      "epoch": 0.06478075580684309,
      "grad_norm": 2.5397369861602783,
      "learning_rate": 0.0001874847944779202,
      "loss": 0.1691,
      "step": 13772
    },
    {
      "epoch": 0.06478545960845555,
      "grad_norm": 2.0112175941467285,
      "learning_rate": 0.0001874838514998067,
      "loss": 0.2819,
      "step": 13773
    },
    {
      "epoch": 0.06479016341006802,
      "grad_norm": 3.0995826721191406,
      "learning_rate": 0.00018748290852169323,
      "loss": 0.1821,
      "step": 13774
    },
    {
      "epoch": 0.06479486721168048,
      "grad_norm": 1.2215068340301514,
      "learning_rate": 0.00018748196554357974,
      "loss": 0.1824,
      "step": 13775
    },
    {
      "epoch": 0.06479957101329294,
      "grad_norm": 0.8861352801322937,
      "learning_rate": 0.00018748102256546626,
      "loss": 0.0943,
      "step": 13776
    },
    {
      "epoch": 0.0648042748149054,
      "grad_norm": 3.18001651763916,
      "learning_rate": 0.00018748007958735278,
      "loss": 0.4193,
      "step": 13777
    },
    {
      "epoch": 0.06480897861651787,
      "grad_norm": 1.6730674505233765,
      "learning_rate": 0.0001874791366092393,
      "loss": 0.2923,
      "step": 13778
    },
    {
      "epoch": 0.06481368241813033,
      "grad_norm": 5.106671333312988,
      "learning_rate": 0.00018747819363112582,
      "loss": 0.3927,
      "step": 13779
    },
    {
      "epoch": 0.0648183862197428,
      "grad_norm": 0.5073078274726868,
      "learning_rate": 0.00018747725065301236,
      "loss": 0.0438,
      "step": 13780
    },
    {
      "epoch": 0.06482309002135526,
      "grad_norm": 1.9874472618103027,
      "learning_rate": 0.00018747630767489888,
      "loss": 0.215,
      "step": 13781
    },
    {
      "epoch": 0.06482779382296772,
      "grad_norm": 1.6079844236373901,
      "learning_rate": 0.0001874753646967854,
      "loss": 0.1372,
      "step": 13782
    },
    {
      "epoch": 0.06483249762458018,
      "grad_norm": 1.307539939880371,
      "learning_rate": 0.00018747442171867192,
      "loss": 0.1273,
      "step": 13783
    },
    {
      "epoch": 0.06483720142619265,
      "grad_norm": 1.1654409170150757,
      "learning_rate": 0.00018747347874055844,
      "loss": 0.1671,
      "step": 13784
    },
    {
      "epoch": 0.06484190522780511,
      "grad_norm": 2.640535354614258,
      "learning_rate": 0.00018747253576244496,
      "loss": 0.2235,
      "step": 13785
    },
    {
      "epoch": 0.06484660902941758,
      "grad_norm": 1.0987848043441772,
      "learning_rate": 0.00018747159278433148,
      "loss": 0.1139,
      "step": 13786
    },
    {
      "epoch": 0.06485131283103004,
      "grad_norm": 1.7665538787841797,
      "learning_rate": 0.000187470649806218,
      "loss": 0.167,
      "step": 13787
    },
    {
      "epoch": 0.0648560166326425,
      "grad_norm": 1.6396090984344482,
      "learning_rate": 0.00018746970682810451,
      "loss": 0.2562,
      "step": 13788
    },
    {
      "epoch": 0.06486072043425496,
      "grad_norm": 2.5509533882141113,
      "learning_rate": 0.00018746876384999106,
      "loss": 0.2565,
      "step": 13789
    },
    {
      "epoch": 0.06486542423586743,
      "grad_norm": 1.0208078622817993,
      "learning_rate": 0.00018746782087187758,
      "loss": 0.0978,
      "step": 13790
    },
    {
      "epoch": 0.0648701280374799,
      "grad_norm": 0.4287233054637909,
      "learning_rate": 0.0001874668778937641,
      "loss": 0.0555,
      "step": 13791
    },
    {
      "epoch": 0.06487483183909236,
      "grad_norm": 1.9167762994766235,
      "learning_rate": 0.00018746593491565062,
      "loss": 0.1298,
      "step": 13792
    },
    {
      "epoch": 0.06487953564070481,
      "grad_norm": 2.638029098510742,
      "learning_rate": 0.00018746499193753716,
      "loss": 0.2894,
      "step": 13793
    },
    {
      "epoch": 0.06488423944231728,
      "grad_norm": 0.15357370674610138,
      "learning_rate": 0.00018746404895942365,
      "loss": 0.0104,
      "step": 13794
    },
    {
      "epoch": 0.06488894324392974,
      "grad_norm": 1.4202461242675781,
      "learning_rate": 0.00018746310598131017,
      "loss": 0.1298,
      "step": 13795
    },
    {
      "epoch": 0.06489364704554221,
      "grad_norm": 1.995476245880127,
      "learning_rate": 0.0001874621630031967,
      "loss": 0.1636,
      "step": 13796
    },
    {
      "epoch": 0.06489835084715467,
      "grad_norm": 1.356994390487671,
      "learning_rate": 0.0001874612200250832,
      "loss": 0.1017,
      "step": 13797
    },
    {
      "epoch": 0.06490305464876714,
      "grad_norm": 2.129148244857788,
      "learning_rate": 0.00018746027704696975,
      "loss": 0.3136,
      "step": 13798
    },
    {
      "epoch": 0.06490775845037959,
      "grad_norm": 1.0658278465270996,
      "learning_rate": 0.00018745933406885627,
      "loss": 0.1076,
      "step": 13799
    },
    {
      "epoch": 0.06491246225199206,
      "grad_norm": 2.1787989139556885,
      "learning_rate": 0.0001874583910907428,
      "loss": 0.4096,
      "step": 13800
    },
    {
      "epoch": 0.06491716605360452,
      "grad_norm": 0.033215656876564026,
      "learning_rate": 0.0001874574481126293,
      "loss": 0.001,
      "step": 13801
    },
    {
      "epoch": 0.06492186985521699,
      "grad_norm": 8.450338363647461,
      "learning_rate": 0.00018745650513451586,
      "loss": 1.171,
      "step": 13802
    },
    {
      "epoch": 0.06492657365682945,
      "grad_norm": 5.161530017852783,
      "learning_rate": 0.00018745556215640237,
      "loss": 0.7684,
      "step": 13803
    },
    {
      "epoch": 0.06493127745844192,
      "grad_norm": 2.7376856803894043,
      "learning_rate": 0.0001874546191782889,
      "loss": 0.1847,
      "step": 13804
    },
    {
      "epoch": 0.06493598126005437,
      "grad_norm": 1.2878059148788452,
      "learning_rate": 0.0001874536762001754,
      "loss": 0.0681,
      "step": 13805
    },
    {
      "epoch": 0.06494068506166684,
      "grad_norm": 5.079019546508789,
      "learning_rate": 0.0001874527332220619,
      "loss": 0.296,
      "step": 13806
    },
    {
      "epoch": 0.0649453888632793,
      "grad_norm": 2.690866470336914,
      "learning_rate": 0.00018745179024394845,
      "loss": 0.6002,
      "step": 13807
    },
    {
      "epoch": 0.06495009266489177,
      "grad_norm": 5.985465049743652,
      "learning_rate": 0.00018745084726583497,
      "loss": 0.4876,
      "step": 13808
    },
    {
      "epoch": 0.06495479646650423,
      "grad_norm": 5.4076690673828125,
      "learning_rate": 0.0001874499042877215,
      "loss": 1.0084,
      "step": 13809
    },
    {
      "epoch": 0.06495950026811668,
      "grad_norm": 3.2876551151275635,
      "learning_rate": 0.000187448961309608,
      "loss": 0.153,
      "step": 13810
    },
    {
      "epoch": 0.06496420406972915,
      "grad_norm": 2.8119513988494873,
      "learning_rate": 0.00018744801833149455,
      "loss": 0.5294,
      "step": 13811
    },
    {
      "epoch": 0.06496890787134162,
      "grad_norm": 2.3756401538848877,
      "learning_rate": 0.00018744707535338107,
      "loss": 0.0857,
      "step": 13812
    },
    {
      "epoch": 0.06497361167295408,
      "grad_norm": 2.4402832984924316,
      "learning_rate": 0.0001874461323752676,
      "loss": 0.1345,
      "step": 13813
    },
    {
      "epoch": 0.06497831547456655,
      "grad_norm": 4.03491735458374,
      "learning_rate": 0.0001874451893971541,
      "loss": 0.4246,
      "step": 13814
    },
    {
      "epoch": 0.06498301927617901,
      "grad_norm": 2.683488130569458,
      "learning_rate": 0.00018744424641904063,
      "loss": 0.2548,
      "step": 13815
    },
    {
      "epoch": 0.06498772307779146,
      "grad_norm": 2.916837453842163,
      "learning_rate": 0.00018744330344092714,
      "loss": 0.4041,
      "step": 13816
    },
    {
      "epoch": 0.06499242687940393,
      "grad_norm": 0.43259814381599426,
      "learning_rate": 0.00018744236046281366,
      "loss": 0.0259,
      "step": 13817
    },
    {
      "epoch": 0.0649971306810164,
      "grad_norm": 4.126270294189453,
      "learning_rate": 0.00018744141748470018,
      "loss": 0.3381,
      "step": 13818
    },
    {
      "epoch": 0.06500183448262886,
      "grad_norm": 2.1330947875976562,
      "learning_rate": 0.0001874404745065867,
      "loss": 0.3375,
      "step": 13819
    },
    {
      "epoch": 0.06500653828424133,
      "grad_norm": 3.582547187805176,
      "learning_rate": 0.00018743953152847322,
      "loss": 0.3275,
      "step": 13820
    },
    {
      "epoch": 0.0650112420858538,
      "grad_norm": 2.3151652812957764,
      "learning_rate": 0.00018743858855035976,
      "loss": 0.2538,
      "step": 13821
    },
    {
      "epoch": 0.06501594588746624,
      "grad_norm": 2.982081413269043,
      "learning_rate": 0.00018743764557224628,
      "loss": 0.3498,
      "step": 13822
    },
    {
      "epoch": 0.06502064968907871,
      "grad_norm": 3.70654034614563,
      "learning_rate": 0.0001874367025941328,
      "loss": 0.5584,
      "step": 13823
    },
    {
      "epoch": 0.06502535349069118,
      "grad_norm": 4.321239471435547,
      "learning_rate": 0.00018743575961601932,
      "loss": 1.2914,
      "step": 13824
    },
    {
      "epoch": 0.06503005729230364,
      "grad_norm": 0.9757585525512695,
      "learning_rate": 0.00018743481663790584,
      "loss": 0.1108,
      "step": 13825
    },
    {
      "epoch": 0.06503476109391611,
      "grad_norm": 1.1370065212249756,
      "learning_rate": 0.00018743387365979236,
      "loss": 0.0937,
      "step": 13826
    },
    {
      "epoch": 0.06503946489552856,
      "grad_norm": 1.3749083280563354,
      "learning_rate": 0.00018743293068167888,
      "loss": 0.3541,
      "step": 13827
    },
    {
      "epoch": 0.06504416869714102,
      "grad_norm": 2.3664653301239014,
      "learning_rate": 0.0001874319877035654,
      "loss": 0.2674,
      "step": 13828
    },
    {
      "epoch": 0.06504887249875349,
      "grad_norm": 0.9846494197845459,
      "learning_rate": 0.00018743104472545191,
      "loss": 0.0833,
      "step": 13829
    },
    {
      "epoch": 0.06505357630036596,
      "grad_norm": 2.2565760612487793,
      "learning_rate": 0.00018743010174733846,
      "loss": 0.2605,
      "step": 13830
    },
    {
      "epoch": 0.06505828010197842,
      "grad_norm": 0.5140002369880676,
      "learning_rate": 0.00018742915876922498,
      "loss": 0.0809,
      "step": 13831
    },
    {
      "epoch": 0.06506298390359089,
      "grad_norm": 0.5724810361862183,
      "learning_rate": 0.0001874282157911115,
      "loss": 0.0992,
      "step": 13832
    },
    {
      "epoch": 0.06506768770520334,
      "grad_norm": 2.0867795944213867,
      "learning_rate": 0.00018742727281299802,
      "loss": 0.3983,
      "step": 13833
    },
    {
      "epoch": 0.0650723915068158,
      "grad_norm": 1.0958168506622314,
      "learning_rate": 0.00018742632983488456,
      "loss": 0.096,
      "step": 13834
    },
    {
      "epoch": 0.06507709530842827,
      "grad_norm": 5.007805824279785,
      "learning_rate": 0.00018742538685677108,
      "loss": 1.0324,
      "step": 13835
    },
    {
      "epoch": 0.06508179911004074,
      "grad_norm": 1.3070957660675049,
      "learning_rate": 0.0001874244438786576,
      "loss": 0.1209,
      "step": 13836
    },
    {
      "epoch": 0.0650865029116532,
      "grad_norm": 2.060559034347534,
      "learning_rate": 0.0001874235009005441,
      "loss": 0.2738,
      "step": 13837
    },
    {
      "epoch": 0.06509120671326567,
      "grad_norm": 2.260113000869751,
      "learning_rate": 0.0001874225579224306,
      "loss": 0.418,
      "step": 13838
    },
    {
      "epoch": 0.06509591051487812,
      "grad_norm": 0.8910456299781799,
      "learning_rate": 0.00018742161494431715,
      "loss": 0.1184,
      "step": 13839
    },
    {
      "epoch": 0.06510061431649058,
      "grad_norm": 2.505784034729004,
      "learning_rate": 0.00018742067196620367,
      "loss": 0.2894,
      "step": 13840
    },
    {
      "epoch": 0.06510531811810305,
      "grad_norm": 2.252462148666382,
      "learning_rate": 0.0001874197289880902,
      "loss": 0.4122,
      "step": 13841
    },
    {
      "epoch": 0.06511002191971552,
      "grad_norm": 2.1880552768707275,
      "learning_rate": 0.0001874187860099767,
      "loss": 0.3649,
      "step": 13842
    },
    {
      "epoch": 0.06511472572132798,
      "grad_norm": 0.6503049731254578,
      "learning_rate": 0.00018741784303186326,
      "loss": 0.0586,
      "step": 13843
    },
    {
      "epoch": 0.06511942952294045,
      "grad_norm": 2.032040596008301,
      "learning_rate": 0.00018741690005374977,
      "loss": 0.4043,
      "step": 13844
    },
    {
      "epoch": 0.0651241333245529,
      "grad_norm": 1.6326662302017212,
      "learning_rate": 0.0001874159570756363,
      "loss": 0.171,
      "step": 13845
    },
    {
      "epoch": 0.06512883712616536,
      "grad_norm": 2.4122495651245117,
      "learning_rate": 0.0001874150140975228,
      "loss": 0.4272,
      "step": 13846
    },
    {
      "epoch": 0.06513354092777783,
      "grad_norm": 4.1358137130737305,
      "learning_rate": 0.00018741407111940933,
      "loss": 1.0796,
      "step": 13847
    },
    {
      "epoch": 0.0651382447293903,
      "grad_norm": 1.8428289890289307,
      "learning_rate": 0.00018741312814129585,
      "loss": 0.366,
      "step": 13848
    },
    {
      "epoch": 0.06514294853100276,
      "grad_norm": 0.3186934292316437,
      "learning_rate": 0.00018741218516318237,
      "loss": 0.0253,
      "step": 13849
    },
    {
      "epoch": 0.06514765233261521,
      "grad_norm": 1.1005433797836304,
      "learning_rate": 0.0001874112421850689,
      "loss": 0.1301,
      "step": 13850
    },
    {
      "epoch": 0.06515235613422768,
      "grad_norm": 3.81951642036438,
      "learning_rate": 0.0001874102992069554,
      "loss": 0.7704,
      "step": 13851
    },
    {
      "epoch": 0.06515705993584014,
      "grad_norm": 2.309079170227051,
      "learning_rate": 0.00018740935622884195,
      "loss": 0.3012,
      "step": 13852
    },
    {
      "epoch": 0.06516176373745261,
      "grad_norm": 4.598762035369873,
      "learning_rate": 0.00018740841325072847,
      "loss": 0.4572,
      "step": 13853
    },
    {
      "epoch": 0.06516646753906508,
      "grad_norm": 0.9912523627281189,
      "learning_rate": 0.000187407470272615,
      "loss": 0.0873,
      "step": 13854
    },
    {
      "epoch": 0.06517117134067754,
      "grad_norm": 1.0020341873168945,
      "learning_rate": 0.0001874065272945015,
      "loss": 0.1348,
      "step": 13855
    },
    {
      "epoch": 0.06517587514229,
      "grad_norm": 0.6969285607337952,
      "learning_rate": 0.00018740558431638803,
      "loss": 0.0882,
      "step": 13856
    },
    {
      "epoch": 0.06518057894390246,
      "grad_norm": 5.319433212280273,
      "learning_rate": 0.00018740464133827454,
      "loss": 0.1936,
      "step": 13857
    },
    {
      "epoch": 0.06518528274551492,
      "grad_norm": 1.222698450088501,
      "learning_rate": 0.00018740369836016106,
      "loss": 0.0968,
      "step": 13858
    },
    {
      "epoch": 0.06518998654712739,
      "grad_norm": 0.65946364402771,
      "learning_rate": 0.00018740275538204758,
      "loss": 0.0573,
      "step": 13859
    },
    {
      "epoch": 0.06519469034873986,
      "grad_norm": 2.4316906929016113,
      "learning_rate": 0.0001874018124039341,
      "loss": 0.42,
      "step": 13860
    },
    {
      "epoch": 0.06519939415035232,
      "grad_norm": 3.2412190437316895,
      "learning_rate": 0.00018740086942582065,
      "loss": 0.3438,
      "step": 13861
    },
    {
      "epoch": 0.06520409795196477,
      "grad_norm": 0.9540268182754517,
      "learning_rate": 0.00018739992644770716,
      "loss": 0.1108,
      "step": 13862
    },
    {
      "epoch": 0.06520880175357724,
      "grad_norm": 3.2868492603302,
      "learning_rate": 0.00018739898346959368,
      "loss": 0.3899,
      "step": 13863
    },
    {
      "epoch": 0.0652135055551897,
      "grad_norm": 1.8658369779586792,
      "learning_rate": 0.0001873980404914802,
      "loss": 0.3299,
      "step": 13864
    },
    {
      "epoch": 0.06521820935680217,
      "grad_norm": 1.7368284463882446,
      "learning_rate": 0.00018739709751336672,
      "loss": 0.2156,
      "step": 13865
    },
    {
      "epoch": 0.06522291315841464,
      "grad_norm": 1.3037692308425903,
      "learning_rate": 0.00018739615453525327,
      "loss": 0.1366,
      "step": 13866
    },
    {
      "epoch": 0.06522761696002709,
      "grad_norm": 2.1642227172851562,
      "learning_rate": 0.00018739521155713978,
      "loss": 0.4711,
      "step": 13867
    },
    {
      "epoch": 0.06523232076163955,
      "grad_norm": 0.29552096128463745,
      "learning_rate": 0.00018739426857902628,
      "loss": 0.0414,
      "step": 13868
    },
    {
      "epoch": 0.06523702456325202,
      "grad_norm": 1.2900861501693726,
      "learning_rate": 0.0001873933256009128,
      "loss": 0.162,
      "step": 13869
    },
    {
      "epoch": 0.06524172836486448,
      "grad_norm": 1.7129260301589966,
      "learning_rate": 0.00018739238262279931,
      "loss": 0.3826,
      "step": 13870
    },
    {
      "epoch": 0.06524643216647695,
      "grad_norm": 1.26346755027771,
      "learning_rate": 0.00018739143964468586,
      "loss": 0.0802,
      "step": 13871
    },
    {
      "epoch": 0.06525113596808942,
      "grad_norm": 3.2660937309265137,
      "learning_rate": 0.00018739049666657238,
      "loss": 0.4808,
      "step": 13872
    },
    {
      "epoch": 0.06525583976970187,
      "grad_norm": 0.10221405327320099,
      "learning_rate": 0.0001873895536884589,
      "loss": 0.0067,
      "step": 13873
    },
    {
      "epoch": 0.06526054357131433,
      "grad_norm": 1.1368684768676758,
      "learning_rate": 0.00018738861071034542,
      "loss": 0.1741,
      "step": 13874
    },
    {
      "epoch": 0.0652652473729268,
      "grad_norm": 2.395596504211426,
      "learning_rate": 0.00018738766773223196,
      "loss": 0.4483,
      "step": 13875
    },
    {
      "epoch": 0.06526995117453926,
      "grad_norm": 3.304597854614258,
      "learning_rate": 0.00018738672475411848,
      "loss": 0.3137,
      "step": 13876
    },
    {
      "epoch": 0.06527465497615173,
      "grad_norm": 1.8870606422424316,
      "learning_rate": 0.000187385781776005,
      "loss": 0.2715,
      "step": 13877
    },
    {
      "epoch": 0.0652793587777642,
      "grad_norm": 1.8239394426345825,
      "learning_rate": 0.00018738483879789152,
      "loss": 0.3977,
      "step": 13878
    },
    {
      "epoch": 0.06528406257937665,
      "grad_norm": 1.9348034858703613,
      "learning_rate": 0.000187383895819778,
      "loss": 0.215,
      "step": 13879
    },
    {
      "epoch": 0.06528876638098911,
      "grad_norm": 2.1620237827301025,
      "learning_rate": 0.00018738295284166455,
      "loss": 0.3772,
      "step": 13880
    },
    {
      "epoch": 0.06529347018260158,
      "grad_norm": 1.6213674545288086,
      "learning_rate": 0.00018738200986355107,
      "loss": 0.1951,
      "step": 13881
    },
    {
      "epoch": 0.06529817398421404,
      "grad_norm": 1.1015615463256836,
      "learning_rate": 0.0001873810668854376,
      "loss": 0.1342,
      "step": 13882
    },
    {
      "epoch": 0.06530287778582651,
      "grad_norm": 3.7017531394958496,
      "learning_rate": 0.0001873801239073241,
      "loss": 0.4379,
      "step": 13883
    },
    {
      "epoch": 0.06530758158743896,
      "grad_norm": 0.2920137643814087,
      "learning_rate": 0.00018737918092921066,
      "loss": 0.0321,
      "step": 13884
    },
    {
      "epoch": 0.06531228538905143,
      "grad_norm": 0.9351584911346436,
      "learning_rate": 0.00018737823795109717,
      "loss": 0.1159,
      "step": 13885
    },
    {
      "epoch": 0.0653169891906639,
      "grad_norm": 2.892664909362793,
      "learning_rate": 0.0001873772949729837,
      "loss": 0.4219,
      "step": 13886
    },
    {
      "epoch": 0.06532169299227636,
      "grad_norm": 2.1721396446228027,
      "learning_rate": 0.0001873763519948702,
      "loss": 0.378,
      "step": 13887
    },
    {
      "epoch": 0.06532639679388882,
      "grad_norm": 0.6207137703895569,
      "learning_rate": 0.00018737540901675673,
      "loss": 0.0574,
      "step": 13888
    },
    {
      "epoch": 0.06533110059550129,
      "grad_norm": 2.1825919151306152,
      "learning_rate": 0.00018737446603864325,
      "loss": 0.401,
      "step": 13889
    },
    {
      "epoch": 0.06533580439711374,
      "grad_norm": 1.309227705001831,
      "learning_rate": 0.00018737352306052977,
      "loss": 0.2129,
      "step": 13890
    },
    {
      "epoch": 0.06534050819872621,
      "grad_norm": 0.41939786076545715,
      "learning_rate": 0.00018737258008241629,
      "loss": 0.0612,
      "step": 13891
    },
    {
      "epoch": 0.06534521200033867,
      "grad_norm": 2.537985324859619,
      "learning_rate": 0.0001873716371043028,
      "loss": 0.4639,
      "step": 13892
    },
    {
      "epoch": 0.06534991580195114,
      "grad_norm": 0.34403184056282043,
      "learning_rate": 0.00018737069412618935,
      "loss": 0.0437,
      "step": 13893
    },
    {
      "epoch": 0.0653546196035636,
      "grad_norm": 2.600945234298706,
      "learning_rate": 0.00018736975114807587,
      "loss": 0.3653,
      "step": 13894
    },
    {
      "epoch": 0.06535932340517607,
      "grad_norm": 2.9672107696533203,
      "learning_rate": 0.0001873688081699624,
      "loss": 0.7787,
      "step": 13895
    },
    {
      "epoch": 0.06536402720678852,
      "grad_norm": 2.0341739654541016,
      "learning_rate": 0.0001873678651918489,
      "loss": 0.51,
      "step": 13896
    },
    {
      "epoch": 0.06536873100840099,
      "grad_norm": 1.070069432258606,
      "learning_rate": 0.00018736692221373543,
      "loss": 0.2807,
      "step": 13897
    },
    {
      "epoch": 0.06537343481001345,
      "grad_norm": 0.28646522760391235,
      "learning_rate": 0.00018736597923562197,
      "loss": 0.0225,
      "step": 13898
    },
    {
      "epoch": 0.06537813861162592,
      "grad_norm": 1.252457618713379,
      "learning_rate": 0.00018736503625750846,
      "loss": 0.145,
      "step": 13899
    },
    {
      "epoch": 0.06538284241323838,
      "grad_norm": 0.734327495098114,
      "learning_rate": 0.00018736409327939498,
      "loss": 0.1422,
      "step": 13900
    },
    {
      "epoch": 0.06538754621485084,
      "grad_norm": 1.3018627166748047,
      "learning_rate": 0.0001873631503012815,
      "loss": 0.0758,
      "step": 13901
    },
    {
      "epoch": 0.0653922500164633,
      "grad_norm": 0.3037669360637665,
      "learning_rate": 0.00018736220732316805,
      "loss": 0.0275,
      "step": 13902
    },
    {
      "epoch": 0.06539695381807577,
      "grad_norm": 2.3851683139801025,
      "learning_rate": 0.00018736126434505456,
      "loss": 0.407,
      "step": 13903
    },
    {
      "epoch": 0.06540165761968823,
      "grad_norm": 0.5732787251472473,
      "learning_rate": 0.00018736032136694108,
      "loss": 0.0778,
      "step": 13904
    },
    {
      "epoch": 0.0654063614213007,
      "grad_norm": 4.60919189453125,
      "learning_rate": 0.0001873593783888276,
      "loss": 0.6037,
      "step": 13905
    },
    {
      "epoch": 0.06541106522291316,
      "grad_norm": 1.7819976806640625,
      "learning_rate": 0.00018735843541071412,
      "loss": 0.1928,
      "step": 13906
    },
    {
      "epoch": 0.06541576902452562,
      "grad_norm": 2.207982301712036,
      "learning_rate": 0.00018735749243260067,
      "loss": 0.3397,
      "step": 13907
    },
    {
      "epoch": 0.06542047282613808,
      "grad_norm": 3.5284738540649414,
      "learning_rate": 0.00018735654945448718,
      "loss": 0.8896,
      "step": 13908
    },
    {
      "epoch": 0.06542517662775055,
      "grad_norm": 2.716928243637085,
      "learning_rate": 0.0001873556064763737,
      "loss": 0.4624,
      "step": 13909
    },
    {
      "epoch": 0.06542988042936301,
      "grad_norm": 1.548526644706726,
      "learning_rate": 0.0001873546634982602,
      "loss": 0.2248,
      "step": 13910
    },
    {
      "epoch": 0.06543458423097548,
      "grad_norm": 3.2908592224121094,
      "learning_rate": 0.00018735372052014674,
      "loss": 0.2808,
      "step": 13911
    },
    {
      "epoch": 0.06543928803258794,
      "grad_norm": 1.3081045150756836,
      "learning_rate": 0.00018735277754203326,
      "loss": 0.3152,
      "step": 13912
    },
    {
      "epoch": 0.0654439918342004,
      "grad_norm": 1.5391442775726318,
      "learning_rate": 0.00018735183456391978,
      "loss": 0.1665,
      "step": 13913
    },
    {
      "epoch": 0.06544869563581286,
      "grad_norm": 0.8082425594329834,
      "learning_rate": 0.0001873508915858063,
      "loss": 0.1731,
      "step": 13914
    },
    {
      "epoch": 0.06545339943742533,
      "grad_norm": 0.8251343369483948,
      "learning_rate": 0.00018734994860769282,
      "loss": 0.1678,
      "step": 13915
    },
    {
      "epoch": 0.0654581032390378,
      "grad_norm": 1.2794489860534668,
      "learning_rate": 0.00018734900562957936,
      "loss": 0.0617,
      "step": 13916
    },
    {
      "epoch": 0.06546280704065026,
      "grad_norm": 2.727341651916504,
      "learning_rate": 0.00018734806265146588,
      "loss": 0.4101,
      "step": 13917
    },
    {
      "epoch": 0.06546751084226271,
      "grad_norm": 2.4922232627868652,
      "learning_rate": 0.0001873471196733524,
      "loss": 0.3314,
      "step": 13918
    },
    {
      "epoch": 0.06547221464387518,
      "grad_norm": 1.0512503385543823,
      "learning_rate": 0.00018734617669523892,
      "loss": 0.205,
      "step": 13919
    },
    {
      "epoch": 0.06547691844548764,
      "grad_norm": 2.6339876651763916,
      "learning_rate": 0.00018734523371712544,
      "loss": 0.3804,
      "step": 13920
    },
    {
      "epoch": 0.06548162224710011,
      "grad_norm": 1.3297075033187866,
      "learning_rate": 0.00018734429073901195,
      "loss": 0.1838,
      "step": 13921
    },
    {
      "epoch": 0.06548632604871257,
      "grad_norm": 3.433352470397949,
      "learning_rate": 0.00018734334776089847,
      "loss": 0.5656,
      "step": 13922
    },
    {
      "epoch": 0.06549102985032504,
      "grad_norm": 2.0295307636260986,
      "learning_rate": 0.000187342404782785,
      "loss": 0.5049,
      "step": 13923
    },
    {
      "epoch": 0.06549573365193749,
      "grad_norm": 3.2258684635162354,
      "learning_rate": 0.0001873414618046715,
      "loss": 0.7284,
      "step": 13924
    },
    {
      "epoch": 0.06550043745354996,
      "grad_norm": 0.9447819590568542,
      "learning_rate": 0.00018734051882655806,
      "loss": 0.1745,
      "step": 13925
    },
    {
      "epoch": 0.06550514125516242,
      "grad_norm": 3.902524948120117,
      "learning_rate": 0.00018733957584844457,
      "loss": 0.765,
      "step": 13926
    },
    {
      "epoch": 0.06550984505677489,
      "grad_norm": 6.648962020874023,
      "learning_rate": 0.0001873386328703311,
      "loss": 0.6869,
      "step": 13927
    },
    {
      "epoch": 0.06551454885838735,
      "grad_norm": 0.4877655506134033,
      "learning_rate": 0.0001873376898922176,
      "loss": 0.0917,
      "step": 13928
    },
    {
      "epoch": 0.06551925265999982,
      "grad_norm": 1.5026519298553467,
      "learning_rate": 0.00018733674691410413,
      "loss": 0.3009,
      "step": 13929
    },
    {
      "epoch": 0.06552395646161227,
      "grad_norm": 0.6984321475028992,
      "learning_rate": 0.00018733580393599065,
      "loss": 0.1194,
      "step": 13930
    },
    {
      "epoch": 0.06552866026322474,
      "grad_norm": 1.0609201192855835,
      "learning_rate": 0.00018733486095787717,
      "loss": 0.1835,
      "step": 13931
    },
    {
      "epoch": 0.0655333640648372,
      "grad_norm": 1.1696951389312744,
      "learning_rate": 0.00018733391797976369,
      "loss": 0.1235,
      "step": 13932
    },
    {
      "epoch": 0.06553806786644967,
      "grad_norm": 0.8540788888931274,
      "learning_rate": 0.0001873329750016502,
      "loss": 0.2141,
      "step": 13933
    },
    {
      "epoch": 0.06554277166806213,
      "grad_norm": 0.641306459903717,
      "learning_rate": 0.00018733203202353675,
      "loss": 0.1243,
      "step": 13934
    },
    {
      "epoch": 0.06554747546967458,
      "grad_norm": 0.7157554030418396,
      "learning_rate": 0.00018733108904542327,
      "loss": 0.1145,
      "step": 13935
    },
    {
      "epoch": 0.06555217927128705,
      "grad_norm": 3.3926682472229004,
      "learning_rate": 0.0001873301460673098,
      "loss": 0.6837,
      "step": 13936
    },
    {
      "epoch": 0.06555688307289952,
      "grad_norm": 1.976470708847046,
      "learning_rate": 0.0001873292030891963,
      "loss": 0.2974,
      "step": 13937
    },
    {
      "epoch": 0.06556158687451198,
      "grad_norm": 1.1402842998504639,
      "learning_rate": 0.00018732826011108283,
      "loss": 0.1638,
      "step": 13938
    },
    {
      "epoch": 0.06556629067612445,
      "grad_norm": 0.4878551959991455,
      "learning_rate": 0.00018732731713296937,
      "loss": 0.0784,
      "step": 13939
    },
    {
      "epoch": 0.06557099447773691,
      "grad_norm": 4.345240116119385,
      "learning_rate": 0.0001873263741548559,
      "loss": 1.0208,
      "step": 13940
    },
    {
      "epoch": 0.06557569827934936,
      "grad_norm": 1.0537164211273193,
      "learning_rate": 0.00018732543117674238,
      "loss": 0.1661,
      "step": 13941
    },
    {
      "epoch": 0.06558040208096183,
      "grad_norm": 1.7767353057861328,
      "learning_rate": 0.0001873244881986289,
      "loss": 0.186,
      "step": 13942
    },
    {
      "epoch": 0.0655851058825743,
      "grad_norm": 2.338665008544922,
      "learning_rate": 0.00018732354522051545,
      "loss": 0.3595,
      "step": 13943
    },
    {
      "epoch": 0.06558980968418676,
      "grad_norm": 0.48836153745651245,
      "learning_rate": 0.00018732260224240196,
      "loss": 0.0372,
      "step": 13944
    },
    {
      "epoch": 0.06559451348579923,
      "grad_norm": 1.4408910274505615,
      "learning_rate": 0.00018732165926428848,
      "loss": 0.1632,
      "step": 13945
    },
    {
      "epoch": 0.06559921728741169,
      "grad_norm": 1.037667989730835,
      "learning_rate": 0.000187320716286175,
      "loss": 0.0697,
      "step": 13946
    },
    {
      "epoch": 0.06560392108902414,
      "grad_norm": 1.4037184715270996,
      "learning_rate": 0.00018731977330806152,
      "loss": 0.3346,
      "step": 13947
    },
    {
      "epoch": 0.06560862489063661,
      "grad_norm": 3.353282928466797,
      "learning_rate": 0.00018731883032994807,
      "loss": 0.4812,
      "step": 13948
    },
    {
      "epoch": 0.06561332869224908,
      "grad_norm": 3.9485394954681396,
      "learning_rate": 0.00018731788735183458,
      "loss": 0.2751,
      "step": 13949
    },
    {
      "epoch": 0.06561803249386154,
      "grad_norm": 1.0464075803756714,
      "learning_rate": 0.0001873169443737211,
      "loss": 0.2147,
      "step": 13950
    },
    {
      "epoch": 0.06562273629547401,
      "grad_norm": 5.457088947296143,
      "learning_rate": 0.00018731600139560762,
      "loss": 0.1717,
      "step": 13951
    },
    {
      "epoch": 0.06562744009708646,
      "grad_norm": 0.3851945698261261,
      "learning_rate": 0.00018731505841749414,
      "loss": 0.0391,
      "step": 13952
    },
    {
      "epoch": 0.06563214389869892,
      "grad_norm": 2.1004998683929443,
      "learning_rate": 0.00018731411543938066,
      "loss": 0.1807,
      "step": 13953
    },
    {
      "epoch": 0.06563684770031139,
      "grad_norm": 2.5525636672973633,
      "learning_rate": 0.00018731317246126718,
      "loss": 0.162,
      "step": 13954
    },
    {
      "epoch": 0.06564155150192386,
      "grad_norm": 3.787743330001831,
      "learning_rate": 0.0001873122294831537,
      "loss": 0.6475,
      "step": 13955
    },
    {
      "epoch": 0.06564625530353632,
      "grad_norm": 3.4814395904541016,
      "learning_rate": 0.00018731128650504022,
      "loss": 0.8048,
      "step": 13956
    },
    {
      "epoch": 0.06565095910514879,
      "grad_norm": 3.9133121967315674,
      "learning_rate": 0.00018731034352692676,
      "loss": 0.5904,
      "step": 13957
    },
    {
      "epoch": 0.06565566290676124,
      "grad_norm": 1.5939240455627441,
      "learning_rate": 0.00018730940054881328,
      "loss": 0.366,
      "step": 13958
    },
    {
      "epoch": 0.0656603667083737,
      "grad_norm": 0.7408804297447205,
      "learning_rate": 0.0001873084575706998,
      "loss": 0.0772,
      "step": 13959
    },
    {
      "epoch": 0.06566507050998617,
      "grad_norm": 4.1670732498168945,
      "learning_rate": 0.00018730751459258632,
      "loss": 0.4418,
      "step": 13960
    },
    {
      "epoch": 0.06566977431159864,
      "grad_norm": 2.694784641265869,
      "learning_rate": 0.00018730657161447284,
      "loss": 0.4621,
      "step": 13961
    },
    {
      "epoch": 0.0656744781132111,
      "grad_norm": 0.6370871067047119,
      "learning_rate": 0.00018730562863635935,
      "loss": 0.0557,
      "step": 13962
    },
    {
      "epoch": 0.06567918191482357,
      "grad_norm": 1.4034637212753296,
      "learning_rate": 0.00018730468565824587,
      "loss": 0.209,
      "step": 13963
    },
    {
      "epoch": 0.06568388571643602,
      "grad_norm": 1.447763442993164,
      "learning_rate": 0.0001873037426801324,
      "loss": 0.2023,
      "step": 13964
    },
    {
      "epoch": 0.06568858951804848,
      "grad_norm": 3.1223857402801514,
      "learning_rate": 0.0001873027997020189,
      "loss": 0.4626,
      "step": 13965
    },
    {
      "epoch": 0.06569329331966095,
      "grad_norm": 1.4353164434432983,
      "learning_rate": 0.00018730185672390546,
      "loss": 0.1513,
      "step": 13966
    },
    {
      "epoch": 0.06569799712127342,
      "grad_norm": 1.125792384147644,
      "learning_rate": 0.00018730091374579197,
      "loss": 0.1145,
      "step": 13967
    },
    {
      "epoch": 0.06570270092288588,
      "grad_norm": 0.3569173216819763,
      "learning_rate": 0.0001872999707676785,
      "loss": 0.0485,
      "step": 13968
    },
    {
      "epoch": 0.06570740472449833,
      "grad_norm": 1.2283058166503906,
      "learning_rate": 0.000187299027789565,
      "loss": 0.2212,
      "step": 13969
    },
    {
      "epoch": 0.0657121085261108,
      "grad_norm": 1.5321044921875,
      "learning_rate": 0.00018729808481145153,
      "loss": 0.3611,
      "step": 13970
    },
    {
      "epoch": 0.06571681232772326,
      "grad_norm": 1.5368610620498657,
      "learning_rate": 0.00018729714183333808,
      "loss": 0.2289,
      "step": 13971
    },
    {
      "epoch": 0.06572151612933573,
      "grad_norm": 1.580683708190918,
      "learning_rate": 0.00018729619885522457,
      "loss": 0.2733,
      "step": 13972
    },
    {
      "epoch": 0.0657262199309482,
      "grad_norm": 2.513592004776001,
      "learning_rate": 0.00018729525587711109,
      "loss": 0.4395,
      "step": 13973
    },
    {
      "epoch": 0.06573092373256066,
      "grad_norm": 2.3446385860443115,
      "learning_rate": 0.0001872943128989976,
      "loss": 0.4099,
      "step": 13974
    },
    {
      "epoch": 0.06573562753417311,
      "grad_norm": 2.490809917449951,
      "learning_rate": 0.00018729336992088415,
      "loss": 0.5645,
      "step": 13975
    },
    {
      "epoch": 0.06574033133578558,
      "grad_norm": 1.8231967687606812,
      "learning_rate": 0.00018729242694277067,
      "loss": 0.3601,
      "step": 13976
    },
    {
      "epoch": 0.06574503513739804,
      "grad_norm": 0.9296476244926453,
      "learning_rate": 0.0001872914839646572,
      "loss": 0.1554,
      "step": 13977
    },
    {
      "epoch": 0.06574973893901051,
      "grad_norm": 1.479222297668457,
      "learning_rate": 0.0001872905409865437,
      "loss": 0.078,
      "step": 13978
    },
    {
      "epoch": 0.06575444274062298,
      "grad_norm": 0.5612878799438477,
      "learning_rate": 0.00018728959800843023,
      "loss": 0.049,
      "step": 13979
    },
    {
      "epoch": 0.06575914654223544,
      "grad_norm": 1.0034211874008179,
      "learning_rate": 0.00018728865503031677,
      "loss": 0.2098,
      "step": 13980
    },
    {
      "epoch": 0.0657638503438479,
      "grad_norm": 0.8917295932769775,
      "learning_rate": 0.0001872877120522033,
      "loss": 0.1065,
      "step": 13981
    },
    {
      "epoch": 0.06576855414546036,
      "grad_norm": 1.3814165592193604,
      "learning_rate": 0.0001872867690740898,
      "loss": 0.1624,
      "step": 13982
    },
    {
      "epoch": 0.06577325794707282,
      "grad_norm": 1.412511944770813,
      "learning_rate": 0.0001872858260959763,
      "loss": 0.2805,
      "step": 13983
    },
    {
      "epoch": 0.06577796174868529,
      "grad_norm": 0.9381830096244812,
      "learning_rate": 0.00018728488311786285,
      "loss": 0.0829,
      "step": 13984
    },
    {
      "epoch": 0.06578266555029776,
      "grad_norm": 1.463756799697876,
      "learning_rate": 0.00018728394013974936,
      "loss": 0.2367,
      "step": 13985
    },
    {
      "epoch": 0.06578736935191021,
      "grad_norm": 1.2857649326324463,
      "learning_rate": 0.00018728299716163588,
      "loss": 0.1908,
      "step": 13986
    },
    {
      "epoch": 0.06579207315352267,
      "grad_norm": 3.1741995811462402,
      "learning_rate": 0.0001872820541835224,
      "loss": 0.4369,
      "step": 13987
    },
    {
      "epoch": 0.06579677695513514,
      "grad_norm": 1.0809581279754639,
      "learning_rate": 0.00018728111120540892,
      "loss": 0.1133,
      "step": 13988
    },
    {
      "epoch": 0.0658014807567476,
      "grad_norm": 3.6362392902374268,
      "learning_rate": 0.00018728016822729547,
      "loss": 0.9629,
      "step": 13989
    },
    {
      "epoch": 0.06580618455836007,
      "grad_norm": 1.80527663230896,
      "learning_rate": 0.00018727922524918198,
      "loss": 0.342,
      "step": 13990
    },
    {
      "epoch": 0.06581088835997254,
      "grad_norm": 1.0488698482513428,
      "learning_rate": 0.0001872782822710685,
      "loss": 0.1409,
      "step": 13991
    },
    {
      "epoch": 0.06581559216158499,
      "grad_norm": 1.3479527235031128,
      "learning_rate": 0.00018727733929295502,
      "loss": 0.2792,
      "step": 13992
    },
    {
      "epoch": 0.06582029596319745,
      "grad_norm": 0.7053807377815247,
      "learning_rate": 0.00018727639631484154,
      "loss": 0.0646,
      "step": 13993
    },
    {
      "epoch": 0.06582499976480992,
      "grad_norm": 1.0275020599365234,
      "learning_rate": 0.00018727545333672806,
      "loss": 0.1143,
      "step": 13994
    },
    {
      "epoch": 0.06582970356642238,
      "grad_norm": 1.0218571424484253,
      "learning_rate": 0.00018727451035861458,
      "loss": 0.1246,
      "step": 13995
    },
    {
      "epoch": 0.06583440736803485,
      "grad_norm": 3.1661148071289062,
      "learning_rate": 0.0001872735673805011,
      "loss": 0.5138,
      "step": 13996
    },
    {
      "epoch": 0.06583911116964732,
      "grad_norm": 2.3563928604125977,
      "learning_rate": 0.00018727262440238762,
      "loss": 0.1994,
      "step": 13997
    },
    {
      "epoch": 0.06584381497125977,
      "grad_norm": 2.077841281890869,
      "learning_rate": 0.00018727168142427416,
      "loss": 0.2384,
      "step": 13998
    },
    {
      "epoch": 0.06584851877287223,
      "grad_norm": 2.4063069820404053,
      "learning_rate": 0.00018727073844616068,
      "loss": 0.1973,
      "step": 13999
    },
    {
      "epoch": 0.0658532225744847,
      "grad_norm": 0.7628015279769897,
      "learning_rate": 0.0001872697954680472,
      "loss": 0.0704,
      "step": 14000
    },
    {
      "epoch": 0.06585792637609716,
      "grad_norm": 5.057799816131592,
      "learning_rate": 0.00018726885248993372,
      "loss": 0.7766,
      "step": 14001
    },
    {
      "epoch": 0.06586263017770963,
      "grad_norm": 4.436938285827637,
      "learning_rate": 0.00018726790951182026,
      "loss": 0.5386,
      "step": 14002
    },
    {
      "epoch": 0.06586733397932208,
      "grad_norm": 2.5878686904907227,
      "learning_rate": 0.00018726696653370675,
      "loss": 0.1985,
      "step": 14003
    },
    {
      "epoch": 0.06587203778093455,
      "grad_norm": 1.8560261726379395,
      "learning_rate": 0.00018726602355559327,
      "loss": 0.1963,
      "step": 14004
    },
    {
      "epoch": 0.06587674158254701,
      "grad_norm": 1.9098862409591675,
      "learning_rate": 0.0001872650805774798,
      "loss": 0.1835,
      "step": 14005
    },
    {
      "epoch": 0.06588144538415948,
      "grad_norm": 2.8765032291412354,
      "learning_rate": 0.0001872641375993663,
      "loss": 0.4627,
      "step": 14006
    },
    {
      "epoch": 0.06588614918577194,
      "grad_norm": 4.4858317375183105,
      "learning_rate": 0.00018726319462125286,
      "loss": 0.721,
      "step": 14007
    },
    {
      "epoch": 0.06589085298738441,
      "grad_norm": 0.930147111415863,
      "learning_rate": 0.00018726225164313937,
      "loss": 0.0697,
      "step": 14008
    },
    {
      "epoch": 0.06589555678899686,
      "grad_norm": 0.8548622727394104,
      "learning_rate": 0.0001872613086650259,
      "loss": 0.0985,
      "step": 14009
    },
    {
      "epoch": 0.06590026059060933,
      "grad_norm": 1.4038804769515991,
      "learning_rate": 0.0001872603656869124,
      "loss": 0.127,
      "step": 14010
    },
    {
      "epoch": 0.0659049643922218,
      "grad_norm": 0.9104323983192444,
      "learning_rate": 0.00018725942270879896,
      "loss": 0.1063,
      "step": 14011
    },
    {
      "epoch": 0.06590966819383426,
      "grad_norm": 2.5792160034179688,
      "learning_rate": 0.00018725847973068548,
      "loss": 0.3785,
      "step": 14012
    },
    {
      "epoch": 0.06591437199544672,
      "grad_norm": 2.1634252071380615,
      "learning_rate": 0.000187257536752572,
      "loss": 0.1742,
      "step": 14013
    },
    {
      "epoch": 0.06591907579705919,
      "grad_norm": 2.045041084289551,
      "learning_rate": 0.00018725659377445849,
      "loss": 0.3096,
      "step": 14014
    },
    {
      "epoch": 0.06592377959867164,
      "grad_norm": 2.497396230697632,
      "learning_rate": 0.000187255650796345,
      "loss": 0.486,
      "step": 14015
    },
    {
      "epoch": 0.06592848340028411,
      "grad_norm": 2.1908786296844482,
      "learning_rate": 0.00018725470781823155,
      "loss": 0.1871,
      "step": 14016
    },
    {
      "epoch": 0.06593318720189657,
      "grad_norm": 1.9825619459152222,
      "learning_rate": 0.00018725376484011807,
      "loss": 0.0928,
      "step": 14017
    },
    {
      "epoch": 0.06593789100350904,
      "grad_norm": 3.804225206375122,
      "learning_rate": 0.0001872528218620046,
      "loss": 0.3878,
      "step": 14018
    },
    {
      "epoch": 0.0659425948051215,
      "grad_norm": 2.117223024368286,
      "learning_rate": 0.0001872518788838911,
      "loss": 0.2247,
      "step": 14019
    },
    {
      "epoch": 0.06594729860673396,
      "grad_norm": 2.0655343532562256,
      "learning_rate": 0.00018725093590577765,
      "loss": 0.1309,
      "step": 14020
    },
    {
      "epoch": 0.06595200240834642,
      "grad_norm": 2.88234281539917,
      "learning_rate": 0.00018724999292766417,
      "loss": 0.4153,
      "step": 14021
    },
    {
      "epoch": 0.06595670620995889,
      "grad_norm": 0.900626003742218,
      "learning_rate": 0.0001872490499495507,
      "loss": 0.0892,
      "step": 14022
    },
    {
      "epoch": 0.06596141001157135,
      "grad_norm": 1.8330912590026855,
      "learning_rate": 0.0001872481069714372,
      "loss": 0.2197,
      "step": 14023
    },
    {
      "epoch": 0.06596611381318382,
      "grad_norm": 3.1302638053894043,
      "learning_rate": 0.00018724716399332373,
      "loss": 0.6219,
      "step": 14024
    },
    {
      "epoch": 0.06597081761479628,
      "grad_norm": 1.8143600225448608,
      "learning_rate": 0.00018724622101521025,
      "loss": 0.1978,
      "step": 14025
    },
    {
      "epoch": 0.06597552141640874,
      "grad_norm": 2.452388286590576,
      "learning_rate": 0.00018724527803709676,
      "loss": 0.4035,
      "step": 14026
    },
    {
      "epoch": 0.0659802252180212,
      "grad_norm": 1.711190938949585,
      "learning_rate": 0.00018724433505898328,
      "loss": 0.2048,
      "step": 14027
    },
    {
      "epoch": 0.06598492901963367,
      "grad_norm": 1.2075459957122803,
      "learning_rate": 0.0001872433920808698,
      "loss": 0.087,
      "step": 14028
    },
    {
      "epoch": 0.06598963282124613,
      "grad_norm": 2.10183048248291,
      "learning_rate": 0.00018724244910275632,
      "loss": 0.166,
      "step": 14029
    },
    {
      "epoch": 0.0659943366228586,
      "grad_norm": 3.339761734008789,
      "learning_rate": 0.00018724150612464287,
      "loss": 0.4047,
      "step": 14030
    },
    {
      "epoch": 0.06599904042447106,
      "grad_norm": 1.8477469682693481,
      "learning_rate": 0.00018724056314652938,
      "loss": 0.1697,
      "step": 14031
    },
    {
      "epoch": 0.06600374422608352,
      "grad_norm": 1.000549554824829,
      "learning_rate": 0.0001872396201684159,
      "loss": 0.0574,
      "step": 14032
    },
    {
      "epoch": 0.06600844802769598,
      "grad_norm": 0.6577686071395874,
      "learning_rate": 0.00018723867719030242,
      "loss": 0.0514,
      "step": 14033
    },
    {
      "epoch": 0.06601315182930845,
      "grad_norm": 4.274438381195068,
      "learning_rate": 0.00018723773421218894,
      "loss": 0.693,
      "step": 14034
    },
    {
      "epoch": 0.06601785563092091,
      "grad_norm": 3.891822099685669,
      "learning_rate": 0.00018723679123407546,
      "loss": 0.5274,
      "step": 14035
    },
    {
      "epoch": 0.06602255943253338,
      "grad_norm": 3.7628493309020996,
      "learning_rate": 0.00018723584825596198,
      "loss": 0.6877,
      "step": 14036
    },
    {
      "epoch": 0.06602726323414583,
      "grad_norm": 2.228023052215576,
      "learning_rate": 0.0001872349052778485,
      "loss": 0.11,
      "step": 14037
    },
    {
      "epoch": 0.0660319670357583,
      "grad_norm": 0.3669898808002472,
      "learning_rate": 0.00018723396229973502,
      "loss": 0.0261,
      "step": 14038
    },
    {
      "epoch": 0.06603667083737076,
      "grad_norm": 3.4393866062164307,
      "learning_rate": 0.00018723301932162156,
      "loss": 0.3653,
      "step": 14039
    },
    {
      "epoch": 0.06604137463898323,
      "grad_norm": 1.5177710056304932,
      "learning_rate": 0.00018723207634350808,
      "loss": 0.1488,
      "step": 14040
    },
    {
      "epoch": 0.06604607844059569,
      "grad_norm": 3.212796926498413,
      "learning_rate": 0.0001872311333653946,
      "loss": 0.5309,
      "step": 14041
    },
    {
      "epoch": 0.06605078224220816,
      "grad_norm": 2.1330111026763916,
      "learning_rate": 0.00018723019038728112,
      "loss": 0.4702,
      "step": 14042
    },
    {
      "epoch": 0.06605548604382061,
      "grad_norm": 1.2792010307312012,
      "learning_rate": 0.00018722924740916766,
      "loss": 0.2451,
      "step": 14043
    },
    {
      "epoch": 0.06606018984543308,
      "grad_norm": 3.582632541656494,
      "learning_rate": 0.00018722830443105418,
      "loss": 0.7121,
      "step": 14044
    },
    {
      "epoch": 0.06606489364704554,
      "grad_norm": 1.5033446550369263,
      "learning_rate": 0.00018722736145294067,
      "loss": 0.1761,
      "step": 14045
    },
    {
      "epoch": 0.06606959744865801,
      "grad_norm": 2.165395498275757,
      "learning_rate": 0.0001872264184748272,
      "loss": 0.4562,
      "step": 14046
    },
    {
      "epoch": 0.06607430125027047,
      "grad_norm": 0.590144693851471,
      "learning_rate": 0.0001872254754967137,
      "loss": 0.059,
      "step": 14047
    },
    {
      "epoch": 0.06607900505188294,
      "grad_norm": 1.7014573812484741,
      "learning_rate": 0.00018722453251860026,
      "loss": 0.2935,
      "step": 14048
    },
    {
      "epoch": 0.06608370885349539,
      "grad_norm": 3.80998158454895,
      "learning_rate": 0.00018722358954048677,
      "loss": 0.6312,
      "step": 14049
    },
    {
      "epoch": 0.06608841265510786,
      "grad_norm": 1.3755238056182861,
      "learning_rate": 0.0001872226465623733,
      "loss": 0.2247,
      "step": 14050
    },
    {
      "epoch": 0.06609311645672032,
      "grad_norm": 4.606720924377441,
      "learning_rate": 0.0001872217035842598,
      "loss": 0.3665,
      "step": 14051
    },
    {
      "epoch": 0.06609782025833279,
      "grad_norm": 3.2710933685302734,
      "learning_rate": 0.00018722076060614636,
      "loss": 0.4507,
      "step": 14052
    },
    {
      "epoch": 0.06610252405994525,
      "grad_norm": 6.548547267913818,
      "learning_rate": 0.00018721981762803288,
      "loss": 0.5072,
      "step": 14053
    },
    {
      "epoch": 0.0661072278615577,
      "grad_norm": 2.5548183917999268,
      "learning_rate": 0.0001872188746499194,
      "loss": 0.3067,
      "step": 14054
    },
    {
      "epoch": 0.06611193166317017,
      "grad_norm": 3.8280158042907715,
      "learning_rate": 0.0001872179316718059,
      "loss": 0.4924,
      "step": 14055
    },
    {
      "epoch": 0.06611663546478264,
      "grad_norm": 1.1216014623641968,
      "learning_rate": 0.00018721698869369243,
      "loss": 0.209,
      "step": 14056
    },
    {
      "epoch": 0.0661213392663951,
      "grad_norm": 0.7444973587989807,
      "learning_rate": 0.00018721604571557895,
      "loss": 0.134,
      "step": 14057
    },
    {
      "epoch": 0.06612604306800757,
      "grad_norm": 0.9565136432647705,
      "learning_rate": 0.00018721510273746547,
      "loss": 0.1184,
      "step": 14058
    },
    {
      "epoch": 0.06613074686962003,
      "grad_norm": 2.637575626373291,
      "learning_rate": 0.000187214159759352,
      "loss": 0.5039,
      "step": 14059
    },
    {
      "epoch": 0.06613545067123248,
      "grad_norm": 1.7190190553665161,
      "learning_rate": 0.0001872132167812385,
      "loss": 0.1849,
      "step": 14060
    },
    {
      "epoch": 0.06614015447284495,
      "grad_norm": 0.9858790636062622,
      "learning_rate": 0.00018721227380312505,
      "loss": 0.0889,
      "step": 14061
    },
    {
      "epoch": 0.06614485827445742,
      "grad_norm": 1.9660439491271973,
      "learning_rate": 0.00018721133082501157,
      "loss": 0.4236,
      "step": 14062
    },
    {
      "epoch": 0.06614956207606988,
      "grad_norm": 1.186974287033081,
      "learning_rate": 0.0001872103878468981,
      "loss": 0.1891,
      "step": 14063
    },
    {
      "epoch": 0.06615426587768235,
      "grad_norm": 1.7952312231063843,
      "learning_rate": 0.0001872094448687846,
      "loss": 0.2469,
      "step": 14064
    },
    {
      "epoch": 0.06615896967929481,
      "grad_norm": 2.258527994155884,
      "learning_rate": 0.00018720850189067113,
      "loss": 0.4626,
      "step": 14065
    },
    {
      "epoch": 0.06616367348090726,
      "grad_norm": 4.076094150543213,
      "learning_rate": 0.00018720755891255765,
      "loss": 0.6829,
      "step": 14066
    },
    {
      "epoch": 0.06616837728251973,
      "grad_norm": 0.8942338824272156,
      "learning_rate": 0.00018720661593444416,
      "loss": 0.1089,
      "step": 14067
    },
    {
      "epoch": 0.0661730810841322,
      "grad_norm": 0.8843437433242798,
      "learning_rate": 0.00018720567295633068,
      "loss": 0.0772,
      "step": 14068
    },
    {
      "epoch": 0.06617778488574466,
      "grad_norm": 3.983107328414917,
      "learning_rate": 0.0001872047299782172,
      "loss": 0.6389,
      "step": 14069
    },
    {
      "epoch": 0.06618248868735713,
      "grad_norm": 2.296663522720337,
      "learning_rate": 0.00018720378700010375,
      "loss": 0.2301,
      "step": 14070
    },
    {
      "epoch": 0.06618719248896958,
      "grad_norm": 0.734079897403717,
      "learning_rate": 0.00018720284402199027,
      "loss": 0.0709,
      "step": 14071
    },
    {
      "epoch": 0.06619189629058204,
      "grad_norm": 0.8087893724441528,
      "learning_rate": 0.00018720190104387678,
      "loss": 0.0881,
      "step": 14072
    },
    {
      "epoch": 0.06619660009219451,
      "grad_norm": 0.5550053119659424,
      "learning_rate": 0.0001872009580657633,
      "loss": 0.051,
      "step": 14073
    },
    {
      "epoch": 0.06620130389380698,
      "grad_norm": 1.1807690858840942,
      "learning_rate": 0.00018720001508764982,
      "loss": 0.1859,
      "step": 14074
    },
    {
      "epoch": 0.06620600769541944,
      "grad_norm": 1.7764304876327515,
      "learning_rate": 0.00018719907210953637,
      "loss": 0.3314,
      "step": 14075
    },
    {
      "epoch": 0.06621071149703191,
      "grad_norm": 2.091111421585083,
      "learning_rate": 0.00018719812913142286,
      "loss": 0.4499,
      "step": 14076
    },
    {
      "epoch": 0.06621541529864436,
      "grad_norm": 0.28146421909332275,
      "learning_rate": 0.00018719718615330938,
      "loss": 0.0232,
      "step": 14077
    },
    {
      "epoch": 0.06622011910025682,
      "grad_norm": 1.2999958992004395,
      "learning_rate": 0.0001871962431751959,
      "loss": 0.1316,
      "step": 14078
    },
    {
      "epoch": 0.06622482290186929,
      "grad_norm": 3.328298568725586,
      "learning_rate": 0.00018719530019708242,
      "loss": 0.3082,
      "step": 14079
    },
    {
      "epoch": 0.06622952670348176,
      "grad_norm": 2.859386920928955,
      "learning_rate": 0.00018719435721896896,
      "loss": 0.5385,
      "step": 14080
    },
    {
      "epoch": 0.06623423050509422,
      "grad_norm": 3.773430109024048,
      "learning_rate": 0.00018719341424085548,
      "loss": 0.5142,
      "step": 14081
    },
    {
      "epoch": 0.06623893430670669,
      "grad_norm": 2.427963972091675,
      "learning_rate": 0.000187192471262742,
      "loss": 0.3984,
      "step": 14082
    },
    {
      "epoch": 0.06624363810831914,
      "grad_norm": 1.9209483861923218,
      "learning_rate": 0.00018719152828462852,
      "loss": 0.274,
      "step": 14083
    },
    {
      "epoch": 0.0662483419099316,
      "grad_norm": 1.9045480489730835,
      "learning_rate": 0.00018719058530651506,
      "loss": 0.2763,
      "step": 14084
    },
    {
      "epoch": 0.06625304571154407,
      "grad_norm": 2.9400370121002197,
      "learning_rate": 0.00018718964232840158,
      "loss": 0.3083,
      "step": 14085
    },
    {
      "epoch": 0.06625774951315654,
      "grad_norm": 1.3989042043685913,
      "learning_rate": 0.0001871886993502881,
      "loss": 0.1628,
      "step": 14086
    },
    {
      "epoch": 0.066262453314769,
      "grad_norm": 0.35264068841934204,
      "learning_rate": 0.00018718775637217462,
      "loss": 0.0372,
      "step": 14087
    },
    {
      "epoch": 0.06626715711638145,
      "grad_norm": 3.8837780952453613,
      "learning_rate": 0.0001871868133940611,
      "loss": 1.1737,
      "step": 14088
    },
    {
      "epoch": 0.06627186091799392,
      "grad_norm": 1.6570240259170532,
      "learning_rate": 0.00018718587041594766,
      "loss": 0.3639,
      "step": 14089
    },
    {
      "epoch": 0.06627656471960638,
      "grad_norm": 1.0422340631484985,
      "learning_rate": 0.00018718492743783417,
      "loss": 0.0948,
      "step": 14090
    },
    {
      "epoch": 0.06628126852121885,
      "grad_norm": 1.1469181776046753,
      "learning_rate": 0.0001871839844597207,
      "loss": 0.2869,
      "step": 14091
    },
    {
      "epoch": 0.06628597232283132,
      "grad_norm": 4.057197570800781,
      "learning_rate": 0.0001871830414816072,
      "loss": 0.3763,
      "step": 14092
    },
    {
      "epoch": 0.06629067612444378,
      "grad_norm": 2.1283318996429443,
      "learning_rate": 0.00018718209850349376,
      "loss": 0.2105,
      "step": 14093
    },
    {
      "epoch": 0.06629537992605623,
      "grad_norm": 0.12011506408452988,
      "learning_rate": 0.00018718115552538028,
      "loss": 0.0082,
      "step": 14094
    },
    {
      "epoch": 0.0663000837276687,
      "grad_norm": 0.509490430355072,
      "learning_rate": 0.0001871802125472668,
      "loss": 0.0462,
      "step": 14095
    },
    {
      "epoch": 0.06630478752928116,
      "grad_norm": 1.6602370738983154,
      "learning_rate": 0.0001871792695691533,
      "loss": 0.2215,
      "step": 14096
    },
    {
      "epoch": 0.06630949133089363,
      "grad_norm": 2.3717408180236816,
      "learning_rate": 0.00018717832659103983,
      "loss": 0.4828,
      "step": 14097
    },
    {
      "epoch": 0.0663141951325061,
      "grad_norm": 1.3561716079711914,
      "learning_rate": 0.00018717738361292635,
      "loss": 0.1227,
      "step": 14098
    },
    {
      "epoch": 0.06631889893411856,
      "grad_norm": 1.4152302742004395,
      "learning_rate": 0.00018717644063481287,
      "loss": 0.1288,
      "step": 14099
    },
    {
      "epoch": 0.06632360273573101,
      "grad_norm": 0.852042019367218,
      "learning_rate": 0.0001871754976566994,
      "loss": 0.1484,
      "step": 14100
    },
    {
      "epoch": 0.06632830653734348,
      "grad_norm": 1.4220380783081055,
      "learning_rate": 0.0001871745546785859,
      "loss": 0.1261,
      "step": 14101
    },
    {
      "epoch": 0.06633301033895594,
      "grad_norm": 2.514256238937378,
      "learning_rate": 0.00018717361170047245,
      "loss": 0.378,
      "step": 14102
    },
    {
      "epoch": 0.06633771414056841,
      "grad_norm": 13.9488525390625,
      "learning_rate": 0.00018717266872235897,
      "loss": 0.1793,
      "step": 14103
    },
    {
      "epoch": 0.06634241794218088,
      "grad_norm": 0.8177239298820496,
      "learning_rate": 0.0001871717257442455,
      "loss": 0.0756,
      "step": 14104
    },
    {
      "epoch": 0.06634712174379333,
      "grad_norm": 2.290137529373169,
      "learning_rate": 0.000187170782766132,
      "loss": 0.2553,
      "step": 14105
    },
    {
      "epoch": 0.0663518255454058,
      "grad_norm": 3.0816118717193604,
      "learning_rate": 0.00018716983978801853,
      "loss": 0.3833,
      "step": 14106
    },
    {
      "epoch": 0.06635652934701826,
      "grad_norm": 2.3516080379486084,
      "learning_rate": 0.00018716889680990505,
      "loss": 0.1642,
      "step": 14107
    },
    {
      "epoch": 0.06636123314863072,
      "grad_norm": 2.609842538833618,
      "learning_rate": 0.00018716795383179156,
      "loss": 0.4141,
      "step": 14108
    },
    {
      "epoch": 0.06636593695024319,
      "grad_norm": 3.579080104827881,
      "learning_rate": 0.00018716701085367808,
      "loss": 0.4682,
      "step": 14109
    },
    {
      "epoch": 0.06637064075185566,
      "grad_norm": 4.077352046966553,
      "learning_rate": 0.0001871660678755646,
      "loss": 0.2416,
      "step": 14110
    },
    {
      "epoch": 0.06637534455346811,
      "grad_norm": 2.5711469650268555,
      "learning_rate": 0.00018716512489745115,
      "loss": 0.3449,
      "step": 14111
    },
    {
      "epoch": 0.06638004835508057,
      "grad_norm": 2.370250701904297,
      "learning_rate": 0.00018716418191933767,
      "loss": 0.3278,
      "step": 14112
    },
    {
      "epoch": 0.06638475215669304,
      "grad_norm": 0.34386900067329407,
      "learning_rate": 0.00018716323894122418,
      "loss": 0.0183,
      "step": 14113
    },
    {
      "epoch": 0.0663894559583055,
      "grad_norm": 4.648247241973877,
      "learning_rate": 0.0001871622959631107,
      "loss": 0.5718,
      "step": 14114
    },
    {
      "epoch": 0.06639415975991797,
      "grad_norm": 5.450961112976074,
      "learning_rate": 0.00018716135298499722,
      "loss": 0.3386,
      "step": 14115
    },
    {
      "epoch": 0.06639886356153044,
      "grad_norm": 3.7364912033081055,
      "learning_rate": 0.00018716041000688377,
      "loss": 0.6464,
      "step": 14116
    },
    {
      "epoch": 0.06640356736314289,
      "grad_norm": 3.26287579536438,
      "learning_rate": 0.00018715946702877029,
      "loss": 0.5056,
      "step": 14117
    },
    {
      "epoch": 0.06640827116475535,
      "grad_norm": 1.5971676111221313,
      "learning_rate": 0.0001871585240506568,
      "loss": 0.1374,
      "step": 14118
    },
    {
      "epoch": 0.06641297496636782,
      "grad_norm": 0.93947434425354,
      "learning_rate": 0.0001871575810725433,
      "loss": 0.0923,
      "step": 14119
    },
    {
      "epoch": 0.06641767876798028,
      "grad_norm": 2.490744113922119,
      "learning_rate": 0.00018715663809442984,
      "loss": 0.6728,
      "step": 14120
    },
    {
      "epoch": 0.06642238256959275,
      "grad_norm": 2.0188794136047363,
      "learning_rate": 0.00018715569511631636,
      "loss": 0.2723,
      "step": 14121
    },
    {
      "epoch": 0.0664270863712052,
      "grad_norm": 2.1406190395355225,
      "learning_rate": 0.00018715475213820288,
      "loss": 0.2447,
      "step": 14122
    },
    {
      "epoch": 0.06643179017281767,
      "grad_norm": 3.176612138748169,
      "learning_rate": 0.0001871538091600894,
      "loss": 0.4143,
      "step": 14123
    },
    {
      "epoch": 0.06643649397443013,
      "grad_norm": 0.7567830681800842,
      "learning_rate": 0.00018715286618197592,
      "loss": 0.0532,
      "step": 14124
    },
    {
      "epoch": 0.0664411977760426,
      "grad_norm": 4.6478681564331055,
      "learning_rate": 0.00018715192320386246,
      "loss": 0.8033,
      "step": 14125
    },
    {
      "epoch": 0.06644590157765506,
      "grad_norm": 1.8461591005325317,
      "learning_rate": 0.00018715098022574898,
      "loss": 0.2293,
      "step": 14126
    },
    {
      "epoch": 0.06645060537926753,
      "grad_norm": 3.3680241107940674,
      "learning_rate": 0.0001871500372476355,
      "loss": 0.5859,
      "step": 14127
    },
    {
      "epoch": 0.06645530918087998,
      "grad_norm": 1.9479656219482422,
      "learning_rate": 0.00018714909426952202,
      "loss": 0.2758,
      "step": 14128
    },
    {
      "epoch": 0.06646001298249245,
      "grad_norm": 2.4199514389038086,
      "learning_rate": 0.00018714815129140854,
      "loss": 0.2703,
      "step": 14129
    },
    {
      "epoch": 0.06646471678410491,
      "grad_norm": 0.7594711184501648,
      "learning_rate": 0.00018714720831329506,
      "loss": 0.0775,
      "step": 14130
    },
    {
      "epoch": 0.06646942058571738,
      "grad_norm": 4.015528202056885,
      "learning_rate": 0.00018714626533518157,
      "loss": 0.5368,
      "step": 14131
    },
    {
      "epoch": 0.06647412438732984,
      "grad_norm": 3.2146453857421875,
      "learning_rate": 0.0001871453223570681,
      "loss": 0.4947,
      "step": 14132
    },
    {
      "epoch": 0.06647882818894231,
      "grad_norm": 0.6555899977684021,
      "learning_rate": 0.0001871443793789546,
      "loss": 0.0739,
      "step": 14133
    },
    {
      "epoch": 0.06648353199055476,
      "grad_norm": 2.139218807220459,
      "learning_rate": 0.00018714343640084116,
      "loss": 0.3212,
      "step": 14134
    },
    {
      "epoch": 0.06648823579216723,
      "grad_norm": 0.521892786026001,
      "learning_rate": 0.00018714249342272768,
      "loss": 0.04,
      "step": 14135
    },
    {
      "epoch": 0.06649293959377969,
      "grad_norm": 2.2291343212127686,
      "learning_rate": 0.0001871415504446142,
      "loss": 0.1893,
      "step": 14136
    },
    {
      "epoch": 0.06649764339539216,
      "grad_norm": 1.290645956993103,
      "learning_rate": 0.0001871406074665007,
      "loss": 0.1384,
      "step": 14137
    },
    {
      "epoch": 0.06650234719700462,
      "grad_norm": 2.599867582321167,
      "learning_rate": 0.00018713966448838723,
      "loss": 0.3515,
      "step": 14138
    },
    {
      "epoch": 0.06650705099861708,
      "grad_norm": 1.2458035945892334,
      "learning_rate": 0.00018713872151027375,
      "loss": 0.2753,
      "step": 14139
    },
    {
      "epoch": 0.06651175480022954,
      "grad_norm": 1.596216082572937,
      "learning_rate": 0.00018713777853216027,
      "loss": 0.2204,
      "step": 14140
    },
    {
      "epoch": 0.06651645860184201,
      "grad_norm": 1.4395915269851685,
      "learning_rate": 0.0001871368355540468,
      "loss": 0.1535,
      "step": 14141
    },
    {
      "epoch": 0.06652116240345447,
      "grad_norm": 1.734187364578247,
      "learning_rate": 0.0001871358925759333,
      "loss": 0.2886,
      "step": 14142
    },
    {
      "epoch": 0.06652586620506694,
      "grad_norm": 3.0500528812408447,
      "learning_rate": 0.00018713494959781985,
      "loss": 0.4318,
      "step": 14143
    },
    {
      "epoch": 0.0665305700066794,
      "grad_norm": 2.677022695541382,
      "learning_rate": 0.00018713400661970637,
      "loss": 0.5156,
      "step": 14144
    },
    {
      "epoch": 0.06653527380829186,
      "grad_norm": 1.306256651878357,
      "learning_rate": 0.0001871330636415929,
      "loss": 0.1543,
      "step": 14145
    },
    {
      "epoch": 0.06653997760990432,
      "grad_norm": 1.3446576595306396,
      "learning_rate": 0.0001871321206634794,
      "loss": 0.3256,
      "step": 14146
    },
    {
      "epoch": 0.06654468141151679,
      "grad_norm": 1.6803724765777588,
      "learning_rate": 0.00018713117768536593,
      "loss": 0.3547,
      "step": 14147
    },
    {
      "epoch": 0.06654938521312925,
      "grad_norm": 1.7687441110610962,
      "learning_rate": 0.00018713023470725247,
      "loss": 0.2699,
      "step": 14148
    },
    {
      "epoch": 0.06655408901474172,
      "grad_norm": 1.0058785676956177,
      "learning_rate": 0.000187129291729139,
      "loss": 0.1524,
      "step": 14149
    },
    {
      "epoch": 0.06655879281635418,
      "grad_norm": 1.9934135675430298,
      "learning_rate": 0.00018712834875102548,
      "loss": 0.4189,
      "step": 14150
    },
    {
      "epoch": 0.06656349661796664,
      "grad_norm": 2.2541604042053223,
      "learning_rate": 0.000187127405772912,
      "loss": 0.1355,
      "step": 14151
    },
    {
      "epoch": 0.0665682004195791,
      "grad_norm": 0.8436242341995239,
      "learning_rate": 0.00018712646279479855,
      "loss": 0.0871,
      "step": 14152
    },
    {
      "epoch": 0.06657290422119157,
      "grad_norm": 2.9383280277252197,
      "learning_rate": 0.00018712551981668507,
      "loss": 0.5159,
      "step": 14153
    },
    {
      "epoch": 0.06657760802280403,
      "grad_norm": 3.2271227836608887,
      "learning_rate": 0.00018712457683857158,
      "loss": 0.7251,
      "step": 14154
    },
    {
      "epoch": 0.0665823118244165,
      "grad_norm": 2.023881435394287,
      "learning_rate": 0.0001871236338604581,
      "loss": 0.4514,
      "step": 14155
    },
    {
      "epoch": 0.06658701562602895,
      "grad_norm": 4.4979047775268555,
      "learning_rate": 0.00018712269088234462,
      "loss": 0.3494,
      "step": 14156
    },
    {
      "epoch": 0.06659171942764142,
      "grad_norm": 1.9911160469055176,
      "learning_rate": 0.00018712174790423117,
      "loss": 0.231,
      "step": 14157
    },
    {
      "epoch": 0.06659642322925388,
      "grad_norm": 1.8950835466384888,
      "learning_rate": 0.00018712080492611769,
      "loss": 0.2676,
      "step": 14158
    },
    {
      "epoch": 0.06660112703086635,
      "grad_norm": 0.8688288331031799,
      "learning_rate": 0.0001871198619480042,
      "loss": 0.0857,
      "step": 14159
    },
    {
      "epoch": 0.06660583083247881,
      "grad_norm": 2.1441638469696045,
      "learning_rate": 0.00018711891896989072,
      "loss": 0.3096,
      "step": 14160
    },
    {
      "epoch": 0.06661053463409128,
      "grad_norm": 2.49515438079834,
      "learning_rate": 0.00018711797599177724,
      "loss": 0.3389,
      "step": 14161
    },
    {
      "epoch": 0.06661523843570373,
      "grad_norm": 3.224839210510254,
      "learning_rate": 0.00018711703301366376,
      "loss": 0.6653,
      "step": 14162
    },
    {
      "epoch": 0.0666199422373162,
      "grad_norm": 1.318995475769043,
      "learning_rate": 0.00018711609003555028,
      "loss": 0.1221,
      "step": 14163
    },
    {
      "epoch": 0.06662464603892866,
      "grad_norm": 4.115784168243408,
      "learning_rate": 0.0001871151470574368,
      "loss": 0.8442,
      "step": 14164
    },
    {
      "epoch": 0.06662934984054113,
      "grad_norm": 1.6333692073822021,
      "learning_rate": 0.00018711420407932332,
      "loss": 0.4135,
      "step": 14165
    },
    {
      "epoch": 0.06663405364215359,
      "grad_norm": 2.721998453140259,
      "learning_rate": 0.00018711326110120986,
      "loss": 0.4474,
      "step": 14166
    },
    {
      "epoch": 0.06663875744376606,
      "grad_norm": 2.2801997661590576,
      "learning_rate": 0.00018711231812309638,
      "loss": 0.2827,
      "step": 14167
    },
    {
      "epoch": 0.06664346124537851,
      "grad_norm": 5.321609973907471,
      "learning_rate": 0.0001871113751449829,
      "loss": 0.2183,
      "step": 14168
    },
    {
      "epoch": 0.06664816504699098,
      "grad_norm": 3.2830967903137207,
      "learning_rate": 0.00018711043216686942,
      "loss": 0.4066,
      "step": 14169
    },
    {
      "epoch": 0.06665286884860344,
      "grad_norm": 0.4945201575756073,
      "learning_rate": 0.00018710948918875594,
      "loss": 0.067,
      "step": 14170
    },
    {
      "epoch": 0.06665757265021591,
      "grad_norm": 3.836367607116699,
      "learning_rate": 0.00018710854621064246,
      "loss": 0.9709,
      "step": 14171
    },
    {
      "epoch": 0.06666227645182837,
      "grad_norm": 2.2249584197998047,
      "learning_rate": 0.00018710760323252897,
      "loss": 0.4777,
      "step": 14172
    },
    {
      "epoch": 0.06666698025344082,
      "grad_norm": 2.0809385776519775,
      "learning_rate": 0.0001871066602544155,
      "loss": 0.2864,
      "step": 14173
    },
    {
      "epoch": 0.06667168405505329,
      "grad_norm": 1.6460607051849365,
      "learning_rate": 0.000187105717276302,
      "loss": 0.2572,
      "step": 14174
    },
    {
      "epoch": 0.06667638785666576,
      "grad_norm": 1.9085403680801392,
      "learning_rate": 0.00018710477429818856,
      "loss": 0.3487,
      "step": 14175
    },
    {
      "epoch": 0.06668109165827822,
      "grad_norm": 1.110916256904602,
      "learning_rate": 0.00018710383132007508,
      "loss": 0.2229,
      "step": 14176
    },
    {
      "epoch": 0.06668579545989069,
      "grad_norm": 2.041063070297241,
      "learning_rate": 0.0001871028883419616,
      "loss": 0.2884,
      "step": 14177
    },
    {
      "epoch": 0.06669049926150315,
      "grad_norm": 1.6844911575317383,
      "learning_rate": 0.0001871019453638481,
      "loss": 0.4084,
      "step": 14178
    },
    {
      "epoch": 0.0666952030631156,
      "grad_norm": 0.9651014804840088,
      "learning_rate": 0.00018710100238573463,
      "loss": 0.1665,
      "step": 14179
    },
    {
      "epoch": 0.06669990686472807,
      "grad_norm": 0.6342907547950745,
      "learning_rate": 0.00018710005940762118,
      "loss": 0.1267,
      "step": 14180
    },
    {
      "epoch": 0.06670461066634054,
      "grad_norm": 0.6677762866020203,
      "learning_rate": 0.00018709911642950767,
      "loss": 0.1066,
      "step": 14181
    },
    {
      "epoch": 0.066709314467953,
      "grad_norm": 2.8767478466033936,
      "learning_rate": 0.0001870981734513942,
      "loss": 0.9062,
      "step": 14182
    },
    {
      "epoch": 0.06671401826956547,
      "grad_norm": 0.7121046185493469,
      "learning_rate": 0.0001870972304732807,
      "loss": 0.1221,
      "step": 14183
    },
    {
      "epoch": 0.06671872207117793,
      "grad_norm": 3.741116523742676,
      "learning_rate": 0.00018709628749516725,
      "loss": 0.3762,
      "step": 14184
    },
    {
      "epoch": 0.06672342587279038,
      "grad_norm": 2.5946755409240723,
      "learning_rate": 0.00018709534451705377,
      "loss": 0.562,
      "step": 14185
    },
    {
      "epoch": 0.06672812967440285,
      "grad_norm": 1.1912603378295898,
      "learning_rate": 0.0001870944015389403,
      "loss": 0.1129,
      "step": 14186
    },
    {
      "epoch": 0.06673283347601532,
      "grad_norm": 1.4310609102249146,
      "learning_rate": 0.0001870934585608268,
      "loss": 0.1824,
      "step": 14187
    },
    {
      "epoch": 0.06673753727762778,
      "grad_norm": 3.620140314102173,
      "learning_rate": 0.00018709251558271333,
      "loss": 0.6942,
      "step": 14188
    },
    {
      "epoch": 0.06674224107924025,
      "grad_norm": 0.3695999085903168,
      "learning_rate": 0.00018709157260459987,
      "loss": 0.0342,
      "step": 14189
    },
    {
      "epoch": 0.0667469448808527,
      "grad_norm": 2.6685433387756348,
      "learning_rate": 0.0001870906296264864,
      "loss": 0.4846,
      "step": 14190
    },
    {
      "epoch": 0.06675164868246516,
      "grad_norm": 2.4850621223449707,
      "learning_rate": 0.0001870896866483729,
      "loss": 0.5425,
      "step": 14191
    },
    {
      "epoch": 0.06675635248407763,
      "grad_norm": 3.2443976402282715,
      "learning_rate": 0.0001870887436702594,
      "loss": 0.3161,
      "step": 14192
    },
    {
      "epoch": 0.0667610562856901,
      "grad_norm": 1.5507253408432007,
      "learning_rate": 0.00018708780069214595,
      "loss": 0.2148,
      "step": 14193
    },
    {
      "epoch": 0.06676576008730256,
      "grad_norm": 1.635922908782959,
      "learning_rate": 0.00018708685771403247,
      "loss": 0.2326,
      "step": 14194
    },
    {
      "epoch": 0.06677046388891503,
      "grad_norm": 2.8306849002838135,
      "learning_rate": 0.00018708591473591898,
      "loss": 0.3891,
      "step": 14195
    },
    {
      "epoch": 0.06677516769052748,
      "grad_norm": 1.0923247337341309,
      "learning_rate": 0.0001870849717578055,
      "loss": 0.1041,
      "step": 14196
    },
    {
      "epoch": 0.06677987149213994,
      "grad_norm": 0.5726380944252014,
      "learning_rate": 0.00018708402877969202,
      "loss": 0.055,
      "step": 14197
    },
    {
      "epoch": 0.06678457529375241,
      "grad_norm": 2.6428914070129395,
      "learning_rate": 0.00018708308580157857,
      "loss": 0.7408,
      "step": 14198
    },
    {
      "epoch": 0.06678927909536488,
      "grad_norm": 1.0864388942718506,
      "learning_rate": 0.00018708214282346509,
      "loss": 0.2464,
      "step": 14199
    },
    {
      "epoch": 0.06679398289697734,
      "grad_norm": 0.41328734159469604,
      "learning_rate": 0.0001870811998453516,
      "loss": 0.0411,
      "step": 14200
    },
    {
      "epoch": 0.06679868669858981,
      "grad_norm": 1.2218189239501953,
      "learning_rate": 0.00018708025686723812,
      "loss": 0.1063,
      "step": 14201
    },
    {
      "epoch": 0.06680339050020226,
      "grad_norm": 3.6997182369232178,
      "learning_rate": 0.00018707931388912464,
      "loss": 0.2614,
      "step": 14202
    },
    {
      "epoch": 0.06680809430181472,
      "grad_norm": 1.509541392326355,
      "learning_rate": 0.00018707837091101116,
      "loss": 0.1233,
      "step": 14203
    },
    {
      "epoch": 0.06681279810342719,
      "grad_norm": 3.9309241771698,
      "learning_rate": 0.00018707742793289768,
      "loss": 0.6476,
      "step": 14204
    },
    {
      "epoch": 0.06681750190503966,
      "grad_norm": 1.6019811630249023,
      "learning_rate": 0.0001870764849547842,
      "loss": 0.1094,
      "step": 14205
    },
    {
      "epoch": 0.06682220570665212,
      "grad_norm": 2.0098876953125,
      "learning_rate": 0.00018707554197667072,
      "loss": 0.3895,
      "step": 14206
    },
    {
      "epoch": 0.06682690950826457,
      "grad_norm": 0.37418296933174133,
      "learning_rate": 0.00018707459899855726,
      "loss": 0.03,
      "step": 14207
    },
    {
      "epoch": 0.06683161330987704,
      "grad_norm": 3.830449104309082,
      "learning_rate": 0.00018707365602044378,
      "loss": 0.1551,
      "step": 14208
    },
    {
      "epoch": 0.0668363171114895,
      "grad_norm": 3.017533779144287,
      "learning_rate": 0.0001870727130423303,
      "loss": 0.6937,
      "step": 14209
    },
    {
      "epoch": 0.06684102091310197,
      "grad_norm": 3.052611827850342,
      "learning_rate": 0.00018707177006421682,
      "loss": 0.5786,
      "step": 14210
    },
    {
      "epoch": 0.06684572471471444,
      "grad_norm": 0.7774400115013123,
      "learning_rate": 0.00018707082708610336,
      "loss": 0.0649,
      "step": 14211
    },
    {
      "epoch": 0.0668504285163269,
      "grad_norm": 0.9645211696624756,
      "learning_rate": 0.00018706988410798986,
      "loss": 0.1127,
      "step": 14212
    },
    {
      "epoch": 0.06685513231793935,
      "grad_norm": 2.8041272163391113,
      "learning_rate": 0.00018706894112987637,
      "loss": 0.616,
      "step": 14213
    },
    {
      "epoch": 0.06685983611955182,
      "grad_norm": 1.5521881580352783,
      "learning_rate": 0.0001870679981517629,
      "loss": 0.1425,
      "step": 14214
    },
    {
      "epoch": 0.06686453992116428,
      "grad_norm": 2.388983726501465,
      "learning_rate": 0.0001870670551736494,
      "loss": 0.246,
      "step": 14215
    },
    {
      "epoch": 0.06686924372277675,
      "grad_norm": 3.151275873184204,
      "learning_rate": 0.00018706611219553596,
      "loss": 0.4717,
      "step": 14216
    },
    {
      "epoch": 0.06687394752438922,
      "grad_norm": 2.4368703365325928,
      "learning_rate": 0.00018706516921742248,
      "loss": 0.4251,
      "step": 14217
    },
    {
      "epoch": 0.06687865132600168,
      "grad_norm": 0.7144129276275635,
      "learning_rate": 0.000187064226239309,
      "loss": 0.068,
      "step": 14218
    },
    {
      "epoch": 0.06688335512761413,
      "grad_norm": 2.809389352798462,
      "learning_rate": 0.0001870632832611955,
      "loss": 0.588,
      "step": 14219
    },
    {
      "epoch": 0.0668880589292266,
      "grad_norm": 2.5024783611297607,
      "learning_rate": 0.00018706234028308206,
      "loss": 0.4552,
      "step": 14220
    },
    {
      "epoch": 0.06689276273083906,
      "grad_norm": 4.32569694519043,
      "learning_rate": 0.00018706139730496858,
      "loss": 0.8034,
      "step": 14221
    },
    {
      "epoch": 0.06689746653245153,
      "grad_norm": 2.8960204124450684,
      "learning_rate": 0.0001870604543268551,
      "loss": 0.5571,
      "step": 14222
    },
    {
      "epoch": 0.066902170334064,
      "grad_norm": 1.7977750301361084,
      "learning_rate": 0.0001870595113487416,
      "loss": 0.175,
      "step": 14223
    },
    {
      "epoch": 0.06690687413567645,
      "grad_norm": 2.617384910583496,
      "learning_rate": 0.0001870585683706281,
      "loss": 0.3933,
      "step": 14224
    },
    {
      "epoch": 0.06691157793728891,
      "grad_norm": 1.090829610824585,
      "learning_rate": 0.00018705762539251465,
      "loss": 0.1558,
      "step": 14225
    },
    {
      "epoch": 0.06691628173890138,
      "grad_norm": 1.5009715557098389,
      "learning_rate": 0.00018705668241440117,
      "loss": 0.283,
      "step": 14226
    },
    {
      "epoch": 0.06692098554051384,
      "grad_norm": 1.5975333452224731,
      "learning_rate": 0.0001870557394362877,
      "loss": 0.3585,
      "step": 14227
    },
    {
      "epoch": 0.06692568934212631,
      "grad_norm": 2.8503031730651855,
      "learning_rate": 0.0001870547964581742,
      "loss": 0.2327,
      "step": 14228
    },
    {
      "epoch": 0.06693039314373878,
      "grad_norm": 1.5186797380447388,
      "learning_rate": 0.00018705385348006075,
      "loss": 0.392,
      "step": 14229
    },
    {
      "epoch": 0.06693509694535123,
      "grad_norm": 0.860144317150116,
      "learning_rate": 0.00018705291050194727,
      "loss": 0.1395,
      "step": 14230
    },
    {
      "epoch": 0.06693980074696369,
      "grad_norm": 1.3406187295913696,
      "learning_rate": 0.0001870519675238338,
      "loss": 0.1514,
      "step": 14231
    },
    {
      "epoch": 0.06694450454857616,
      "grad_norm": 3.278350353240967,
      "learning_rate": 0.0001870510245457203,
      "loss": 0.631,
      "step": 14232
    },
    {
      "epoch": 0.06694920835018862,
      "grad_norm": 1.4502596855163574,
      "learning_rate": 0.00018705008156760683,
      "loss": 0.3457,
      "step": 14233
    },
    {
      "epoch": 0.06695391215180109,
      "grad_norm": 1.0210870504379272,
      "learning_rate": 0.00018704913858949335,
      "loss": 0.1709,
      "step": 14234
    },
    {
      "epoch": 0.06695861595341356,
      "grad_norm": 0.6227014660835266,
      "learning_rate": 0.00018704819561137987,
      "loss": 0.0867,
      "step": 14235
    },
    {
      "epoch": 0.06696331975502601,
      "grad_norm": 0.4841931462287903,
      "learning_rate": 0.00018704725263326638,
      "loss": 0.0822,
      "step": 14236
    },
    {
      "epoch": 0.06696802355663847,
      "grad_norm": 1.0491251945495605,
      "learning_rate": 0.0001870463096551529,
      "loss": 0.1517,
      "step": 14237
    },
    {
      "epoch": 0.06697272735825094,
      "grad_norm": 2.8463449478149414,
      "learning_rate": 0.00018704536667703942,
      "loss": 0.3649,
      "step": 14238
    },
    {
      "epoch": 0.0669774311598634,
      "grad_norm": 2.166776180267334,
      "learning_rate": 0.00018704442369892597,
      "loss": 0.2584,
      "step": 14239
    },
    {
      "epoch": 0.06698213496147587,
      "grad_norm": 1.5020586252212524,
      "learning_rate": 0.00018704348072081249,
      "loss": 0.4169,
      "step": 14240
    },
    {
      "epoch": 0.06698683876308832,
      "grad_norm": 1.327943205833435,
      "learning_rate": 0.000187042537742699,
      "loss": 0.1505,
      "step": 14241
    },
    {
      "epoch": 0.06699154256470079,
      "grad_norm": 3.004506826400757,
      "learning_rate": 0.00018704159476458552,
      "loss": 0.4486,
      "step": 14242
    },
    {
      "epoch": 0.06699624636631325,
      "grad_norm": 1.3156800270080566,
      "learning_rate": 0.00018704065178647204,
      "loss": 0.3067,
      "step": 14243
    },
    {
      "epoch": 0.06700095016792572,
      "grad_norm": 1.484239101409912,
      "learning_rate": 0.00018703970880835856,
      "loss": 0.4787,
      "step": 14244
    },
    {
      "epoch": 0.06700565396953818,
      "grad_norm": 0.7567095756530762,
      "learning_rate": 0.00018703876583024508,
      "loss": 0.0884,
      "step": 14245
    },
    {
      "epoch": 0.06701035777115065,
      "grad_norm": 2.2259914875030518,
      "learning_rate": 0.0001870378228521316,
      "loss": 0.3329,
      "step": 14246
    },
    {
      "epoch": 0.0670150615727631,
      "grad_norm": 0.7919079661369324,
      "learning_rate": 0.00018703687987401812,
      "loss": 0.1875,
      "step": 14247
    },
    {
      "epoch": 0.06701976537437557,
      "grad_norm": 2.2115530967712402,
      "learning_rate": 0.00018703593689590466,
      "loss": 0.2237,
      "step": 14248
    },
    {
      "epoch": 0.06702446917598803,
      "grad_norm": 1.4434077739715576,
      "learning_rate": 0.00018703499391779118,
      "loss": 0.403,
      "step": 14249
    },
    {
      "epoch": 0.0670291729776005,
      "grad_norm": 3.2123043537139893,
      "learning_rate": 0.0001870340509396777,
      "loss": 0.889,
      "step": 14250
    },
    {
      "epoch": 0.06703387677921296,
      "grad_norm": 2.066330671310425,
      "learning_rate": 0.00018703310796156422,
      "loss": 0.1333,
      "step": 14251
    },
    {
      "epoch": 0.06703858058082543,
      "grad_norm": 1.4993000030517578,
      "learning_rate": 0.00018703216498345076,
      "loss": 0.1295,
      "step": 14252
    },
    {
      "epoch": 0.06704328438243788,
      "grad_norm": 1.6239345073699951,
      "learning_rate": 0.00018703122200533728,
      "loss": 0.2037,
      "step": 14253
    },
    {
      "epoch": 0.06704798818405035,
      "grad_norm": 1.7118202447891235,
      "learning_rate": 0.00018703027902722377,
      "loss": 0.1732,
      "step": 14254
    },
    {
      "epoch": 0.06705269198566281,
      "grad_norm": 3.9764177799224854,
      "learning_rate": 0.0001870293360491103,
      "loss": 0.9703,
      "step": 14255
    },
    {
      "epoch": 0.06705739578727528,
      "grad_norm": 2.2302193641662598,
      "learning_rate": 0.0001870283930709968,
      "loss": 0.2696,
      "step": 14256
    },
    {
      "epoch": 0.06706209958888774,
      "grad_norm": 0.657902181148529,
      "learning_rate": 0.00018702745009288336,
      "loss": 0.0477,
      "step": 14257
    },
    {
      "epoch": 0.0670668033905002,
      "grad_norm": 0.8494134545326233,
      "learning_rate": 0.00018702650711476988,
      "loss": 0.0605,
      "step": 14258
    },
    {
      "epoch": 0.06707150719211266,
      "grad_norm": 0.943739116191864,
      "learning_rate": 0.0001870255641366564,
      "loss": 0.1462,
      "step": 14259
    },
    {
      "epoch": 0.06707621099372513,
      "grad_norm": 1.2361748218536377,
      "learning_rate": 0.0001870246211585429,
      "loss": 0.105,
      "step": 14260
    },
    {
      "epoch": 0.06708091479533759,
      "grad_norm": 3.598649740219116,
      "learning_rate": 0.00018702367818042946,
      "loss": 0.4379,
      "step": 14261
    },
    {
      "epoch": 0.06708561859695006,
      "grad_norm": 1.7468328475952148,
      "learning_rate": 0.00018702273520231598,
      "loss": 0.1461,
      "step": 14262
    },
    {
      "epoch": 0.06709032239856252,
      "grad_norm": 4.669889450073242,
      "learning_rate": 0.0001870217922242025,
      "loss": 0.8501,
      "step": 14263
    },
    {
      "epoch": 0.06709502620017498,
      "grad_norm": 4.548510551452637,
      "learning_rate": 0.00018702084924608901,
      "loss": 0.3385,
      "step": 14264
    },
    {
      "epoch": 0.06709973000178744,
      "grad_norm": 1.7668845653533936,
      "learning_rate": 0.0001870199062679755,
      "loss": 0.3712,
      "step": 14265
    },
    {
      "epoch": 0.06710443380339991,
      "grad_norm": 2.4455795288085938,
      "learning_rate": 0.00018701896328986205,
      "loss": 0.3645,
      "step": 14266
    },
    {
      "epoch": 0.06710913760501237,
      "grad_norm": 0.5496904253959656,
      "learning_rate": 0.00018701802031174857,
      "loss": 0.0539,
      "step": 14267
    },
    {
      "epoch": 0.06711384140662484,
      "grad_norm": 2.6905717849731445,
      "learning_rate": 0.0001870170773336351,
      "loss": 0.8261,
      "step": 14268
    },
    {
      "epoch": 0.0671185452082373,
      "grad_norm": 2.2362382411956787,
      "learning_rate": 0.0001870161343555216,
      "loss": 0.4745,
      "step": 14269
    },
    {
      "epoch": 0.06712324900984976,
      "grad_norm": 1.1632449626922607,
      "learning_rate": 0.00018701519137740815,
      "loss": 0.2152,
      "step": 14270
    },
    {
      "epoch": 0.06712795281146222,
      "grad_norm": 1.1841320991516113,
      "learning_rate": 0.00018701424839929467,
      "loss": 0.2412,
      "step": 14271
    },
    {
      "epoch": 0.06713265661307469,
      "grad_norm": 0.9542952179908752,
      "learning_rate": 0.0001870133054211812,
      "loss": 0.2316,
      "step": 14272
    },
    {
      "epoch": 0.06713736041468715,
      "grad_norm": 1.1746066808700562,
      "learning_rate": 0.0001870123624430677,
      "loss": 0.2859,
      "step": 14273
    },
    {
      "epoch": 0.06714206421629962,
      "grad_norm": 1.1819647550582886,
      "learning_rate": 0.00018701141946495423,
      "loss": 0.2063,
      "step": 14274
    },
    {
      "epoch": 0.06714676801791207,
      "grad_norm": 0.43681004643440247,
      "learning_rate": 0.00018701047648684075,
      "loss": 0.0392,
      "step": 14275
    },
    {
      "epoch": 0.06715147181952454,
      "grad_norm": 0.6209303140640259,
      "learning_rate": 0.00018700953350872727,
      "loss": 0.1065,
      "step": 14276
    },
    {
      "epoch": 0.067156175621137,
      "grad_norm": 2.1346213817596436,
      "learning_rate": 0.00018700859053061378,
      "loss": 0.3613,
      "step": 14277
    },
    {
      "epoch": 0.06716087942274947,
      "grad_norm": 1.0578035116195679,
      "learning_rate": 0.0001870076475525003,
      "loss": 0.1325,
      "step": 14278
    },
    {
      "epoch": 0.06716558322436193,
      "grad_norm": 0.3332967460155487,
      "learning_rate": 0.00018700670457438685,
      "loss": 0.0308,
      "step": 14279
    },
    {
      "epoch": 0.0671702870259744,
      "grad_norm": 3.189075469970703,
      "learning_rate": 0.00018700576159627337,
      "loss": 0.408,
      "step": 14280
    },
    {
      "epoch": 0.06717499082758685,
      "grad_norm": 2.3730275630950928,
      "learning_rate": 0.00018700481861815989,
      "loss": 0.4655,
      "step": 14281
    },
    {
      "epoch": 0.06717969462919932,
      "grad_norm": 0.9197088479995728,
      "learning_rate": 0.0001870038756400464,
      "loss": 0.0794,
      "step": 14282
    },
    {
      "epoch": 0.06718439843081178,
      "grad_norm": 0.46581554412841797,
      "learning_rate": 0.00018700293266193292,
      "loss": 0.039,
      "step": 14283
    },
    {
      "epoch": 0.06718910223242425,
      "grad_norm": 0.7015842199325562,
      "learning_rate": 0.00018700198968381947,
      "loss": 0.0861,
      "step": 14284
    },
    {
      "epoch": 0.06719380603403671,
      "grad_norm": 2.6472997665405273,
      "learning_rate": 0.00018700104670570596,
      "loss": 0.3212,
      "step": 14285
    },
    {
      "epoch": 0.06719850983564918,
      "grad_norm": 1.521083116531372,
      "learning_rate": 0.00018700010372759248,
      "loss": 0.141,
      "step": 14286
    },
    {
      "epoch": 0.06720321363726163,
      "grad_norm": 0.9069626331329346,
      "learning_rate": 0.000186999160749479,
      "loss": 0.0981,
      "step": 14287
    },
    {
      "epoch": 0.0672079174388741,
      "grad_norm": 1.3091574907302856,
      "learning_rate": 0.00018699821777136552,
      "loss": 0.1555,
      "step": 14288
    },
    {
      "epoch": 0.06721262124048656,
      "grad_norm": 2.885326623916626,
      "learning_rate": 0.00018699727479325206,
      "loss": 0.3615,
      "step": 14289
    },
    {
      "epoch": 0.06721732504209903,
      "grad_norm": 5.212926387786865,
      "learning_rate": 0.00018699633181513858,
      "loss": 0.7604,
      "step": 14290
    },
    {
      "epoch": 0.06722202884371149,
      "grad_norm": 2.1804628372192383,
      "learning_rate": 0.0001869953888370251,
      "loss": 0.2052,
      "step": 14291
    },
    {
      "epoch": 0.06722673264532394,
      "grad_norm": 3.6531436443328857,
      "learning_rate": 0.00018699444585891162,
      "loss": 0.919,
      "step": 14292
    },
    {
      "epoch": 0.06723143644693641,
      "grad_norm": 0.13453586399555206,
      "learning_rate": 0.00018699350288079816,
      "loss": 0.0094,
      "step": 14293
    },
    {
      "epoch": 0.06723614024854888,
      "grad_norm": 1.9561127424240112,
      "learning_rate": 0.00018699255990268468,
      "loss": 0.1136,
      "step": 14294
    },
    {
      "epoch": 0.06724084405016134,
      "grad_norm": 1.4103084802627563,
      "learning_rate": 0.0001869916169245712,
      "loss": 0.2123,
      "step": 14295
    },
    {
      "epoch": 0.06724554785177381,
      "grad_norm": 2.0761759281158447,
      "learning_rate": 0.0001869906739464577,
      "loss": 0.2353,
      "step": 14296
    },
    {
      "epoch": 0.06725025165338627,
      "grad_norm": 3.2931020259857178,
      "learning_rate": 0.0001869897309683442,
      "loss": 0.8047,
      "step": 14297
    },
    {
      "epoch": 0.06725495545499872,
      "grad_norm": 2.307583808898926,
      "learning_rate": 0.00018698878799023076,
      "loss": 0.3305,
      "step": 14298
    },
    {
      "epoch": 0.06725965925661119,
      "grad_norm": 1.7544376850128174,
      "learning_rate": 0.00018698784501211728,
      "loss": 0.3334,
      "step": 14299
    },
    {
      "epoch": 0.06726436305822366,
      "grad_norm": 2.852982759475708,
      "learning_rate": 0.0001869869020340038,
      "loss": 0.3219,
      "step": 14300
    },
    {
      "epoch": 0.06726906685983612,
      "grad_norm": 2.953859806060791,
      "learning_rate": 0.0001869859590558903,
      "loss": 0.3363,
      "step": 14301
    },
    {
      "epoch": 0.06727377066144859,
      "grad_norm": 8.587910652160645,
      "learning_rate": 0.00018698501607777686,
      "loss": 0.6362,
      "step": 14302
    },
    {
      "epoch": 0.06727847446306105,
      "grad_norm": 2.3686211109161377,
      "learning_rate": 0.00018698407309966338,
      "loss": 0.2081,
      "step": 14303
    },
    {
      "epoch": 0.0672831782646735,
      "grad_norm": 2.347731113433838,
      "learning_rate": 0.0001869831301215499,
      "loss": 0.281,
      "step": 14304
    },
    {
      "epoch": 0.06728788206628597,
      "grad_norm": 2.819256067276001,
      "learning_rate": 0.00018698218714343641,
      "loss": 0.4799,
      "step": 14305
    },
    {
      "epoch": 0.06729258586789844,
      "grad_norm": 3.586928129196167,
      "learning_rate": 0.00018698124416532293,
      "loss": 0.4062,
      "step": 14306
    },
    {
      "epoch": 0.0672972896695109,
      "grad_norm": 3.1327407360076904,
      "learning_rate": 0.00018698030118720945,
      "loss": 0.7196,
      "step": 14307
    },
    {
      "epoch": 0.06730199347112337,
      "grad_norm": 1.8502082824707031,
      "learning_rate": 0.00018697935820909597,
      "loss": 0.2594,
      "step": 14308
    },
    {
      "epoch": 0.06730669727273582,
      "grad_norm": 2.322870969772339,
      "learning_rate": 0.0001869784152309825,
      "loss": 0.485,
      "step": 14309
    },
    {
      "epoch": 0.06731140107434828,
      "grad_norm": 2.2500507831573486,
      "learning_rate": 0.000186977472252869,
      "loss": 0.4147,
      "step": 14310
    },
    {
      "epoch": 0.06731610487596075,
      "grad_norm": 2.8858394622802734,
      "learning_rate": 0.00018697652927475555,
      "loss": 0.5502,
      "step": 14311
    },
    {
      "epoch": 0.06732080867757322,
      "grad_norm": 0.7701494693756104,
      "learning_rate": 0.00018697558629664207,
      "loss": 0.1046,
      "step": 14312
    },
    {
      "epoch": 0.06732551247918568,
      "grad_norm": 1.3620846271514893,
      "learning_rate": 0.0001869746433185286,
      "loss": 0.1701,
      "step": 14313
    },
    {
      "epoch": 0.06733021628079815,
      "grad_norm": 1.1690088510513306,
      "learning_rate": 0.0001869737003404151,
      "loss": 0.1305,
      "step": 14314
    },
    {
      "epoch": 0.0673349200824106,
      "grad_norm": 1.4056177139282227,
      "learning_rate": 0.00018697275736230163,
      "loss": 0.1948,
      "step": 14315
    },
    {
      "epoch": 0.06733962388402306,
      "grad_norm": 1.3063154220581055,
      "learning_rate": 0.00018697181438418815,
      "loss": 0.1451,
      "step": 14316
    },
    {
      "epoch": 0.06734432768563553,
      "grad_norm": 3.0120630264282227,
      "learning_rate": 0.00018697087140607467,
      "loss": 0.4373,
      "step": 14317
    },
    {
      "epoch": 0.067349031487248,
      "grad_norm": 1.4843251705169678,
      "learning_rate": 0.00018696992842796118,
      "loss": 0.16,
      "step": 14318
    },
    {
      "epoch": 0.06735373528886046,
      "grad_norm": 2.693498373031616,
      "learning_rate": 0.0001869689854498477,
      "loss": 0.4298,
      "step": 14319
    },
    {
      "epoch": 0.06735843909047293,
      "grad_norm": 2.8653640747070312,
      "learning_rate": 0.00018696804247173425,
      "loss": 0.263,
      "step": 14320
    },
    {
      "epoch": 0.06736314289208538,
      "grad_norm": 2.5880956649780273,
      "learning_rate": 0.00018696709949362077,
      "loss": 0.2026,
      "step": 14321
    },
    {
      "epoch": 0.06736784669369784,
      "grad_norm": 1.813491940498352,
      "learning_rate": 0.00018696615651550729,
      "loss": 0.2509,
      "step": 14322
    },
    {
      "epoch": 0.06737255049531031,
      "grad_norm": 2.3721680641174316,
      "learning_rate": 0.0001869652135373938,
      "loss": 0.3501,
      "step": 14323
    },
    {
      "epoch": 0.06737725429692278,
      "grad_norm": 1.8791972398757935,
      "learning_rate": 0.00018696427055928032,
      "loss": 0.1823,
      "step": 14324
    },
    {
      "epoch": 0.06738195809853524,
      "grad_norm": 1.670088291168213,
      "learning_rate": 0.00018696332758116687,
      "loss": 0.3005,
      "step": 14325
    },
    {
      "epoch": 0.06738666190014769,
      "grad_norm": 1.6393094062805176,
      "learning_rate": 0.0001869623846030534,
      "loss": 0.1332,
      "step": 14326
    },
    {
      "epoch": 0.06739136570176016,
      "grad_norm": 0.7738620638847351,
      "learning_rate": 0.00018696144162493988,
      "loss": 0.0924,
      "step": 14327
    },
    {
      "epoch": 0.06739606950337262,
      "grad_norm": 6.3394904136657715,
      "learning_rate": 0.0001869604986468264,
      "loss": 0.3705,
      "step": 14328
    },
    {
      "epoch": 0.06740077330498509,
      "grad_norm": 3.6311473846435547,
      "learning_rate": 0.00018695955566871294,
      "loss": 0.5636,
      "step": 14329
    },
    {
      "epoch": 0.06740547710659756,
      "grad_norm": 1.1731104850769043,
      "learning_rate": 0.00018695861269059946,
      "loss": 0.0829,
      "step": 14330
    },
    {
      "epoch": 0.06741018090821002,
      "grad_norm": 2.3950653076171875,
      "learning_rate": 0.00018695766971248598,
      "loss": 0.3545,
      "step": 14331
    },
    {
      "epoch": 0.06741488470982247,
      "grad_norm": 0.727440357208252,
      "learning_rate": 0.0001869567267343725,
      "loss": 0.057,
      "step": 14332
    },
    {
      "epoch": 0.06741958851143494,
      "grad_norm": 0.9347617626190186,
      "learning_rate": 0.00018695578375625902,
      "loss": 0.0898,
      "step": 14333
    },
    {
      "epoch": 0.0674242923130474,
      "grad_norm": 4.088936805725098,
      "learning_rate": 0.00018695484077814556,
      "loss": 0.9301,
      "step": 14334
    },
    {
      "epoch": 0.06742899611465987,
      "grad_norm": 0.7403949499130249,
      "learning_rate": 0.00018695389780003208,
      "loss": 0.0588,
      "step": 14335
    },
    {
      "epoch": 0.06743369991627234,
      "grad_norm": 3.172119140625,
      "learning_rate": 0.0001869529548219186,
      "loss": 0.5606,
      "step": 14336
    },
    {
      "epoch": 0.0674384037178848,
      "grad_norm": 2.2193732261657715,
      "learning_rate": 0.00018695201184380512,
      "loss": 0.5686,
      "step": 14337
    },
    {
      "epoch": 0.06744310751949725,
      "grad_norm": 3.5698070526123047,
      "learning_rate": 0.00018695106886569164,
      "loss": 0.9857,
      "step": 14338
    },
    {
      "epoch": 0.06744781132110972,
      "grad_norm": 6.643877983093262,
      "learning_rate": 0.00018695012588757816,
      "loss": 0.6085,
      "step": 14339
    },
    {
      "epoch": 0.06745251512272218,
      "grad_norm": 1.9212661981582642,
      "learning_rate": 0.00018694918290946468,
      "loss": 0.2133,
      "step": 14340
    },
    {
      "epoch": 0.06745721892433465,
      "grad_norm": 1.9855986833572388,
      "learning_rate": 0.0001869482399313512,
      "loss": 0.4778,
      "step": 14341
    },
    {
      "epoch": 0.06746192272594712,
      "grad_norm": 0.7870177626609802,
      "learning_rate": 0.0001869472969532377,
      "loss": 0.1216,
      "step": 14342
    },
    {
      "epoch": 0.06746662652755957,
      "grad_norm": 2.351696252822876,
      "learning_rate": 0.00018694635397512426,
      "loss": 0.3854,
      "step": 14343
    },
    {
      "epoch": 0.06747133032917203,
      "grad_norm": 0.8706855177879333,
      "learning_rate": 0.00018694541099701078,
      "loss": 0.1302,
      "step": 14344
    },
    {
      "epoch": 0.0674760341307845,
      "grad_norm": 2.887314558029175,
      "learning_rate": 0.0001869444680188973,
      "loss": 0.5896,
      "step": 14345
    },
    {
      "epoch": 0.06748073793239696,
      "grad_norm": 1.7572071552276611,
      "learning_rate": 0.00018694352504078381,
      "loss": 0.3493,
      "step": 14346
    },
    {
      "epoch": 0.06748544173400943,
      "grad_norm": 1.404296636581421,
      "learning_rate": 0.00018694258206267033,
      "loss": 0.2234,
      "step": 14347
    },
    {
      "epoch": 0.0674901455356219,
      "grad_norm": 1.937398076057434,
      "learning_rate": 0.00018694163908455685,
      "loss": 0.2431,
      "step": 14348
    },
    {
      "epoch": 0.06749484933723435,
      "grad_norm": 1.803599238395691,
      "learning_rate": 0.00018694069610644337,
      "loss": 0.3733,
      "step": 14349
    },
    {
      "epoch": 0.06749955313884681,
      "grad_norm": 1.257541298866272,
      "learning_rate": 0.0001869397531283299,
      "loss": 0.155,
      "step": 14350
    },
    {
      "epoch": 0.06750425694045928,
      "grad_norm": 1.1743290424346924,
      "learning_rate": 0.0001869388101502164,
      "loss": 0.1466,
      "step": 14351
    },
    {
      "epoch": 0.06750896074207174,
      "grad_norm": 3.0358710289001465,
      "learning_rate": 0.00018693786717210295,
      "loss": 0.6298,
      "step": 14352
    },
    {
      "epoch": 0.06751366454368421,
      "grad_norm": 1.3205095529556274,
      "learning_rate": 0.00018693692419398947,
      "loss": 0.098,
      "step": 14353
    },
    {
      "epoch": 0.06751836834529668,
      "grad_norm": 1.5911542177200317,
      "learning_rate": 0.000186935981215876,
      "loss": 0.1865,
      "step": 14354
    },
    {
      "epoch": 0.06752307214690913,
      "grad_norm": 4.610871315002441,
      "learning_rate": 0.0001869350382377625,
      "loss": 0.5979,
      "step": 14355
    },
    {
      "epoch": 0.06752777594852159,
      "grad_norm": 1.7413787841796875,
      "learning_rate": 0.00018693409525964903,
      "loss": 0.2289,
      "step": 14356
    },
    {
      "epoch": 0.06753247975013406,
      "grad_norm": 3.4398515224456787,
      "learning_rate": 0.00018693315228153557,
      "loss": 0.5644,
      "step": 14357
    },
    {
      "epoch": 0.06753718355174652,
      "grad_norm": 2.4906558990478516,
      "learning_rate": 0.00018693220930342207,
      "loss": 0.2874,
      "step": 14358
    },
    {
      "epoch": 0.06754188735335899,
      "grad_norm": 3.1010401248931885,
      "learning_rate": 0.00018693126632530858,
      "loss": 0.5458,
      "step": 14359
    },
    {
      "epoch": 0.06754659115497144,
      "grad_norm": 2.3273866176605225,
      "learning_rate": 0.0001869303233471951,
      "loss": 0.2769,
      "step": 14360
    },
    {
      "epoch": 0.06755129495658391,
      "grad_norm": 0.8875685930252075,
      "learning_rate": 0.00018692938036908165,
      "loss": 0.0819,
      "step": 14361
    },
    {
      "epoch": 0.06755599875819637,
      "grad_norm": 0.9760785102844238,
      "learning_rate": 0.00018692843739096817,
      "loss": 0.1048,
      "step": 14362
    },
    {
      "epoch": 0.06756070255980884,
      "grad_norm": 1.8311262130737305,
      "learning_rate": 0.00018692749441285469,
      "loss": 0.27,
      "step": 14363
    },
    {
      "epoch": 0.0675654063614213,
      "grad_norm": 5.630275726318359,
      "learning_rate": 0.0001869265514347412,
      "loss": 0.749,
      "step": 14364
    },
    {
      "epoch": 0.06757011016303377,
      "grad_norm": 1.0026639699935913,
      "learning_rate": 0.00018692560845662772,
      "loss": 0.096,
      "step": 14365
    },
    {
      "epoch": 0.06757481396464622,
      "grad_norm": 1.9464499950408936,
      "learning_rate": 0.00018692466547851427,
      "loss": 0.1284,
      "step": 14366
    },
    {
      "epoch": 0.06757951776625869,
      "grad_norm": 2.9095489978790283,
      "learning_rate": 0.0001869237225004008,
      "loss": 0.2706,
      "step": 14367
    },
    {
      "epoch": 0.06758422156787115,
      "grad_norm": 2.5887398719787598,
      "learning_rate": 0.0001869227795222873,
      "loss": 0.2216,
      "step": 14368
    },
    {
      "epoch": 0.06758892536948362,
      "grad_norm": 3.14864182472229,
      "learning_rate": 0.00018692183654417382,
      "loss": 0.3859,
      "step": 14369
    },
    {
      "epoch": 0.06759362917109608,
      "grad_norm": 1.581922173500061,
      "learning_rate": 0.00018692089356606034,
      "loss": 0.1253,
      "step": 14370
    },
    {
      "epoch": 0.06759833297270855,
      "grad_norm": 2.545595169067383,
      "learning_rate": 0.00018691995058794686,
      "loss": 0.258,
      "step": 14371
    },
    {
      "epoch": 0.067603036774321,
      "grad_norm": 1.7680245637893677,
      "learning_rate": 0.00018691900760983338,
      "loss": 0.1757,
      "step": 14372
    },
    {
      "epoch": 0.06760774057593347,
      "grad_norm": 3.650939464569092,
      "learning_rate": 0.0001869180646317199,
      "loss": 0.235,
      "step": 14373
    },
    {
      "epoch": 0.06761244437754593,
      "grad_norm": 0.8023325204849243,
      "learning_rate": 0.00018691712165360642,
      "loss": 0.0676,
      "step": 14374
    },
    {
      "epoch": 0.0676171481791584,
      "grad_norm": 1.7329411506652832,
      "learning_rate": 0.00018691617867549296,
      "loss": 0.1377,
      "step": 14375
    },
    {
      "epoch": 0.06762185198077086,
      "grad_norm": 2.067816972732544,
      "learning_rate": 0.00018691523569737948,
      "loss": 0.2164,
      "step": 14376
    },
    {
      "epoch": 0.06762655578238332,
      "grad_norm": 3.369697093963623,
      "learning_rate": 0.000186914292719266,
      "loss": 0.4591,
      "step": 14377
    },
    {
      "epoch": 0.06763125958399578,
      "grad_norm": 3.120265483856201,
      "learning_rate": 0.00018691334974115252,
      "loss": 0.579,
      "step": 14378
    },
    {
      "epoch": 0.06763596338560825,
      "grad_norm": 0.5697875022888184,
      "learning_rate": 0.00018691240676303904,
      "loss": 0.0477,
      "step": 14379
    },
    {
      "epoch": 0.06764066718722071,
      "grad_norm": 4.392893314361572,
      "learning_rate": 0.00018691146378492556,
      "loss": 0.281,
      "step": 14380
    },
    {
      "epoch": 0.06764537098883318,
      "grad_norm": 3.5078647136688232,
      "learning_rate": 0.00018691052080681208,
      "loss": 0.5039,
      "step": 14381
    },
    {
      "epoch": 0.06765007479044564,
      "grad_norm": 0.3907465636730194,
      "learning_rate": 0.0001869095778286986,
      "loss": 0.0303,
      "step": 14382
    },
    {
      "epoch": 0.0676547785920581,
      "grad_norm": 3.2545690536499023,
      "learning_rate": 0.0001869086348505851,
      "loss": 0.4602,
      "step": 14383
    },
    {
      "epoch": 0.06765948239367056,
      "grad_norm": 2.137899160385132,
      "learning_rate": 0.00018690769187247166,
      "loss": 0.1531,
      "step": 14384
    },
    {
      "epoch": 0.06766418619528303,
      "grad_norm": 0.41140711307525635,
      "learning_rate": 0.00018690674889435818,
      "loss": 0.0541,
      "step": 14385
    },
    {
      "epoch": 0.06766888999689549,
      "grad_norm": 1.6987476348876953,
      "learning_rate": 0.0001869058059162447,
      "loss": 0.2448,
      "step": 14386
    },
    {
      "epoch": 0.06767359379850796,
      "grad_norm": 0.8260011672973633,
      "learning_rate": 0.00018690486293813121,
      "loss": 0.0629,
      "step": 14387
    },
    {
      "epoch": 0.06767829760012042,
      "grad_norm": 2.873861789703369,
      "learning_rate": 0.00018690391996001773,
      "loss": 0.4584,
      "step": 14388
    },
    {
      "epoch": 0.06768300140173288,
      "grad_norm": 0.4920302927494049,
      "learning_rate": 0.00018690297698190425,
      "loss": 0.0589,
      "step": 14389
    },
    {
      "epoch": 0.06768770520334534,
      "grad_norm": 2.254441499710083,
      "learning_rate": 0.00018690203400379077,
      "loss": 0.257,
      "step": 14390
    },
    {
      "epoch": 0.06769240900495781,
      "grad_norm": 0.28457722067832947,
      "learning_rate": 0.0001869010910256773,
      "loss": 0.0213,
      "step": 14391
    },
    {
      "epoch": 0.06769711280657027,
      "grad_norm": 0.4413844645023346,
      "learning_rate": 0.0001869001480475638,
      "loss": 0.0403,
      "step": 14392
    },
    {
      "epoch": 0.06770181660818274,
      "grad_norm": 1.3105792999267578,
      "learning_rate": 0.00018689920506945035,
      "loss": 0.1223,
      "step": 14393
    },
    {
      "epoch": 0.06770652040979519,
      "grad_norm": 2.3675971031188965,
      "learning_rate": 0.00018689826209133687,
      "loss": 0.2974,
      "step": 14394
    },
    {
      "epoch": 0.06771122421140766,
      "grad_norm": 4.852762222290039,
      "learning_rate": 0.0001868973191132234,
      "loss": 0.72,
      "step": 14395
    },
    {
      "epoch": 0.06771592801302012,
      "grad_norm": 2.1036477088928223,
      "learning_rate": 0.0001868963761351099,
      "loss": 0.3346,
      "step": 14396
    },
    {
      "epoch": 0.06772063181463259,
      "grad_norm": 0.40229907631874084,
      "learning_rate": 0.00018689543315699643,
      "loss": 0.0374,
      "step": 14397
    },
    {
      "epoch": 0.06772533561624505,
      "grad_norm": 5.050704479217529,
      "learning_rate": 0.00018689449017888297,
      "loss": 0.6811,
      "step": 14398
    },
    {
      "epoch": 0.06773003941785752,
      "grad_norm": 0.16306616365909576,
      "learning_rate": 0.0001868935472007695,
      "loss": 0.0112,
      "step": 14399
    },
    {
      "epoch": 0.06773474321946997,
      "grad_norm": 2.0220413208007812,
      "learning_rate": 0.000186892604222656,
      "loss": 0.1234,
      "step": 14400
    },
    {
      "epoch": 0.06773944702108244,
      "grad_norm": 0.368196576833725,
      "learning_rate": 0.0001868916612445425,
      "loss": 0.0187,
      "step": 14401
    },
    {
      "epoch": 0.0677441508226949,
      "grad_norm": 7.14991569519043,
      "learning_rate": 0.00018689071826642905,
      "loss": 0.5563,
      "step": 14402
    },
    {
      "epoch": 0.06774885462430737,
      "grad_norm": 5.84648323059082,
      "learning_rate": 0.00018688977528831557,
      "loss": 0.8875,
      "step": 14403
    },
    {
      "epoch": 0.06775355842591983,
      "grad_norm": 3.2455384731292725,
      "learning_rate": 0.00018688883231020209,
      "loss": 0.5817,
      "step": 14404
    },
    {
      "epoch": 0.0677582622275323,
      "grad_norm": 3.2932090759277344,
      "learning_rate": 0.0001868878893320886,
      "loss": 0.345,
      "step": 14405
    },
    {
      "epoch": 0.06776296602914475,
      "grad_norm": 2.220581531524658,
      "learning_rate": 0.00018688694635397512,
      "loss": 0.329,
      "step": 14406
    },
    {
      "epoch": 0.06776766983075722,
      "grad_norm": 1.2100880146026611,
      "learning_rate": 0.00018688600337586167,
      "loss": 0.11,
      "step": 14407
    },
    {
      "epoch": 0.06777237363236968,
      "grad_norm": 1.557103157043457,
      "learning_rate": 0.0001868850603977482,
      "loss": 0.1403,
      "step": 14408
    },
    {
      "epoch": 0.06777707743398215,
      "grad_norm": 5.397396087646484,
      "learning_rate": 0.0001868841174196347,
      "loss": 0.9702,
      "step": 14409
    },
    {
      "epoch": 0.06778178123559461,
      "grad_norm": 1.9412188529968262,
      "learning_rate": 0.00018688317444152122,
      "loss": 0.0939,
      "step": 14410
    },
    {
      "epoch": 0.06778648503720706,
      "grad_norm": 3.4896504878997803,
      "learning_rate": 0.00018688223146340774,
      "loss": 0.3653,
      "step": 14411
    },
    {
      "epoch": 0.06779118883881953,
      "grad_norm": 3.1135880947113037,
      "learning_rate": 0.00018688128848529426,
      "loss": 0.1341,
      "step": 14412
    },
    {
      "epoch": 0.067795892640432,
      "grad_norm": 4.951903820037842,
      "learning_rate": 0.00018688034550718078,
      "loss": 0.873,
      "step": 14413
    },
    {
      "epoch": 0.06780059644204446,
      "grad_norm": 2.740175724029541,
      "learning_rate": 0.0001868794025290673,
      "loss": 0.1252,
      "step": 14414
    },
    {
      "epoch": 0.06780530024365693,
      "grad_norm": 3.49977707862854,
      "learning_rate": 0.00018687845955095382,
      "loss": 0.2902,
      "step": 14415
    },
    {
      "epoch": 0.06781000404526939,
      "grad_norm": 2.1682817935943604,
      "learning_rate": 0.00018687751657284036,
      "loss": 0.1522,
      "step": 14416
    },
    {
      "epoch": 0.06781470784688184,
      "grad_norm": 2.192314863204956,
      "learning_rate": 0.00018687657359472688,
      "loss": 0.168,
      "step": 14417
    },
    {
      "epoch": 0.06781941164849431,
      "grad_norm": 5.253167152404785,
      "learning_rate": 0.0001868756306166134,
      "loss": 0.638,
      "step": 14418
    },
    {
      "epoch": 0.06782411545010678,
      "grad_norm": 2.6981489658355713,
      "learning_rate": 0.00018687468763849992,
      "loss": 0.417,
      "step": 14419
    },
    {
      "epoch": 0.06782881925171924,
      "grad_norm": 1.137186884880066,
      "learning_rate": 0.00018687374466038644,
      "loss": 0.2065,
      "step": 14420
    },
    {
      "epoch": 0.0678335230533317,
      "grad_norm": 1.0386435985565186,
      "learning_rate": 0.00018687280168227296,
      "loss": 0.1015,
      "step": 14421
    },
    {
      "epoch": 0.06783822685494417,
      "grad_norm": 1.4690492153167725,
      "learning_rate": 0.00018687185870415948,
      "loss": 0.1589,
      "step": 14422
    },
    {
      "epoch": 0.06784293065655662,
      "grad_norm": 2.8058998584747314,
      "learning_rate": 0.000186870915726046,
      "loss": 0.4021,
      "step": 14423
    },
    {
      "epoch": 0.06784763445816909,
      "grad_norm": 1.6138368844985962,
      "learning_rate": 0.0001868699727479325,
      "loss": 0.1751,
      "step": 14424
    },
    {
      "epoch": 0.06785233825978156,
      "grad_norm": 1.9464824199676514,
      "learning_rate": 0.00018686902976981906,
      "loss": 0.3856,
      "step": 14425
    },
    {
      "epoch": 0.06785704206139402,
      "grad_norm": 2.7375400066375732,
      "learning_rate": 0.00018686808679170558,
      "loss": 0.3671,
      "step": 14426
    },
    {
      "epoch": 0.06786174586300649,
      "grad_norm": 2.660767078399658,
      "learning_rate": 0.0001868671438135921,
      "loss": 0.4043,
      "step": 14427
    },
    {
      "epoch": 0.06786644966461894,
      "grad_norm": 1.893491506576538,
      "learning_rate": 0.00018686620083547861,
      "loss": 0.2922,
      "step": 14428
    },
    {
      "epoch": 0.0678711534662314,
      "grad_norm": 2.2756145000457764,
      "learning_rate": 0.00018686525785736516,
      "loss": 0.5417,
      "step": 14429
    },
    {
      "epoch": 0.06787585726784387,
      "grad_norm": 1.649543285369873,
      "learning_rate": 0.00018686431487925168,
      "loss": 0.2537,
      "step": 14430
    },
    {
      "epoch": 0.06788056106945634,
      "grad_norm": 1.532522201538086,
      "learning_rate": 0.0001868633719011382,
      "loss": 0.3821,
      "step": 14431
    },
    {
      "epoch": 0.0678852648710688,
      "grad_norm": 1.785552978515625,
      "learning_rate": 0.0001868624289230247,
      "loss": 0.3438,
      "step": 14432
    },
    {
      "epoch": 0.06788996867268127,
      "grad_norm": 0.9901124835014343,
      "learning_rate": 0.0001868614859449112,
      "loss": 0.2209,
      "step": 14433
    },
    {
      "epoch": 0.06789467247429372,
      "grad_norm": 0.6948243975639343,
      "learning_rate": 0.00018686054296679775,
      "loss": 0.077,
      "step": 14434
    },
    {
      "epoch": 0.06789937627590618,
      "grad_norm": 1.0558006763458252,
      "learning_rate": 0.00018685959998868427,
      "loss": 0.1564,
      "step": 14435
    },
    {
      "epoch": 0.06790408007751865,
      "grad_norm": 0.9742788076400757,
      "learning_rate": 0.0001868586570105708,
      "loss": 0.0873,
      "step": 14436
    },
    {
      "epoch": 0.06790878387913112,
      "grad_norm": 1.5159121751785278,
      "learning_rate": 0.0001868577140324573,
      "loss": 0.3471,
      "step": 14437
    },
    {
      "epoch": 0.06791348768074358,
      "grad_norm": 1.441777229309082,
      "learning_rate": 0.00018685677105434383,
      "loss": 0.2853,
      "step": 14438
    },
    {
      "epoch": 0.06791819148235605,
      "grad_norm": 2.0508310794830322,
      "learning_rate": 0.00018685582807623037,
      "loss": 0.3938,
      "step": 14439
    },
    {
      "epoch": 0.0679228952839685,
      "grad_norm": 2.5929713249206543,
      "learning_rate": 0.0001868548850981169,
      "loss": 0.4597,
      "step": 14440
    },
    {
      "epoch": 0.06792759908558096,
      "grad_norm": 1.3197988271713257,
      "learning_rate": 0.0001868539421200034,
      "loss": 0.2889,
      "step": 14441
    },
    {
      "epoch": 0.06793230288719343,
      "grad_norm": 0.814910352230072,
      "learning_rate": 0.00018685299914188993,
      "loss": 0.1526,
      "step": 14442
    },
    {
      "epoch": 0.0679370066888059,
      "grad_norm": 2.9519362449645996,
      "learning_rate": 0.00018685205616377645,
      "loss": 0.4151,
      "step": 14443
    },
    {
      "epoch": 0.06794171049041836,
      "grad_norm": 1.8986999988555908,
      "learning_rate": 0.00018685111318566297,
      "loss": 0.1964,
      "step": 14444
    },
    {
      "epoch": 0.06794641429203081,
      "grad_norm": 0.4668242037296295,
      "learning_rate": 0.00018685017020754949,
      "loss": 0.051,
      "step": 14445
    },
    {
      "epoch": 0.06795111809364328,
      "grad_norm": 4.195460796356201,
      "learning_rate": 0.000186849227229436,
      "loss": 0.6601,
      "step": 14446
    },
    {
      "epoch": 0.06795582189525574,
      "grad_norm": 6.01731538772583,
      "learning_rate": 0.00018684828425132252,
      "loss": 0.4194,
      "step": 14447
    },
    {
      "epoch": 0.06796052569686821,
      "grad_norm": 4.334481716156006,
      "learning_rate": 0.00018684734127320907,
      "loss": 0.7209,
      "step": 14448
    },
    {
      "epoch": 0.06796522949848068,
      "grad_norm": 1.4726064205169678,
      "learning_rate": 0.0001868463982950956,
      "loss": 0.1891,
      "step": 14449
    },
    {
      "epoch": 0.06796993330009314,
      "grad_norm": 3.15061092376709,
      "learning_rate": 0.0001868454553169821,
      "loss": 0.5776,
      "step": 14450
    },
    {
      "epoch": 0.06797463710170559,
      "grad_norm": 2.6470117568969727,
      "learning_rate": 0.00018684451233886862,
      "loss": 0.3914,
      "step": 14451
    },
    {
      "epoch": 0.06797934090331806,
      "grad_norm": 2.996593475341797,
      "learning_rate": 0.00018684356936075514,
      "loss": 0.3875,
      "step": 14452
    },
    {
      "epoch": 0.06798404470493052,
      "grad_norm": 1.8972164392471313,
      "learning_rate": 0.00018684262638264166,
      "loss": 0.184,
      "step": 14453
    },
    {
      "epoch": 0.06798874850654299,
      "grad_norm": 2.896367073059082,
      "learning_rate": 0.00018684168340452818,
      "loss": 0.3107,
      "step": 14454
    },
    {
      "epoch": 0.06799345230815546,
      "grad_norm": 3.7678439617156982,
      "learning_rate": 0.0001868407404264147,
      "loss": 0.4595,
      "step": 14455
    },
    {
      "epoch": 0.06799815610976792,
      "grad_norm": 1.025118350982666,
      "learning_rate": 0.00018683979744830122,
      "loss": 0.085,
      "step": 14456
    },
    {
      "epoch": 0.06800285991138037,
      "grad_norm": 4.840388298034668,
      "learning_rate": 0.00018683885447018776,
      "loss": 0.7378,
      "step": 14457
    },
    {
      "epoch": 0.06800756371299284,
      "grad_norm": 0.8784361481666565,
      "learning_rate": 0.00018683791149207428,
      "loss": 0.0802,
      "step": 14458
    },
    {
      "epoch": 0.0680122675146053,
      "grad_norm": 2.942446231842041,
      "learning_rate": 0.0001868369685139608,
      "loss": 0.4589,
      "step": 14459
    },
    {
      "epoch": 0.06801697131621777,
      "grad_norm": 0.7844642400741577,
      "learning_rate": 0.00018683602553584732,
      "loss": 0.0752,
      "step": 14460
    },
    {
      "epoch": 0.06802167511783024,
      "grad_norm": 1.284900188446045,
      "learning_rate": 0.00018683508255773387,
      "loss": 0.1819,
      "step": 14461
    },
    {
      "epoch": 0.06802637891944269,
      "grad_norm": 1.0402897596359253,
      "learning_rate": 0.00018683413957962038,
      "loss": 0.1046,
      "step": 14462
    },
    {
      "epoch": 0.06803108272105515,
      "grad_norm": 1.0198676586151123,
      "learning_rate": 0.00018683319660150688,
      "loss": 0.1497,
      "step": 14463
    },
    {
      "epoch": 0.06803578652266762,
      "grad_norm": 1.16646409034729,
      "learning_rate": 0.0001868322536233934,
      "loss": 0.129,
      "step": 14464
    },
    {
      "epoch": 0.06804049032428008,
      "grad_norm": 0.7787487506866455,
      "learning_rate": 0.0001868313106452799,
      "loss": 0.0785,
      "step": 14465
    },
    {
      "epoch": 0.06804519412589255,
      "grad_norm": 1.800034999847412,
      "learning_rate": 0.00018683036766716646,
      "loss": 0.1443,
      "step": 14466
    },
    {
      "epoch": 0.06804989792750502,
      "grad_norm": 1.132782220840454,
      "learning_rate": 0.00018682942468905298,
      "loss": 0.1018,
      "step": 14467
    },
    {
      "epoch": 0.06805460172911747,
      "grad_norm": 1.6388397216796875,
      "learning_rate": 0.0001868284817109395,
      "loss": 0.1345,
      "step": 14468
    },
    {
      "epoch": 0.06805930553072993,
      "grad_norm": 2.8081741333007812,
      "learning_rate": 0.00018682753873282601,
      "loss": 0.3239,
      "step": 14469
    },
    {
      "epoch": 0.0680640093323424,
      "grad_norm": 0.6798483729362488,
      "learning_rate": 0.00018682659575471256,
      "loss": 0.0719,
      "step": 14470
    },
    {
      "epoch": 0.06806871313395486,
      "grad_norm": 1.4479422569274902,
      "learning_rate": 0.00018682565277659908,
      "loss": 0.1045,
      "step": 14471
    },
    {
      "epoch": 0.06807341693556733,
      "grad_norm": 2.3193981647491455,
      "learning_rate": 0.0001868247097984856,
      "loss": 0.386,
      "step": 14472
    },
    {
      "epoch": 0.0680781207371798,
      "grad_norm": 1.5058681964874268,
      "learning_rate": 0.00018682376682037212,
      "loss": 0.1536,
      "step": 14473
    },
    {
      "epoch": 0.06808282453879225,
      "grad_norm": 1.8587371110916138,
      "learning_rate": 0.0001868228238422586,
      "loss": 0.1998,
      "step": 14474
    },
    {
      "epoch": 0.06808752834040471,
      "grad_norm": 4.5214080810546875,
      "learning_rate": 0.00018682188086414515,
      "loss": 0.8314,
      "step": 14475
    },
    {
      "epoch": 0.06809223214201718,
      "grad_norm": 1.6192973852157593,
      "learning_rate": 0.00018682093788603167,
      "loss": 0.1445,
      "step": 14476
    },
    {
      "epoch": 0.06809693594362964,
      "grad_norm": 3.4200282096862793,
      "learning_rate": 0.0001868199949079182,
      "loss": 0.3418,
      "step": 14477
    },
    {
      "epoch": 0.06810163974524211,
      "grad_norm": 1.99220609664917,
      "learning_rate": 0.0001868190519298047,
      "loss": 0.2878,
      "step": 14478
    },
    {
      "epoch": 0.06810634354685456,
      "grad_norm": 0.15884944796562195,
      "learning_rate": 0.00018681810895169126,
      "loss": 0.008,
      "step": 14479
    },
    {
      "epoch": 0.06811104734846703,
      "grad_norm": 1.2275437116622925,
      "learning_rate": 0.00018681716597357777,
      "loss": 0.0761,
      "step": 14480
    },
    {
      "epoch": 0.06811575115007949,
      "grad_norm": 2.5750792026519775,
      "learning_rate": 0.0001868162229954643,
      "loss": 0.5605,
      "step": 14481
    },
    {
      "epoch": 0.06812045495169196,
      "grad_norm": 1.1601054668426514,
      "learning_rate": 0.0001868152800173508,
      "loss": 0.0615,
      "step": 14482
    },
    {
      "epoch": 0.06812515875330442,
      "grad_norm": 3.0131306648254395,
      "learning_rate": 0.00018681433703923733,
      "loss": 0.2915,
      "step": 14483
    },
    {
      "epoch": 0.06812986255491689,
      "grad_norm": 1.9695560932159424,
      "learning_rate": 0.00018681339406112385,
      "loss": 0.2322,
      "step": 14484
    },
    {
      "epoch": 0.06813456635652934,
      "grad_norm": 4.953418254852295,
      "learning_rate": 0.00018681245108301037,
      "loss": 0.5835,
      "step": 14485
    },
    {
      "epoch": 0.06813927015814181,
      "grad_norm": 0.4494767487049103,
      "learning_rate": 0.00018681150810489689,
      "loss": 0.0279,
      "step": 14486
    },
    {
      "epoch": 0.06814397395975427,
      "grad_norm": 1.438939094543457,
      "learning_rate": 0.0001868105651267834,
      "loss": 0.1046,
      "step": 14487
    },
    {
      "epoch": 0.06814867776136674,
      "grad_norm": 2.4927375316619873,
      "learning_rate": 0.00018680962214866995,
      "loss": 0.325,
      "step": 14488
    },
    {
      "epoch": 0.0681533815629792,
      "grad_norm": 1.7068156003952026,
      "learning_rate": 0.00018680867917055647,
      "loss": 0.0869,
      "step": 14489
    },
    {
      "epoch": 0.06815808536459167,
      "grad_norm": 0.819536566734314,
      "learning_rate": 0.000186807736192443,
      "loss": 0.0546,
      "step": 14490
    },
    {
      "epoch": 0.06816278916620412,
      "grad_norm": 3.644963026046753,
      "learning_rate": 0.0001868067932143295,
      "loss": 0.6444,
      "step": 14491
    },
    {
      "epoch": 0.06816749296781659,
      "grad_norm": 1.897934913635254,
      "learning_rate": 0.00018680585023621602,
      "loss": 0.245,
      "step": 14492
    },
    {
      "epoch": 0.06817219676942905,
      "grad_norm": 0.6269446611404419,
      "learning_rate": 0.00018680490725810257,
      "loss": 0.0505,
      "step": 14493
    },
    {
      "epoch": 0.06817690057104152,
      "grad_norm": 2.468775510787964,
      "learning_rate": 0.00018680396427998906,
      "loss": 0.316,
      "step": 14494
    },
    {
      "epoch": 0.06818160437265398,
      "grad_norm": 0.5866081118583679,
      "learning_rate": 0.00018680302130187558,
      "loss": 0.0304,
      "step": 14495
    },
    {
      "epoch": 0.06818630817426644,
      "grad_norm": 2.6892948150634766,
      "learning_rate": 0.0001868020783237621,
      "loss": 0.3328,
      "step": 14496
    },
    {
      "epoch": 0.0681910119758789,
      "grad_norm": 2.372610569000244,
      "learning_rate": 0.00018680113534564862,
      "loss": 0.2393,
      "step": 14497
    },
    {
      "epoch": 0.06819571577749137,
      "grad_norm": 2.756549835205078,
      "learning_rate": 0.00018680019236753516,
      "loss": 0.3298,
      "step": 14498
    },
    {
      "epoch": 0.06820041957910383,
      "grad_norm": 0.22568006813526154,
      "learning_rate": 0.00018679924938942168,
      "loss": 0.0143,
      "step": 14499
    },
    {
      "epoch": 0.0682051233807163,
      "grad_norm": 2.666156768798828,
      "learning_rate": 0.0001867983064113082,
      "loss": 0.2656,
      "step": 14500
    },
    {
      "epoch": 0.06820982718232876,
      "grad_norm": 0.6068707704544067,
      "learning_rate": 0.00018679736343319472,
      "loss": 0.0304,
      "step": 14501
    },
    {
      "epoch": 0.06821453098394122,
      "grad_norm": 0.8811517357826233,
      "learning_rate": 0.00018679642045508127,
      "loss": 0.0708,
      "step": 14502
    },
    {
      "epoch": 0.06821923478555368,
      "grad_norm": 7.065384387969971,
      "learning_rate": 0.00018679547747696778,
      "loss": 0.2918,
      "step": 14503
    },
    {
      "epoch": 0.06822393858716615,
      "grad_norm": 1.389862060546875,
      "learning_rate": 0.0001867945344988543,
      "loss": 0.1119,
      "step": 14504
    },
    {
      "epoch": 0.06822864238877861,
      "grad_norm": 0.609020471572876,
      "learning_rate": 0.0001867935915207408,
      "loss": 0.04,
      "step": 14505
    },
    {
      "epoch": 0.06823334619039108,
      "grad_norm": 0.8243741393089294,
      "learning_rate": 0.0001867926485426273,
      "loss": 0.0549,
      "step": 14506
    },
    {
      "epoch": 0.06823804999200354,
      "grad_norm": 0.6595146656036377,
      "learning_rate": 0.00018679170556451386,
      "loss": 0.034,
      "step": 14507
    },
    {
      "epoch": 0.068242753793616,
      "grad_norm": 0.019038749858736992,
      "learning_rate": 0.00018679076258640038,
      "loss": 0.0009,
      "step": 14508
    },
    {
      "epoch": 0.06824745759522846,
      "grad_norm": 3.5889642238616943,
      "learning_rate": 0.0001867898196082869,
      "loss": 0.2692,
      "step": 14509
    },
    {
      "epoch": 0.06825216139684093,
      "grad_norm": 6.172726631164551,
      "learning_rate": 0.00018678887663017341,
      "loss": 0.9199,
      "step": 14510
    },
    {
      "epoch": 0.06825686519845339,
      "grad_norm": 4.243801593780518,
      "learning_rate": 0.00018678793365205996,
      "loss": 0.5251,
      "step": 14511
    },
    {
      "epoch": 0.06826156900006586,
      "grad_norm": 3.408464193344116,
      "learning_rate": 0.00018678699067394648,
      "loss": 0.6934,
      "step": 14512
    },
    {
      "epoch": 0.06826627280167831,
      "grad_norm": 2.840993642807007,
      "learning_rate": 0.000186786047695833,
      "loss": 0.3445,
      "step": 14513
    },
    {
      "epoch": 0.06827097660329078,
      "grad_norm": 2.396939516067505,
      "learning_rate": 0.00018678510471771952,
      "loss": 0.2192,
      "step": 14514
    },
    {
      "epoch": 0.06827568040490324,
      "grad_norm": 0.3917495310306549,
      "learning_rate": 0.00018678416173960604,
      "loss": 0.0421,
      "step": 14515
    },
    {
      "epoch": 0.0682803842065157,
      "grad_norm": 1.4034993648529053,
      "learning_rate": 0.00018678321876149255,
      "loss": 0.0742,
      "step": 14516
    },
    {
      "epoch": 0.06828508800812817,
      "grad_norm": 4.2541680335998535,
      "learning_rate": 0.00018678227578337907,
      "loss": 0.505,
      "step": 14517
    },
    {
      "epoch": 0.06828979180974064,
      "grad_norm": 0.7140349745750427,
      "learning_rate": 0.0001867813328052656,
      "loss": 0.0426,
      "step": 14518
    },
    {
      "epoch": 0.06829449561135309,
      "grad_norm": 3.0038349628448486,
      "learning_rate": 0.0001867803898271521,
      "loss": 0.293,
      "step": 14519
    },
    {
      "epoch": 0.06829919941296556,
      "grad_norm": 2.530264139175415,
      "learning_rate": 0.00018677944684903866,
      "loss": 0.3939,
      "step": 14520
    },
    {
      "epoch": 0.06830390321457802,
      "grad_norm": 0.4101097881793976,
      "learning_rate": 0.00018677850387092517,
      "loss": 0.0292,
      "step": 14521
    },
    {
      "epoch": 0.06830860701619049,
      "grad_norm": 1.6166164875030518,
      "learning_rate": 0.0001867775608928117,
      "loss": 0.1357,
      "step": 14522
    },
    {
      "epoch": 0.06831331081780295,
      "grad_norm": 2.8069920539855957,
      "learning_rate": 0.0001867766179146982,
      "loss": 0.4229,
      "step": 14523
    },
    {
      "epoch": 0.06831801461941542,
      "grad_norm": 0.841340959072113,
      "learning_rate": 0.00018677567493658473,
      "loss": 0.069,
      "step": 14524
    },
    {
      "epoch": 0.06832271842102787,
      "grad_norm": 0.37934890389442444,
      "learning_rate": 0.00018677473195847125,
      "loss": 0.0231,
      "step": 14525
    },
    {
      "epoch": 0.06832742222264034,
      "grad_norm": 2.733989953994751,
      "learning_rate": 0.00018677378898035777,
      "loss": 0.5253,
      "step": 14526
    },
    {
      "epoch": 0.0683321260242528,
      "grad_norm": 5.057613372802734,
      "learning_rate": 0.00018677284600224429,
      "loss": 0.8707,
      "step": 14527
    },
    {
      "epoch": 0.06833682982586527,
      "grad_norm": 2.7709925174713135,
      "learning_rate": 0.0001867719030241308,
      "loss": 0.2442,
      "step": 14528
    },
    {
      "epoch": 0.06834153362747773,
      "grad_norm": 3.988548994064331,
      "learning_rate": 0.00018677096004601735,
      "loss": 0.5534,
      "step": 14529
    },
    {
      "epoch": 0.06834623742909018,
      "grad_norm": 4.865002632141113,
      "learning_rate": 0.00018677001706790387,
      "loss": 0.9707,
      "step": 14530
    },
    {
      "epoch": 0.06835094123070265,
      "grad_norm": 0.19240424036979675,
      "learning_rate": 0.0001867690740897904,
      "loss": 0.013,
      "step": 14531
    },
    {
      "epoch": 0.06835564503231512,
      "grad_norm": 0.6555005311965942,
      "learning_rate": 0.0001867681311116769,
      "loss": 0.0617,
      "step": 14532
    },
    {
      "epoch": 0.06836034883392758,
      "grad_norm": 4.2536540031433105,
      "learning_rate": 0.00018676718813356342,
      "loss": 0.8493,
      "step": 14533
    },
    {
      "epoch": 0.06836505263554005,
      "grad_norm": 3.972130060195923,
      "learning_rate": 0.00018676624515544997,
      "loss": 0.5788,
      "step": 14534
    },
    {
      "epoch": 0.06836975643715251,
      "grad_norm": 2.5953376293182373,
      "learning_rate": 0.0001867653021773365,
      "loss": 0.2527,
      "step": 14535
    },
    {
      "epoch": 0.06837446023876496,
      "grad_norm": 2.2961838245391846,
      "learning_rate": 0.00018676435919922298,
      "loss": 0.2291,
      "step": 14536
    },
    {
      "epoch": 0.06837916404037743,
      "grad_norm": 0.15760865807533264,
      "learning_rate": 0.0001867634162211095,
      "loss": 0.0084,
      "step": 14537
    },
    {
      "epoch": 0.0683838678419899,
      "grad_norm": 2.8610095977783203,
      "learning_rate": 0.00018676247324299605,
      "loss": 0.3668,
      "step": 14538
    },
    {
      "epoch": 0.06838857164360236,
      "grad_norm": 1.4119882583618164,
      "learning_rate": 0.00018676153026488256,
      "loss": 0.1586,
      "step": 14539
    },
    {
      "epoch": 0.06839327544521483,
      "grad_norm": 0.33594244718551636,
      "learning_rate": 0.00018676058728676908,
      "loss": 0.0254,
      "step": 14540
    },
    {
      "epoch": 0.06839797924682729,
      "grad_norm": 2.413358688354492,
      "learning_rate": 0.0001867596443086556,
      "loss": 0.1946,
      "step": 14541
    },
    {
      "epoch": 0.06840268304843974,
      "grad_norm": 3.019440174102783,
      "learning_rate": 0.00018675870133054212,
      "loss": 0.5417,
      "step": 14542
    },
    {
      "epoch": 0.06840738685005221,
      "grad_norm": 0.3149568438529968,
      "learning_rate": 0.00018675775835242867,
      "loss": 0.0246,
      "step": 14543
    },
    {
      "epoch": 0.06841209065166468,
      "grad_norm": 1.6073477268218994,
      "learning_rate": 0.00018675681537431518,
      "loss": 0.3055,
      "step": 14544
    },
    {
      "epoch": 0.06841679445327714,
      "grad_norm": 1.6748627424240112,
      "learning_rate": 0.0001867558723962017,
      "loss": 0.2018,
      "step": 14545
    },
    {
      "epoch": 0.0684214982548896,
      "grad_norm": 2.6139678955078125,
      "learning_rate": 0.00018675492941808822,
      "loss": 0.3156,
      "step": 14546
    },
    {
      "epoch": 0.06842620205650206,
      "grad_norm": 2.20831561088562,
      "learning_rate": 0.0001867539864399747,
      "loss": 0.3794,
      "step": 14547
    },
    {
      "epoch": 0.06843090585811452,
      "grad_norm": 2.495893716812134,
      "learning_rate": 0.00018675304346186126,
      "loss": 0.4676,
      "step": 14548
    },
    {
      "epoch": 0.06843560965972699,
      "grad_norm": 0.6421440243721008,
      "learning_rate": 0.00018675210048374778,
      "loss": 0.078,
      "step": 14549
    },
    {
      "epoch": 0.06844031346133946,
      "grad_norm": 0.848874032497406,
      "learning_rate": 0.0001867511575056343,
      "loss": 0.0786,
      "step": 14550
    },
    {
      "epoch": 0.06844501726295192,
      "grad_norm": 0.41954442858695984,
      "learning_rate": 0.00018675021452752081,
      "loss": 0.0292,
      "step": 14551
    },
    {
      "epoch": 0.06844972106456439,
      "grad_norm": 2.897993803024292,
      "learning_rate": 0.00018674927154940736,
      "loss": 0.3063,
      "step": 14552
    },
    {
      "epoch": 0.06845442486617684,
      "grad_norm": 3.758183240890503,
      "learning_rate": 0.00018674832857129388,
      "loss": 0.3696,
      "step": 14553
    },
    {
      "epoch": 0.0684591286677893,
      "grad_norm": 3.7899510860443115,
      "learning_rate": 0.0001867473855931804,
      "loss": 0.6992,
      "step": 14554
    },
    {
      "epoch": 0.06846383246940177,
      "grad_norm": 4.193732261657715,
      "learning_rate": 0.00018674644261506692,
      "loss": 0.6565,
      "step": 14555
    },
    {
      "epoch": 0.06846853627101424,
      "grad_norm": 0.8262122273445129,
      "learning_rate": 0.00018674549963695344,
      "loss": 0.0985,
      "step": 14556
    },
    {
      "epoch": 0.0684732400726267,
      "grad_norm": 3.205756187438965,
      "learning_rate": 0.00018674455665883995,
      "loss": 0.2127,
      "step": 14557
    },
    {
      "epoch": 0.06847794387423917,
      "grad_norm": 1.048365592956543,
      "learning_rate": 0.00018674361368072647,
      "loss": 0.1152,
      "step": 14558
    },
    {
      "epoch": 0.06848264767585162,
      "grad_norm": 2.125890016555786,
      "learning_rate": 0.000186742670702613,
      "loss": 0.1597,
      "step": 14559
    },
    {
      "epoch": 0.06848735147746408,
      "grad_norm": 2.993887424468994,
      "learning_rate": 0.0001867417277244995,
      "loss": 0.2738,
      "step": 14560
    },
    {
      "epoch": 0.06849205527907655,
      "grad_norm": 3.3416409492492676,
      "learning_rate": 0.00018674078474638606,
      "loss": 0.2488,
      "step": 14561
    },
    {
      "epoch": 0.06849675908068902,
      "grad_norm": 1.27560293674469,
      "learning_rate": 0.00018673984176827257,
      "loss": 0.0857,
      "step": 14562
    },
    {
      "epoch": 0.06850146288230148,
      "grad_norm": 1.8039822578430176,
      "learning_rate": 0.0001867388987901591,
      "loss": 0.1452,
      "step": 14563
    },
    {
      "epoch": 0.06850616668391393,
      "grad_norm": 0.2799263000488281,
      "learning_rate": 0.0001867379558120456,
      "loss": 0.0255,
      "step": 14564
    },
    {
      "epoch": 0.0685108704855264,
      "grad_norm": 0.8651581406593323,
      "learning_rate": 0.00018673701283393213,
      "loss": 0.0624,
      "step": 14565
    },
    {
      "epoch": 0.06851557428713886,
      "grad_norm": 1.6886602640151978,
      "learning_rate": 0.00018673606985581868,
      "loss": 0.2008,
      "step": 14566
    },
    {
      "epoch": 0.06852027808875133,
      "grad_norm": 3.042682647705078,
      "learning_rate": 0.00018673512687770517,
      "loss": 0.2371,
      "step": 14567
    },
    {
      "epoch": 0.0685249818903638,
      "grad_norm": 2.0867884159088135,
      "learning_rate": 0.00018673418389959169,
      "loss": 0.1826,
      "step": 14568
    },
    {
      "epoch": 0.06852968569197626,
      "grad_norm": 2.030257225036621,
      "learning_rate": 0.0001867332409214782,
      "loss": 0.1907,
      "step": 14569
    },
    {
      "epoch": 0.06853438949358871,
      "grad_norm": 0.9326158761978149,
      "learning_rate": 0.00018673229794336475,
      "loss": 0.0602,
      "step": 14570
    },
    {
      "epoch": 0.06853909329520118,
      "grad_norm": 1.013051986694336,
      "learning_rate": 0.00018673135496525127,
      "loss": 0.0568,
      "step": 14571
    },
    {
      "epoch": 0.06854379709681364,
      "grad_norm": 1.7917512655258179,
      "learning_rate": 0.0001867304119871378,
      "loss": 0.096,
      "step": 14572
    },
    {
      "epoch": 0.06854850089842611,
      "grad_norm": 3.4964160919189453,
      "learning_rate": 0.0001867294690090243,
      "loss": 0.327,
      "step": 14573
    },
    {
      "epoch": 0.06855320470003858,
      "grad_norm": 3.699213743209839,
      "learning_rate": 0.00018672852603091082,
      "loss": 0.6145,
      "step": 14574
    },
    {
      "epoch": 0.06855790850165104,
      "grad_norm": 0.39187562465667725,
      "learning_rate": 0.00018672758305279737,
      "loss": 0.0235,
      "step": 14575
    },
    {
      "epoch": 0.06856261230326349,
      "grad_norm": 2.81329607963562,
      "learning_rate": 0.0001867266400746839,
      "loss": 0.1733,
      "step": 14576
    },
    {
      "epoch": 0.06856731610487596,
      "grad_norm": 3.37577748298645,
      "learning_rate": 0.0001867256970965704,
      "loss": 0.2385,
      "step": 14577
    },
    {
      "epoch": 0.06857201990648842,
      "grad_norm": 5.544543266296387,
      "learning_rate": 0.0001867247541184569,
      "loss": 1.0839,
      "step": 14578
    },
    {
      "epoch": 0.06857672370810089,
      "grad_norm": 2.7329325675964355,
      "learning_rate": 0.00018672381114034345,
      "loss": 0.2304,
      "step": 14579
    },
    {
      "epoch": 0.06858142750971336,
      "grad_norm": 3.5028152465820312,
      "learning_rate": 0.00018672286816222996,
      "loss": 0.5741,
      "step": 14580
    },
    {
      "epoch": 0.06858613131132581,
      "grad_norm": 2.1317806243896484,
      "learning_rate": 0.00018672192518411648,
      "loss": 0.117,
      "step": 14581
    },
    {
      "epoch": 0.06859083511293827,
      "grad_norm": 2.6397359371185303,
      "learning_rate": 0.000186720982206003,
      "loss": 0.2545,
      "step": 14582
    },
    {
      "epoch": 0.06859553891455074,
      "grad_norm": 2.429748773574829,
      "learning_rate": 0.00018672003922788952,
      "loss": 0.2276,
      "step": 14583
    },
    {
      "epoch": 0.0686002427161632,
      "grad_norm": 4.016275882720947,
      "learning_rate": 0.00018671909624977607,
      "loss": 0.8459,
      "step": 14584
    },
    {
      "epoch": 0.06860494651777567,
      "grad_norm": 2.5060319900512695,
      "learning_rate": 0.00018671815327166258,
      "loss": 0.4515,
      "step": 14585
    },
    {
      "epoch": 0.06860965031938814,
      "grad_norm": 3.220472812652588,
      "learning_rate": 0.0001867172102935491,
      "loss": 0.6221,
      "step": 14586
    },
    {
      "epoch": 0.06861435412100059,
      "grad_norm": 0.1210983544588089,
      "learning_rate": 0.00018671626731543562,
      "loss": 0.0082,
      "step": 14587
    },
    {
      "epoch": 0.06861905792261305,
      "grad_norm": 3.8084142208099365,
      "learning_rate": 0.00018671532433732214,
      "loss": 0.8194,
      "step": 14588
    },
    {
      "epoch": 0.06862376172422552,
      "grad_norm": 1.494367003440857,
      "learning_rate": 0.00018671438135920866,
      "loss": 0.1045,
      "step": 14589
    },
    {
      "epoch": 0.06862846552583798,
      "grad_norm": 0.9129217863082886,
      "learning_rate": 0.00018671343838109518,
      "loss": 0.091,
      "step": 14590
    },
    {
      "epoch": 0.06863316932745045,
      "grad_norm": 2.08457612991333,
      "learning_rate": 0.0001867124954029817,
      "loss": 0.3688,
      "step": 14591
    },
    {
      "epoch": 0.06863787312906292,
      "grad_norm": 1.6469171047210693,
      "learning_rate": 0.00018671155242486821,
      "loss": 0.3698,
      "step": 14592
    },
    {
      "epoch": 0.06864257693067537,
      "grad_norm": 2.609417200088501,
      "learning_rate": 0.00018671060944675476,
      "loss": 0.6842,
      "step": 14593
    },
    {
      "epoch": 0.06864728073228783,
      "grad_norm": 1.4703240394592285,
      "learning_rate": 0.00018670966646864128,
      "loss": 0.1541,
      "step": 14594
    },
    {
      "epoch": 0.0686519845339003,
      "grad_norm": 2.0168514251708984,
      "learning_rate": 0.0001867087234905278,
      "loss": 0.2382,
      "step": 14595
    },
    {
      "epoch": 0.06865668833551276,
      "grad_norm": 2.6509008407592773,
      "learning_rate": 0.00018670778051241432,
      "loss": 0.47,
      "step": 14596
    },
    {
      "epoch": 0.06866139213712523,
      "grad_norm": 0.338661253452301,
      "learning_rate": 0.00018670683753430083,
      "loss": 0.0281,
      "step": 14597
    },
    {
      "epoch": 0.06866609593873768,
      "grad_norm": 2.6483471393585205,
      "learning_rate": 0.00018670589455618735,
      "loss": 0.316,
      "step": 14598
    },
    {
      "epoch": 0.06867079974035015,
      "grad_norm": 0.5399158000946045,
      "learning_rate": 0.00018670495157807387,
      "loss": 0.086,
      "step": 14599
    },
    {
      "epoch": 0.06867550354196261,
      "grad_norm": 0.7866127490997314,
      "learning_rate": 0.0001867040085999604,
      "loss": 0.1768,
      "step": 14600
    },
    {
      "epoch": 0.06868020734357508,
      "grad_norm": 0.5771021246910095,
      "learning_rate": 0.0001867030656218469,
      "loss": 0.0446,
      "step": 14601
    },
    {
      "epoch": 0.06868491114518754,
      "grad_norm": 0.6117757558822632,
      "learning_rate": 0.00018670212264373346,
      "loss": 0.0661,
      "step": 14602
    },
    {
      "epoch": 0.06868961494680001,
      "grad_norm": 0.422648549079895,
      "learning_rate": 0.00018670117966561997,
      "loss": 0.0395,
      "step": 14603
    },
    {
      "epoch": 0.06869431874841246,
      "grad_norm": 2.7205066680908203,
      "learning_rate": 0.0001867002366875065,
      "loss": 0.5214,
      "step": 14604
    },
    {
      "epoch": 0.06869902255002493,
      "grad_norm": 0.7106183767318726,
      "learning_rate": 0.000186699293709393,
      "loss": 0.045,
      "step": 14605
    },
    {
      "epoch": 0.06870372635163739,
      "grad_norm": 0.573402464389801,
      "learning_rate": 0.00018669835073127953,
      "loss": 0.0514,
      "step": 14606
    },
    {
      "epoch": 0.06870843015324986,
      "grad_norm": 1.0247783660888672,
      "learning_rate": 0.00018669740775316608,
      "loss": 0.135,
      "step": 14607
    },
    {
      "epoch": 0.06871313395486232,
      "grad_norm": 0.6080950498580933,
      "learning_rate": 0.0001866964647750526,
      "loss": 0.0833,
      "step": 14608
    },
    {
      "epoch": 0.06871783775647479,
      "grad_norm": 3.1962790489196777,
      "learning_rate": 0.00018669552179693909,
      "loss": 0.0778,
      "step": 14609
    },
    {
      "epoch": 0.06872254155808724,
      "grad_norm": 3.101043939590454,
      "learning_rate": 0.0001866945788188256,
      "loss": 0.2253,
      "step": 14610
    },
    {
      "epoch": 0.0687272453596997,
      "grad_norm": 1.1076966524124146,
      "learning_rate": 0.00018669363584071215,
      "loss": 0.0826,
      "step": 14611
    },
    {
      "epoch": 0.06873194916131217,
      "grad_norm": 2.7127280235290527,
      "learning_rate": 0.00018669269286259867,
      "loss": 0.4715,
      "step": 14612
    },
    {
      "epoch": 0.06873665296292464,
      "grad_norm": 4.906462669372559,
      "learning_rate": 0.0001866917498844852,
      "loss": 0.6726,
      "step": 14613
    },
    {
      "epoch": 0.0687413567645371,
      "grad_norm": 2.4028480052948,
      "learning_rate": 0.0001866908069063717,
      "loss": 0.2597,
      "step": 14614
    },
    {
      "epoch": 0.06874606056614956,
      "grad_norm": 0.6807560324668884,
      "learning_rate": 0.00018668986392825822,
      "loss": 0.0779,
      "step": 14615
    },
    {
      "epoch": 0.06875076436776202,
      "grad_norm": 2.140159845352173,
      "learning_rate": 0.00018668892095014477,
      "loss": 0.2846,
      "step": 14616
    },
    {
      "epoch": 0.06875546816937449,
      "grad_norm": 1.7473490238189697,
      "learning_rate": 0.0001866879779720313,
      "loss": 0.1724,
      "step": 14617
    },
    {
      "epoch": 0.06876017197098695,
      "grad_norm": 2.8993732929229736,
      "learning_rate": 0.0001866870349939178,
      "loss": 0.221,
      "step": 14618
    },
    {
      "epoch": 0.06876487577259942,
      "grad_norm": 1.0618095397949219,
      "learning_rate": 0.00018668609201580433,
      "loss": 0.0835,
      "step": 14619
    },
    {
      "epoch": 0.06876957957421188,
      "grad_norm": 1.4809077978134155,
      "learning_rate": 0.00018668514903769085,
      "loss": 0.1005,
      "step": 14620
    },
    {
      "epoch": 0.06877428337582434,
      "grad_norm": 1.95719313621521,
      "learning_rate": 0.00018668420605957736,
      "loss": 0.2118,
      "step": 14621
    },
    {
      "epoch": 0.0687789871774368,
      "grad_norm": 0.5326840877532959,
      "learning_rate": 0.00018668326308146388,
      "loss": 0.0332,
      "step": 14622
    },
    {
      "epoch": 0.06878369097904927,
      "grad_norm": 3.95872163772583,
      "learning_rate": 0.0001866823201033504,
      "loss": 0.2044,
      "step": 14623
    },
    {
      "epoch": 0.06878839478066173,
      "grad_norm": 4.4245285987854,
      "learning_rate": 0.00018668137712523692,
      "loss": 0.289,
      "step": 14624
    },
    {
      "epoch": 0.0687930985822742,
      "grad_norm": 2.6666321754455566,
      "learning_rate": 0.00018668043414712347,
      "loss": 0.3051,
      "step": 14625
    },
    {
      "epoch": 0.06879780238388666,
      "grad_norm": 2.629356861114502,
      "learning_rate": 0.00018667949116900998,
      "loss": 0.4753,
      "step": 14626
    },
    {
      "epoch": 0.06880250618549912,
      "grad_norm": 2.7313740253448486,
      "learning_rate": 0.0001866785481908965,
      "loss": 0.31,
      "step": 14627
    },
    {
      "epoch": 0.06880720998711158,
      "grad_norm": 1.9265178442001343,
      "learning_rate": 0.00018667760521278302,
      "loss": 0.3467,
      "step": 14628
    },
    {
      "epoch": 0.06881191378872405,
      "grad_norm": 0.4787796139717102,
      "learning_rate": 0.00018667666223466954,
      "loss": 0.0401,
      "step": 14629
    },
    {
      "epoch": 0.06881661759033651,
      "grad_norm": 3.8238298892974854,
      "learning_rate": 0.00018667571925655606,
      "loss": 0.3041,
      "step": 14630
    },
    {
      "epoch": 0.06882132139194898,
      "grad_norm": 2.712761163711548,
      "learning_rate": 0.00018667477627844258,
      "loss": 0.3471,
      "step": 14631
    },
    {
      "epoch": 0.06882602519356143,
      "grad_norm": 1.4800268411636353,
      "learning_rate": 0.0001866738333003291,
      "loss": 0.137,
      "step": 14632
    },
    {
      "epoch": 0.0688307289951739,
      "grad_norm": 0.468418687582016,
      "learning_rate": 0.00018667289032221561,
      "loss": 0.0469,
      "step": 14633
    },
    {
      "epoch": 0.06883543279678636,
      "grad_norm": 3.178776979446411,
      "learning_rate": 0.00018667194734410216,
      "loss": 0.409,
      "step": 14634
    },
    {
      "epoch": 0.06884013659839883,
      "grad_norm": 1.1357049942016602,
      "learning_rate": 0.00018667100436598868,
      "loss": 0.108,
      "step": 14635
    },
    {
      "epoch": 0.06884484040001129,
      "grad_norm": 2.794682264328003,
      "learning_rate": 0.0001866700613878752,
      "loss": 0.5828,
      "step": 14636
    },
    {
      "epoch": 0.06884954420162376,
      "grad_norm": 1.93800687789917,
      "learning_rate": 0.00018666911840976172,
      "loss": 0.1619,
      "step": 14637
    },
    {
      "epoch": 0.06885424800323621,
      "grad_norm": 4.032487869262695,
      "learning_rate": 0.00018666817543164826,
      "loss": 0.5087,
      "step": 14638
    },
    {
      "epoch": 0.06885895180484868,
      "grad_norm": 1.524993658065796,
      "learning_rate": 0.00018666723245353478,
      "loss": 0.1803,
      "step": 14639
    },
    {
      "epoch": 0.06886365560646114,
      "grad_norm": 1.823332667350769,
      "learning_rate": 0.00018666628947542127,
      "loss": 0.3555,
      "step": 14640
    },
    {
      "epoch": 0.0688683594080736,
      "grad_norm": 2.8364474773406982,
      "learning_rate": 0.0001866653464973078,
      "loss": 0.4743,
      "step": 14641
    },
    {
      "epoch": 0.06887306320968607,
      "grad_norm": 1.8692606687545776,
      "learning_rate": 0.0001866644035191943,
      "loss": 0.126,
      "step": 14642
    },
    {
      "epoch": 0.06887776701129854,
      "grad_norm": 2.306478977203369,
      "learning_rate": 0.00018666346054108086,
      "loss": 0.1309,
      "step": 14643
    },
    {
      "epoch": 0.06888247081291099,
      "grad_norm": 0.8195476531982422,
      "learning_rate": 0.00018666251756296737,
      "loss": 0.0871,
      "step": 14644
    },
    {
      "epoch": 0.06888717461452346,
      "grad_norm": 1.009020447731018,
      "learning_rate": 0.0001866615745848539,
      "loss": 0.1607,
      "step": 14645
    },
    {
      "epoch": 0.06889187841613592,
      "grad_norm": 0.38550159335136414,
      "learning_rate": 0.0001866606316067404,
      "loss": 0.0287,
      "step": 14646
    },
    {
      "epoch": 0.06889658221774839,
      "grad_norm": 0.3060513436794281,
      "learning_rate": 0.00018665968862862693,
      "loss": 0.0244,
      "step": 14647
    },
    {
      "epoch": 0.06890128601936085,
      "grad_norm": 1.8852946758270264,
      "learning_rate": 0.00018665874565051348,
      "loss": 0.5119,
      "step": 14648
    },
    {
      "epoch": 0.0689059898209733,
      "grad_norm": 1.6492853164672852,
      "learning_rate": 0.0001866578026724,
      "loss": 0.3225,
      "step": 14649
    },
    {
      "epoch": 0.06891069362258577,
      "grad_norm": 3.129133939743042,
      "learning_rate": 0.0001866568596942865,
      "loss": 0.593,
      "step": 14650
    },
    {
      "epoch": 0.06891539742419824,
      "grad_norm": 1.8188420534133911,
      "learning_rate": 0.00018665591671617303,
      "loss": 0.1007,
      "step": 14651
    },
    {
      "epoch": 0.0689201012258107,
      "grad_norm": 2.532932758331299,
      "learning_rate": 0.00018665497373805955,
      "loss": 0.2847,
      "step": 14652
    },
    {
      "epoch": 0.06892480502742317,
      "grad_norm": 3.9009437561035156,
      "learning_rate": 0.00018665403075994607,
      "loss": 0.3618,
      "step": 14653
    },
    {
      "epoch": 0.06892950882903563,
      "grad_norm": 2.88041615486145,
      "learning_rate": 0.0001866530877818326,
      "loss": 0.2971,
      "step": 14654
    },
    {
      "epoch": 0.06893421263064808,
      "grad_norm": 5.201412677764893,
      "learning_rate": 0.0001866521448037191,
      "loss": 0.617,
      "step": 14655
    },
    {
      "epoch": 0.06893891643226055,
      "grad_norm": 0.9477441310882568,
      "learning_rate": 0.00018665120182560562,
      "loss": 0.0737,
      "step": 14656
    },
    {
      "epoch": 0.06894362023387302,
      "grad_norm": 2.227189779281616,
      "learning_rate": 0.00018665025884749217,
      "loss": 0.1751,
      "step": 14657
    },
    {
      "epoch": 0.06894832403548548,
      "grad_norm": 3.890103578567505,
      "learning_rate": 0.0001866493158693787,
      "loss": 0.5915,
      "step": 14658
    },
    {
      "epoch": 0.06895302783709795,
      "grad_norm": 2.5338289737701416,
      "learning_rate": 0.0001866483728912652,
      "loss": 0.3089,
      "step": 14659
    },
    {
      "epoch": 0.06895773163871041,
      "grad_norm": 3.5941450595855713,
      "learning_rate": 0.00018664742991315173,
      "loss": 0.4398,
      "step": 14660
    },
    {
      "epoch": 0.06896243544032286,
      "grad_norm": 2.4621191024780273,
      "learning_rate": 0.00018664648693503825,
      "loss": 0.3793,
      "step": 14661
    },
    {
      "epoch": 0.06896713924193533,
      "grad_norm": 6.551860809326172,
      "learning_rate": 0.00018664554395692476,
      "loss": 1.0207,
      "step": 14662
    },
    {
      "epoch": 0.0689718430435478,
      "grad_norm": 2.289823055267334,
      "learning_rate": 0.00018664460097881128,
      "loss": 0.3823,
      "step": 14663
    },
    {
      "epoch": 0.06897654684516026,
      "grad_norm": 1.8996039628982544,
      "learning_rate": 0.0001866436580006978,
      "loss": 0.3962,
      "step": 14664
    },
    {
      "epoch": 0.06898125064677273,
      "grad_norm": 2.146636486053467,
      "learning_rate": 0.00018664271502258432,
      "loss": 0.3243,
      "step": 14665
    },
    {
      "epoch": 0.06898595444838518,
      "grad_norm": 2.6234817504882812,
      "learning_rate": 0.00018664177204447087,
      "loss": 0.7814,
      "step": 14666
    },
    {
      "epoch": 0.06899065824999764,
      "grad_norm": 1.5764480829238892,
      "learning_rate": 0.00018664082906635738,
      "loss": 0.2279,
      "step": 14667
    },
    {
      "epoch": 0.06899536205161011,
      "grad_norm": 2.629849433898926,
      "learning_rate": 0.0001866398860882439,
      "loss": 0.6186,
      "step": 14668
    },
    {
      "epoch": 0.06900006585322258,
      "grad_norm": 1.2545225620269775,
      "learning_rate": 0.00018663894311013042,
      "loss": 0.1389,
      "step": 14669
    },
    {
      "epoch": 0.06900476965483504,
      "grad_norm": 3.79339337348938,
      "learning_rate": 0.00018663800013201697,
      "loss": 0.5632,
      "step": 14670
    },
    {
      "epoch": 0.0690094734564475,
      "grad_norm": 2.6611993312835693,
      "learning_rate": 0.00018663705715390346,
      "loss": 0.5175,
      "step": 14671
    },
    {
      "epoch": 0.06901417725805996,
      "grad_norm": 2.9049134254455566,
      "learning_rate": 0.00018663611417578998,
      "loss": 0.2782,
      "step": 14672
    },
    {
      "epoch": 0.06901888105967242,
      "grad_norm": 3.3665473461151123,
      "learning_rate": 0.0001866351711976765,
      "loss": 0.6085,
      "step": 14673
    },
    {
      "epoch": 0.06902358486128489,
      "grad_norm": 1.737037181854248,
      "learning_rate": 0.00018663422821956301,
      "loss": 0.2947,
      "step": 14674
    },
    {
      "epoch": 0.06902828866289736,
      "grad_norm": 1.2586809396743774,
      "learning_rate": 0.00018663328524144956,
      "loss": 0.3505,
      "step": 14675
    },
    {
      "epoch": 0.06903299246450982,
      "grad_norm": 2.2857372760772705,
      "learning_rate": 0.00018663234226333608,
      "loss": 0.4006,
      "step": 14676
    },
    {
      "epoch": 0.06903769626612229,
      "grad_norm": 1.5164763927459717,
      "learning_rate": 0.0001866313992852226,
      "loss": 0.2298,
      "step": 14677
    },
    {
      "epoch": 0.06904240006773474,
      "grad_norm": 1.2046070098876953,
      "learning_rate": 0.00018663045630710912,
      "loss": 0.2906,
      "step": 14678
    },
    {
      "epoch": 0.0690471038693472,
      "grad_norm": 0.8809193968772888,
      "learning_rate": 0.00018662951332899566,
      "loss": 0.1414,
      "step": 14679
    },
    {
      "epoch": 0.06905180767095967,
      "grad_norm": 1.5588418245315552,
      "learning_rate": 0.00018662857035088218,
      "loss": 0.4447,
      "step": 14680
    },
    {
      "epoch": 0.06905651147257214,
      "grad_norm": 1.6411534547805786,
      "learning_rate": 0.0001866276273727687,
      "loss": 0.2301,
      "step": 14681
    },
    {
      "epoch": 0.0690612152741846,
      "grad_norm": 0.9701086282730103,
      "learning_rate": 0.00018662668439465522,
      "loss": 0.3076,
      "step": 14682
    },
    {
      "epoch": 0.06906591907579705,
      "grad_norm": 0.4410889148712158,
      "learning_rate": 0.0001866257414165417,
      "loss": 0.0465,
      "step": 14683
    },
    {
      "epoch": 0.06907062287740952,
      "grad_norm": 3.0035383701324463,
      "learning_rate": 0.00018662479843842826,
      "loss": 0.571,
      "step": 14684
    },
    {
      "epoch": 0.06907532667902198,
      "grad_norm": 0.6361493468284607,
      "learning_rate": 0.00018662385546031477,
      "loss": 0.1006,
      "step": 14685
    },
    {
      "epoch": 0.06908003048063445,
      "grad_norm": 2.5663270950317383,
      "learning_rate": 0.0001866229124822013,
      "loss": 0.8347,
      "step": 14686
    },
    {
      "epoch": 0.06908473428224692,
      "grad_norm": 0.7952426671981812,
      "learning_rate": 0.0001866219695040878,
      "loss": 0.0973,
      "step": 14687
    },
    {
      "epoch": 0.06908943808385938,
      "grad_norm": 0.8925920128822327,
      "learning_rate": 0.00018662102652597436,
      "loss": 0.1021,
      "step": 14688
    },
    {
      "epoch": 0.06909414188547183,
      "grad_norm": 0.6341919898986816,
      "learning_rate": 0.00018662008354786088,
      "loss": 0.0773,
      "step": 14689
    },
    {
      "epoch": 0.0690988456870843,
      "grad_norm": 1.7130587100982666,
      "learning_rate": 0.0001866191405697474,
      "loss": 0.2407,
      "step": 14690
    },
    {
      "epoch": 0.06910354948869676,
      "grad_norm": 1.9975402355194092,
      "learning_rate": 0.0001866181975916339,
      "loss": 0.5624,
      "step": 14691
    },
    {
      "epoch": 0.06910825329030923,
      "grad_norm": 1.9140726327896118,
      "learning_rate": 0.00018661725461352043,
      "loss": 0.2727,
      "step": 14692
    },
    {
      "epoch": 0.0691129570919217,
      "grad_norm": 0.9662513732910156,
      "learning_rate": 0.00018661631163540695,
      "loss": 0.1268,
      "step": 14693
    },
    {
      "epoch": 0.06911766089353416,
      "grad_norm": 2.0704965591430664,
      "learning_rate": 0.00018661536865729347,
      "loss": 0.245,
      "step": 14694
    },
    {
      "epoch": 0.06912236469514661,
      "grad_norm": 1.001028060913086,
      "learning_rate": 0.00018661442567918,
      "loss": 0.1396,
      "step": 14695
    },
    {
      "epoch": 0.06912706849675908,
      "grad_norm": 2.495836019515991,
      "learning_rate": 0.0001866134827010665,
      "loss": 0.1581,
      "step": 14696
    },
    {
      "epoch": 0.06913177229837154,
      "grad_norm": 2.378067970275879,
      "learning_rate": 0.00018661253972295302,
      "loss": 0.5124,
      "step": 14697
    },
    {
      "epoch": 0.06913647609998401,
      "grad_norm": 0.8783842921257019,
      "learning_rate": 0.00018661159674483957,
      "loss": 0.0835,
      "step": 14698
    },
    {
      "epoch": 0.06914117990159648,
      "grad_norm": 1.2275331020355225,
      "learning_rate": 0.0001866106537667261,
      "loss": 0.1123,
      "step": 14699
    },
    {
      "epoch": 0.06914588370320893,
      "grad_norm": 1.821363091468811,
      "learning_rate": 0.0001866097107886126,
      "loss": 0.477,
      "step": 14700
    },
    {
      "epoch": 0.06915058750482139,
      "grad_norm": 1.6078206300735474,
      "learning_rate": 0.00018660876781049913,
      "loss": 0.0549,
      "step": 14701
    },
    {
      "epoch": 0.06915529130643386,
      "grad_norm": 4.932856559753418,
      "learning_rate": 0.00018660782483238565,
      "loss": 0.7879,
      "step": 14702
    },
    {
      "epoch": 0.06915999510804632,
      "grad_norm": 1.3364335298538208,
      "learning_rate": 0.00018660688185427216,
      "loss": 0.1453,
      "step": 14703
    },
    {
      "epoch": 0.06916469890965879,
      "grad_norm": 1.687027096748352,
      "learning_rate": 0.00018660593887615868,
      "loss": 0.1021,
      "step": 14704
    },
    {
      "epoch": 0.06916940271127126,
      "grad_norm": 3.344463348388672,
      "learning_rate": 0.0001866049958980452,
      "loss": 0.4985,
      "step": 14705
    },
    {
      "epoch": 0.0691741065128837,
      "grad_norm": 3.313593864440918,
      "learning_rate": 0.00018660405291993172,
      "loss": 0.465,
      "step": 14706
    },
    {
      "epoch": 0.06917881031449617,
      "grad_norm": 5.015538692474365,
      "learning_rate": 0.00018660310994181827,
      "loss": 0.3383,
      "step": 14707
    },
    {
      "epoch": 0.06918351411610864,
      "grad_norm": 4.362626075744629,
      "learning_rate": 0.00018660216696370478,
      "loss": 0.4596,
      "step": 14708
    },
    {
      "epoch": 0.0691882179177211,
      "grad_norm": 1.7497646808624268,
      "learning_rate": 0.0001866012239855913,
      "loss": 0.1994,
      "step": 14709
    },
    {
      "epoch": 0.06919292171933357,
      "grad_norm": 4.381258964538574,
      "learning_rate": 0.00018660028100747782,
      "loss": 0.5209,
      "step": 14710
    },
    {
      "epoch": 0.06919762552094604,
      "grad_norm": 1.8367239236831665,
      "learning_rate": 0.00018659933802936437,
      "loss": 0.1259,
      "step": 14711
    },
    {
      "epoch": 0.06920232932255849,
      "grad_norm": 0.6696908473968506,
      "learning_rate": 0.00018659839505125089,
      "loss": 0.0664,
      "step": 14712
    },
    {
      "epoch": 0.06920703312417095,
      "grad_norm": 1.6835469007492065,
      "learning_rate": 0.0001865974520731374,
      "loss": 0.2296,
      "step": 14713
    },
    {
      "epoch": 0.06921173692578342,
      "grad_norm": 2.652838945388794,
      "learning_rate": 0.0001865965090950239,
      "loss": 0.5024,
      "step": 14714
    },
    {
      "epoch": 0.06921644072739588,
      "grad_norm": 2.7853057384490967,
      "learning_rate": 0.00018659556611691041,
      "loss": 0.4577,
      "step": 14715
    },
    {
      "epoch": 0.06922114452900835,
      "grad_norm": 0.5866193771362305,
      "learning_rate": 0.00018659462313879696,
      "loss": 0.0539,
      "step": 14716
    },
    {
      "epoch": 0.0692258483306208,
      "grad_norm": 0.42151591181755066,
      "learning_rate": 0.00018659368016068348,
      "loss": 0.0643,
      "step": 14717
    },
    {
      "epoch": 0.06923055213223327,
      "grad_norm": 3.2644543647766113,
      "learning_rate": 0.00018659273718257,
      "loss": 0.3167,
      "step": 14718
    },
    {
      "epoch": 0.06923525593384573,
      "grad_norm": 2.5792291164398193,
      "learning_rate": 0.00018659179420445652,
      "loss": 0.2766,
      "step": 14719
    },
    {
      "epoch": 0.0692399597354582,
      "grad_norm": 5.678136348724365,
      "learning_rate": 0.00018659085122634306,
      "loss": 0.7628,
      "step": 14720
    },
    {
      "epoch": 0.06924466353707066,
      "grad_norm": 2.7841999530792236,
      "learning_rate": 0.00018658990824822958,
      "loss": 0.5341,
      "step": 14721
    },
    {
      "epoch": 0.06924936733868313,
      "grad_norm": 1.6452558040618896,
      "learning_rate": 0.0001865889652701161,
      "loss": 0.198,
      "step": 14722
    },
    {
      "epoch": 0.06925407114029558,
      "grad_norm": 1.5495495796203613,
      "learning_rate": 0.00018658802229200262,
      "loss": 0.2262,
      "step": 14723
    },
    {
      "epoch": 0.06925877494190805,
      "grad_norm": 3.0347483158111572,
      "learning_rate": 0.00018658707931388914,
      "loss": 0.6161,
      "step": 14724
    },
    {
      "epoch": 0.06926347874352051,
      "grad_norm": 2.0085794925689697,
      "learning_rate": 0.00018658613633577566,
      "loss": 0.1775,
      "step": 14725
    },
    {
      "epoch": 0.06926818254513298,
      "grad_norm": 2.4796204566955566,
      "learning_rate": 0.00018658519335766217,
      "loss": 0.377,
      "step": 14726
    },
    {
      "epoch": 0.06927288634674544,
      "grad_norm": 1.0459023714065552,
      "learning_rate": 0.0001865842503795487,
      "loss": 0.0971,
      "step": 14727
    },
    {
      "epoch": 0.06927759014835791,
      "grad_norm": 0.42588111758232117,
      "learning_rate": 0.0001865833074014352,
      "loss": 0.0365,
      "step": 14728
    },
    {
      "epoch": 0.06928229394997036,
      "grad_norm": 0.6220449209213257,
      "learning_rate": 0.00018658236442332176,
      "loss": 0.0554,
      "step": 14729
    },
    {
      "epoch": 0.06928699775158283,
      "grad_norm": 1.6193486452102661,
      "learning_rate": 0.00018658142144520828,
      "loss": 0.1896,
      "step": 14730
    },
    {
      "epoch": 0.06929170155319529,
      "grad_norm": 3.4432525634765625,
      "learning_rate": 0.0001865804784670948,
      "loss": 0.5786,
      "step": 14731
    },
    {
      "epoch": 0.06929640535480776,
      "grad_norm": 2.568056106567383,
      "learning_rate": 0.0001865795354889813,
      "loss": 0.4303,
      "step": 14732
    },
    {
      "epoch": 0.06930110915642022,
      "grad_norm": 0.3403241038322449,
      "learning_rate": 0.00018657859251086783,
      "loss": 0.0252,
      "step": 14733
    },
    {
      "epoch": 0.06930581295803268,
      "grad_norm": 0.8748241662979126,
      "learning_rate": 0.00018657764953275435,
      "loss": 0.097,
      "step": 14734
    },
    {
      "epoch": 0.06931051675964514,
      "grad_norm": 0.7390950322151184,
      "learning_rate": 0.00018657670655464087,
      "loss": 0.1214,
      "step": 14735
    },
    {
      "epoch": 0.0693152205612576,
      "grad_norm": 0.5917409658432007,
      "learning_rate": 0.0001865757635765274,
      "loss": 0.07,
      "step": 14736
    },
    {
      "epoch": 0.06931992436287007,
      "grad_norm": 3.4095523357391357,
      "learning_rate": 0.0001865748205984139,
      "loss": 0.4596,
      "step": 14737
    },
    {
      "epoch": 0.06932462816448254,
      "grad_norm": 5.098273277282715,
      "learning_rate": 0.00018657387762030045,
      "loss": 0.5061,
      "step": 14738
    },
    {
      "epoch": 0.069329331966095,
      "grad_norm": 2.1600821018218994,
      "learning_rate": 0.00018657293464218697,
      "loss": 0.2132,
      "step": 14739
    },
    {
      "epoch": 0.06933403576770746,
      "grad_norm": 3.4384381771087646,
      "learning_rate": 0.0001865719916640735,
      "loss": 0.215,
      "step": 14740
    },
    {
      "epoch": 0.06933873956931992,
      "grad_norm": 2.44936203956604,
      "learning_rate": 0.00018657104868596,
      "loss": 0.3722,
      "step": 14741
    },
    {
      "epoch": 0.06934344337093239,
      "grad_norm": 0.4219493567943573,
      "learning_rate": 0.00018657010570784653,
      "loss": 0.0457,
      "step": 14742
    },
    {
      "epoch": 0.06934814717254485,
      "grad_norm": 1.4731389284133911,
      "learning_rate": 0.00018656916272973307,
      "loss": 0.1293,
      "step": 14743
    },
    {
      "epoch": 0.06935285097415732,
      "grad_norm": 4.509004592895508,
      "learning_rate": 0.0001865682197516196,
      "loss": 0.6487,
      "step": 14744
    },
    {
      "epoch": 0.06935755477576978,
      "grad_norm": 1.9912930727005005,
      "learning_rate": 0.00018656727677350608,
      "loss": 0.1923,
      "step": 14745
    },
    {
      "epoch": 0.06936225857738224,
      "grad_norm": 2.409508466720581,
      "learning_rate": 0.0001865663337953926,
      "loss": 0.2015,
      "step": 14746
    },
    {
      "epoch": 0.0693669623789947,
      "grad_norm": 1.0789533853530884,
      "learning_rate": 0.00018656539081727915,
      "loss": 0.1215,
      "step": 14747
    },
    {
      "epoch": 0.06937166618060717,
      "grad_norm": 3.8560903072357178,
      "learning_rate": 0.00018656444783916567,
      "loss": 0.9587,
      "step": 14748
    },
    {
      "epoch": 0.06937636998221963,
      "grad_norm": 0.5275442600250244,
      "learning_rate": 0.00018656350486105218,
      "loss": 0.033,
      "step": 14749
    },
    {
      "epoch": 0.0693810737838321,
      "grad_norm": 2.471444606781006,
      "learning_rate": 0.0001865625618829387,
      "loss": 0.2525,
      "step": 14750
    },
    {
      "epoch": 0.06938577758544455,
      "grad_norm": 0.3283465802669525,
      "learning_rate": 0.00018656161890482522,
      "loss": 0.0258,
      "step": 14751
    },
    {
      "epoch": 0.06939048138705702,
      "grad_norm": 1.3367146253585815,
      "learning_rate": 0.00018656067592671177,
      "loss": 0.0572,
      "step": 14752
    },
    {
      "epoch": 0.06939518518866948,
      "grad_norm": 3.144721031188965,
      "learning_rate": 0.00018655973294859829,
      "loss": 0.5448,
      "step": 14753
    },
    {
      "epoch": 0.06939988899028195,
      "grad_norm": 3.9524428844451904,
      "learning_rate": 0.0001865587899704848,
      "loss": 0.5894,
      "step": 14754
    },
    {
      "epoch": 0.06940459279189441,
      "grad_norm": 1.9797526597976685,
      "learning_rate": 0.00018655784699237132,
      "loss": 0.1336,
      "step": 14755
    },
    {
      "epoch": 0.06940929659350688,
      "grad_norm": 4.154269218444824,
      "learning_rate": 0.00018655690401425781,
      "loss": 0.5173,
      "step": 14756
    },
    {
      "epoch": 0.06941400039511933,
      "grad_norm": 5.050866603851318,
      "learning_rate": 0.00018655596103614436,
      "loss": 0.5046,
      "step": 14757
    },
    {
      "epoch": 0.0694187041967318,
      "grad_norm": 1.587592601776123,
      "learning_rate": 0.00018655501805803088,
      "loss": 0.1001,
      "step": 14758
    },
    {
      "epoch": 0.06942340799834426,
      "grad_norm": 2.493560314178467,
      "learning_rate": 0.0001865540750799174,
      "loss": 0.2268,
      "step": 14759
    },
    {
      "epoch": 0.06942811179995673,
      "grad_norm": 1.8679218292236328,
      "learning_rate": 0.00018655313210180392,
      "loss": 0.1905,
      "step": 14760
    },
    {
      "epoch": 0.06943281560156919,
      "grad_norm": 2.310899019241333,
      "learning_rate": 0.00018655218912369046,
      "loss": 0.2131,
      "step": 14761
    },
    {
      "epoch": 0.06943751940318166,
      "grad_norm": 3.101407766342163,
      "learning_rate": 0.00018655124614557698,
      "loss": 0.2447,
      "step": 14762
    },
    {
      "epoch": 0.06944222320479411,
      "grad_norm": 4.1411542892456055,
      "learning_rate": 0.0001865503031674635,
      "loss": 0.6075,
      "step": 14763
    },
    {
      "epoch": 0.06944692700640658,
      "grad_norm": 1.637996792793274,
      "learning_rate": 0.00018654936018935002,
      "loss": 0.0807,
      "step": 14764
    },
    {
      "epoch": 0.06945163080801904,
      "grad_norm": 1.7720011472702026,
      "learning_rate": 0.00018654841721123654,
      "loss": 0.172,
      "step": 14765
    },
    {
      "epoch": 0.0694563346096315,
      "grad_norm": 3.359858989715576,
      "learning_rate": 0.00018654747423312306,
      "loss": 0.3018,
      "step": 14766
    },
    {
      "epoch": 0.06946103841124397,
      "grad_norm": 2.7403178215026855,
      "learning_rate": 0.00018654653125500957,
      "loss": 0.1914,
      "step": 14767
    },
    {
      "epoch": 0.06946574221285642,
      "grad_norm": 1.1692674160003662,
      "learning_rate": 0.0001865455882768961,
      "loss": 0.0767,
      "step": 14768
    },
    {
      "epoch": 0.06947044601446889,
      "grad_norm": 2.6266162395477295,
      "learning_rate": 0.0001865446452987826,
      "loss": 0.3366,
      "step": 14769
    },
    {
      "epoch": 0.06947514981608136,
      "grad_norm": 4.537539005279541,
      "learning_rate": 0.00018654370232066916,
      "loss": 0.7944,
      "step": 14770
    },
    {
      "epoch": 0.06947985361769382,
      "grad_norm": 0.7776322960853577,
      "learning_rate": 0.00018654275934255568,
      "loss": 0.0647,
      "step": 14771
    },
    {
      "epoch": 0.06948455741930629,
      "grad_norm": 2.013148069381714,
      "learning_rate": 0.0001865418163644422,
      "loss": 0.2811,
      "step": 14772
    },
    {
      "epoch": 0.06948926122091875,
      "grad_norm": 1.9186899662017822,
      "learning_rate": 0.0001865408733863287,
      "loss": 0.1563,
      "step": 14773
    },
    {
      "epoch": 0.0694939650225312,
      "grad_norm": 3.118232488632202,
      "learning_rate": 0.00018653993040821523,
      "loss": 0.3115,
      "step": 14774
    },
    {
      "epoch": 0.06949866882414367,
      "grad_norm": 1.751295566558838,
      "learning_rate": 0.00018653898743010178,
      "loss": 0.1916,
      "step": 14775
    },
    {
      "epoch": 0.06950337262575614,
      "grad_norm": 1.1823338270187378,
      "learning_rate": 0.00018653804445198827,
      "loss": 0.0863,
      "step": 14776
    },
    {
      "epoch": 0.0695080764273686,
      "grad_norm": 0.21076224744319916,
      "learning_rate": 0.0001865371014738748,
      "loss": 0.0237,
      "step": 14777
    },
    {
      "epoch": 0.06951278022898107,
      "grad_norm": 1.9089479446411133,
      "learning_rate": 0.0001865361584957613,
      "loss": 0.1534,
      "step": 14778
    },
    {
      "epoch": 0.06951748403059353,
      "grad_norm": 2.216134786605835,
      "learning_rate": 0.00018653521551764785,
      "loss": 0.2587,
      "step": 14779
    },
    {
      "epoch": 0.06952218783220598,
      "grad_norm": 5.618656635284424,
      "learning_rate": 0.00018653427253953437,
      "loss": 0.4746,
      "step": 14780
    },
    {
      "epoch": 0.06952689163381845,
      "grad_norm": 1.9381505250930786,
      "learning_rate": 0.0001865333295614209,
      "loss": 0.1688,
      "step": 14781
    },
    {
      "epoch": 0.06953159543543092,
      "grad_norm": 0.8347586393356323,
      "learning_rate": 0.0001865323865833074,
      "loss": 0.101,
      "step": 14782
    },
    {
      "epoch": 0.06953629923704338,
      "grad_norm": 1.0494686365127563,
      "learning_rate": 0.00018653144360519393,
      "loss": 0.0633,
      "step": 14783
    },
    {
      "epoch": 0.06954100303865585,
      "grad_norm": 0.20900791883468628,
      "learning_rate": 0.00018653050062708047,
      "loss": 0.0095,
      "step": 14784
    },
    {
      "epoch": 0.0695457068402683,
      "grad_norm": 0.6716387867927551,
      "learning_rate": 0.000186529557648967,
      "loss": 0.0568,
      "step": 14785
    },
    {
      "epoch": 0.06955041064188076,
      "grad_norm": 4.128871440887451,
      "learning_rate": 0.0001865286146708535,
      "loss": 0.7159,
      "step": 14786
    },
    {
      "epoch": 0.06955511444349323,
      "grad_norm": 5.333256244659424,
      "learning_rate": 0.00018652767169274,
      "loss": 0.6841,
      "step": 14787
    },
    {
      "epoch": 0.0695598182451057,
      "grad_norm": 3.677128553390503,
      "learning_rate": 0.00018652672871462655,
      "loss": 0.4431,
      "step": 14788
    },
    {
      "epoch": 0.06956452204671816,
      "grad_norm": 2.3852925300598145,
      "learning_rate": 0.00018652578573651307,
      "loss": 0.1599,
      "step": 14789
    },
    {
      "epoch": 0.06956922584833063,
      "grad_norm": 1.302330493927002,
      "learning_rate": 0.00018652484275839958,
      "loss": 0.0733,
      "step": 14790
    },
    {
      "epoch": 0.06957392964994308,
      "grad_norm": 2.4952380657196045,
      "learning_rate": 0.0001865238997802861,
      "loss": 0.2089,
      "step": 14791
    },
    {
      "epoch": 0.06957863345155554,
      "grad_norm": 0.2897787094116211,
      "learning_rate": 0.00018652295680217262,
      "loss": 0.0351,
      "step": 14792
    },
    {
      "epoch": 0.06958333725316801,
      "grad_norm": 0.07498354464769363,
      "learning_rate": 0.00018652201382405917,
      "loss": 0.0033,
      "step": 14793
    },
    {
      "epoch": 0.06958804105478048,
      "grad_norm": 1.6761083602905273,
      "learning_rate": 0.00018652107084594569,
      "loss": 0.0505,
      "step": 14794
    },
    {
      "epoch": 0.06959274485639294,
      "grad_norm": 2.3342111110687256,
      "learning_rate": 0.0001865201278678322,
      "loss": 0.275,
      "step": 14795
    },
    {
      "epoch": 0.0695974486580054,
      "grad_norm": 0.2131544053554535,
      "learning_rate": 0.00018651918488971872,
      "loss": 0.0203,
      "step": 14796
    },
    {
      "epoch": 0.06960215245961786,
      "grad_norm": 0.1532088816165924,
      "learning_rate": 0.00018651824191160524,
      "loss": 0.0096,
      "step": 14797
    },
    {
      "epoch": 0.06960685626123032,
      "grad_norm": 3.981450080871582,
      "learning_rate": 0.00018651729893349176,
      "loss": 0.4343,
      "step": 14798
    },
    {
      "epoch": 0.06961156006284279,
      "grad_norm": 4.123570442199707,
      "learning_rate": 0.00018651635595537828,
      "loss": 0.456,
      "step": 14799
    },
    {
      "epoch": 0.06961626386445526,
      "grad_norm": 4.86378812789917,
      "learning_rate": 0.0001865154129772648,
      "loss": 0.7668,
      "step": 14800
    },
    {
      "epoch": 0.06962096766606772,
      "grad_norm": 3.5950474739074707,
      "learning_rate": 0.00018651446999915132,
      "loss": 0.1829,
      "step": 14801
    },
    {
      "epoch": 0.06962567146768017,
      "grad_norm": 4.153420925140381,
      "learning_rate": 0.00018651352702103786,
      "loss": 0.4044,
      "step": 14802
    },
    {
      "epoch": 0.06963037526929264,
      "grad_norm": 0.2934139668941498,
      "learning_rate": 0.00018651258404292438,
      "loss": 0.0218,
      "step": 14803
    },
    {
      "epoch": 0.0696350790709051,
      "grad_norm": 2.4829304218292236,
      "learning_rate": 0.0001865116410648109,
      "loss": 0.2309,
      "step": 14804
    },
    {
      "epoch": 0.06963978287251757,
      "grad_norm": 5.268500328063965,
      "learning_rate": 0.00018651069808669742,
      "loss": 0.3892,
      "step": 14805
    },
    {
      "epoch": 0.06964448667413004,
      "grad_norm": 4.135249614715576,
      "learning_rate": 0.00018650975510858394,
      "loss": 0.4431,
      "step": 14806
    },
    {
      "epoch": 0.0696491904757425,
      "grad_norm": 3.598200559616089,
      "learning_rate": 0.00018650881213047046,
      "loss": 0.7259,
      "step": 14807
    },
    {
      "epoch": 0.06965389427735495,
      "grad_norm": 2.910153388977051,
      "learning_rate": 0.00018650786915235697,
      "loss": 0.214,
      "step": 14808
    },
    {
      "epoch": 0.06965859807896742,
      "grad_norm": 3.641932249069214,
      "learning_rate": 0.0001865069261742435,
      "loss": 0.5037,
      "step": 14809
    },
    {
      "epoch": 0.06966330188057988,
      "grad_norm": 2.7082724571228027,
      "learning_rate": 0.00018650598319613,
      "loss": 0.3113,
      "step": 14810
    },
    {
      "epoch": 0.06966800568219235,
      "grad_norm": 4.848553657531738,
      "learning_rate": 0.00018650504021801656,
      "loss": 0.6357,
      "step": 14811
    },
    {
      "epoch": 0.06967270948380481,
      "grad_norm": 2.4872019290924072,
      "learning_rate": 0.00018650409723990308,
      "loss": 0.2017,
      "step": 14812
    },
    {
      "epoch": 0.06967741328541728,
      "grad_norm": 7.353672504425049,
      "learning_rate": 0.0001865031542617896,
      "loss": 0.416,
      "step": 14813
    },
    {
      "epoch": 0.06968211708702973,
      "grad_norm": 2.5482513904571533,
      "learning_rate": 0.0001865022112836761,
      "loss": 0.2554,
      "step": 14814
    },
    {
      "epoch": 0.0696868208886422,
      "grad_norm": 2.792541027069092,
      "learning_rate": 0.00018650126830556263,
      "loss": 0.5894,
      "step": 14815
    },
    {
      "epoch": 0.06969152469025466,
      "grad_norm": 2.706540584564209,
      "learning_rate": 0.00018650032532744918,
      "loss": 0.2818,
      "step": 14816
    },
    {
      "epoch": 0.06969622849186713,
      "grad_norm": 1.9749163389205933,
      "learning_rate": 0.0001864993823493357,
      "loss": 0.2046,
      "step": 14817
    },
    {
      "epoch": 0.0697009322934796,
      "grad_norm": 0.1473001092672348,
      "learning_rate": 0.0001864984393712222,
      "loss": 0.0161,
      "step": 14818
    },
    {
      "epoch": 0.06970563609509205,
      "grad_norm": 2.8311245441436768,
      "learning_rate": 0.0001864974963931087,
      "loss": 0.4406,
      "step": 14819
    },
    {
      "epoch": 0.06971033989670451,
      "grad_norm": 2.504704236984253,
      "learning_rate": 0.00018649655341499525,
      "loss": 0.4609,
      "step": 14820
    },
    {
      "epoch": 0.06971504369831698,
      "grad_norm": 1.5111420154571533,
      "learning_rate": 0.00018649561043688177,
      "loss": 0.1634,
      "step": 14821
    },
    {
      "epoch": 0.06971974749992944,
      "grad_norm": 0.4951435923576355,
      "learning_rate": 0.0001864946674587683,
      "loss": 0.0416,
      "step": 14822
    },
    {
      "epoch": 0.06972445130154191,
      "grad_norm": 4.107355117797852,
      "learning_rate": 0.0001864937244806548,
      "loss": 0.6187,
      "step": 14823
    },
    {
      "epoch": 0.06972915510315437,
      "grad_norm": 4.012157917022705,
      "learning_rate": 0.00018649278150254133,
      "loss": 0.5257,
      "step": 14824
    },
    {
      "epoch": 0.06973385890476683,
      "grad_norm": 0.38965651392936707,
      "learning_rate": 0.00018649183852442787,
      "loss": 0.0299,
      "step": 14825
    },
    {
      "epoch": 0.06973856270637929,
      "grad_norm": 3.8240621089935303,
      "learning_rate": 0.0001864908955463144,
      "loss": 0.7351,
      "step": 14826
    },
    {
      "epoch": 0.06974326650799176,
      "grad_norm": 2.490887403488159,
      "learning_rate": 0.0001864899525682009,
      "loss": 0.2385,
      "step": 14827
    },
    {
      "epoch": 0.06974797030960422,
      "grad_norm": 2.846003293991089,
      "learning_rate": 0.00018648900959008743,
      "loss": 0.2558,
      "step": 14828
    },
    {
      "epoch": 0.06975267411121669,
      "grad_norm": 6.559866428375244,
      "learning_rate": 0.00018648806661197395,
      "loss": 0.5697,
      "step": 14829
    },
    {
      "epoch": 0.06975737791282915,
      "grad_norm": 2.3296897411346436,
      "learning_rate": 0.00018648712363386047,
      "loss": 0.2647,
      "step": 14830
    },
    {
      "epoch": 0.0697620817144416,
      "grad_norm": 1.262308955192566,
      "learning_rate": 0.00018648618065574698,
      "loss": 0.1667,
      "step": 14831
    },
    {
      "epoch": 0.06976678551605407,
      "grad_norm": 0.6262804269790649,
      "learning_rate": 0.0001864852376776335,
      "loss": 0.1079,
      "step": 14832
    },
    {
      "epoch": 0.06977148931766654,
      "grad_norm": 1.4364594221115112,
      "learning_rate": 0.00018648429469952002,
      "loss": 0.1363,
      "step": 14833
    },
    {
      "epoch": 0.069776193119279,
      "grad_norm": 2.4154152870178223,
      "learning_rate": 0.00018648335172140657,
      "loss": 0.3067,
      "step": 14834
    },
    {
      "epoch": 0.06978089692089147,
      "grad_norm": 2.1768605709075928,
      "learning_rate": 0.00018648240874329309,
      "loss": 0.2576,
      "step": 14835
    },
    {
      "epoch": 0.06978560072250392,
      "grad_norm": 1.6294586658477783,
      "learning_rate": 0.0001864814657651796,
      "loss": 0.1826,
      "step": 14836
    },
    {
      "epoch": 0.06979030452411639,
      "grad_norm": 0.5785303115844727,
      "learning_rate": 0.00018648052278706612,
      "loss": 0.0432,
      "step": 14837
    },
    {
      "epoch": 0.06979500832572885,
      "grad_norm": 5.437489986419678,
      "learning_rate": 0.00018647957980895264,
      "loss": 0.5835,
      "step": 14838
    },
    {
      "epoch": 0.06979971212734132,
      "grad_norm": 1.1913667917251587,
      "learning_rate": 0.00018647863683083916,
      "loss": 0.1632,
      "step": 14839
    },
    {
      "epoch": 0.06980441592895378,
      "grad_norm": 0.8780132532119751,
      "learning_rate": 0.00018647769385272568,
      "loss": 0.0921,
      "step": 14840
    },
    {
      "epoch": 0.06980911973056625,
      "grad_norm": 1.5136287212371826,
      "learning_rate": 0.0001864767508746122,
      "loss": 0.1344,
      "step": 14841
    },
    {
      "epoch": 0.0698138235321787,
      "grad_norm": 2.1663286685943604,
      "learning_rate": 0.00018647580789649872,
      "loss": 0.2577,
      "step": 14842
    },
    {
      "epoch": 0.06981852733379117,
      "grad_norm": 0.6189574599266052,
      "learning_rate": 0.00018647486491838526,
      "loss": 0.0493,
      "step": 14843
    },
    {
      "epoch": 0.06982323113540363,
      "grad_norm": 0.9113321900367737,
      "learning_rate": 0.00018647392194027178,
      "loss": 0.1047,
      "step": 14844
    },
    {
      "epoch": 0.0698279349370161,
      "grad_norm": 3.5811896324157715,
      "learning_rate": 0.0001864729789621583,
      "loss": 0.2335,
      "step": 14845
    },
    {
      "epoch": 0.06983263873862856,
      "grad_norm": 1.5792577266693115,
      "learning_rate": 0.00018647203598404482,
      "loss": 0.1776,
      "step": 14846
    },
    {
      "epoch": 0.06983734254024103,
      "grad_norm": 2.5192148685455322,
      "learning_rate": 0.00018647109300593136,
      "loss": 0.6207,
      "step": 14847
    },
    {
      "epoch": 0.06984204634185348,
      "grad_norm": 3.039886236190796,
      "learning_rate": 0.00018647015002781788,
      "loss": 0.6672,
      "step": 14848
    },
    {
      "epoch": 0.06984675014346595,
      "grad_norm": 2.8443779945373535,
      "learning_rate": 0.00018646920704970437,
      "loss": 0.3996,
      "step": 14849
    },
    {
      "epoch": 0.06985145394507841,
      "grad_norm": 4.9224348068237305,
      "learning_rate": 0.0001864682640715909,
      "loss": 0.7144,
      "step": 14850
    },
    {
      "epoch": 0.06985615774669088,
      "grad_norm": 1.1042529344558716,
      "learning_rate": 0.0001864673210934774,
      "loss": 0.0877,
      "step": 14851
    },
    {
      "epoch": 0.06986086154830334,
      "grad_norm": 1.452719807624817,
      "learning_rate": 0.00018646637811536396,
      "loss": 0.106,
      "step": 14852
    },
    {
      "epoch": 0.0698655653499158,
      "grad_norm": 2.8881661891937256,
      "learning_rate": 0.00018646543513725048,
      "loss": 0.4021,
      "step": 14853
    },
    {
      "epoch": 0.06987026915152826,
      "grad_norm": 3.187378168106079,
      "learning_rate": 0.000186464492159137,
      "loss": 0.5879,
      "step": 14854
    },
    {
      "epoch": 0.06987497295314073,
      "grad_norm": 0.18963155150413513,
      "learning_rate": 0.0001864635491810235,
      "loss": 0.0089,
      "step": 14855
    },
    {
      "epoch": 0.06987967675475319,
      "grad_norm": 3.7580490112304688,
      "learning_rate": 0.00018646260620291003,
      "loss": 0.4012,
      "step": 14856
    },
    {
      "epoch": 0.06988438055636566,
      "grad_norm": 2.129490375518799,
      "learning_rate": 0.00018646166322479658,
      "loss": 0.2831,
      "step": 14857
    },
    {
      "epoch": 0.06988908435797812,
      "grad_norm": 8.793686866760254,
      "learning_rate": 0.0001864607202466831,
      "loss": 0.4318,
      "step": 14858
    },
    {
      "epoch": 0.06989378815959058,
      "grad_norm": 3.4160778522491455,
      "learning_rate": 0.00018645977726856961,
      "loss": 0.5668,
      "step": 14859
    },
    {
      "epoch": 0.06989849196120304,
      "grad_norm": 3.6373291015625,
      "learning_rate": 0.0001864588342904561,
      "loss": 0.6094,
      "step": 14860
    },
    {
      "epoch": 0.0699031957628155,
      "grad_norm": 0.29284965991973877,
      "learning_rate": 0.00018645789131234265,
      "loss": 0.0106,
      "step": 14861
    },
    {
      "epoch": 0.06990789956442797,
      "grad_norm": 1.3961870670318604,
      "learning_rate": 0.00018645694833422917,
      "loss": 0.1432,
      "step": 14862
    },
    {
      "epoch": 0.06991260336604044,
      "grad_norm": 3.364305257797241,
      "learning_rate": 0.0001864560053561157,
      "loss": 0.5488,
      "step": 14863
    },
    {
      "epoch": 0.0699173071676529,
      "grad_norm": 0.7317660450935364,
      "learning_rate": 0.0001864550623780022,
      "loss": 0.0666,
      "step": 14864
    },
    {
      "epoch": 0.06992201096926536,
      "grad_norm": 2.553971529006958,
      "learning_rate": 0.00018645411939988873,
      "loss": 0.3138,
      "step": 14865
    },
    {
      "epoch": 0.06992671477087782,
      "grad_norm": 3.9626317024230957,
      "learning_rate": 0.00018645317642177527,
      "loss": 0.3113,
      "step": 14866
    },
    {
      "epoch": 0.06993141857249029,
      "grad_norm": 2.4122304916381836,
      "learning_rate": 0.0001864522334436618,
      "loss": 0.3779,
      "step": 14867
    },
    {
      "epoch": 0.06993612237410275,
      "grad_norm": 3.833477258682251,
      "learning_rate": 0.0001864512904655483,
      "loss": 0.4867,
      "step": 14868
    },
    {
      "epoch": 0.06994082617571522,
      "grad_norm": 2.177661895751953,
      "learning_rate": 0.00018645034748743483,
      "loss": 0.2843,
      "step": 14869
    },
    {
      "epoch": 0.06994552997732767,
      "grad_norm": 3.742319107055664,
      "learning_rate": 0.00018644940450932135,
      "loss": 0.2769,
      "step": 14870
    },
    {
      "epoch": 0.06995023377894014,
      "grad_norm": 1.8550745248794556,
      "learning_rate": 0.00018644846153120787,
      "loss": 0.1387,
      "step": 14871
    },
    {
      "epoch": 0.0699549375805526,
      "grad_norm": 2.3608791828155518,
      "learning_rate": 0.00018644751855309438,
      "loss": 0.3295,
      "step": 14872
    },
    {
      "epoch": 0.06995964138216507,
      "grad_norm": 1.5847519636154175,
      "learning_rate": 0.0001864465755749809,
      "loss": 0.181,
      "step": 14873
    },
    {
      "epoch": 0.06996434518377753,
      "grad_norm": 1.4780560731887817,
      "learning_rate": 0.00018644563259686742,
      "loss": 0.1224,
      "step": 14874
    },
    {
      "epoch": 0.06996904898539,
      "grad_norm": 1.0070319175720215,
      "learning_rate": 0.00018644468961875397,
      "loss": 0.0725,
      "step": 14875
    },
    {
      "epoch": 0.06997375278700245,
      "grad_norm": 2.2591123580932617,
      "learning_rate": 0.00018644374664064049,
      "loss": 0.2157,
      "step": 14876
    },
    {
      "epoch": 0.06997845658861492,
      "grad_norm": 1.5551786422729492,
      "learning_rate": 0.000186442803662527,
      "loss": 0.1868,
      "step": 14877
    },
    {
      "epoch": 0.06998316039022738,
      "grad_norm": 0.7404497861862183,
      "learning_rate": 0.00018644186068441352,
      "loss": 0.0834,
      "step": 14878
    },
    {
      "epoch": 0.06998786419183985,
      "grad_norm": 1.100997805595398,
      "learning_rate": 0.00018644091770630007,
      "loss": 0.084,
      "step": 14879
    },
    {
      "epoch": 0.06999256799345231,
      "grad_norm": 0.7876387238502502,
      "learning_rate": 0.00018643997472818656,
      "loss": 0.071,
      "step": 14880
    },
    {
      "epoch": 0.06999727179506478,
      "grad_norm": 2.190096378326416,
      "learning_rate": 0.00018643903175007308,
      "loss": 0.2482,
      "step": 14881
    },
    {
      "epoch": 0.07000197559667723,
      "grad_norm": 2.3119335174560547,
      "learning_rate": 0.0001864380887719596,
      "loss": 0.3733,
      "step": 14882
    },
    {
      "epoch": 0.0700066793982897,
      "grad_norm": 3.263808250427246,
      "learning_rate": 0.00018643714579384612,
      "loss": 0.5429,
      "step": 14883
    },
    {
      "epoch": 0.07001138319990216,
      "grad_norm": 2.226104497909546,
      "learning_rate": 0.00018643620281573266,
      "loss": 0.1408,
      "step": 14884
    },
    {
      "epoch": 0.07001608700151463,
      "grad_norm": 1.195852279663086,
      "learning_rate": 0.00018643525983761918,
      "loss": 0.1363,
      "step": 14885
    },
    {
      "epoch": 0.07002079080312709,
      "grad_norm": 3.122947931289673,
      "learning_rate": 0.0001864343168595057,
      "loss": 0.4404,
      "step": 14886
    },
    {
      "epoch": 0.07002549460473954,
      "grad_norm": 2.8621368408203125,
      "learning_rate": 0.00018643337388139222,
      "loss": 0.5483,
      "step": 14887
    },
    {
      "epoch": 0.07003019840635201,
      "grad_norm": 2.4416310787200928,
      "learning_rate": 0.00018643243090327876,
      "loss": 0.4119,
      "step": 14888
    },
    {
      "epoch": 0.07003490220796448,
      "grad_norm": 3.781639814376831,
      "learning_rate": 0.00018643148792516528,
      "loss": 0.7036,
      "step": 14889
    },
    {
      "epoch": 0.07003960600957694,
      "grad_norm": 1.5423659086227417,
      "learning_rate": 0.0001864305449470518,
      "loss": 0.2017,
      "step": 14890
    },
    {
      "epoch": 0.0700443098111894,
      "grad_norm": 1.8006348609924316,
      "learning_rate": 0.0001864296019689383,
      "loss": 0.189,
      "step": 14891
    },
    {
      "epoch": 0.07004901361280187,
      "grad_norm": 1.6832361221313477,
      "learning_rate": 0.0001864286589908248,
      "loss": 0.2761,
      "step": 14892
    },
    {
      "epoch": 0.07005371741441432,
      "grad_norm": 0.9730182886123657,
      "learning_rate": 0.00018642771601271136,
      "loss": 0.0917,
      "step": 14893
    },
    {
      "epoch": 0.07005842121602679,
      "grad_norm": 1.4279576539993286,
      "learning_rate": 0.00018642677303459788,
      "loss": 0.2,
      "step": 14894
    },
    {
      "epoch": 0.07006312501763926,
      "grad_norm": 1.5990318059921265,
      "learning_rate": 0.0001864258300564844,
      "loss": 0.1678,
      "step": 14895
    },
    {
      "epoch": 0.07006782881925172,
      "grad_norm": 2.2949960231781006,
      "learning_rate": 0.0001864248870783709,
      "loss": 0.3175,
      "step": 14896
    },
    {
      "epoch": 0.07007253262086419,
      "grad_norm": 0.9593467116355896,
      "learning_rate": 0.00018642394410025746,
      "loss": 0.1202,
      "step": 14897
    },
    {
      "epoch": 0.07007723642247665,
      "grad_norm": 0.5770426988601685,
      "learning_rate": 0.00018642300112214398,
      "loss": 0.0832,
      "step": 14898
    },
    {
      "epoch": 0.0700819402240891,
      "grad_norm": 1.1148786544799805,
      "learning_rate": 0.0001864220581440305,
      "loss": 0.1148,
      "step": 14899
    },
    {
      "epoch": 0.07008664402570157,
      "grad_norm": 2.5197744369506836,
      "learning_rate": 0.00018642111516591701,
      "loss": 0.5923,
      "step": 14900
    },
    {
      "epoch": 0.07009134782731404,
      "grad_norm": 1.1800459623336792,
      "learning_rate": 0.00018642017218780353,
      "loss": 0.0957,
      "step": 14901
    },
    {
      "epoch": 0.0700960516289265,
      "grad_norm": 2.3197550773620605,
      "learning_rate": 0.00018641922920969005,
      "loss": 0.1651,
      "step": 14902
    },
    {
      "epoch": 0.07010075543053897,
      "grad_norm": 3.3228964805603027,
      "learning_rate": 0.00018641828623157657,
      "loss": 0.6443,
      "step": 14903
    },
    {
      "epoch": 0.07010545923215142,
      "grad_norm": 0.7799232006072998,
      "learning_rate": 0.0001864173432534631,
      "loss": 0.034,
      "step": 14904
    },
    {
      "epoch": 0.07011016303376388,
      "grad_norm": 1.330046534538269,
      "learning_rate": 0.0001864164002753496,
      "loss": 0.1108,
      "step": 14905
    },
    {
      "epoch": 0.07011486683537635,
      "grad_norm": 2.285316228866577,
      "learning_rate": 0.00018641545729723613,
      "loss": 0.3769,
      "step": 14906
    },
    {
      "epoch": 0.07011957063698881,
      "grad_norm": 1.8200839757919312,
      "learning_rate": 0.00018641451431912267,
      "loss": 0.2199,
      "step": 14907
    },
    {
      "epoch": 0.07012427443860128,
      "grad_norm": 1.4121602773666382,
      "learning_rate": 0.0001864135713410092,
      "loss": 0.1684,
      "step": 14908
    },
    {
      "epoch": 0.07012897824021375,
      "grad_norm": 1.774410367012024,
      "learning_rate": 0.0001864126283628957,
      "loss": 0.2283,
      "step": 14909
    },
    {
      "epoch": 0.0701336820418262,
      "grad_norm": 1.1984634399414062,
      "learning_rate": 0.00018641168538478223,
      "loss": 0.1328,
      "step": 14910
    },
    {
      "epoch": 0.07013838584343866,
      "grad_norm": 1.3450671434402466,
      "learning_rate": 0.00018641074240666875,
      "loss": 0.1047,
      "step": 14911
    },
    {
      "epoch": 0.07014308964505113,
      "grad_norm": 1.972313642501831,
      "learning_rate": 0.00018640979942855527,
      "loss": 0.2621,
      "step": 14912
    },
    {
      "epoch": 0.0701477934466636,
      "grad_norm": 1.124161720275879,
      "learning_rate": 0.00018640885645044178,
      "loss": 0.1506,
      "step": 14913
    },
    {
      "epoch": 0.07015249724827606,
      "grad_norm": 1.6465084552764893,
      "learning_rate": 0.0001864079134723283,
      "loss": 0.1709,
      "step": 14914
    },
    {
      "epoch": 0.07015720104988853,
      "grad_norm": 2.7927627563476562,
      "learning_rate": 0.00018640697049421482,
      "loss": 0.2356,
      "step": 14915
    },
    {
      "epoch": 0.07016190485150098,
      "grad_norm": 0.4808308482170105,
      "learning_rate": 0.00018640602751610137,
      "loss": 0.073,
      "step": 14916
    },
    {
      "epoch": 0.07016660865311344,
      "grad_norm": 4.504615306854248,
      "learning_rate": 0.00018640508453798789,
      "loss": 0.5207,
      "step": 14917
    },
    {
      "epoch": 0.07017131245472591,
      "grad_norm": 1.9485771656036377,
      "learning_rate": 0.0001864041415598744,
      "loss": 0.3266,
      "step": 14918
    },
    {
      "epoch": 0.07017601625633837,
      "grad_norm": 2.578457832336426,
      "learning_rate": 0.00018640319858176092,
      "loss": 0.2587,
      "step": 14919
    },
    {
      "epoch": 0.07018072005795084,
      "grad_norm": 1.1120705604553223,
      "learning_rate": 0.00018640225560364747,
      "loss": 0.1262,
      "step": 14920
    },
    {
      "epoch": 0.07018542385956329,
      "grad_norm": 2.946208953857422,
      "learning_rate": 0.000186401312625534,
      "loss": 0.3541,
      "step": 14921
    },
    {
      "epoch": 0.07019012766117576,
      "grad_norm": 2.8965795040130615,
      "learning_rate": 0.00018640036964742048,
      "loss": 0.3559,
      "step": 14922
    },
    {
      "epoch": 0.07019483146278822,
      "grad_norm": 0.944067120552063,
      "learning_rate": 0.000186399426669307,
      "loss": 0.0614,
      "step": 14923
    },
    {
      "epoch": 0.07019953526440069,
      "grad_norm": 3.4642856121063232,
      "learning_rate": 0.00018639848369119352,
      "loss": 0.6029,
      "step": 14924
    },
    {
      "epoch": 0.07020423906601315,
      "grad_norm": 2.2873923778533936,
      "learning_rate": 0.00018639754071308006,
      "loss": 0.195,
      "step": 14925
    },
    {
      "epoch": 0.07020894286762562,
      "grad_norm": 2.385472536087036,
      "learning_rate": 0.00018639659773496658,
      "loss": 0.2128,
      "step": 14926
    },
    {
      "epoch": 0.07021364666923807,
      "grad_norm": 1.5671790838241577,
      "learning_rate": 0.0001863956547568531,
      "loss": 0.1928,
      "step": 14927
    },
    {
      "epoch": 0.07021835047085054,
      "grad_norm": 0.13458189368247986,
      "learning_rate": 0.00018639471177873962,
      "loss": 0.0081,
      "step": 14928
    },
    {
      "epoch": 0.070223054272463,
      "grad_norm": 2.646881103515625,
      "learning_rate": 0.00018639376880062616,
      "loss": 0.4549,
      "step": 14929
    },
    {
      "epoch": 0.07022775807407547,
      "grad_norm": 1.6617848873138428,
      "learning_rate": 0.00018639282582251268,
      "loss": 0.2875,
      "step": 14930
    },
    {
      "epoch": 0.07023246187568793,
      "grad_norm": 3.598202705383301,
      "learning_rate": 0.0001863918828443992,
      "loss": 0.4036,
      "step": 14931
    },
    {
      "epoch": 0.0702371656773004,
      "grad_norm": 0.5239800214767456,
      "learning_rate": 0.00018639093986628572,
      "loss": 0.0458,
      "step": 14932
    },
    {
      "epoch": 0.07024186947891285,
      "grad_norm": 0.9048784375190735,
      "learning_rate": 0.00018638999688817224,
      "loss": 0.0948,
      "step": 14933
    },
    {
      "epoch": 0.07024657328052532,
      "grad_norm": 0.3861272633075714,
      "learning_rate": 0.00018638905391005876,
      "loss": 0.0313,
      "step": 14934
    },
    {
      "epoch": 0.07025127708213778,
      "grad_norm": 1.3395092487335205,
      "learning_rate": 0.00018638811093194528,
      "loss": 0.1404,
      "step": 14935
    },
    {
      "epoch": 0.07025598088375025,
      "grad_norm": 3.927307605743408,
      "learning_rate": 0.0001863871679538318,
      "loss": 0.6467,
      "step": 14936
    },
    {
      "epoch": 0.07026068468536271,
      "grad_norm": 0.6999592781066895,
      "learning_rate": 0.0001863862249757183,
      "loss": 0.0719,
      "step": 14937
    },
    {
      "epoch": 0.07026538848697517,
      "grad_norm": 1.866947054862976,
      "learning_rate": 0.00018638528199760486,
      "loss": 0.399,
      "step": 14938
    },
    {
      "epoch": 0.07027009228858763,
      "grad_norm": 2.7407467365264893,
      "learning_rate": 0.00018638433901949138,
      "loss": 0.8614,
      "step": 14939
    },
    {
      "epoch": 0.0702747960902001,
      "grad_norm": 1.015268325805664,
      "learning_rate": 0.0001863833960413779,
      "loss": 0.071,
      "step": 14940
    },
    {
      "epoch": 0.07027949989181256,
      "grad_norm": 0.405404269695282,
      "learning_rate": 0.00018638245306326441,
      "loss": 0.0225,
      "step": 14941
    },
    {
      "epoch": 0.07028420369342503,
      "grad_norm": 1.9921232461929321,
      "learning_rate": 0.00018638151008515093,
      "loss": 0.3093,
      "step": 14942
    },
    {
      "epoch": 0.0702889074950375,
      "grad_norm": 1.4838463068008423,
      "learning_rate": 0.00018638056710703745,
      "loss": 0.3167,
      "step": 14943
    },
    {
      "epoch": 0.07029361129664995,
      "grad_norm": 3.8132729530334473,
      "learning_rate": 0.00018637962412892397,
      "loss": 0.7588,
      "step": 14944
    },
    {
      "epoch": 0.07029831509826241,
      "grad_norm": 0.4412124752998352,
      "learning_rate": 0.0001863786811508105,
      "loss": 0.0554,
      "step": 14945
    },
    {
      "epoch": 0.07030301889987488,
      "grad_norm": 1.667541742324829,
      "learning_rate": 0.000186377738172697,
      "loss": 0.1231,
      "step": 14946
    },
    {
      "epoch": 0.07030772270148734,
      "grad_norm": 2.636606216430664,
      "learning_rate": 0.00018637679519458355,
      "loss": 0.7114,
      "step": 14947
    },
    {
      "epoch": 0.07031242650309981,
      "grad_norm": 0.6108112335205078,
      "learning_rate": 0.00018637585221647007,
      "loss": 0.0543,
      "step": 14948
    },
    {
      "epoch": 0.07031713030471227,
      "grad_norm": 1.1726484298706055,
      "learning_rate": 0.0001863749092383566,
      "loss": 0.1413,
      "step": 14949
    },
    {
      "epoch": 0.07032183410632473,
      "grad_norm": 1.2294871807098389,
      "learning_rate": 0.0001863739662602431,
      "loss": 0.2417,
      "step": 14950
    },
    {
      "epoch": 0.07032653790793719,
      "grad_norm": 6.102664947509766,
      "learning_rate": 0.00018637302328212963,
      "loss": 0.2931,
      "step": 14951
    },
    {
      "epoch": 0.07033124170954966,
      "grad_norm": 2.7790212631225586,
      "learning_rate": 0.00018637208030401617,
      "loss": 1.2221,
      "step": 14952
    },
    {
      "epoch": 0.07033594551116212,
      "grad_norm": 5.121633052825928,
      "learning_rate": 0.00018637113732590267,
      "loss": 0.4818,
      "step": 14953
    },
    {
      "epoch": 0.07034064931277459,
      "grad_norm": 4.045684337615967,
      "learning_rate": 0.00018637019434778918,
      "loss": 0.5532,
      "step": 14954
    },
    {
      "epoch": 0.07034535311438704,
      "grad_norm": 1.6740779876708984,
      "learning_rate": 0.0001863692513696757,
      "loss": 0.1626,
      "step": 14955
    },
    {
      "epoch": 0.0703500569159995,
      "grad_norm": 1.5269643068313599,
      "learning_rate": 0.00018636830839156225,
      "loss": 0.2049,
      "step": 14956
    },
    {
      "epoch": 0.07035476071761197,
      "grad_norm": 1.1151599884033203,
      "learning_rate": 0.00018636736541344877,
      "loss": 0.0978,
      "step": 14957
    },
    {
      "epoch": 0.07035946451922444,
      "grad_norm": 1.7468256950378418,
      "learning_rate": 0.00018636642243533529,
      "loss": 0.208,
      "step": 14958
    },
    {
      "epoch": 0.0703641683208369,
      "grad_norm": 1.3258013725280762,
      "learning_rate": 0.0001863654794572218,
      "loss": 0.1637,
      "step": 14959
    },
    {
      "epoch": 0.07036887212244937,
      "grad_norm": 1.5784755945205688,
      "learning_rate": 0.00018636453647910832,
      "loss": 0.1536,
      "step": 14960
    },
    {
      "epoch": 0.07037357592406182,
      "grad_norm": 2.210819959640503,
      "learning_rate": 0.00018636359350099487,
      "loss": 0.2876,
      "step": 14961
    },
    {
      "epoch": 0.07037827972567429,
      "grad_norm": 2.7740817070007324,
      "learning_rate": 0.0001863626505228814,
      "loss": 0.2604,
      "step": 14962
    },
    {
      "epoch": 0.07038298352728675,
      "grad_norm": 1.828840970993042,
      "learning_rate": 0.0001863617075447679,
      "loss": 0.2343,
      "step": 14963
    },
    {
      "epoch": 0.07038768732889922,
      "grad_norm": 3.1812405586242676,
      "learning_rate": 0.00018636076456665442,
      "loss": 0.3617,
      "step": 14964
    },
    {
      "epoch": 0.07039239113051168,
      "grad_norm": 0.8427241444587708,
      "learning_rate": 0.00018635982158854092,
      "loss": 0.1384,
      "step": 14965
    },
    {
      "epoch": 0.07039709493212415,
      "grad_norm": 0.808119535446167,
      "learning_rate": 0.00018635887861042746,
      "loss": 0.0604,
      "step": 14966
    },
    {
      "epoch": 0.0704017987337366,
      "grad_norm": 1.9193363189697266,
      "learning_rate": 0.00018635793563231398,
      "loss": 0.2724,
      "step": 14967
    },
    {
      "epoch": 0.07040650253534907,
      "grad_norm": 0.6869484782218933,
      "learning_rate": 0.0001863569926542005,
      "loss": 0.099,
      "step": 14968
    },
    {
      "epoch": 0.07041120633696153,
      "grad_norm": 0.767302930355072,
      "learning_rate": 0.00018635604967608702,
      "loss": 0.0712,
      "step": 14969
    },
    {
      "epoch": 0.070415910138574,
      "grad_norm": 1.3871474266052246,
      "learning_rate": 0.00018635510669797356,
      "loss": 0.0956,
      "step": 14970
    },
    {
      "epoch": 0.07042061394018646,
      "grad_norm": 0.8336803913116455,
      "learning_rate": 0.00018635416371986008,
      "loss": 0.0834,
      "step": 14971
    },
    {
      "epoch": 0.07042531774179892,
      "grad_norm": 3.0744781494140625,
      "learning_rate": 0.0001863532207417466,
      "loss": 0.3371,
      "step": 14972
    },
    {
      "epoch": 0.07043002154341138,
      "grad_norm": 0.41048961877822876,
      "learning_rate": 0.00018635227776363312,
      "loss": 0.0343,
      "step": 14973
    },
    {
      "epoch": 0.07043472534502385,
      "grad_norm": 4.191776275634766,
      "learning_rate": 0.00018635133478551964,
      "loss": 0.5404,
      "step": 14974
    },
    {
      "epoch": 0.07043942914663631,
      "grad_norm": 1.8818153142929077,
      "learning_rate": 0.00018635039180740616,
      "loss": 0.3871,
      "step": 14975
    },
    {
      "epoch": 0.07044413294824878,
      "grad_norm": 1.4494316577911377,
      "learning_rate": 0.00018634944882929268,
      "loss": 0.1978,
      "step": 14976
    },
    {
      "epoch": 0.07044883674986124,
      "grad_norm": 3.0658257007598877,
      "learning_rate": 0.0001863485058511792,
      "loss": 0.6851,
      "step": 14977
    },
    {
      "epoch": 0.0704535405514737,
      "grad_norm": 2.614767551422119,
      "learning_rate": 0.0001863475628730657,
      "loss": 0.6964,
      "step": 14978
    },
    {
      "epoch": 0.07045824435308616,
      "grad_norm": 2.3401081562042236,
      "learning_rate": 0.00018634661989495226,
      "loss": 0.4372,
      "step": 14979
    },
    {
      "epoch": 0.07046294815469863,
      "grad_norm": 2.5193111896514893,
      "learning_rate": 0.00018634567691683878,
      "loss": 0.3327,
      "step": 14980
    },
    {
      "epoch": 0.07046765195631109,
      "grad_norm": 0.6869310140609741,
      "learning_rate": 0.0001863447339387253,
      "loss": 0.0389,
      "step": 14981
    },
    {
      "epoch": 0.07047235575792356,
      "grad_norm": 2.285229444503784,
      "learning_rate": 0.00018634379096061181,
      "loss": 0.1844,
      "step": 14982
    },
    {
      "epoch": 0.07047705955953602,
      "grad_norm": 1.5327630043029785,
      "learning_rate": 0.00018634284798249833,
      "loss": 0.1159,
      "step": 14983
    },
    {
      "epoch": 0.07048176336114848,
      "grad_norm": 2.883105754852295,
      "learning_rate": 0.00018634190500438485,
      "loss": 0.4712,
      "step": 14984
    },
    {
      "epoch": 0.07048646716276094,
      "grad_norm": 1.1007094383239746,
      "learning_rate": 0.00018634096202627137,
      "loss": 0.0819,
      "step": 14985
    },
    {
      "epoch": 0.0704911709643734,
      "grad_norm": 0.6799697875976562,
      "learning_rate": 0.0001863400190481579,
      "loss": 0.0735,
      "step": 14986
    },
    {
      "epoch": 0.07049587476598587,
      "grad_norm": 2.2804794311523438,
      "learning_rate": 0.0001863390760700444,
      "loss": 0.4099,
      "step": 14987
    },
    {
      "epoch": 0.07050057856759834,
      "grad_norm": 2.1410953998565674,
      "learning_rate": 0.00018633813309193095,
      "loss": 0.1922,
      "step": 14988
    },
    {
      "epoch": 0.07050528236921079,
      "grad_norm": 4.4121809005737305,
      "learning_rate": 0.00018633719011381747,
      "loss": 0.4468,
      "step": 14989
    },
    {
      "epoch": 0.07050998617082326,
      "grad_norm": 3.0847575664520264,
      "learning_rate": 0.000186336247135704,
      "loss": 0.6543,
      "step": 14990
    },
    {
      "epoch": 0.07051468997243572,
      "grad_norm": 1.704171061515808,
      "learning_rate": 0.0001863353041575905,
      "loss": 0.4263,
      "step": 14991
    },
    {
      "epoch": 0.07051939377404819,
      "grad_norm": 1.1512572765350342,
      "learning_rate": 0.00018633436117947703,
      "loss": 0.1636,
      "step": 14992
    },
    {
      "epoch": 0.07052409757566065,
      "grad_norm": 0.1777474284172058,
      "learning_rate": 0.00018633341820136357,
      "loss": 0.0138,
      "step": 14993
    },
    {
      "epoch": 0.07052880137727312,
      "grad_norm": 1.0853347778320312,
      "learning_rate": 0.0001863324752232501,
      "loss": 0.177,
      "step": 14994
    },
    {
      "epoch": 0.07053350517888557,
      "grad_norm": 1.940150260925293,
      "learning_rate": 0.0001863315322451366,
      "loss": 0.2329,
      "step": 14995
    },
    {
      "epoch": 0.07053820898049804,
      "grad_norm": 0.9427176713943481,
      "learning_rate": 0.0001863305892670231,
      "loss": 0.1134,
      "step": 14996
    },
    {
      "epoch": 0.0705429127821105,
      "grad_norm": 0.7554200887680054,
      "learning_rate": 0.00018632964628890965,
      "loss": 0.1112,
      "step": 14997
    },
    {
      "epoch": 0.07054761658372297,
      "grad_norm": 0.1190577819943428,
      "learning_rate": 0.00018632870331079617,
      "loss": 0.0085,
      "step": 14998
    },
    {
      "epoch": 0.07055232038533543,
      "grad_norm": 0.45333436131477356,
      "learning_rate": 0.00018632776033268269,
      "loss": 0.0472,
      "step": 14999
    },
    {
      "epoch": 0.0705570241869479,
      "grad_norm": 1.1697992086410522,
      "learning_rate": 0.0001863268173545692,
      "loss": 0.2281,
      "step": 15000
    },
    {
      "epoch": 0.07056172798856035,
      "grad_norm": 0.8825897574424744,
      "learning_rate": 0.00018632587437645572,
      "loss": 0.0617,
      "step": 15001
    },
    {
      "epoch": 0.07056643179017281,
      "grad_norm": 3.243190050125122,
      "learning_rate": 0.00018632493139834227,
      "loss": 0.4193,
      "step": 15002
    },
    {
      "epoch": 0.07057113559178528,
      "grad_norm": 1.3976904153823853,
      "learning_rate": 0.0001863239884202288,
      "loss": 0.0504,
      "step": 15003
    },
    {
      "epoch": 0.07057583939339775,
      "grad_norm": 0.5125732421875,
      "learning_rate": 0.0001863230454421153,
      "loss": 0.0335,
      "step": 15004
    },
    {
      "epoch": 0.07058054319501021,
      "grad_norm": 3.7332923412323,
      "learning_rate": 0.00018632210246400182,
      "loss": 0.5395,
      "step": 15005
    },
    {
      "epoch": 0.07058524699662266,
      "grad_norm": 0.3977709710597992,
      "learning_rate": 0.00018632115948588834,
      "loss": 0.0416,
      "step": 15006
    },
    {
      "epoch": 0.07058995079823513,
      "grad_norm": 2.4109275341033936,
      "learning_rate": 0.00018632021650777486,
      "loss": 0.1765,
      "step": 15007
    },
    {
      "epoch": 0.0705946545998476,
      "grad_norm": 3.3172266483306885,
      "learning_rate": 0.00018631927352966138,
      "loss": 0.4054,
      "step": 15008
    },
    {
      "epoch": 0.07059935840146006,
      "grad_norm": 0.8466638326644897,
      "learning_rate": 0.0001863183305515479,
      "loss": 0.0752,
      "step": 15009
    },
    {
      "epoch": 0.07060406220307253,
      "grad_norm": 3.3382985591888428,
      "learning_rate": 0.00018631738757343442,
      "loss": 0.885,
      "step": 15010
    },
    {
      "epoch": 0.07060876600468499,
      "grad_norm": 2.002774715423584,
      "learning_rate": 0.00018631644459532096,
      "loss": 0.3881,
      "step": 15011
    },
    {
      "epoch": 0.07061346980629744,
      "grad_norm": 3.932030200958252,
      "learning_rate": 0.00018631550161720748,
      "loss": 0.4203,
      "step": 15012
    },
    {
      "epoch": 0.07061817360790991,
      "grad_norm": 1.875031590461731,
      "learning_rate": 0.000186314558639094,
      "loss": 0.1641,
      "step": 15013
    },
    {
      "epoch": 0.07062287740952237,
      "grad_norm": 2.5580010414123535,
      "learning_rate": 0.00018631361566098052,
      "loss": 0.1917,
      "step": 15014
    },
    {
      "epoch": 0.07062758121113484,
      "grad_norm": 0.8134272694587708,
      "learning_rate": 0.00018631267268286704,
      "loss": 0.0708,
      "step": 15015
    },
    {
      "epoch": 0.0706322850127473,
      "grad_norm": 2.0868890285491943,
      "learning_rate": 0.00018631172970475356,
      "loss": 0.3524,
      "step": 15016
    },
    {
      "epoch": 0.07063698881435977,
      "grad_norm": 3.058670997619629,
      "learning_rate": 0.00018631078672664008,
      "loss": 0.7328,
      "step": 15017
    },
    {
      "epoch": 0.07064169261597222,
      "grad_norm": 0.801667332649231,
      "learning_rate": 0.0001863098437485266,
      "loss": 0.0704,
      "step": 15018
    },
    {
      "epoch": 0.07064639641758469,
      "grad_norm": 3.873610258102417,
      "learning_rate": 0.0001863089007704131,
      "loss": 0.6477,
      "step": 15019
    },
    {
      "epoch": 0.07065110021919715,
      "grad_norm": 3.803333044052124,
      "learning_rate": 0.00018630795779229966,
      "loss": 0.4084,
      "step": 15020
    },
    {
      "epoch": 0.07065580402080962,
      "grad_norm": 3.8071484565734863,
      "learning_rate": 0.00018630701481418618,
      "loss": 0.6821,
      "step": 15021
    },
    {
      "epoch": 0.07066050782242209,
      "grad_norm": 7.442157745361328,
      "learning_rate": 0.0001863060718360727,
      "loss": 0.1904,
      "step": 15022
    },
    {
      "epoch": 0.07066521162403454,
      "grad_norm": 2.8887252807617188,
      "learning_rate": 0.00018630512885795921,
      "loss": 0.6388,
      "step": 15023
    },
    {
      "epoch": 0.070669915425647,
      "grad_norm": 2.657114267349243,
      "learning_rate": 0.00018630418587984573,
      "loss": 0.5797,
      "step": 15024
    },
    {
      "epoch": 0.07067461922725947,
      "grad_norm": 0.9999599456787109,
      "learning_rate": 0.00018630324290173228,
      "loss": 0.1014,
      "step": 15025
    },
    {
      "epoch": 0.07067932302887193,
      "grad_norm": 1.0647127628326416,
      "learning_rate": 0.0001863022999236188,
      "loss": 0.0825,
      "step": 15026
    },
    {
      "epoch": 0.0706840268304844,
      "grad_norm": 2.0854451656341553,
      "learning_rate": 0.0001863013569455053,
      "loss": 0.338,
      "step": 15027
    },
    {
      "epoch": 0.07068873063209687,
      "grad_norm": 8.542781829833984,
      "learning_rate": 0.0001863004139673918,
      "loss": 0.8804,
      "step": 15028
    },
    {
      "epoch": 0.07069343443370932,
      "grad_norm": 1.208980917930603,
      "learning_rate": 0.00018629947098927835,
      "loss": 0.1307,
      "step": 15029
    },
    {
      "epoch": 0.07069813823532178,
      "grad_norm": 5.059811115264893,
      "learning_rate": 0.00018629852801116487,
      "loss": 0.3652,
      "step": 15030
    },
    {
      "epoch": 0.07070284203693425,
      "grad_norm": 2.6997618675231934,
      "learning_rate": 0.0001862975850330514,
      "loss": 0.1851,
      "step": 15031
    },
    {
      "epoch": 0.07070754583854671,
      "grad_norm": 0.5305600166320801,
      "learning_rate": 0.0001862966420549379,
      "loss": 0.0557,
      "step": 15032
    },
    {
      "epoch": 0.07071224964015918,
      "grad_norm": 1.8815199136734009,
      "learning_rate": 0.00018629569907682443,
      "loss": 0.5125,
      "step": 15033
    },
    {
      "epoch": 0.07071695344177165,
      "grad_norm": 2.001060724258423,
      "learning_rate": 0.00018629475609871097,
      "loss": 0.2649,
      "step": 15034
    },
    {
      "epoch": 0.0707216572433841,
      "grad_norm": 1.2839537858963013,
      "learning_rate": 0.0001862938131205975,
      "loss": 0.1105,
      "step": 15035
    },
    {
      "epoch": 0.07072636104499656,
      "grad_norm": 2.5193793773651123,
      "learning_rate": 0.000186292870142484,
      "loss": 0.4074,
      "step": 15036
    },
    {
      "epoch": 0.07073106484660903,
      "grad_norm": 1.4590137004852295,
      "learning_rate": 0.00018629192716437053,
      "loss": 0.1718,
      "step": 15037
    },
    {
      "epoch": 0.0707357686482215,
      "grad_norm": 0.468039333820343,
      "learning_rate": 0.00018629098418625705,
      "loss": 0.0506,
      "step": 15038
    },
    {
      "epoch": 0.07074047244983396,
      "grad_norm": 2.1718502044677734,
      "learning_rate": 0.00018629004120814357,
      "loss": 0.315,
      "step": 15039
    },
    {
      "epoch": 0.07074517625144641,
      "grad_norm": 1.089430570602417,
      "learning_rate": 0.00018628909823003009,
      "loss": 0.1278,
      "step": 15040
    },
    {
      "epoch": 0.07074988005305888,
      "grad_norm": 2.848860740661621,
      "learning_rate": 0.0001862881552519166,
      "loss": 0.5236,
      "step": 15041
    },
    {
      "epoch": 0.07075458385467134,
      "grad_norm": 2.2082011699676514,
      "learning_rate": 0.00018628721227380312,
      "loss": 0.348,
      "step": 15042
    },
    {
      "epoch": 0.07075928765628381,
      "grad_norm": 2.104519844055176,
      "learning_rate": 0.00018628626929568967,
      "loss": 0.2776,
      "step": 15043
    },
    {
      "epoch": 0.07076399145789627,
      "grad_norm": 1.5281063318252563,
      "learning_rate": 0.0001862853263175762,
      "loss": 0.3648,
      "step": 15044
    },
    {
      "epoch": 0.07076869525950874,
      "grad_norm": 1.418812870979309,
      "learning_rate": 0.0001862843833394627,
      "loss": 0.1586,
      "step": 15045
    },
    {
      "epoch": 0.07077339906112119,
      "grad_norm": 0.32386642694473267,
      "learning_rate": 0.00018628344036134922,
      "loss": 0.0252,
      "step": 15046
    },
    {
      "epoch": 0.07077810286273366,
      "grad_norm": 2.073319673538208,
      "learning_rate": 0.00018628249738323574,
      "loss": 0.3994,
      "step": 15047
    },
    {
      "epoch": 0.07078280666434612,
      "grad_norm": 0.8272822499275208,
      "learning_rate": 0.00018628155440512226,
      "loss": 0.1168,
      "step": 15048
    },
    {
      "epoch": 0.07078751046595859,
      "grad_norm": 1.4937541484832764,
      "learning_rate": 0.00018628061142700878,
      "loss": 0.1369,
      "step": 15049
    },
    {
      "epoch": 0.07079221426757105,
      "grad_norm": 1.5173989534378052,
      "learning_rate": 0.0001862796684488953,
      "loss": 0.1993,
      "step": 15050
    },
    {
      "epoch": 0.07079691806918352,
      "grad_norm": 3.1387500762939453,
      "learning_rate": 0.00018627872547078182,
      "loss": 0.2729,
      "step": 15051
    },
    {
      "epoch": 0.07080162187079597,
      "grad_norm": 0.8418073058128357,
      "learning_rate": 0.00018627778249266836,
      "loss": 0.0804,
      "step": 15052
    },
    {
      "epoch": 0.07080632567240844,
      "grad_norm": 3.5360493659973145,
      "learning_rate": 0.00018627683951455488,
      "loss": 0.3219,
      "step": 15053
    },
    {
      "epoch": 0.0708110294740209,
      "grad_norm": 4.997922420501709,
      "learning_rate": 0.0001862758965364414,
      "loss": 0.6179,
      "step": 15054
    },
    {
      "epoch": 0.07081573327563337,
      "grad_norm": 3.678894519805908,
      "learning_rate": 0.00018627495355832792,
      "loss": 0.6907,
      "step": 15055
    },
    {
      "epoch": 0.07082043707724583,
      "grad_norm": 1.8205599784851074,
      "learning_rate": 0.00018627401058021447,
      "loss": 0.2077,
      "step": 15056
    },
    {
      "epoch": 0.07082514087885829,
      "grad_norm": 3.705040216445923,
      "learning_rate": 0.00018627306760210098,
      "loss": 0.6808,
      "step": 15057
    },
    {
      "epoch": 0.07082984468047075,
      "grad_norm": 2.314704179763794,
      "learning_rate": 0.00018627212462398748,
      "loss": 0.3238,
      "step": 15058
    },
    {
      "epoch": 0.07083454848208322,
      "grad_norm": 0.45750585198402405,
      "learning_rate": 0.000186271181645874,
      "loss": 0.0265,
      "step": 15059
    },
    {
      "epoch": 0.07083925228369568,
      "grad_norm": 1.143690824508667,
      "learning_rate": 0.0001862702386677605,
      "loss": 0.096,
      "step": 15060
    },
    {
      "epoch": 0.07084395608530815,
      "grad_norm": 3.2587218284606934,
      "learning_rate": 0.00018626929568964706,
      "loss": 0.3362,
      "step": 15061
    },
    {
      "epoch": 0.07084865988692061,
      "grad_norm": 0.19953276216983795,
      "learning_rate": 0.00018626835271153358,
      "loss": 0.013,
      "step": 15062
    },
    {
      "epoch": 0.07085336368853307,
      "grad_norm": 1.9536793231964111,
      "learning_rate": 0.0001862674097334201,
      "loss": 0.195,
      "step": 15063
    },
    {
      "epoch": 0.07085806749014553,
      "grad_norm": 0.9389645457267761,
      "learning_rate": 0.00018626646675530661,
      "loss": 0.1086,
      "step": 15064
    },
    {
      "epoch": 0.070862771291758,
      "grad_norm": 2.0771045684814453,
      "learning_rate": 0.00018626552377719313,
      "loss": 0.1578,
      "step": 15065
    },
    {
      "epoch": 0.07086747509337046,
      "grad_norm": 1.669161081314087,
      "learning_rate": 0.00018626458079907968,
      "loss": 0.1285,
      "step": 15066
    },
    {
      "epoch": 0.07087217889498293,
      "grad_norm": 1.7847446203231812,
      "learning_rate": 0.0001862636378209662,
      "loss": 0.288,
      "step": 15067
    },
    {
      "epoch": 0.0708768826965954,
      "grad_norm": 1.8017581701278687,
      "learning_rate": 0.00018626269484285272,
      "loss": 0.1946,
      "step": 15068
    },
    {
      "epoch": 0.07088158649820785,
      "grad_norm": 0.573515772819519,
      "learning_rate": 0.0001862617518647392,
      "loss": 0.0598,
      "step": 15069
    },
    {
      "epoch": 0.07088629029982031,
      "grad_norm": 1.4445514678955078,
      "learning_rate": 0.00018626080888662575,
      "loss": 0.1398,
      "step": 15070
    },
    {
      "epoch": 0.07089099410143278,
      "grad_norm": 1.5614792108535767,
      "learning_rate": 0.00018625986590851227,
      "loss": 0.0941,
      "step": 15071
    },
    {
      "epoch": 0.07089569790304524,
      "grad_norm": 0.6225542426109314,
      "learning_rate": 0.0001862589229303988,
      "loss": 0.0476,
      "step": 15072
    },
    {
      "epoch": 0.07090040170465771,
      "grad_norm": 4.230462074279785,
      "learning_rate": 0.0001862579799522853,
      "loss": 0.5157,
      "step": 15073
    },
    {
      "epoch": 0.07090510550627016,
      "grad_norm": 2.8622243404388428,
      "learning_rate": 0.00018625703697417183,
      "loss": 0.3493,
      "step": 15074
    },
    {
      "epoch": 0.07090980930788263,
      "grad_norm": 2.7283706665039062,
      "learning_rate": 0.00018625609399605837,
      "loss": 0.4091,
      "step": 15075
    },
    {
      "epoch": 0.07091451310949509,
      "grad_norm": 0.12218648940324783,
      "learning_rate": 0.0001862551510179449,
      "loss": 0.0084,
      "step": 15076
    },
    {
      "epoch": 0.07091921691110756,
      "grad_norm": 2.0830159187316895,
      "learning_rate": 0.0001862542080398314,
      "loss": 0.2623,
      "step": 15077
    },
    {
      "epoch": 0.07092392071272002,
      "grad_norm": 0.5412463545799255,
      "learning_rate": 0.00018625326506171793,
      "loss": 0.0594,
      "step": 15078
    },
    {
      "epoch": 0.07092862451433249,
      "grad_norm": 2.660822868347168,
      "learning_rate": 0.00018625232208360445,
      "loss": 0.7205,
      "step": 15079
    },
    {
      "epoch": 0.07093332831594494,
      "grad_norm": 4.339542865753174,
      "learning_rate": 0.00018625137910549097,
      "loss": 1.1877,
      "step": 15080
    },
    {
      "epoch": 0.0709380321175574,
      "grad_norm": 1.6723226308822632,
      "learning_rate": 0.00018625043612737749,
      "loss": 0.2157,
      "step": 15081
    },
    {
      "epoch": 0.07094273591916987,
      "grad_norm": 1.7777420282363892,
      "learning_rate": 0.000186249493149264,
      "loss": 0.1033,
      "step": 15082
    },
    {
      "epoch": 0.07094743972078234,
      "grad_norm": 2.5053560733795166,
      "learning_rate": 0.00018624855017115052,
      "loss": 0.5117,
      "step": 15083
    },
    {
      "epoch": 0.0709521435223948,
      "grad_norm": 2.1937429904937744,
      "learning_rate": 0.00018624760719303707,
      "loss": 0.2439,
      "step": 15084
    },
    {
      "epoch": 0.07095684732400727,
      "grad_norm": 1.3522050380706787,
      "learning_rate": 0.0001862466642149236,
      "loss": 0.1093,
      "step": 15085
    },
    {
      "epoch": 0.07096155112561972,
      "grad_norm": 2.35490345954895,
      "learning_rate": 0.0001862457212368101,
      "loss": 0.471,
      "step": 15086
    },
    {
      "epoch": 0.07096625492723219,
      "grad_norm": 0.5046837329864502,
      "learning_rate": 0.00018624477825869662,
      "loss": 0.0577,
      "step": 15087
    },
    {
      "epoch": 0.07097095872884465,
      "grad_norm": 3.026567220687866,
      "learning_rate": 0.00018624383528058314,
      "loss": 0.7555,
      "step": 15088
    },
    {
      "epoch": 0.07097566253045712,
      "grad_norm": 1.8768800497055054,
      "learning_rate": 0.00018624289230246966,
      "loss": 0.121,
      "step": 15089
    },
    {
      "epoch": 0.07098036633206958,
      "grad_norm": 1.3695284128189087,
      "learning_rate": 0.00018624194932435618,
      "loss": 0.1592,
      "step": 15090
    },
    {
      "epoch": 0.07098507013368203,
      "grad_norm": 0.5075039863586426,
      "learning_rate": 0.0001862410063462427,
      "loss": 0.0558,
      "step": 15091
    },
    {
      "epoch": 0.0709897739352945,
      "grad_norm": 1.8373547792434692,
      "learning_rate": 0.00018624006336812922,
      "loss": 0.2061,
      "step": 15092
    },
    {
      "epoch": 0.07099447773690697,
      "grad_norm": 2.601773738861084,
      "learning_rate": 0.00018623912039001576,
      "loss": 0.4907,
      "step": 15093
    },
    {
      "epoch": 0.07099918153851943,
      "grad_norm": 0.4889392554759979,
      "learning_rate": 0.00018623817741190228,
      "loss": 0.0496,
      "step": 15094
    },
    {
      "epoch": 0.0710038853401319,
      "grad_norm": 0.1953265219926834,
      "learning_rate": 0.0001862372344337888,
      "loss": 0.0149,
      "step": 15095
    },
    {
      "epoch": 0.07100858914174436,
      "grad_norm": 3.759066343307495,
      "learning_rate": 0.00018623629145567532,
      "loss": 0.4498,
      "step": 15096
    },
    {
      "epoch": 0.07101329294335681,
      "grad_norm": 6.0553998947143555,
      "learning_rate": 0.00018623534847756187,
      "loss": 0.4573,
      "step": 15097
    },
    {
      "epoch": 0.07101799674496928,
      "grad_norm": 0.9433313608169556,
      "learning_rate": 0.00018623440549944838,
      "loss": 0.1348,
      "step": 15098
    },
    {
      "epoch": 0.07102270054658175,
      "grad_norm": 1.708081603050232,
      "learning_rate": 0.0001862334625213349,
      "loss": 0.3224,
      "step": 15099
    },
    {
      "epoch": 0.07102740434819421,
      "grad_norm": 2.632993221282959,
      "learning_rate": 0.0001862325195432214,
      "loss": 0.3951,
      "step": 15100
    },
    {
      "epoch": 0.07103210814980668,
      "grad_norm": 2.543980360031128,
      "learning_rate": 0.0001862315765651079,
      "loss": 0.3371,
      "step": 15101
    },
    {
      "epoch": 0.07103681195141914,
      "grad_norm": 1.5775688886642456,
      "learning_rate": 0.00018623063358699446,
      "loss": 0.1544,
      "step": 15102
    },
    {
      "epoch": 0.0710415157530316,
      "grad_norm": 2.3398125171661377,
      "learning_rate": 0.00018622969060888098,
      "loss": 0.3082,
      "step": 15103
    },
    {
      "epoch": 0.07104621955464406,
      "grad_norm": 2.933515787124634,
      "learning_rate": 0.0001862287476307675,
      "loss": 0.3136,
      "step": 15104
    },
    {
      "epoch": 0.07105092335625653,
      "grad_norm": 1.2109744548797607,
      "learning_rate": 0.00018622780465265401,
      "loss": 0.1208,
      "step": 15105
    },
    {
      "epoch": 0.07105562715786899,
      "grad_norm": 0.763849139213562,
      "learning_rate": 0.00018622686167454056,
      "loss": 0.0873,
      "step": 15106
    },
    {
      "epoch": 0.07106033095948146,
      "grad_norm": 4.037274360656738,
      "learning_rate": 0.00018622591869642708,
      "loss": 0.2196,
      "step": 15107
    },
    {
      "epoch": 0.07106503476109391,
      "grad_norm": 1.9271678924560547,
      "learning_rate": 0.0001862249757183136,
      "loss": 0.1635,
      "step": 15108
    },
    {
      "epoch": 0.07106973856270637,
      "grad_norm": 10.409367561340332,
      "learning_rate": 0.00018622403274020012,
      "loss": 0.6596,
      "step": 15109
    },
    {
      "epoch": 0.07107444236431884,
      "grad_norm": 0.49746567010879517,
      "learning_rate": 0.00018622308976208663,
      "loss": 0.0491,
      "step": 15110
    },
    {
      "epoch": 0.0710791461659313,
      "grad_norm": 3.8127682209014893,
      "learning_rate": 0.00018622214678397315,
      "loss": 0.7866,
      "step": 15111
    },
    {
      "epoch": 0.07108384996754377,
      "grad_norm": 3.3902735710144043,
      "learning_rate": 0.00018622120380585967,
      "loss": 0.4107,
      "step": 15112
    },
    {
      "epoch": 0.07108855376915624,
      "grad_norm": 2.0303614139556885,
      "learning_rate": 0.0001862202608277462,
      "loss": 0.2711,
      "step": 15113
    },
    {
      "epoch": 0.07109325757076869,
      "grad_norm": 0.9260364174842834,
      "learning_rate": 0.0001862193178496327,
      "loss": 0.0822,
      "step": 15114
    },
    {
      "epoch": 0.07109796137238115,
      "grad_norm": 2.3248684406280518,
      "learning_rate": 0.00018621837487151923,
      "loss": 0.2307,
      "step": 15115
    },
    {
      "epoch": 0.07110266517399362,
      "grad_norm": 2.6255974769592285,
      "learning_rate": 0.00018621743189340577,
      "loss": 0.3815,
      "step": 15116
    },
    {
      "epoch": 0.07110736897560609,
      "grad_norm": 2.4119999408721924,
      "learning_rate": 0.0001862164889152923,
      "loss": 0.2721,
      "step": 15117
    },
    {
      "epoch": 0.07111207277721855,
      "grad_norm": 3.648339033126831,
      "learning_rate": 0.0001862155459371788,
      "loss": 0.6116,
      "step": 15118
    },
    {
      "epoch": 0.07111677657883102,
      "grad_norm": 1.729713797569275,
      "learning_rate": 0.00018621460295906533,
      "loss": 0.1991,
      "step": 15119
    },
    {
      "epoch": 0.07112148038044347,
      "grad_norm": 5.504459857940674,
      "learning_rate": 0.00018621365998095185,
      "loss": 0.3373,
      "step": 15120
    },
    {
      "epoch": 0.07112618418205593,
      "grad_norm": 1.409153699874878,
      "learning_rate": 0.00018621271700283837,
      "loss": 0.0811,
      "step": 15121
    },
    {
      "epoch": 0.0711308879836684,
      "grad_norm": 1.9651914834976196,
      "learning_rate": 0.00018621177402472489,
      "loss": 0.4142,
      "step": 15122
    },
    {
      "epoch": 0.07113559178528087,
      "grad_norm": 0.9869135618209839,
      "learning_rate": 0.0001862108310466114,
      "loss": 0.0845,
      "step": 15123
    },
    {
      "epoch": 0.07114029558689333,
      "grad_norm": 2.887924909591675,
      "learning_rate": 0.00018620988806849792,
      "loss": 0.1667,
      "step": 15124
    },
    {
      "epoch": 0.07114499938850578,
      "grad_norm": 0.8500590920448303,
      "learning_rate": 0.00018620894509038447,
      "loss": 0.0629,
      "step": 15125
    },
    {
      "epoch": 0.07114970319011825,
      "grad_norm": 1.880280613899231,
      "learning_rate": 0.000186208002112271,
      "loss": 0.1254,
      "step": 15126
    },
    {
      "epoch": 0.07115440699173071,
      "grad_norm": 2.5955636501312256,
      "learning_rate": 0.0001862070591341575,
      "loss": 0.383,
      "step": 15127
    },
    {
      "epoch": 0.07115911079334318,
      "grad_norm": 2.8625879287719727,
      "learning_rate": 0.00018620611615604402,
      "loss": 0.6876,
      "step": 15128
    },
    {
      "epoch": 0.07116381459495565,
      "grad_norm": 3.5058138370513916,
      "learning_rate": 0.00018620517317793057,
      "loss": 0.6603,
      "step": 15129
    },
    {
      "epoch": 0.07116851839656811,
      "grad_norm": 1.8744540214538574,
      "learning_rate": 0.0001862042301998171,
      "loss": 0.3004,
      "step": 15130
    },
    {
      "epoch": 0.07117322219818056,
      "grad_norm": 2.935438632965088,
      "learning_rate": 0.00018620328722170358,
      "loss": 0.2716,
      "step": 15131
    },
    {
      "epoch": 0.07117792599979303,
      "grad_norm": 3.703336715698242,
      "learning_rate": 0.0001862023442435901,
      "loss": 0.3902,
      "step": 15132
    },
    {
      "epoch": 0.0711826298014055,
      "grad_norm": 3.916853427886963,
      "learning_rate": 0.00018620140126547662,
      "loss": 0.4755,
      "step": 15133
    },
    {
      "epoch": 0.07118733360301796,
      "grad_norm": 3.730287551879883,
      "learning_rate": 0.00018620045828736316,
      "loss": 0.4719,
      "step": 15134
    },
    {
      "epoch": 0.07119203740463043,
      "grad_norm": 1.5486233234405518,
      "learning_rate": 0.00018619951530924968,
      "loss": 0.1052,
      "step": 15135
    },
    {
      "epoch": 0.07119674120624289,
      "grad_norm": 1.576646327972412,
      "learning_rate": 0.0001861985723311362,
      "loss": 0.1418,
      "step": 15136
    },
    {
      "epoch": 0.07120144500785534,
      "grad_norm": 0.5391992330551147,
      "learning_rate": 0.00018619762935302272,
      "loss": 0.0419,
      "step": 15137
    },
    {
      "epoch": 0.07120614880946781,
      "grad_norm": 1.171054482460022,
      "learning_rate": 0.00018619668637490927,
      "loss": 0.0919,
      "step": 15138
    },
    {
      "epoch": 0.07121085261108027,
      "grad_norm": 1.7237422466278076,
      "learning_rate": 0.00018619574339679578,
      "loss": 0.2901,
      "step": 15139
    },
    {
      "epoch": 0.07121555641269274,
      "grad_norm": 1.579757809638977,
      "learning_rate": 0.0001861948004186823,
      "loss": 0.3904,
      "step": 15140
    },
    {
      "epoch": 0.0712202602143052,
      "grad_norm": 1.3163923025131226,
      "learning_rate": 0.00018619385744056882,
      "loss": 0.1294,
      "step": 15141
    },
    {
      "epoch": 0.07122496401591766,
      "grad_norm": 2.074215888977051,
      "learning_rate": 0.0001861929144624553,
      "loss": 0.2006,
      "step": 15142
    },
    {
      "epoch": 0.07122966781753012,
      "grad_norm": 2.972545862197876,
      "learning_rate": 0.00018619197148434186,
      "loss": 0.4023,
      "step": 15143
    },
    {
      "epoch": 0.07123437161914259,
      "grad_norm": 1.9696067571640015,
      "learning_rate": 0.00018619102850622838,
      "loss": 0.244,
      "step": 15144
    },
    {
      "epoch": 0.07123907542075505,
      "grad_norm": 0.38324469327926636,
      "learning_rate": 0.0001861900855281149,
      "loss": 0.035,
      "step": 15145
    },
    {
      "epoch": 0.07124377922236752,
      "grad_norm": 0.8861801028251648,
      "learning_rate": 0.00018618914255000141,
      "loss": 0.0918,
      "step": 15146
    },
    {
      "epoch": 0.07124848302397999,
      "grad_norm": 2.04292893409729,
      "learning_rate": 0.00018618819957188796,
      "loss": 0.3518,
      "step": 15147
    },
    {
      "epoch": 0.07125318682559244,
      "grad_norm": 1.8351410627365112,
      "learning_rate": 0.00018618725659377448,
      "loss": 0.4391,
      "step": 15148
    },
    {
      "epoch": 0.0712578906272049,
      "grad_norm": 1.9552758932113647,
      "learning_rate": 0.000186186313615661,
      "loss": 0.4784,
      "step": 15149
    },
    {
      "epoch": 0.07126259442881737,
      "grad_norm": 1.7413203716278076,
      "learning_rate": 0.00018618537063754752,
      "loss": 0.3227,
      "step": 15150
    },
    {
      "epoch": 0.07126729823042983,
      "grad_norm": 7.397194862365723,
      "learning_rate": 0.00018618442765943403,
      "loss": 0.5133,
      "step": 15151
    },
    {
      "epoch": 0.0712720020320423,
      "grad_norm": 4.422793388366699,
      "learning_rate": 0.00018618348468132055,
      "loss": 0.4508,
      "step": 15152
    },
    {
      "epoch": 0.07127670583365477,
      "grad_norm": 3.3132119178771973,
      "learning_rate": 0.00018618254170320707,
      "loss": 0.3728,
      "step": 15153
    },
    {
      "epoch": 0.07128140963526722,
      "grad_norm": 9.089497566223145,
      "learning_rate": 0.0001861815987250936,
      "loss": 0.5453,
      "step": 15154
    },
    {
      "epoch": 0.07128611343687968,
      "grad_norm": 4.640714645385742,
      "learning_rate": 0.0001861806557469801,
      "loss": 0.7849,
      "step": 15155
    },
    {
      "epoch": 0.07129081723849215,
      "grad_norm": 1.0610653162002563,
      "learning_rate": 0.00018617971276886665,
      "loss": 0.1328,
      "step": 15156
    },
    {
      "epoch": 0.07129552104010461,
      "grad_norm": 2.060856580734253,
      "learning_rate": 0.00018617876979075317,
      "loss": 0.2578,
      "step": 15157
    },
    {
      "epoch": 0.07130022484171708,
      "grad_norm": 2.4804108142852783,
      "learning_rate": 0.0001861778268126397,
      "loss": 0.2432,
      "step": 15158
    },
    {
      "epoch": 0.07130492864332953,
      "grad_norm": 3.6473233699798584,
      "learning_rate": 0.0001861768838345262,
      "loss": 0.7582,
      "step": 15159
    },
    {
      "epoch": 0.071309632444942,
      "grad_norm": 0.09972093254327774,
      "learning_rate": 0.00018617594085641273,
      "loss": 0.0081,
      "step": 15160
    },
    {
      "epoch": 0.07131433624655446,
      "grad_norm": 1.285946249961853,
      "learning_rate": 0.00018617499787829928,
      "loss": 0.2664,
      "step": 15161
    },
    {
      "epoch": 0.07131904004816693,
      "grad_norm": 2.4135706424713135,
      "learning_rate": 0.00018617405490018577,
      "loss": 0.2925,
      "step": 15162
    },
    {
      "epoch": 0.0713237438497794,
      "grad_norm": 3.666055202484131,
      "learning_rate": 0.00018617311192207229,
      "loss": 0.6095,
      "step": 15163
    },
    {
      "epoch": 0.07132844765139186,
      "grad_norm": 2.479705810546875,
      "learning_rate": 0.0001861721689439588,
      "loss": 0.3116,
      "step": 15164
    },
    {
      "epoch": 0.07133315145300431,
      "grad_norm": 3.2124264240264893,
      "learning_rate": 0.00018617122596584532,
      "loss": 0.4636,
      "step": 15165
    },
    {
      "epoch": 0.07133785525461678,
      "grad_norm": 1.9881106615066528,
      "learning_rate": 0.00018617028298773187,
      "loss": 0.3586,
      "step": 15166
    },
    {
      "epoch": 0.07134255905622924,
      "grad_norm": 2.4802660942077637,
      "learning_rate": 0.0001861693400096184,
      "loss": 0.4603,
      "step": 15167
    },
    {
      "epoch": 0.07134726285784171,
      "grad_norm": 1.4633510112762451,
      "learning_rate": 0.0001861683970315049,
      "loss": 0.227,
      "step": 15168
    },
    {
      "epoch": 0.07135196665945417,
      "grad_norm": 0.8407073020935059,
      "learning_rate": 0.00018616745405339142,
      "loss": 0.0826,
      "step": 15169
    },
    {
      "epoch": 0.07135667046106664,
      "grad_norm": 1.7131752967834473,
      "learning_rate": 0.00018616651107527797,
      "loss": 0.3158,
      "step": 15170
    },
    {
      "epoch": 0.07136137426267909,
      "grad_norm": 2.215747356414795,
      "learning_rate": 0.0001861655680971645,
      "loss": 0.4112,
      "step": 15171
    },
    {
      "epoch": 0.07136607806429156,
      "grad_norm": 1.7742773294448853,
      "learning_rate": 0.000186164625119051,
      "loss": 0.4032,
      "step": 15172
    },
    {
      "epoch": 0.07137078186590402,
      "grad_norm": 3.926553726196289,
      "learning_rate": 0.0001861636821409375,
      "loss": 0.4019,
      "step": 15173
    },
    {
      "epoch": 0.07137548566751649,
      "grad_norm": 1.2625885009765625,
      "learning_rate": 0.00018616273916282402,
      "loss": 0.2406,
      "step": 15174
    },
    {
      "epoch": 0.07138018946912895,
      "grad_norm": 1.8452913761138916,
      "learning_rate": 0.00018616179618471056,
      "loss": 0.3271,
      "step": 15175
    },
    {
      "epoch": 0.0713848932707414,
      "grad_norm": 2.1369428634643555,
      "learning_rate": 0.00018616085320659708,
      "loss": 0.2408,
      "step": 15176
    },
    {
      "epoch": 0.07138959707235387,
      "grad_norm": 1.6849582195281982,
      "learning_rate": 0.0001861599102284836,
      "loss": 0.2564,
      "step": 15177
    },
    {
      "epoch": 0.07139430087396634,
      "grad_norm": 1.7418752908706665,
      "learning_rate": 0.00018615896725037012,
      "loss": 0.1802,
      "step": 15178
    },
    {
      "epoch": 0.0713990046755788,
      "grad_norm": 1.345086932182312,
      "learning_rate": 0.00018615802427225666,
      "loss": 0.2763,
      "step": 15179
    },
    {
      "epoch": 0.07140370847719127,
      "grad_norm": 3.41152024269104,
      "learning_rate": 0.00018615708129414318,
      "loss": 0.6648,
      "step": 15180
    },
    {
      "epoch": 0.07140841227880373,
      "grad_norm": 1.9966036081314087,
      "learning_rate": 0.0001861561383160297,
      "loss": 0.3336,
      "step": 15181
    },
    {
      "epoch": 0.07141311608041619,
      "grad_norm": 1.2020906209945679,
      "learning_rate": 0.00018615519533791622,
      "loss": 0.1484,
      "step": 15182
    },
    {
      "epoch": 0.07141781988202865,
      "grad_norm": 1.3609619140625,
      "learning_rate": 0.00018615425235980274,
      "loss": 0.1365,
      "step": 15183
    },
    {
      "epoch": 0.07142252368364112,
      "grad_norm": 2.6333703994750977,
      "learning_rate": 0.00018615330938168926,
      "loss": 0.5667,
      "step": 15184
    },
    {
      "epoch": 0.07142722748525358,
      "grad_norm": 0.8396187424659729,
      "learning_rate": 0.00018615236640357578,
      "loss": 0.1929,
      "step": 15185
    },
    {
      "epoch": 0.07143193128686605,
      "grad_norm": 2.7343175411224365,
      "learning_rate": 0.0001861514234254623,
      "loss": 0.442,
      "step": 15186
    },
    {
      "epoch": 0.07143663508847851,
      "grad_norm": 2.6152992248535156,
      "learning_rate": 0.00018615048044734881,
      "loss": 0.3684,
      "step": 15187
    },
    {
      "epoch": 0.07144133889009097,
      "grad_norm": 0.9824849963188171,
      "learning_rate": 0.00018614953746923536,
      "loss": 0.1656,
      "step": 15188
    },
    {
      "epoch": 0.07144604269170343,
      "grad_norm": 2.018019914627075,
      "learning_rate": 0.00018614859449112188,
      "loss": 0.3627,
      "step": 15189
    },
    {
      "epoch": 0.0714507464933159,
      "grad_norm": 1.406687617301941,
      "learning_rate": 0.0001861476515130084,
      "loss": 0.2321,
      "step": 15190
    },
    {
      "epoch": 0.07145545029492836,
      "grad_norm": 4.233415603637695,
      "learning_rate": 0.00018614670853489492,
      "loss": 0.8598,
      "step": 15191
    },
    {
      "epoch": 0.07146015409654083,
      "grad_norm": 2.1751108169555664,
      "learning_rate": 0.00018614576555678143,
      "loss": 0.3942,
      "step": 15192
    },
    {
      "epoch": 0.07146485789815328,
      "grad_norm": 1.789292573928833,
      "learning_rate": 0.00018614482257866795,
      "loss": 0.3446,
      "step": 15193
    },
    {
      "epoch": 0.07146956169976575,
      "grad_norm": 1.169967532157898,
      "learning_rate": 0.00018614387960055447,
      "loss": 0.1926,
      "step": 15194
    },
    {
      "epoch": 0.07147426550137821,
      "grad_norm": 1.9114129543304443,
      "learning_rate": 0.000186142936622441,
      "loss": 0.4034,
      "step": 15195
    },
    {
      "epoch": 0.07147896930299068,
      "grad_norm": 1.4471138715744019,
      "learning_rate": 0.0001861419936443275,
      "loss": 0.2384,
      "step": 15196
    },
    {
      "epoch": 0.07148367310460314,
      "grad_norm": 4.349975109100342,
      "learning_rate": 0.00018614105066621405,
      "loss": 0.2816,
      "step": 15197
    },
    {
      "epoch": 0.07148837690621561,
      "grad_norm": 0.5761474967002869,
      "learning_rate": 0.00018614010768810057,
      "loss": 0.0567,
      "step": 15198
    },
    {
      "epoch": 0.07149308070782806,
      "grad_norm": 1.0719963312149048,
      "learning_rate": 0.0001861391647099871,
      "loss": 0.2172,
      "step": 15199
    },
    {
      "epoch": 0.07149778450944053,
      "grad_norm": 1.623067021369934,
      "learning_rate": 0.0001861382217318736,
      "loss": 0.1752,
      "step": 15200
    },
    {
      "epoch": 0.07150248831105299,
      "grad_norm": 9.52252197265625,
      "learning_rate": 0.00018613727875376013,
      "loss": 0.6174,
      "step": 15201
    },
    {
      "epoch": 0.07150719211266546,
      "grad_norm": 1.6014680862426758,
      "learning_rate": 0.00018613633577564668,
      "loss": 0.2414,
      "step": 15202
    },
    {
      "epoch": 0.07151189591427792,
      "grad_norm": 0.6099051833152771,
      "learning_rate": 0.0001861353927975332,
      "loss": 0.0234,
      "step": 15203
    },
    {
      "epoch": 0.07151659971589039,
      "grad_norm": 1.7457042932510376,
      "learning_rate": 0.00018613444981941969,
      "loss": 0.1325,
      "step": 15204
    },
    {
      "epoch": 0.07152130351750284,
      "grad_norm": 1.017687201499939,
      "learning_rate": 0.0001861335068413062,
      "loss": 0.1715,
      "step": 15205
    },
    {
      "epoch": 0.0715260073191153,
      "grad_norm": 1.9930024147033691,
      "learning_rate": 0.00018613256386319275,
      "loss": 0.2894,
      "step": 15206
    },
    {
      "epoch": 0.07153071112072777,
      "grad_norm": 2.1971287727355957,
      "learning_rate": 0.00018613162088507927,
      "loss": 0.0601,
      "step": 15207
    },
    {
      "epoch": 0.07153541492234024,
      "grad_norm": 3.978933811187744,
      "learning_rate": 0.0001861306779069658,
      "loss": 0.4853,
      "step": 15208
    },
    {
      "epoch": 0.0715401187239527,
      "grad_norm": 2.184065103530884,
      "learning_rate": 0.0001861297349288523,
      "loss": 0.2677,
      "step": 15209
    },
    {
      "epoch": 0.07154482252556515,
      "grad_norm": 3.5419647693634033,
      "learning_rate": 0.00018612879195073882,
      "loss": 0.5287,
      "step": 15210
    },
    {
      "epoch": 0.07154952632717762,
      "grad_norm": 3.24594783782959,
      "learning_rate": 0.00018612784897262537,
      "loss": 0.4786,
      "step": 15211
    },
    {
      "epoch": 0.07155423012879009,
      "grad_norm": 4.698718070983887,
      "learning_rate": 0.0001861269059945119,
      "loss": 0.5247,
      "step": 15212
    },
    {
      "epoch": 0.07155893393040255,
      "grad_norm": 0.637984573841095,
      "learning_rate": 0.0001861259630163984,
      "loss": 0.0903,
      "step": 15213
    },
    {
      "epoch": 0.07156363773201502,
      "grad_norm": 1.1891694068908691,
      "learning_rate": 0.00018612502003828493,
      "loss": 0.1877,
      "step": 15214
    },
    {
      "epoch": 0.07156834153362748,
      "grad_norm": 1.54973566532135,
      "learning_rate": 0.00018612407706017144,
      "loss": 0.221,
      "step": 15215
    },
    {
      "epoch": 0.07157304533523993,
      "grad_norm": 0.8065470457077026,
      "learning_rate": 0.00018612313408205796,
      "loss": 0.1492,
      "step": 15216
    },
    {
      "epoch": 0.0715777491368524,
      "grad_norm": 0.9375051856040955,
      "learning_rate": 0.00018612219110394448,
      "loss": 0.0956,
      "step": 15217
    },
    {
      "epoch": 0.07158245293846487,
      "grad_norm": 1.2884931564331055,
      "learning_rate": 0.000186121248125831,
      "loss": 0.2705,
      "step": 15218
    },
    {
      "epoch": 0.07158715674007733,
      "grad_norm": 1.245711088180542,
      "learning_rate": 0.00018612030514771752,
      "loss": 0.1693,
      "step": 15219
    },
    {
      "epoch": 0.0715918605416898,
      "grad_norm": 2.7590017318725586,
      "learning_rate": 0.00018611936216960406,
      "loss": 0.46,
      "step": 15220
    },
    {
      "epoch": 0.07159656434330226,
      "grad_norm": 2.5459392070770264,
      "learning_rate": 0.00018611841919149058,
      "loss": 0.4309,
      "step": 15221
    },
    {
      "epoch": 0.07160126814491471,
      "grad_norm": 0.537925660610199,
      "learning_rate": 0.0001861174762133771,
      "loss": 0.0566,
      "step": 15222
    },
    {
      "epoch": 0.07160597194652718,
      "grad_norm": 0.7347421646118164,
      "learning_rate": 0.00018611653323526362,
      "loss": 0.0588,
      "step": 15223
    },
    {
      "epoch": 0.07161067574813965,
      "grad_norm": 1.9944789409637451,
      "learning_rate": 0.00018611559025715014,
      "loss": 0.1872,
      "step": 15224
    },
    {
      "epoch": 0.07161537954975211,
      "grad_norm": 1.347491979598999,
      "learning_rate": 0.00018611464727903666,
      "loss": 0.1394,
      "step": 15225
    },
    {
      "epoch": 0.07162008335136458,
      "grad_norm": 1.8766484260559082,
      "learning_rate": 0.00018611370430092318,
      "loss": 0.4239,
      "step": 15226
    },
    {
      "epoch": 0.07162478715297703,
      "grad_norm": 1.0323666334152222,
      "learning_rate": 0.0001861127613228097,
      "loss": 0.1243,
      "step": 15227
    },
    {
      "epoch": 0.0716294909545895,
      "grad_norm": 1.3039520978927612,
      "learning_rate": 0.00018611181834469621,
      "loss": 0.1542,
      "step": 15228
    },
    {
      "epoch": 0.07163419475620196,
      "grad_norm": 2.4702413082122803,
      "learning_rate": 0.00018611087536658276,
      "loss": 0.5887,
      "step": 15229
    },
    {
      "epoch": 0.07163889855781443,
      "grad_norm": 4.745028972625732,
      "learning_rate": 0.00018610993238846928,
      "loss": 0.9839,
      "step": 15230
    },
    {
      "epoch": 0.07164360235942689,
      "grad_norm": 0.5391480326652527,
      "learning_rate": 0.0001861089894103558,
      "loss": 0.061,
      "step": 15231
    },
    {
      "epoch": 0.07164830616103936,
      "grad_norm": 0.514886200428009,
      "learning_rate": 0.00018610804643224232,
      "loss": 0.0763,
      "step": 15232
    },
    {
      "epoch": 0.07165300996265181,
      "grad_norm": 2.8833250999450684,
      "learning_rate": 0.00018610710345412883,
      "loss": 0.7399,
      "step": 15233
    },
    {
      "epoch": 0.07165771376426427,
      "grad_norm": 1.0164258480072021,
      "learning_rate": 0.00018610616047601538,
      "loss": 0.1281,
      "step": 15234
    },
    {
      "epoch": 0.07166241756587674,
      "grad_norm": 0.851535439491272,
      "learning_rate": 0.00018610521749790187,
      "loss": 0.1446,
      "step": 15235
    },
    {
      "epoch": 0.0716671213674892,
      "grad_norm": 0.5603683590888977,
      "learning_rate": 0.0001861042745197884,
      "loss": 0.0498,
      "step": 15236
    },
    {
      "epoch": 0.07167182516910167,
      "grad_norm": 2.918792486190796,
      "learning_rate": 0.0001861033315416749,
      "loss": 0.807,
      "step": 15237
    },
    {
      "epoch": 0.07167652897071414,
      "grad_norm": 2.2595648765563965,
      "learning_rate": 0.00018610238856356145,
      "loss": 0.5767,
      "step": 15238
    },
    {
      "epoch": 0.07168123277232659,
      "grad_norm": 2.9313056468963623,
      "learning_rate": 0.00018610144558544797,
      "loss": 0.5478,
      "step": 15239
    },
    {
      "epoch": 0.07168593657393905,
      "grad_norm": 1.4142745733261108,
      "learning_rate": 0.0001861005026073345,
      "loss": 0.1371,
      "step": 15240
    },
    {
      "epoch": 0.07169064037555152,
      "grad_norm": 1.8260631561279297,
      "learning_rate": 0.000186099559629221,
      "loss": 0.2645,
      "step": 15241
    },
    {
      "epoch": 0.07169534417716399,
      "grad_norm": 0.7315365672111511,
      "learning_rate": 0.00018609861665110753,
      "loss": 0.0931,
      "step": 15242
    },
    {
      "epoch": 0.07170004797877645,
      "grad_norm": 0.667639434337616,
      "learning_rate": 0.00018609767367299408,
      "loss": 0.1187,
      "step": 15243
    },
    {
      "epoch": 0.0717047517803889,
      "grad_norm": 1.2368624210357666,
      "learning_rate": 0.0001860967306948806,
      "loss": 0.1482,
      "step": 15244
    },
    {
      "epoch": 0.07170945558200137,
      "grad_norm": 1.500692367553711,
      "learning_rate": 0.0001860957877167671,
      "loss": 0.2559,
      "step": 15245
    },
    {
      "epoch": 0.07171415938361383,
      "grad_norm": 3.9996049404144287,
      "learning_rate": 0.00018609484473865363,
      "loss": 0.9925,
      "step": 15246
    },
    {
      "epoch": 0.0717188631852263,
      "grad_norm": 1.968473196029663,
      "learning_rate": 0.00018609390176054015,
      "loss": 0.3375,
      "step": 15247
    },
    {
      "epoch": 0.07172356698683877,
      "grad_norm": 1.7410155534744263,
      "learning_rate": 0.00018609295878242667,
      "loss": 0.2214,
      "step": 15248
    },
    {
      "epoch": 0.07172827078845123,
      "grad_norm": 1.3518574237823486,
      "learning_rate": 0.0001860920158043132,
      "loss": 0.3672,
      "step": 15249
    },
    {
      "epoch": 0.07173297459006368,
      "grad_norm": 1.2317249774932861,
      "learning_rate": 0.0001860910728261997,
      "loss": 0.2702,
      "step": 15250
    },
    {
      "epoch": 0.07173767839167615,
      "grad_norm": 1.9189929962158203,
      "learning_rate": 0.00018609012984808622,
      "loss": 0.0465,
      "step": 15251
    },
    {
      "epoch": 0.07174238219328861,
      "grad_norm": 2.317197799682617,
      "learning_rate": 0.00018608918686997277,
      "loss": 0.2578,
      "step": 15252
    },
    {
      "epoch": 0.07174708599490108,
      "grad_norm": 2.709303617477417,
      "learning_rate": 0.0001860882438918593,
      "loss": 0.2324,
      "step": 15253
    },
    {
      "epoch": 0.07175178979651355,
      "grad_norm": 1.1933259963989258,
      "learning_rate": 0.0001860873009137458,
      "loss": 0.0895,
      "step": 15254
    },
    {
      "epoch": 0.07175649359812601,
      "grad_norm": 5.459342002868652,
      "learning_rate": 0.00018608635793563233,
      "loss": 0.793,
      "step": 15255
    },
    {
      "epoch": 0.07176119739973846,
      "grad_norm": 1.6671395301818848,
      "learning_rate": 0.00018608541495751884,
      "loss": 0.2335,
      "step": 15256
    },
    {
      "epoch": 0.07176590120135093,
      "grad_norm": 1.0261036157608032,
      "learning_rate": 0.00018608447197940536,
      "loss": 0.1439,
      "step": 15257
    },
    {
      "epoch": 0.0717706050029634,
      "grad_norm": 1.6209123134613037,
      "learning_rate": 0.00018608352900129188,
      "loss": 0.2813,
      "step": 15258
    },
    {
      "epoch": 0.07177530880457586,
      "grad_norm": 1.00261390209198,
      "learning_rate": 0.0001860825860231784,
      "loss": 0.1129,
      "step": 15259
    },
    {
      "epoch": 0.07178001260618833,
      "grad_norm": 1.6049705743789673,
      "learning_rate": 0.00018608164304506492,
      "loss": 0.1914,
      "step": 15260
    },
    {
      "epoch": 0.07178471640780078,
      "grad_norm": 1.3362594842910767,
      "learning_rate": 0.00018608070006695146,
      "loss": 0.2062,
      "step": 15261
    },
    {
      "epoch": 0.07178942020941324,
      "grad_norm": 0.5604564547538757,
      "learning_rate": 0.00018607975708883798,
      "loss": 0.0566,
      "step": 15262
    },
    {
      "epoch": 0.07179412401102571,
      "grad_norm": 4.518731117248535,
      "learning_rate": 0.0001860788141107245,
      "loss": 0.9319,
      "step": 15263
    },
    {
      "epoch": 0.07179882781263817,
      "grad_norm": 2.333259344100952,
      "learning_rate": 0.00018607787113261102,
      "loss": 0.2504,
      "step": 15264
    },
    {
      "epoch": 0.07180353161425064,
      "grad_norm": 3.653724193572998,
      "learning_rate": 0.00018607692815449757,
      "loss": 1.1035,
      "step": 15265
    },
    {
      "epoch": 0.0718082354158631,
      "grad_norm": 4.335841178894043,
      "learning_rate": 0.00018607598517638406,
      "loss": 0.6409,
      "step": 15266
    },
    {
      "epoch": 0.07181293921747556,
      "grad_norm": 1.534596562385559,
      "learning_rate": 0.00018607504219827058,
      "loss": 0.1419,
      "step": 15267
    },
    {
      "epoch": 0.07181764301908802,
      "grad_norm": 3.112929582595825,
      "learning_rate": 0.0001860740992201571,
      "loss": 0.5248,
      "step": 15268
    },
    {
      "epoch": 0.07182234682070049,
      "grad_norm": 2.297645092010498,
      "learning_rate": 0.00018607315624204361,
      "loss": 0.429,
      "step": 15269
    },
    {
      "epoch": 0.07182705062231295,
      "grad_norm": 1.4479364156723022,
      "learning_rate": 0.00018607221326393016,
      "loss": 0.2514,
      "step": 15270
    },
    {
      "epoch": 0.07183175442392542,
      "grad_norm": 1.1166353225708008,
      "learning_rate": 0.00018607127028581668,
      "loss": 0.113,
      "step": 15271
    },
    {
      "epoch": 0.07183645822553789,
      "grad_norm": 0.4309476912021637,
      "learning_rate": 0.0001860703273077032,
      "loss": 0.0579,
      "step": 15272
    },
    {
      "epoch": 0.07184116202715034,
      "grad_norm": 1.6856215000152588,
      "learning_rate": 0.00018606938432958972,
      "loss": 0.247,
      "step": 15273
    },
    {
      "epoch": 0.0718458658287628,
      "grad_norm": 2.87072491645813,
      "learning_rate": 0.00018606844135147623,
      "loss": 0.44,
      "step": 15274
    },
    {
      "epoch": 0.07185056963037527,
      "grad_norm": 0.6913512945175171,
      "learning_rate": 0.00018606749837336278,
      "loss": 0.074,
      "step": 15275
    },
    {
      "epoch": 0.07185527343198773,
      "grad_norm": 2.2629454135894775,
      "learning_rate": 0.0001860665553952493,
      "loss": 0.268,
      "step": 15276
    },
    {
      "epoch": 0.0718599772336002,
      "grad_norm": 1.185423731803894,
      "learning_rate": 0.00018606561241713582,
      "loss": 0.122,
      "step": 15277
    },
    {
      "epoch": 0.07186468103521265,
      "grad_norm": 0.34768617153167725,
      "learning_rate": 0.0001860646694390223,
      "loss": 0.0276,
      "step": 15278
    },
    {
      "epoch": 0.07186938483682512,
      "grad_norm": 7.697158336639404,
      "learning_rate": 0.00018606372646090885,
      "loss": 0.6978,
      "step": 15279
    },
    {
      "epoch": 0.07187408863843758,
      "grad_norm": 0.9523876905441284,
      "learning_rate": 0.00018606278348279537,
      "loss": 0.0896,
      "step": 15280
    },
    {
      "epoch": 0.07187879244005005,
      "grad_norm": 2.585052251815796,
      "learning_rate": 0.0001860618405046819,
      "loss": 0.2041,
      "step": 15281
    },
    {
      "epoch": 0.07188349624166251,
      "grad_norm": 2.1279990673065186,
      "learning_rate": 0.0001860608975265684,
      "loss": 0.4693,
      "step": 15282
    },
    {
      "epoch": 0.07188820004327498,
      "grad_norm": 1.0040138959884644,
      "learning_rate": 0.00018605995454845493,
      "loss": 0.079,
      "step": 15283
    },
    {
      "epoch": 0.07189290384488743,
      "grad_norm": 2.8902387619018555,
      "learning_rate": 0.00018605901157034148,
      "loss": 0.3793,
      "step": 15284
    },
    {
      "epoch": 0.0718976076464999,
      "grad_norm": 0.5521706342697144,
      "learning_rate": 0.000186058068592228,
      "loss": 0.0459,
      "step": 15285
    },
    {
      "epoch": 0.07190231144811236,
      "grad_norm": 1.0559477806091309,
      "learning_rate": 0.0001860571256141145,
      "loss": 0.0772,
      "step": 15286
    },
    {
      "epoch": 0.07190701524972483,
      "grad_norm": 2.9096052646636963,
      "learning_rate": 0.00018605618263600103,
      "loss": 0.6172,
      "step": 15287
    },
    {
      "epoch": 0.0719117190513373,
      "grad_norm": 1.5312038660049438,
      "learning_rate": 0.00018605523965788755,
      "loss": 0.2341,
      "step": 15288
    },
    {
      "epoch": 0.07191642285294976,
      "grad_norm": 0.38894832134246826,
      "learning_rate": 0.00018605429667977407,
      "loss": 0.0665,
      "step": 15289
    },
    {
      "epoch": 0.07192112665456221,
      "grad_norm": 0.3829426169395447,
      "learning_rate": 0.0001860533537016606,
      "loss": 0.0453,
      "step": 15290
    },
    {
      "epoch": 0.07192583045617468,
      "grad_norm": 0.7852017879486084,
      "learning_rate": 0.0001860524107235471,
      "loss": 0.0618,
      "step": 15291
    },
    {
      "epoch": 0.07193053425778714,
      "grad_norm": 0.8317923545837402,
      "learning_rate": 0.00018605146774543362,
      "loss": 0.0954,
      "step": 15292
    },
    {
      "epoch": 0.07193523805939961,
      "grad_norm": 4.2307000160217285,
      "learning_rate": 0.00018605052476732017,
      "loss": 0.6727,
      "step": 15293
    },
    {
      "epoch": 0.07193994186101207,
      "grad_norm": 0.19637809693813324,
      "learning_rate": 0.0001860495817892067,
      "loss": 0.015,
      "step": 15294
    },
    {
      "epoch": 0.07194464566262454,
      "grad_norm": 5.278148651123047,
      "learning_rate": 0.0001860486388110932,
      "loss": 1.0289,
      "step": 15295
    },
    {
      "epoch": 0.07194934946423699,
      "grad_norm": 3.4033191204071045,
      "learning_rate": 0.00018604769583297973,
      "loss": 0.6566,
      "step": 15296
    },
    {
      "epoch": 0.07195405326584946,
      "grad_norm": 2.80500864982605,
      "learning_rate": 0.00018604675285486624,
      "loss": 0.4607,
      "step": 15297
    },
    {
      "epoch": 0.07195875706746192,
      "grad_norm": 2.847013473510742,
      "learning_rate": 0.00018604580987675276,
      "loss": 0.6028,
      "step": 15298
    },
    {
      "epoch": 0.07196346086907439,
      "grad_norm": 1.8940694332122803,
      "learning_rate": 0.00018604486689863928,
      "loss": 0.1967,
      "step": 15299
    },
    {
      "epoch": 0.07196816467068685,
      "grad_norm": 1.9682356119155884,
      "learning_rate": 0.0001860439239205258,
      "loss": 0.1671,
      "step": 15300
    },
    {
      "epoch": 0.0719728684722993,
      "grad_norm": 2.5356268882751465,
      "learning_rate": 0.00018604298094241232,
      "loss": 0.2234,
      "step": 15301
    },
    {
      "epoch": 0.07197757227391177,
      "grad_norm": 0.3559991717338562,
      "learning_rate": 0.00018604203796429886,
      "loss": 0.0406,
      "step": 15302
    },
    {
      "epoch": 0.07198227607552424,
      "grad_norm": 0.9531630873680115,
      "learning_rate": 0.00018604109498618538,
      "loss": 0.065,
      "step": 15303
    },
    {
      "epoch": 0.0719869798771367,
      "grad_norm": 2.424170970916748,
      "learning_rate": 0.0001860401520080719,
      "loss": 0.198,
      "step": 15304
    },
    {
      "epoch": 0.07199168367874917,
      "grad_norm": 3.9337215423583984,
      "learning_rate": 0.00018603920902995842,
      "loss": 0.4638,
      "step": 15305
    },
    {
      "epoch": 0.07199638748036163,
      "grad_norm": 2.698748826980591,
      "learning_rate": 0.00018603826605184497,
      "loss": 0.2363,
      "step": 15306
    },
    {
      "epoch": 0.07200109128197409,
      "grad_norm": 2.549349784851074,
      "learning_rate": 0.00018603732307373149,
      "loss": 0.1435,
      "step": 15307
    },
    {
      "epoch": 0.07200579508358655,
      "grad_norm": 1.81629478931427,
      "learning_rate": 0.000186036380095618,
      "loss": 0.1671,
      "step": 15308
    },
    {
      "epoch": 0.07201049888519902,
      "grad_norm": 2.873657464981079,
      "learning_rate": 0.0001860354371175045,
      "loss": 0.3059,
      "step": 15309
    },
    {
      "epoch": 0.07201520268681148,
      "grad_norm": 5.99575662612915,
      "learning_rate": 0.00018603449413939101,
      "loss": 0.5038,
      "step": 15310
    },
    {
      "epoch": 0.07201990648842395,
      "grad_norm": 1.640400767326355,
      "learning_rate": 0.00018603355116127756,
      "loss": 0.0356,
      "step": 15311
    },
    {
      "epoch": 0.07202461029003641,
      "grad_norm": 12.238204956054688,
      "learning_rate": 0.00018603260818316408,
      "loss": 0.5362,
      "step": 15312
    },
    {
      "epoch": 0.07202931409164887,
      "grad_norm": 4.334359169006348,
      "learning_rate": 0.0001860316652050506,
      "loss": 0.4054,
      "step": 15313
    },
    {
      "epoch": 0.07203401789326133,
      "grad_norm": 1.2743803262710571,
      "learning_rate": 0.00018603072222693712,
      "loss": 0.0633,
      "step": 15314
    },
    {
      "epoch": 0.0720387216948738,
      "grad_norm": 3.175534963607788,
      "learning_rate": 0.00018602977924882366,
      "loss": 0.3984,
      "step": 15315
    },
    {
      "epoch": 0.07204342549648626,
      "grad_norm": 3.5645735263824463,
      "learning_rate": 0.00018602883627071018,
      "loss": 0.6252,
      "step": 15316
    },
    {
      "epoch": 0.07204812929809873,
      "grad_norm": 2.6427149772644043,
      "learning_rate": 0.0001860278932925967,
      "loss": 0.4102,
      "step": 15317
    },
    {
      "epoch": 0.07205283309971118,
      "grad_norm": 2.719698905944824,
      "learning_rate": 0.00018602695031448322,
      "loss": 0.3342,
      "step": 15318
    },
    {
      "epoch": 0.07205753690132365,
      "grad_norm": 1.404101848602295,
      "learning_rate": 0.00018602600733636974,
      "loss": 0.1184,
      "step": 15319
    },
    {
      "epoch": 0.07206224070293611,
      "grad_norm": 2.5989136695861816,
      "learning_rate": 0.00018602506435825625,
      "loss": 0.4707,
      "step": 15320
    },
    {
      "epoch": 0.07206694450454858,
      "grad_norm": 4.286805629730225,
      "learning_rate": 0.00018602412138014277,
      "loss": 0.6397,
      "step": 15321
    },
    {
      "epoch": 0.07207164830616104,
      "grad_norm": 2.0849978923797607,
      "learning_rate": 0.0001860231784020293,
      "loss": 0.2353,
      "step": 15322
    },
    {
      "epoch": 0.07207635210777351,
      "grad_norm": 2.431812286376953,
      "learning_rate": 0.0001860222354239158,
      "loss": 0.3626,
      "step": 15323
    },
    {
      "epoch": 0.07208105590938596,
      "grad_norm": 2.7082250118255615,
      "learning_rate": 0.00018602129244580233,
      "loss": 0.7219,
      "step": 15324
    },
    {
      "epoch": 0.07208575971099843,
      "grad_norm": 3.291743755340576,
      "learning_rate": 0.00018602034946768888,
      "loss": 0.2214,
      "step": 15325
    },
    {
      "epoch": 0.07209046351261089,
      "grad_norm": 1.496486783027649,
      "learning_rate": 0.0001860194064895754,
      "loss": 0.2026,
      "step": 15326
    },
    {
      "epoch": 0.07209516731422336,
      "grad_norm": 1.231097936630249,
      "learning_rate": 0.0001860184635114619,
      "loss": 0.1272,
      "step": 15327
    },
    {
      "epoch": 0.07209987111583582,
      "grad_norm": 3.799938917160034,
      "learning_rate": 0.00018601752053334843,
      "loss": 0.4119,
      "step": 15328
    },
    {
      "epoch": 0.07210457491744829,
      "grad_norm": 1.006794810295105,
      "learning_rate": 0.00018601657755523495,
      "loss": 0.1269,
      "step": 15329
    },
    {
      "epoch": 0.07210927871906074,
      "grad_norm": 3.3244426250457764,
      "learning_rate": 0.00018601563457712147,
      "loss": 0.4346,
      "step": 15330
    },
    {
      "epoch": 0.0721139825206732,
      "grad_norm": 3.1804699897766113,
      "learning_rate": 0.000186014691599008,
      "loss": 0.5138,
      "step": 15331
    },
    {
      "epoch": 0.07211868632228567,
      "grad_norm": 4.066898345947266,
      "learning_rate": 0.0001860137486208945,
      "loss": 0.693,
      "step": 15332
    },
    {
      "epoch": 0.07212339012389814,
      "grad_norm": 1.3950614929199219,
      "learning_rate": 0.00018601280564278102,
      "loss": 0.1726,
      "step": 15333
    },
    {
      "epoch": 0.0721280939255106,
      "grad_norm": 1.552281141281128,
      "learning_rate": 0.00018601186266466757,
      "loss": 0.2187,
      "step": 15334
    },
    {
      "epoch": 0.07213279772712305,
      "grad_norm": 1.7211909294128418,
      "learning_rate": 0.0001860109196865541,
      "loss": 0.21,
      "step": 15335
    },
    {
      "epoch": 0.07213750152873552,
      "grad_norm": 0.5897058844566345,
      "learning_rate": 0.0001860099767084406,
      "loss": 0.0513,
      "step": 15336
    },
    {
      "epoch": 0.07214220533034799,
      "grad_norm": 2.727313280105591,
      "learning_rate": 0.00018600903373032713,
      "loss": 0.3644,
      "step": 15337
    },
    {
      "epoch": 0.07214690913196045,
      "grad_norm": 0.6939703822135925,
      "learning_rate": 0.00018600809075221367,
      "loss": 0.1079,
      "step": 15338
    },
    {
      "epoch": 0.07215161293357292,
      "grad_norm": 0.6946753263473511,
      "learning_rate": 0.00018600714777410016,
      "loss": 0.0652,
      "step": 15339
    },
    {
      "epoch": 0.07215631673518538,
      "grad_norm": 0.662129819393158,
      "learning_rate": 0.00018600620479598668,
      "loss": 0.0597,
      "step": 15340
    },
    {
      "epoch": 0.07216102053679783,
      "grad_norm": 0.6679953932762146,
      "learning_rate": 0.0001860052618178732,
      "loss": 0.0505,
      "step": 15341
    },
    {
      "epoch": 0.0721657243384103,
      "grad_norm": 1.8976397514343262,
      "learning_rate": 0.00018600431883975972,
      "loss": 0.5437,
      "step": 15342
    },
    {
      "epoch": 0.07217042814002277,
      "grad_norm": 2.0225493907928467,
      "learning_rate": 0.00018600337586164626,
      "loss": 0.2842,
      "step": 15343
    },
    {
      "epoch": 0.07217513194163523,
      "grad_norm": 2.953498601913452,
      "learning_rate": 0.00018600243288353278,
      "loss": 0.5338,
      "step": 15344
    },
    {
      "epoch": 0.0721798357432477,
      "grad_norm": 3.168783187866211,
      "learning_rate": 0.0001860014899054193,
      "loss": 0.4226,
      "step": 15345
    },
    {
      "epoch": 0.07218453954486016,
      "grad_norm": 0.3949979543685913,
      "learning_rate": 0.00018600054692730582,
      "loss": 0.0448,
      "step": 15346
    },
    {
      "epoch": 0.07218924334647261,
      "grad_norm": 0.6405779719352722,
      "learning_rate": 0.00018599960394919237,
      "loss": 0.0653,
      "step": 15347
    },
    {
      "epoch": 0.07219394714808508,
      "grad_norm": 2.251351833343506,
      "learning_rate": 0.00018599866097107889,
      "loss": 0.4457,
      "step": 15348
    },
    {
      "epoch": 0.07219865094969755,
      "grad_norm": 1.324066400527954,
      "learning_rate": 0.0001859977179929654,
      "loss": 0.1429,
      "step": 15349
    },
    {
      "epoch": 0.07220335475131001,
      "grad_norm": 0.5792312026023865,
      "learning_rate": 0.00018599677501485192,
      "loss": 0.0721,
      "step": 15350
    },
    {
      "epoch": 0.07220805855292248,
      "grad_norm": 4.162364482879639,
      "learning_rate": 0.00018599583203673841,
      "loss": 0.1854,
      "step": 15351
    },
    {
      "epoch": 0.07221276235453493,
      "grad_norm": 4.0863471031188965,
      "learning_rate": 0.00018599488905862496,
      "loss": 0.8867,
      "step": 15352
    },
    {
      "epoch": 0.0722174661561474,
      "grad_norm": 4.118898868560791,
      "learning_rate": 0.00018599394608051148,
      "loss": 0.7434,
      "step": 15353
    },
    {
      "epoch": 0.07222216995775986,
      "grad_norm": 3.3050529956817627,
      "learning_rate": 0.000185993003102398,
      "loss": 0.3314,
      "step": 15354
    },
    {
      "epoch": 0.07222687375937233,
      "grad_norm": 2.8976402282714844,
      "learning_rate": 0.00018599206012428452,
      "loss": 0.3495,
      "step": 15355
    },
    {
      "epoch": 0.07223157756098479,
      "grad_norm": 0.9832403063774109,
      "learning_rate": 0.00018599111714617106,
      "loss": 0.08,
      "step": 15356
    },
    {
      "epoch": 0.07223628136259726,
      "grad_norm": 1.7998911142349243,
      "learning_rate": 0.00018599017416805758,
      "loss": 0.2361,
      "step": 15357
    },
    {
      "epoch": 0.07224098516420971,
      "grad_norm": 4.144931316375732,
      "learning_rate": 0.0001859892311899441,
      "loss": 0.318,
      "step": 15358
    },
    {
      "epoch": 0.07224568896582217,
      "grad_norm": 0.8575618267059326,
      "learning_rate": 0.00018598828821183062,
      "loss": 0.0634,
      "step": 15359
    },
    {
      "epoch": 0.07225039276743464,
      "grad_norm": 2.8631374835968018,
      "learning_rate": 0.00018598734523371714,
      "loss": 0.3024,
      "step": 15360
    },
    {
      "epoch": 0.0722550965690471,
      "grad_norm": 1.4282084703445435,
      "learning_rate": 0.00018598640225560365,
      "loss": 0.1584,
      "step": 15361
    },
    {
      "epoch": 0.07225980037065957,
      "grad_norm": 0.33743855357170105,
      "learning_rate": 0.00018598545927749017,
      "loss": 0.0332,
      "step": 15362
    },
    {
      "epoch": 0.07226450417227204,
      "grad_norm": 5.1584320068359375,
      "learning_rate": 0.0001859845162993767,
      "loss": 0.5187,
      "step": 15363
    },
    {
      "epoch": 0.07226920797388449,
      "grad_norm": 2.932914972305298,
      "learning_rate": 0.0001859835733212632,
      "loss": 0.3658,
      "step": 15364
    },
    {
      "epoch": 0.07227391177549695,
      "grad_norm": 3.582801103591919,
      "learning_rate": 0.00018598263034314976,
      "loss": 0.5045,
      "step": 15365
    },
    {
      "epoch": 0.07227861557710942,
      "grad_norm": 2.0043060779571533,
      "learning_rate": 0.00018598168736503627,
      "loss": 0.1675,
      "step": 15366
    },
    {
      "epoch": 0.07228331937872189,
      "grad_norm": 1.6710577011108398,
      "learning_rate": 0.0001859807443869228,
      "loss": 0.1681,
      "step": 15367
    },
    {
      "epoch": 0.07228802318033435,
      "grad_norm": 2.4698336124420166,
      "learning_rate": 0.0001859798014088093,
      "loss": 0.4399,
      "step": 15368
    },
    {
      "epoch": 0.0722927269819468,
      "grad_norm": 2.4500954151153564,
      "learning_rate": 0.00018597885843069583,
      "loss": 0.1151,
      "step": 15369
    },
    {
      "epoch": 0.07229743078355927,
      "grad_norm": 1.4626424312591553,
      "learning_rate": 0.00018597791545258235,
      "loss": 0.1836,
      "step": 15370
    },
    {
      "epoch": 0.07230213458517173,
      "grad_norm": 1.4562872648239136,
      "learning_rate": 0.00018597697247446887,
      "loss": 0.2187,
      "step": 15371
    },
    {
      "epoch": 0.0723068383867842,
      "grad_norm": 0.3201825022697449,
      "learning_rate": 0.0001859760294963554,
      "loss": 0.0281,
      "step": 15372
    },
    {
      "epoch": 0.07231154218839667,
      "grad_norm": 0.10190707445144653,
      "learning_rate": 0.0001859750865182419,
      "loss": 0.0068,
      "step": 15373
    },
    {
      "epoch": 0.07231624599000913,
      "grad_norm": 2.542269706726074,
      "learning_rate": 0.00018597414354012842,
      "loss": 0.4249,
      "step": 15374
    },
    {
      "epoch": 0.07232094979162158,
      "grad_norm": 1.450711965560913,
      "learning_rate": 0.00018597320056201497,
      "loss": 0.1923,
      "step": 15375
    },
    {
      "epoch": 0.07232565359323405,
      "grad_norm": 0.9867236018180847,
      "learning_rate": 0.0001859722575839015,
      "loss": 0.0961,
      "step": 15376
    },
    {
      "epoch": 0.07233035739484651,
      "grad_norm": 0.9029000401496887,
      "learning_rate": 0.000185971314605788,
      "loss": 0.0979,
      "step": 15377
    },
    {
      "epoch": 0.07233506119645898,
      "grad_norm": 1.7746777534484863,
      "learning_rate": 0.00018597037162767453,
      "loss": 0.3865,
      "step": 15378
    },
    {
      "epoch": 0.07233976499807145,
      "grad_norm": 1.6304306983947754,
      "learning_rate": 0.00018596942864956107,
      "loss": 0.381,
      "step": 15379
    },
    {
      "epoch": 0.07234446879968391,
      "grad_norm": 2.0317230224609375,
      "learning_rate": 0.0001859684856714476,
      "loss": 0.2316,
      "step": 15380
    },
    {
      "epoch": 0.07234917260129636,
      "grad_norm": 2.5033254623413086,
      "learning_rate": 0.0001859675426933341,
      "loss": 0.6143,
      "step": 15381
    },
    {
      "epoch": 0.07235387640290883,
      "grad_norm": 0.7856011390686035,
      "learning_rate": 0.0001859665997152206,
      "loss": 0.0818,
      "step": 15382
    },
    {
      "epoch": 0.0723585802045213,
      "grad_norm": 0.20691673457622528,
      "learning_rate": 0.00018596565673710712,
      "loss": 0.0279,
      "step": 15383
    },
    {
      "epoch": 0.07236328400613376,
      "grad_norm": 5.036700248718262,
      "learning_rate": 0.00018596471375899366,
      "loss": 0.3527,
      "step": 15384
    },
    {
      "epoch": 0.07236798780774623,
      "grad_norm": 1.0504053831100464,
      "learning_rate": 0.00018596377078088018,
      "loss": 0.0746,
      "step": 15385
    },
    {
      "epoch": 0.07237269160935868,
      "grad_norm": 0.9699094891548157,
      "learning_rate": 0.0001859628278027667,
      "loss": 0.1128,
      "step": 15386
    },
    {
      "epoch": 0.07237739541097114,
      "grad_norm": 1.2714860439300537,
      "learning_rate": 0.00018596188482465322,
      "loss": 0.1287,
      "step": 15387
    },
    {
      "epoch": 0.07238209921258361,
      "grad_norm": 1.9217451810836792,
      "learning_rate": 0.00018596094184653977,
      "loss": 0.4586,
      "step": 15388
    },
    {
      "epoch": 0.07238680301419607,
      "grad_norm": 3.6765708923339844,
      "learning_rate": 0.00018595999886842629,
      "loss": 0.4188,
      "step": 15389
    },
    {
      "epoch": 0.07239150681580854,
      "grad_norm": 1.3478580713272095,
      "learning_rate": 0.0001859590558903128,
      "loss": 0.0925,
      "step": 15390
    },
    {
      "epoch": 0.072396210617421,
      "grad_norm": 3.8932878971099854,
      "learning_rate": 0.00018595811291219932,
      "loss": 0.6708,
      "step": 15391
    },
    {
      "epoch": 0.07240091441903346,
      "grad_norm": 4.843471527099609,
      "learning_rate": 0.00018595716993408584,
      "loss": 0.9671,
      "step": 15392
    },
    {
      "epoch": 0.07240561822064592,
      "grad_norm": 1.4464600086212158,
      "learning_rate": 0.00018595622695597236,
      "loss": 0.1432,
      "step": 15393
    },
    {
      "epoch": 0.07241032202225839,
      "grad_norm": 1.4711806774139404,
      "learning_rate": 0.00018595528397785888,
      "loss": 0.1212,
      "step": 15394
    },
    {
      "epoch": 0.07241502582387085,
      "grad_norm": 2.0111443996429443,
      "learning_rate": 0.0001859543409997454,
      "loss": 0.4566,
      "step": 15395
    },
    {
      "epoch": 0.07241972962548332,
      "grad_norm": 0.437267005443573,
      "learning_rate": 0.00018595339802163192,
      "loss": 0.0318,
      "step": 15396
    },
    {
      "epoch": 0.07242443342709579,
      "grad_norm": 0.6227884292602539,
      "learning_rate": 0.00018595245504351846,
      "loss": 0.105,
      "step": 15397
    },
    {
      "epoch": 0.07242913722870824,
      "grad_norm": 2.300642251968384,
      "learning_rate": 0.00018595151206540498,
      "loss": 0.1947,
      "step": 15398
    },
    {
      "epoch": 0.0724338410303207,
      "grad_norm": 1.2745046615600586,
      "learning_rate": 0.0001859505690872915,
      "loss": 0.1254,
      "step": 15399
    },
    {
      "epoch": 0.07243854483193317,
      "grad_norm": 1.2108466625213623,
      "learning_rate": 0.00018594962610917802,
      "loss": 0.18,
      "step": 15400
    },
    {
      "epoch": 0.07244324863354563,
      "grad_norm": 8.230388641357422,
      "learning_rate": 0.00018594868313106454,
      "loss": 0.5412,
      "step": 15401
    },
    {
      "epoch": 0.0724479524351581,
      "grad_norm": 2.840789318084717,
      "learning_rate": 0.00018594774015295105,
      "loss": 0.2959,
      "step": 15402
    },
    {
      "epoch": 0.07245265623677055,
      "grad_norm": 3.353524923324585,
      "learning_rate": 0.00018594679717483757,
      "loss": 0.339,
      "step": 15403
    },
    {
      "epoch": 0.07245736003838302,
      "grad_norm": 4.069082736968994,
      "learning_rate": 0.0001859458541967241,
      "loss": 0.4337,
      "step": 15404
    },
    {
      "epoch": 0.07246206383999548,
      "grad_norm": 2.5261154174804688,
      "learning_rate": 0.0001859449112186106,
      "loss": 0.3279,
      "step": 15405
    },
    {
      "epoch": 0.07246676764160795,
      "grad_norm": 2.177307367324829,
      "learning_rate": 0.00018594396824049716,
      "loss": 0.2421,
      "step": 15406
    },
    {
      "epoch": 0.07247147144322041,
      "grad_norm": 3.174903154373169,
      "learning_rate": 0.00018594302526238367,
      "loss": 0.4093,
      "step": 15407
    },
    {
      "epoch": 0.07247617524483288,
      "grad_norm": 0.8370819687843323,
      "learning_rate": 0.0001859420822842702,
      "loss": 0.1051,
      "step": 15408
    },
    {
      "epoch": 0.07248087904644533,
      "grad_norm": 2.282365083694458,
      "learning_rate": 0.0001859411393061567,
      "loss": 0.2907,
      "step": 15409
    },
    {
      "epoch": 0.0724855828480578,
      "grad_norm": 0.5833197236061096,
      "learning_rate": 0.00018594019632804323,
      "loss": 0.0548,
      "step": 15410
    },
    {
      "epoch": 0.07249028664967026,
      "grad_norm": 4.71148681640625,
      "learning_rate": 0.00018593925334992978,
      "loss": 0.6359,
      "step": 15411
    },
    {
      "epoch": 0.07249499045128273,
      "grad_norm": 3.6614162921905518,
      "learning_rate": 0.0001859383103718163,
      "loss": 0.3821,
      "step": 15412
    },
    {
      "epoch": 0.0724996942528952,
      "grad_norm": 0.9249755144119263,
      "learning_rate": 0.0001859373673937028,
      "loss": 0.091,
      "step": 15413
    },
    {
      "epoch": 0.07250439805450766,
      "grad_norm": 0.2628941535949707,
      "learning_rate": 0.0001859364244155893,
      "loss": 0.0301,
      "step": 15414
    },
    {
      "epoch": 0.07250910185612011,
      "grad_norm": 1.6673252582550049,
      "learning_rate": 0.00018593548143747585,
      "loss": 0.2604,
      "step": 15415
    },
    {
      "epoch": 0.07251380565773258,
      "grad_norm": 0.43031108379364014,
      "learning_rate": 0.00018593453845936237,
      "loss": 0.0347,
      "step": 15416
    },
    {
      "epoch": 0.07251850945934504,
      "grad_norm": 2.718104124069214,
      "learning_rate": 0.0001859335954812489,
      "loss": 0.3296,
      "step": 15417
    },
    {
      "epoch": 0.07252321326095751,
      "grad_norm": 2.053509473800659,
      "learning_rate": 0.0001859326525031354,
      "loss": 0.2338,
      "step": 15418
    },
    {
      "epoch": 0.07252791706256997,
      "grad_norm": 1.686836838722229,
      "learning_rate": 0.00018593170952502193,
      "loss": 0.1407,
      "step": 15419
    },
    {
      "epoch": 0.07253262086418243,
      "grad_norm": 0.8554093837738037,
      "learning_rate": 0.00018593076654690847,
      "loss": 0.0606,
      "step": 15420
    },
    {
      "epoch": 0.07253732466579489,
      "grad_norm": 1.2633750438690186,
      "learning_rate": 0.000185929823568795,
      "loss": 0.0845,
      "step": 15421
    },
    {
      "epoch": 0.07254202846740736,
      "grad_norm": 1.2010183334350586,
      "learning_rate": 0.0001859288805906815,
      "loss": 0.0916,
      "step": 15422
    },
    {
      "epoch": 0.07254673226901982,
      "grad_norm": 2.293330192565918,
      "learning_rate": 0.00018592793761256803,
      "loss": 0.3823,
      "step": 15423
    },
    {
      "epoch": 0.07255143607063229,
      "grad_norm": 2.7153360843658447,
      "learning_rate": 0.00018592699463445452,
      "loss": 0.3756,
      "step": 15424
    },
    {
      "epoch": 0.07255613987224475,
      "grad_norm": 1.434635877609253,
      "learning_rate": 0.00018592605165634106,
      "loss": 0.1225,
      "step": 15425
    },
    {
      "epoch": 0.0725608436738572,
      "grad_norm": 3.688565254211426,
      "learning_rate": 0.00018592510867822758,
      "loss": 0.5235,
      "step": 15426
    },
    {
      "epoch": 0.07256554747546967,
      "grad_norm": 3.7709250450134277,
      "learning_rate": 0.0001859241657001141,
      "loss": 0.4779,
      "step": 15427
    },
    {
      "epoch": 0.07257025127708214,
      "grad_norm": 0.7065211534500122,
      "learning_rate": 0.00018592322272200062,
      "loss": 0.049,
      "step": 15428
    },
    {
      "epoch": 0.0725749550786946,
      "grad_norm": 0.5601608753204346,
      "learning_rate": 0.00018592227974388717,
      "loss": 0.118,
      "step": 15429
    },
    {
      "epoch": 0.07257965888030707,
      "grad_norm": 1.809714913368225,
      "learning_rate": 0.00018592133676577369,
      "loss": 0.167,
      "step": 15430
    },
    {
      "epoch": 0.07258436268191953,
      "grad_norm": 5.4874587059021,
      "learning_rate": 0.0001859203937876602,
      "loss": 1.3076,
      "step": 15431
    },
    {
      "epoch": 0.07258906648353199,
      "grad_norm": 0.3891878128051758,
      "learning_rate": 0.00018591945080954672,
      "loss": 0.0373,
      "step": 15432
    },
    {
      "epoch": 0.07259377028514445,
      "grad_norm": 0.8333114385604858,
      "learning_rate": 0.00018591850783143324,
      "loss": 0.0651,
      "step": 15433
    },
    {
      "epoch": 0.07259847408675692,
      "grad_norm": 1.254146933555603,
      "learning_rate": 0.00018591756485331976,
      "loss": 0.0811,
      "step": 15434
    },
    {
      "epoch": 0.07260317788836938,
      "grad_norm": 3.410937547683716,
      "learning_rate": 0.00018591662187520628,
      "loss": 0.9071,
      "step": 15435
    },
    {
      "epoch": 0.07260788168998185,
      "grad_norm": 2.231282949447632,
      "learning_rate": 0.0001859156788970928,
      "loss": 0.2343,
      "step": 15436
    },
    {
      "epoch": 0.0726125854915943,
      "grad_norm": 3.3888094425201416,
      "learning_rate": 0.00018591473591897932,
      "loss": 0.7275,
      "step": 15437
    },
    {
      "epoch": 0.07261728929320677,
      "grad_norm": 2.715566873550415,
      "learning_rate": 0.00018591379294086586,
      "loss": 0.2329,
      "step": 15438
    },
    {
      "epoch": 0.07262199309481923,
      "grad_norm": 0.8576138019561768,
      "learning_rate": 0.00018591284996275238,
      "loss": 0.0823,
      "step": 15439
    },
    {
      "epoch": 0.0726266968964317,
      "grad_norm": 1.679573655128479,
      "learning_rate": 0.0001859119069846389,
      "loss": 0.2275,
      "step": 15440
    },
    {
      "epoch": 0.07263140069804416,
      "grad_norm": 1.9493088722229004,
      "learning_rate": 0.00018591096400652542,
      "loss": 0.2195,
      "step": 15441
    },
    {
      "epoch": 0.07263610449965663,
      "grad_norm": 5.025146961212158,
      "learning_rate": 0.00018591002102841194,
      "loss": 0.6815,
      "step": 15442
    },
    {
      "epoch": 0.07264080830126908,
      "grad_norm": 1.1812154054641724,
      "learning_rate": 0.00018590907805029848,
      "loss": 0.0934,
      "step": 15443
    },
    {
      "epoch": 0.07264551210288155,
      "grad_norm": 2.14139986038208,
      "learning_rate": 0.00018590813507218497,
      "loss": 0.1819,
      "step": 15444
    },
    {
      "epoch": 0.07265021590449401,
      "grad_norm": 1.8110040426254272,
      "learning_rate": 0.0001859071920940715,
      "loss": 0.388,
      "step": 15445
    },
    {
      "epoch": 0.07265491970610648,
      "grad_norm": 2.3005642890930176,
      "learning_rate": 0.000185906249115958,
      "loss": 0.2491,
      "step": 15446
    },
    {
      "epoch": 0.07265962350771894,
      "grad_norm": 1.483547329902649,
      "learning_rate": 0.00018590530613784456,
      "loss": 0.2609,
      "step": 15447
    },
    {
      "epoch": 0.07266432730933141,
      "grad_norm": 3.1204464435577393,
      "learning_rate": 0.00018590436315973107,
      "loss": 0.5187,
      "step": 15448
    },
    {
      "epoch": 0.07266903111094386,
      "grad_norm": 1.5639636516571045,
      "learning_rate": 0.0001859034201816176,
      "loss": 0.1275,
      "step": 15449
    },
    {
      "epoch": 0.07267373491255633,
      "grad_norm": 1.11322820186615,
      "learning_rate": 0.0001859024772035041,
      "loss": 0.1528,
      "step": 15450
    },
    {
      "epoch": 0.07267843871416879,
      "grad_norm": 1.0585850477218628,
      "learning_rate": 0.00018590153422539063,
      "loss": 0.0459,
      "step": 15451
    },
    {
      "epoch": 0.07268314251578126,
      "grad_norm": 5.3550615310668945,
      "learning_rate": 0.00018590059124727718,
      "loss": 0.2881,
      "step": 15452
    },
    {
      "epoch": 0.07268784631739372,
      "grad_norm": 4.4714837074279785,
      "learning_rate": 0.0001858996482691637,
      "loss": 0.6814,
      "step": 15453
    },
    {
      "epoch": 0.07269255011900617,
      "grad_norm": 3.1951401233673096,
      "learning_rate": 0.00018589870529105021,
      "loss": 0.194,
      "step": 15454
    },
    {
      "epoch": 0.07269725392061864,
      "grad_norm": 7.59105110168457,
      "learning_rate": 0.0001858977623129367,
      "loss": 0.5082,
      "step": 15455
    },
    {
      "epoch": 0.0727019577222311,
      "grad_norm": 2.4874022006988525,
      "learning_rate": 0.00018589681933482325,
      "loss": 0.1694,
      "step": 15456
    },
    {
      "epoch": 0.07270666152384357,
      "grad_norm": 3.918818235397339,
      "learning_rate": 0.00018589587635670977,
      "loss": 0.3231,
      "step": 15457
    },
    {
      "epoch": 0.07271136532545604,
      "grad_norm": 1.627492070198059,
      "learning_rate": 0.0001858949333785963,
      "loss": 0.1099,
      "step": 15458
    },
    {
      "epoch": 0.0727160691270685,
      "grad_norm": 1.186126470565796,
      "learning_rate": 0.0001858939904004828,
      "loss": 0.092,
      "step": 15459
    },
    {
      "epoch": 0.07272077292868095,
      "grad_norm": 1.3042644262313843,
      "learning_rate": 0.00018589304742236933,
      "loss": 0.0999,
      "step": 15460
    },
    {
      "epoch": 0.07272547673029342,
      "grad_norm": 2.1603193283081055,
      "learning_rate": 0.00018589210444425587,
      "loss": 0.2322,
      "step": 15461
    },
    {
      "epoch": 0.07273018053190589,
      "grad_norm": 4.069153308868408,
      "learning_rate": 0.0001858911614661424,
      "loss": 0.5948,
      "step": 15462
    },
    {
      "epoch": 0.07273488433351835,
      "grad_norm": 2.397412061691284,
      "learning_rate": 0.0001858902184880289,
      "loss": 0.3507,
      "step": 15463
    },
    {
      "epoch": 0.07273958813513082,
      "grad_norm": 0.7973043918609619,
      "learning_rate": 0.00018588927550991543,
      "loss": 0.0501,
      "step": 15464
    },
    {
      "epoch": 0.07274429193674328,
      "grad_norm": 4.982278347015381,
      "learning_rate": 0.00018588833253180195,
      "loss": 1.1894,
      "step": 15465
    },
    {
      "epoch": 0.07274899573835573,
      "grad_norm": 5.52224588394165,
      "learning_rate": 0.00018588738955368846,
      "loss": 0.977,
      "step": 15466
    },
    {
      "epoch": 0.0727536995399682,
      "grad_norm": 0.4964110255241394,
      "learning_rate": 0.00018588644657557498,
      "loss": 0.0389,
      "step": 15467
    },
    {
      "epoch": 0.07275840334158067,
      "grad_norm": 6.050590515136719,
      "learning_rate": 0.0001858855035974615,
      "loss": 0.5532,
      "step": 15468
    },
    {
      "epoch": 0.07276310714319313,
      "grad_norm": 4.464722633361816,
      "learning_rate": 0.00018588456061934802,
      "loss": 0.9077,
      "step": 15469
    },
    {
      "epoch": 0.0727678109448056,
      "grad_norm": 2.3121585845947266,
      "learning_rate": 0.00018588361764123457,
      "loss": 0.2331,
      "step": 15470
    },
    {
      "epoch": 0.07277251474641805,
      "grad_norm": 1.5410974025726318,
      "learning_rate": 0.00018588267466312109,
      "loss": 0.2746,
      "step": 15471
    },
    {
      "epoch": 0.07277721854803051,
      "grad_norm": 0.3125077486038208,
      "learning_rate": 0.0001858817316850076,
      "loss": 0.0214,
      "step": 15472
    },
    {
      "epoch": 0.07278192234964298,
      "grad_norm": 3.0205793380737305,
      "learning_rate": 0.00018588078870689412,
      "loss": 0.3006,
      "step": 15473
    },
    {
      "epoch": 0.07278662615125545,
      "grad_norm": 0.7239862680435181,
      "learning_rate": 0.00018587984572878067,
      "loss": 0.0731,
      "step": 15474
    },
    {
      "epoch": 0.07279132995286791,
      "grad_norm": 3.0740344524383545,
      "learning_rate": 0.00018587890275066716,
      "loss": 0.6926,
      "step": 15475
    },
    {
      "epoch": 0.07279603375448038,
      "grad_norm": 1.1620150804519653,
      "learning_rate": 0.00018587795977255368,
      "loss": 0.1774,
      "step": 15476
    },
    {
      "epoch": 0.07280073755609283,
      "grad_norm": 8.269485473632812,
      "learning_rate": 0.0001858770167944402,
      "loss": 0.7201,
      "step": 15477
    },
    {
      "epoch": 0.0728054413577053,
      "grad_norm": 2.9852025508880615,
      "learning_rate": 0.00018587607381632672,
      "loss": 0.2559,
      "step": 15478
    },
    {
      "epoch": 0.07281014515931776,
      "grad_norm": 1.4830330610275269,
      "learning_rate": 0.00018587513083821326,
      "loss": 0.1678,
      "step": 15479
    },
    {
      "epoch": 0.07281484896093023,
      "grad_norm": 2.4250051975250244,
      "learning_rate": 0.00018587418786009978,
      "loss": 0.2454,
      "step": 15480
    },
    {
      "epoch": 0.07281955276254269,
      "grad_norm": 1.885219931602478,
      "learning_rate": 0.0001858732448819863,
      "loss": 0.3369,
      "step": 15481
    },
    {
      "epoch": 0.07282425656415516,
      "grad_norm": 2.963247299194336,
      "learning_rate": 0.00018587230190387282,
      "loss": 0.3402,
      "step": 15482
    },
    {
      "epoch": 0.07282896036576761,
      "grad_norm": 0.9794490337371826,
      "learning_rate": 0.00018587135892575934,
      "loss": 0.0822,
      "step": 15483
    },
    {
      "epoch": 0.07283366416738007,
      "grad_norm": 5.3830084800720215,
      "learning_rate": 0.00018587041594764588,
      "loss": 0.7689,
      "step": 15484
    },
    {
      "epoch": 0.07283836796899254,
      "grad_norm": 6.196926116943359,
      "learning_rate": 0.0001858694729695324,
      "loss": 0.6222,
      "step": 15485
    },
    {
      "epoch": 0.072843071770605,
      "grad_norm": 1.6343965530395508,
      "learning_rate": 0.0001858685299914189,
      "loss": 0.1634,
      "step": 15486
    },
    {
      "epoch": 0.07284777557221747,
      "grad_norm": 1.1394200325012207,
      "learning_rate": 0.0001858675870133054,
      "loss": 0.1447,
      "step": 15487
    },
    {
      "epoch": 0.07285247937382992,
      "grad_norm": 0.5806893110275269,
      "learning_rate": 0.00018586664403519196,
      "loss": 0.0486,
      "step": 15488
    },
    {
      "epoch": 0.07285718317544239,
      "grad_norm": 2.5448851585388184,
      "learning_rate": 0.00018586570105707847,
      "loss": 0.5406,
      "step": 15489
    },
    {
      "epoch": 0.07286188697705485,
      "grad_norm": 2.238680124282837,
      "learning_rate": 0.000185864758078965,
      "loss": 0.248,
      "step": 15490
    },
    {
      "epoch": 0.07286659077866732,
      "grad_norm": 2.525486946105957,
      "learning_rate": 0.0001858638151008515,
      "loss": 0.4909,
      "step": 15491
    },
    {
      "epoch": 0.07287129458027979,
      "grad_norm": 1.1509547233581543,
      "learning_rate": 0.00018586287212273803,
      "loss": 0.1903,
      "step": 15492
    },
    {
      "epoch": 0.07287599838189225,
      "grad_norm": 1.391059398651123,
      "learning_rate": 0.00018586192914462458,
      "loss": 0.225,
      "step": 15493
    },
    {
      "epoch": 0.0728807021835047,
      "grad_norm": 2.4130637645721436,
      "learning_rate": 0.0001858609861665111,
      "loss": 0.2727,
      "step": 15494
    },
    {
      "epoch": 0.07288540598511717,
      "grad_norm": 2.2880947589874268,
      "learning_rate": 0.00018586004318839761,
      "loss": 0.3236,
      "step": 15495
    },
    {
      "epoch": 0.07289010978672963,
      "grad_norm": 1.7605082988739014,
      "learning_rate": 0.00018585910021028413,
      "loss": 0.3802,
      "step": 15496
    },
    {
      "epoch": 0.0728948135883421,
      "grad_norm": 0.8690764307975769,
      "learning_rate": 0.00018585815723217065,
      "loss": 0.1392,
      "step": 15497
    },
    {
      "epoch": 0.07289951738995457,
      "grad_norm": 5.325560092926025,
      "learning_rate": 0.00018585721425405717,
      "loss": 0.4534,
      "step": 15498
    },
    {
      "epoch": 0.07290422119156703,
      "grad_norm": 2.391641139984131,
      "learning_rate": 0.0001858562712759437,
      "loss": 0.4043,
      "step": 15499
    },
    {
      "epoch": 0.07290892499317948,
      "grad_norm": 1.2033309936523438,
      "learning_rate": 0.0001858553282978302,
      "loss": 0.3842,
      "step": 15500
    },
    {
      "epoch": 0.07291362879479195,
      "grad_norm": 1.4475396871566772,
      "learning_rate": 0.00018585438531971673,
      "loss": 0.1288,
      "step": 15501
    },
    {
      "epoch": 0.07291833259640441,
      "grad_norm": 2.298614263534546,
      "learning_rate": 0.00018585344234160327,
      "loss": 0.2018,
      "step": 15502
    },
    {
      "epoch": 0.07292303639801688,
      "grad_norm": 2.355593681335449,
      "learning_rate": 0.0001858524993634898,
      "loss": 0.3,
      "step": 15503
    },
    {
      "epoch": 0.07292774019962935,
      "grad_norm": 0.5322896838188171,
      "learning_rate": 0.0001858515563853763,
      "loss": 0.0408,
      "step": 15504
    },
    {
      "epoch": 0.0729324440012418,
      "grad_norm": 3.547337055206299,
      "learning_rate": 0.00018585061340726283,
      "loss": 0.2892,
      "step": 15505
    },
    {
      "epoch": 0.07293714780285426,
      "grad_norm": 1.0974966287612915,
      "learning_rate": 0.00018584967042914935,
      "loss": 0.0942,
      "step": 15506
    },
    {
      "epoch": 0.07294185160446673,
      "grad_norm": 4.6193366050720215,
      "learning_rate": 0.00018584872745103586,
      "loss": 0.4332,
      "step": 15507
    },
    {
      "epoch": 0.0729465554060792,
      "grad_norm": 1.4007482528686523,
      "learning_rate": 0.00018584778447292238,
      "loss": 0.1556,
      "step": 15508
    },
    {
      "epoch": 0.07295125920769166,
      "grad_norm": 1.5955513715744019,
      "learning_rate": 0.0001858468414948089,
      "loss": 0.2475,
      "step": 15509
    },
    {
      "epoch": 0.07295596300930413,
      "grad_norm": 1.259290337562561,
      "learning_rate": 0.00018584589851669542,
      "loss": 0.1408,
      "step": 15510
    },
    {
      "epoch": 0.07296066681091658,
      "grad_norm": 1.0296894311904907,
      "learning_rate": 0.00018584495553858197,
      "loss": 0.0405,
      "step": 15511
    },
    {
      "epoch": 0.07296537061252904,
      "grad_norm": 1.529889702796936,
      "learning_rate": 0.00018584401256046849,
      "loss": 0.1994,
      "step": 15512
    },
    {
      "epoch": 0.07297007441414151,
      "grad_norm": 0.47794580459594727,
      "learning_rate": 0.000185843069582355,
      "loss": 0.0749,
      "step": 15513
    },
    {
      "epoch": 0.07297477821575397,
      "grad_norm": 2.573505401611328,
      "learning_rate": 0.00018584212660424152,
      "loss": 0.2216,
      "step": 15514
    },
    {
      "epoch": 0.07297948201736644,
      "grad_norm": 1.000295877456665,
      "learning_rate": 0.00018584118362612807,
      "loss": 0.1073,
      "step": 15515
    },
    {
      "epoch": 0.0729841858189789,
      "grad_norm": 0.6814974546432495,
      "learning_rate": 0.0001858402406480146,
      "loss": 0.0652,
      "step": 15516
    },
    {
      "epoch": 0.07298888962059136,
      "grad_norm": 2.2363333702087402,
      "learning_rate": 0.00018583929766990108,
      "loss": 0.4618,
      "step": 15517
    },
    {
      "epoch": 0.07299359342220382,
      "grad_norm": 3.442716598510742,
      "learning_rate": 0.0001858383546917876,
      "loss": 0.5254,
      "step": 15518
    },
    {
      "epoch": 0.07299829722381629,
      "grad_norm": 0.9370669722557068,
      "learning_rate": 0.00018583741171367412,
      "loss": 0.0798,
      "step": 15519
    },
    {
      "epoch": 0.07300300102542875,
      "grad_norm": 3.5206875801086426,
      "learning_rate": 0.00018583646873556066,
      "loss": 0.3186,
      "step": 15520
    },
    {
      "epoch": 0.07300770482704122,
      "grad_norm": 1.062463402748108,
      "learning_rate": 0.00018583552575744718,
      "loss": 0.0693,
      "step": 15521
    },
    {
      "epoch": 0.07301240862865367,
      "grad_norm": 0.8992617726325989,
      "learning_rate": 0.0001858345827793337,
      "loss": 0.1022,
      "step": 15522
    },
    {
      "epoch": 0.07301711243026614,
      "grad_norm": 2.0001776218414307,
      "learning_rate": 0.00018583363980122022,
      "loss": 0.1611,
      "step": 15523
    },
    {
      "epoch": 0.0730218162318786,
      "grad_norm": 4.222470760345459,
      "learning_rate": 0.00018583269682310676,
      "loss": 0.4602,
      "step": 15524
    },
    {
      "epoch": 0.07302652003349107,
      "grad_norm": 2.804074287414551,
      "learning_rate": 0.00018583175384499328,
      "loss": 0.3337,
      "step": 15525
    },
    {
      "epoch": 0.07303122383510353,
      "grad_norm": 2.0567259788513184,
      "learning_rate": 0.0001858308108668798,
      "loss": 0.3885,
      "step": 15526
    },
    {
      "epoch": 0.073035927636716,
      "grad_norm": 1.114946722984314,
      "learning_rate": 0.00018582986788876632,
      "loss": 0.0944,
      "step": 15527
    },
    {
      "epoch": 0.07304063143832845,
      "grad_norm": 5.4250288009643555,
      "learning_rate": 0.00018582892491065284,
      "loss": 0.9877,
      "step": 15528
    },
    {
      "epoch": 0.07304533523994092,
      "grad_norm": 1.3581804037094116,
      "learning_rate": 0.00018582798193253936,
      "loss": 0.1246,
      "step": 15529
    },
    {
      "epoch": 0.07305003904155338,
      "grad_norm": 2.464555263519287,
      "learning_rate": 0.00018582703895442587,
      "loss": 0.3417,
      "step": 15530
    },
    {
      "epoch": 0.07305474284316585,
      "grad_norm": 2.7860941886901855,
      "learning_rate": 0.0001858260959763124,
      "loss": 0.2206,
      "step": 15531
    },
    {
      "epoch": 0.07305944664477831,
      "grad_norm": 1.2216600179672241,
      "learning_rate": 0.0001858251529981989,
      "loss": 0.104,
      "step": 15532
    },
    {
      "epoch": 0.07306415044639078,
      "grad_norm": 3.3503572940826416,
      "learning_rate": 0.00018582421002008543,
      "loss": 0.693,
      "step": 15533
    },
    {
      "epoch": 0.07306885424800323,
      "grad_norm": 4.506666660308838,
      "learning_rate": 0.00018582326704197198,
      "loss": 0.6605,
      "step": 15534
    },
    {
      "epoch": 0.0730735580496157,
      "grad_norm": 2.35905122756958,
      "learning_rate": 0.0001858223240638585,
      "loss": 0.2182,
      "step": 15535
    },
    {
      "epoch": 0.07307826185122816,
      "grad_norm": 2.7292561531066895,
      "learning_rate": 0.00018582138108574501,
      "loss": 0.2642,
      "step": 15536
    },
    {
      "epoch": 0.07308296565284063,
      "grad_norm": 3.028068780899048,
      "learning_rate": 0.00018582043810763153,
      "loss": 0.2252,
      "step": 15537
    },
    {
      "epoch": 0.0730876694544531,
      "grad_norm": 2.6036651134490967,
      "learning_rate": 0.00018581949512951805,
      "loss": 0.5559,
      "step": 15538
    },
    {
      "epoch": 0.07309237325606555,
      "grad_norm": 0.2876897156238556,
      "learning_rate": 0.00018581855215140457,
      "loss": 0.0373,
      "step": 15539
    },
    {
      "epoch": 0.07309707705767801,
      "grad_norm": 0.8288827538490295,
      "learning_rate": 0.0001858176091732911,
      "loss": 0.0844,
      "step": 15540
    },
    {
      "epoch": 0.07310178085929048,
      "grad_norm": 2.8261899948120117,
      "learning_rate": 0.0001858166661951776,
      "loss": 0.5513,
      "step": 15541
    },
    {
      "epoch": 0.07310648466090294,
      "grad_norm": 0.39045512676239014,
      "learning_rate": 0.00018581572321706413,
      "loss": 0.0561,
      "step": 15542
    },
    {
      "epoch": 0.07311118846251541,
      "grad_norm": 2.7069742679595947,
      "learning_rate": 0.00018581478023895067,
      "loss": 0.5742,
      "step": 15543
    },
    {
      "epoch": 0.07311589226412787,
      "grad_norm": 2.1737148761749268,
      "learning_rate": 0.0001858138372608372,
      "loss": 0.536,
      "step": 15544
    },
    {
      "epoch": 0.07312059606574033,
      "grad_norm": 2.4413881301879883,
      "learning_rate": 0.0001858128942827237,
      "loss": 0.3164,
      "step": 15545
    },
    {
      "epoch": 0.07312529986735279,
      "grad_norm": 1.5555440187454224,
      "learning_rate": 0.00018581195130461023,
      "loss": 0.2213,
      "step": 15546
    },
    {
      "epoch": 0.07313000366896526,
      "grad_norm": 3.715385675430298,
      "learning_rate": 0.00018581100832649677,
      "loss": 1.0797,
      "step": 15547
    },
    {
      "epoch": 0.07313470747057772,
      "grad_norm": 2.069831609725952,
      "learning_rate": 0.00018581006534838326,
      "loss": 0.2187,
      "step": 15548
    },
    {
      "epoch": 0.07313941127219019,
      "grad_norm": 0.7876590490341187,
      "learning_rate": 0.00018580912237026978,
      "loss": 0.0435,
      "step": 15549
    },
    {
      "epoch": 0.07314411507380265,
      "grad_norm": 2.058415651321411,
      "learning_rate": 0.0001858081793921563,
      "loss": 0.2399,
      "step": 15550
    },
    {
      "epoch": 0.0731488188754151,
      "grad_norm": 5.509326457977295,
      "learning_rate": 0.00018580723641404282,
      "loss": 0.339,
      "step": 15551
    },
    {
      "epoch": 0.07315352267702757,
      "grad_norm": 1.7711745500564575,
      "learning_rate": 0.00018580629343592937,
      "loss": 0.1113,
      "step": 15552
    },
    {
      "epoch": 0.07315822647864004,
      "grad_norm": 0.5754250884056091,
      "learning_rate": 0.00018580535045781589,
      "loss": 0.0579,
      "step": 15553
    },
    {
      "epoch": 0.0731629302802525,
      "grad_norm": 4.920558929443359,
      "learning_rate": 0.0001858044074797024,
      "loss": 1.1629,
      "step": 15554
    },
    {
      "epoch": 0.07316763408186497,
      "grad_norm": 2.3736026287078857,
      "learning_rate": 0.00018580346450158892,
      "loss": 0.0985,
      "step": 15555
    },
    {
      "epoch": 0.07317233788347742,
      "grad_norm": 1.7698546648025513,
      "learning_rate": 0.00018580252152347547,
      "loss": 0.0673,
      "step": 15556
    },
    {
      "epoch": 0.07317704168508989,
      "grad_norm": 1.3327815532684326,
      "learning_rate": 0.000185801578545362,
      "loss": 0.1417,
      "step": 15557
    },
    {
      "epoch": 0.07318174548670235,
      "grad_norm": 7.950949192047119,
      "learning_rate": 0.0001858006355672485,
      "loss": 1.5954,
      "step": 15558
    },
    {
      "epoch": 0.07318644928831482,
      "grad_norm": 1.5503532886505127,
      "learning_rate": 0.000185799692589135,
      "loss": 0.0959,
      "step": 15559
    },
    {
      "epoch": 0.07319115308992728,
      "grad_norm": 3.3288896083831787,
      "learning_rate": 0.00018579874961102152,
      "loss": 0.6262,
      "step": 15560
    },
    {
      "epoch": 0.07319585689153975,
      "grad_norm": 4.026527404785156,
      "learning_rate": 0.00018579780663290806,
      "loss": 0.5837,
      "step": 15561
    },
    {
      "epoch": 0.0732005606931522,
      "grad_norm": 2.820220708847046,
      "learning_rate": 0.00018579686365479458,
      "loss": 0.2979,
      "step": 15562
    },
    {
      "epoch": 0.07320526449476467,
      "grad_norm": 3.494093418121338,
      "learning_rate": 0.0001857959206766811,
      "loss": 0.6268,
      "step": 15563
    },
    {
      "epoch": 0.07320996829637713,
      "grad_norm": 1.8445258140563965,
      "learning_rate": 0.00018579497769856762,
      "loss": 0.1356,
      "step": 15564
    },
    {
      "epoch": 0.0732146720979896,
      "grad_norm": 1.8491744995117188,
      "learning_rate": 0.00018579403472045416,
      "loss": 0.124,
      "step": 15565
    },
    {
      "epoch": 0.07321937589960206,
      "grad_norm": 0.5273553133010864,
      "learning_rate": 0.00018579309174234068,
      "loss": 0.044,
      "step": 15566
    },
    {
      "epoch": 0.07322407970121453,
      "grad_norm": 0.18120937049388885,
      "learning_rate": 0.0001857921487642272,
      "loss": 0.013,
      "step": 15567
    },
    {
      "epoch": 0.07322878350282698,
      "grad_norm": 6.966274261474609,
      "learning_rate": 0.00018579120578611372,
      "loss": 0.5244,
      "step": 15568
    },
    {
      "epoch": 0.07323348730443945,
      "grad_norm": 2.761273145675659,
      "learning_rate": 0.00018579026280800024,
      "loss": 0.6552,
      "step": 15569
    },
    {
      "epoch": 0.07323819110605191,
      "grad_norm": 2.9630508422851562,
      "learning_rate": 0.00018578931982988676,
      "loss": 0.3321,
      "step": 15570
    },
    {
      "epoch": 0.07324289490766438,
      "grad_norm": 0.7790998220443726,
      "learning_rate": 0.00018578837685177327,
      "loss": 0.0713,
      "step": 15571
    },
    {
      "epoch": 0.07324759870927684,
      "grad_norm": 1.85476553440094,
      "learning_rate": 0.0001857874338736598,
      "loss": 0.2164,
      "step": 15572
    },
    {
      "epoch": 0.0732523025108893,
      "grad_norm": 5.1565260887146,
      "learning_rate": 0.0001857864908955463,
      "loss": 0.918,
      "step": 15573
    },
    {
      "epoch": 0.07325700631250176,
      "grad_norm": 1.6908397674560547,
      "learning_rate": 0.00018578554791743286,
      "loss": 0.2745,
      "step": 15574
    },
    {
      "epoch": 0.07326171011411423,
      "grad_norm": 4.823449611663818,
      "learning_rate": 0.00018578460493931938,
      "loss": 0.3075,
      "step": 15575
    },
    {
      "epoch": 0.07326641391572669,
      "grad_norm": 0.7680242657661438,
      "learning_rate": 0.0001857836619612059,
      "loss": 0.099,
      "step": 15576
    },
    {
      "epoch": 0.07327111771733916,
      "grad_norm": 1.813620924949646,
      "learning_rate": 0.00018578271898309241,
      "loss": 0.3201,
      "step": 15577
    },
    {
      "epoch": 0.07327582151895162,
      "grad_norm": 0.13048332929611206,
      "learning_rate": 0.00018578177600497893,
      "loss": 0.0089,
      "step": 15578
    },
    {
      "epoch": 0.07328052532056407,
      "grad_norm": 2.197509765625,
      "learning_rate": 0.00018578083302686545,
      "loss": 0.1615,
      "step": 15579
    },
    {
      "epoch": 0.07328522912217654,
      "grad_norm": 0.9884469509124756,
      "learning_rate": 0.00018577989004875197,
      "loss": 0.1052,
      "step": 15580
    },
    {
      "epoch": 0.073289932923789,
      "grad_norm": 0.9427946209907532,
      "learning_rate": 0.0001857789470706385,
      "loss": 0.0884,
      "step": 15581
    },
    {
      "epoch": 0.07329463672540147,
      "grad_norm": 0.09962617605924606,
      "learning_rate": 0.000185778004092525,
      "loss": 0.007,
      "step": 15582
    },
    {
      "epoch": 0.07329934052701394,
      "grad_norm": 2.4260003566741943,
      "learning_rate": 0.00018577706111441153,
      "loss": 0.7119,
      "step": 15583
    },
    {
      "epoch": 0.0733040443286264,
      "grad_norm": 1.9262888431549072,
      "learning_rate": 0.00018577611813629807,
      "loss": 0.1825,
      "step": 15584
    },
    {
      "epoch": 0.07330874813023885,
      "grad_norm": 2.5200488567352295,
      "learning_rate": 0.0001857751751581846,
      "loss": 0.4824,
      "step": 15585
    },
    {
      "epoch": 0.07331345193185132,
      "grad_norm": 0.8313131928443909,
      "learning_rate": 0.0001857742321800711,
      "loss": 0.0877,
      "step": 15586
    },
    {
      "epoch": 0.07331815573346379,
      "grad_norm": 0.5230469107627869,
      "learning_rate": 0.00018577328920195763,
      "loss": 0.0263,
      "step": 15587
    },
    {
      "epoch": 0.07332285953507625,
      "grad_norm": 2.355607271194458,
      "learning_rate": 0.00018577234622384417,
      "loss": 0.1997,
      "step": 15588
    },
    {
      "epoch": 0.07332756333668872,
      "grad_norm": 0.4901045262813568,
      "learning_rate": 0.0001857714032457307,
      "loss": 0.0315,
      "step": 15589
    },
    {
      "epoch": 0.07333226713830117,
      "grad_norm": 0.9754489660263062,
      "learning_rate": 0.00018577046026761718,
      "loss": 0.1212,
      "step": 15590
    },
    {
      "epoch": 0.07333697093991363,
      "grad_norm": 2.6140120029449463,
      "learning_rate": 0.0001857695172895037,
      "loss": 0.4519,
      "step": 15591
    },
    {
      "epoch": 0.0733416747415261,
      "grad_norm": 3.093522071838379,
      "learning_rate": 0.00018576857431139022,
      "loss": 0.5113,
      "step": 15592
    },
    {
      "epoch": 0.07334637854313857,
      "grad_norm": 0.3253926932811737,
      "learning_rate": 0.00018576763133327677,
      "loss": 0.0332,
      "step": 15593
    },
    {
      "epoch": 0.07335108234475103,
      "grad_norm": 4.559832572937012,
      "learning_rate": 0.00018576668835516328,
      "loss": 0.3339,
      "step": 15594
    },
    {
      "epoch": 0.0733557861463635,
      "grad_norm": 4.355771541595459,
      "learning_rate": 0.0001857657453770498,
      "loss": 0.4928,
      "step": 15595
    },
    {
      "epoch": 0.07336048994797595,
      "grad_norm": 0.156348317861557,
      "learning_rate": 0.00018576480239893632,
      "loss": 0.0105,
      "step": 15596
    },
    {
      "epoch": 0.07336519374958841,
      "grad_norm": 0.4294165372848511,
      "learning_rate": 0.00018576385942082287,
      "loss": 0.033,
      "step": 15597
    },
    {
      "epoch": 0.07336989755120088,
      "grad_norm": 1.355170726776123,
      "learning_rate": 0.0001857629164427094,
      "loss": 0.2556,
      "step": 15598
    },
    {
      "epoch": 0.07337460135281335,
      "grad_norm": 0.3158612549304962,
      "learning_rate": 0.0001857619734645959,
      "loss": 0.0218,
      "step": 15599
    },
    {
      "epoch": 0.07337930515442581,
      "grad_norm": 4.556525230407715,
      "learning_rate": 0.00018576103048648242,
      "loss": 0.7456,
      "step": 15600
    },
    {
      "epoch": 0.07338400895603828,
      "grad_norm": 0.8349041938781738,
      "learning_rate": 0.00018576008750836894,
      "loss": 0.067,
      "step": 15601
    },
    {
      "epoch": 0.07338871275765073,
      "grad_norm": 2.399397850036621,
      "learning_rate": 0.00018575914453025546,
      "loss": 0.1586,
      "step": 15602
    },
    {
      "epoch": 0.0733934165592632,
      "grad_norm": 2.9989287853240967,
      "learning_rate": 0.00018575820155214198,
      "loss": 0.5266,
      "step": 15603
    },
    {
      "epoch": 0.07339812036087566,
      "grad_norm": 0.20168346166610718,
      "learning_rate": 0.0001857572585740285,
      "loss": 0.0192,
      "step": 15604
    },
    {
      "epoch": 0.07340282416248813,
      "grad_norm": 8.55834674835205,
      "learning_rate": 0.00018575631559591502,
      "loss": 0.5655,
      "step": 15605
    },
    {
      "epoch": 0.07340752796410059,
      "grad_norm": 16.62664031982422,
      "learning_rate": 0.00018575537261780156,
      "loss": 0.471,
      "step": 15606
    },
    {
      "epoch": 0.07341223176571304,
      "grad_norm": 2.0716326236724854,
      "learning_rate": 0.00018575442963968808,
      "loss": 0.5106,
      "step": 15607
    },
    {
      "epoch": 0.07341693556732551,
      "grad_norm": 1.7957948446273804,
      "learning_rate": 0.0001857534866615746,
      "loss": 0.1277,
      "step": 15608
    },
    {
      "epoch": 0.07342163936893797,
      "grad_norm": 3.9675917625427246,
      "learning_rate": 0.00018575254368346112,
      "loss": 0.4341,
      "step": 15609
    },
    {
      "epoch": 0.07342634317055044,
      "grad_norm": 1.4692168235778809,
      "learning_rate": 0.00018575160070534764,
      "loss": 0.1128,
      "step": 15610
    },
    {
      "epoch": 0.0734310469721629,
      "grad_norm": 2.0150363445281982,
      "learning_rate": 0.00018575065772723416,
      "loss": 0.115,
      "step": 15611
    },
    {
      "epoch": 0.07343575077377537,
      "grad_norm": 6.550388813018799,
      "learning_rate": 0.00018574971474912067,
      "loss": 1.2741,
      "step": 15612
    },
    {
      "epoch": 0.07344045457538782,
      "grad_norm": 0.18729296326637268,
      "learning_rate": 0.0001857487717710072,
      "loss": 0.0088,
      "step": 15613
    },
    {
      "epoch": 0.07344515837700029,
      "grad_norm": 1.4959666728973389,
      "learning_rate": 0.0001857478287928937,
      "loss": 0.1939,
      "step": 15614
    },
    {
      "epoch": 0.07344986217861275,
      "grad_norm": 1.8749299049377441,
      "learning_rate": 0.00018574688581478026,
      "loss": 0.2093,
      "step": 15615
    },
    {
      "epoch": 0.07345456598022522,
      "grad_norm": 9.542832374572754,
      "learning_rate": 0.00018574594283666678,
      "loss": 0.6907,
      "step": 15616
    },
    {
      "epoch": 0.07345926978183769,
      "grad_norm": 4.382299423217773,
      "learning_rate": 0.0001857449998585533,
      "loss": 0.677,
      "step": 15617
    },
    {
      "epoch": 0.07346397358345015,
      "grad_norm": 8.753293991088867,
      "learning_rate": 0.00018574405688043981,
      "loss": 0.9521,
      "step": 15618
    },
    {
      "epoch": 0.0734686773850626,
      "grad_norm": 0.37434080243110657,
      "learning_rate": 0.00018574311390232633,
      "loss": 0.0307,
      "step": 15619
    },
    {
      "epoch": 0.07347338118667507,
      "grad_norm": 6.880590438842773,
      "learning_rate": 0.00018574217092421288,
      "loss": 0.5264,
      "step": 15620
    },
    {
      "epoch": 0.07347808498828753,
      "grad_norm": 2.0945756435394287,
      "learning_rate": 0.00018574122794609937,
      "loss": 0.3128,
      "step": 15621
    },
    {
      "epoch": 0.0734827887899,
      "grad_norm": 0.44366252422332764,
      "learning_rate": 0.0001857402849679859,
      "loss": 0.0495,
      "step": 15622
    },
    {
      "epoch": 0.07348749259151247,
      "grad_norm": 2.462129831314087,
      "learning_rate": 0.0001857393419898724,
      "loss": 0.1192,
      "step": 15623
    },
    {
      "epoch": 0.07349219639312492,
      "grad_norm": 0.8249408006668091,
      "learning_rate": 0.00018573839901175895,
      "loss": 0.0742,
      "step": 15624
    },
    {
      "epoch": 0.07349690019473738,
      "grad_norm": 0.40904700756073,
      "learning_rate": 0.00018573745603364547,
      "loss": 0.0346,
      "step": 15625
    },
    {
      "epoch": 0.07350160399634985,
      "grad_norm": 2.0137994289398193,
      "learning_rate": 0.000185736513055532,
      "loss": 0.3052,
      "step": 15626
    },
    {
      "epoch": 0.07350630779796231,
      "grad_norm": 0.40057551860809326,
      "learning_rate": 0.0001857355700774185,
      "loss": 0.0333,
      "step": 15627
    },
    {
      "epoch": 0.07351101159957478,
      "grad_norm": 3.3408429622650146,
      "learning_rate": 0.00018573462709930503,
      "loss": 0.529,
      "step": 15628
    },
    {
      "epoch": 0.07351571540118725,
      "grad_norm": 0.6487983465194702,
      "learning_rate": 0.00018573368412119157,
      "loss": 0.0919,
      "step": 15629
    },
    {
      "epoch": 0.0735204192027997,
      "grad_norm": 1.1968154907226562,
      "learning_rate": 0.0001857327411430781,
      "loss": 0.1033,
      "step": 15630
    },
    {
      "epoch": 0.07352512300441216,
      "grad_norm": 2.8011887073516846,
      "learning_rate": 0.0001857317981649646,
      "loss": 0.4548,
      "step": 15631
    },
    {
      "epoch": 0.07352982680602463,
      "grad_norm": 3.6538333892822266,
      "learning_rate": 0.00018573085518685113,
      "loss": 0.9696,
      "step": 15632
    },
    {
      "epoch": 0.0735345306076371,
      "grad_norm": 1.552359700202942,
      "learning_rate": 0.00018572991220873762,
      "loss": 0.3303,
      "step": 15633
    },
    {
      "epoch": 0.07353923440924956,
      "grad_norm": 2.5860509872436523,
      "learning_rate": 0.00018572896923062417,
      "loss": 0.8155,
      "step": 15634
    },
    {
      "epoch": 0.07354393821086203,
      "grad_norm": 0.5081166625022888,
      "learning_rate": 0.00018572802625251068,
      "loss": 0.0668,
      "step": 15635
    },
    {
      "epoch": 0.07354864201247448,
      "grad_norm": 2.0018723011016846,
      "learning_rate": 0.0001857270832743972,
      "loss": 0.5058,
      "step": 15636
    },
    {
      "epoch": 0.07355334581408694,
      "grad_norm": 2.3294501304626465,
      "learning_rate": 0.00018572614029628372,
      "loss": 0.2897,
      "step": 15637
    },
    {
      "epoch": 0.07355804961569941,
      "grad_norm": 0.7693590521812439,
      "learning_rate": 0.00018572519731817027,
      "loss": 0.0552,
      "step": 15638
    },
    {
      "epoch": 0.07356275341731187,
      "grad_norm": 3.388953685760498,
      "learning_rate": 0.0001857242543400568,
      "loss": 0.432,
      "step": 15639
    },
    {
      "epoch": 0.07356745721892434,
      "grad_norm": 2.1880640983581543,
      "learning_rate": 0.0001857233113619433,
      "loss": 0.3445,
      "step": 15640
    },
    {
      "epoch": 0.07357216102053679,
      "grad_norm": 2.5182180404663086,
      "learning_rate": 0.00018572236838382982,
      "loss": 0.5323,
      "step": 15641
    },
    {
      "epoch": 0.07357686482214926,
      "grad_norm": 2.19694185256958,
      "learning_rate": 0.00018572142540571634,
      "loss": 0.2518,
      "step": 15642
    },
    {
      "epoch": 0.07358156862376172,
      "grad_norm": 1.0949867963790894,
      "learning_rate": 0.00018572048242760286,
      "loss": 0.1622,
      "step": 15643
    },
    {
      "epoch": 0.07358627242537419,
      "grad_norm": 2.0372140407562256,
      "learning_rate": 0.00018571953944948938,
      "loss": 0.2357,
      "step": 15644
    },
    {
      "epoch": 0.07359097622698665,
      "grad_norm": 1.823296070098877,
      "learning_rate": 0.0001857185964713759,
      "loss": 0.3815,
      "step": 15645
    },
    {
      "epoch": 0.07359568002859912,
      "grad_norm": 2.1981773376464844,
      "learning_rate": 0.00018571765349326242,
      "loss": 0.3757,
      "step": 15646
    },
    {
      "epoch": 0.07360038383021157,
      "grad_norm": 1.1897363662719727,
      "learning_rate": 0.00018571671051514896,
      "loss": 0.1536,
      "step": 15647
    },
    {
      "epoch": 0.07360508763182404,
      "grad_norm": 3.286395788192749,
      "learning_rate": 0.00018571576753703548,
      "loss": 0.7723,
      "step": 15648
    },
    {
      "epoch": 0.0736097914334365,
      "grad_norm": 1.3359273672103882,
      "learning_rate": 0.000185714824558922,
      "loss": 0.1466,
      "step": 15649
    },
    {
      "epoch": 0.07361449523504897,
      "grad_norm": 2.1348602771759033,
      "learning_rate": 0.00018571388158080852,
      "loss": 0.4793,
      "step": 15650
    },
    {
      "epoch": 0.07361919903666143,
      "grad_norm": 1.2375417947769165,
      "learning_rate": 0.00018571293860269504,
      "loss": 0.0691,
      "step": 15651
    },
    {
      "epoch": 0.0736239028382739,
      "grad_norm": 6.9614458084106445,
      "learning_rate": 0.00018571199562458156,
      "loss": 0.5575,
      "step": 15652
    },
    {
      "epoch": 0.07362860663988635,
      "grad_norm": 3.477271795272827,
      "learning_rate": 0.00018571105264646807,
      "loss": 0.4741,
      "step": 15653
    },
    {
      "epoch": 0.07363331044149882,
      "grad_norm": 1.475433111190796,
      "learning_rate": 0.0001857101096683546,
      "loss": 0.115,
      "step": 15654
    },
    {
      "epoch": 0.07363801424311128,
      "grad_norm": 1.9977856874465942,
      "learning_rate": 0.0001857091666902411,
      "loss": 0.2394,
      "step": 15655
    },
    {
      "epoch": 0.07364271804472375,
      "grad_norm": 2.2379417419433594,
      "learning_rate": 0.00018570822371212766,
      "loss": 0.2004,
      "step": 15656
    },
    {
      "epoch": 0.07364742184633621,
      "grad_norm": 2.699469566345215,
      "learning_rate": 0.00018570728073401418,
      "loss": 0.2846,
      "step": 15657
    },
    {
      "epoch": 0.07365212564794867,
      "grad_norm": 0.3571162223815918,
      "learning_rate": 0.0001857063377559007,
      "loss": 0.0413,
      "step": 15658
    },
    {
      "epoch": 0.07365682944956113,
      "grad_norm": 2.5009102821350098,
      "learning_rate": 0.00018570539477778721,
      "loss": 0.3156,
      "step": 15659
    },
    {
      "epoch": 0.0736615332511736,
      "grad_norm": 0.4894905686378479,
      "learning_rate": 0.00018570445179967373,
      "loss": 0.0405,
      "step": 15660
    },
    {
      "epoch": 0.07366623705278606,
      "grad_norm": 2.2461557388305664,
      "learning_rate": 0.00018570350882156028,
      "loss": 0.3374,
      "step": 15661
    },
    {
      "epoch": 0.07367094085439853,
      "grad_norm": 1.139169692993164,
      "learning_rate": 0.0001857025658434468,
      "loss": 0.1135,
      "step": 15662
    },
    {
      "epoch": 0.073675644656011,
      "grad_norm": 2.9026339054107666,
      "learning_rate": 0.00018570162286533332,
      "loss": 0.41,
      "step": 15663
    },
    {
      "epoch": 0.07368034845762345,
      "grad_norm": 2.8773274421691895,
      "learning_rate": 0.0001857006798872198,
      "loss": 0.3409,
      "step": 15664
    },
    {
      "epoch": 0.07368505225923591,
      "grad_norm": 3.3581063747406006,
      "learning_rate": 0.00018569973690910635,
      "loss": 0.5131,
      "step": 15665
    },
    {
      "epoch": 0.07368975606084838,
      "grad_norm": 1.2820640802383423,
      "learning_rate": 0.00018569879393099287,
      "loss": 0.167,
      "step": 15666
    },
    {
      "epoch": 0.07369445986246084,
      "grad_norm": 1.7785524129867554,
      "learning_rate": 0.0001856978509528794,
      "loss": 0.2399,
      "step": 15667
    },
    {
      "epoch": 0.07369916366407331,
      "grad_norm": 2.653914213180542,
      "learning_rate": 0.0001856969079747659,
      "loss": 0.2833,
      "step": 15668
    },
    {
      "epoch": 0.07370386746568577,
      "grad_norm": 1.9917384386062622,
      "learning_rate": 0.00018569596499665243,
      "loss": 0.3049,
      "step": 15669
    },
    {
      "epoch": 0.07370857126729823,
      "grad_norm": 2.4157259464263916,
      "learning_rate": 0.00018569502201853897,
      "loss": 0.3814,
      "step": 15670
    },
    {
      "epoch": 0.07371327506891069,
      "grad_norm": 3.059708595275879,
      "learning_rate": 0.0001856940790404255,
      "loss": 0.1763,
      "step": 15671
    },
    {
      "epoch": 0.07371797887052316,
      "grad_norm": 2.8433399200439453,
      "learning_rate": 0.000185693136062312,
      "loss": 0.5753,
      "step": 15672
    },
    {
      "epoch": 0.07372268267213562,
      "grad_norm": 1.0926313400268555,
      "learning_rate": 0.00018569219308419853,
      "loss": 0.2245,
      "step": 15673
    },
    {
      "epoch": 0.07372738647374809,
      "grad_norm": 2.2898430824279785,
      "learning_rate": 0.00018569125010608505,
      "loss": 0.05,
      "step": 15674
    },
    {
      "epoch": 0.07373209027536054,
      "grad_norm": 0.5620633363723755,
      "learning_rate": 0.00018569030712797157,
      "loss": 0.0692,
      "step": 15675
    },
    {
      "epoch": 0.073736794076973,
      "grad_norm": 1.5669312477111816,
      "learning_rate": 0.00018568936414985808,
      "loss": 0.257,
      "step": 15676
    },
    {
      "epoch": 0.07374149787858547,
      "grad_norm": 1.9840445518493652,
      "learning_rate": 0.0001856884211717446,
      "loss": 0.3178,
      "step": 15677
    },
    {
      "epoch": 0.07374620168019794,
      "grad_norm": 2.991842269897461,
      "learning_rate": 0.00018568747819363112,
      "loss": 0.6728,
      "step": 15678
    },
    {
      "epoch": 0.0737509054818104,
      "grad_norm": 3.13923716545105,
      "learning_rate": 0.00018568653521551767,
      "loss": 0.4102,
      "step": 15679
    },
    {
      "epoch": 0.07375560928342287,
      "grad_norm": 2.658651828765869,
      "learning_rate": 0.0001856855922374042,
      "loss": 0.3115,
      "step": 15680
    },
    {
      "epoch": 0.07376031308503532,
      "grad_norm": 1.3139081001281738,
      "learning_rate": 0.0001856846492592907,
      "loss": 0.163,
      "step": 15681
    },
    {
      "epoch": 0.07376501688664779,
      "grad_norm": 0.6252588033676147,
      "learning_rate": 0.00018568370628117722,
      "loss": 0.0767,
      "step": 15682
    },
    {
      "epoch": 0.07376972068826025,
      "grad_norm": 1.3113293647766113,
      "learning_rate": 0.00018568276330306374,
      "loss": 0.2484,
      "step": 15683
    },
    {
      "epoch": 0.07377442448987272,
      "grad_norm": 3.5845787525177,
      "learning_rate": 0.00018568182032495026,
      "loss": 0.5085,
      "step": 15684
    },
    {
      "epoch": 0.07377912829148518,
      "grad_norm": 3.191157579421997,
      "learning_rate": 0.00018568087734683678,
      "loss": 0.6355,
      "step": 15685
    },
    {
      "epoch": 0.07378383209309765,
      "grad_norm": 2.265049695968628,
      "learning_rate": 0.0001856799343687233,
      "loss": 0.4705,
      "step": 15686
    },
    {
      "epoch": 0.0737885358947101,
      "grad_norm": 6.0171732902526855,
      "learning_rate": 0.00018567899139060982,
      "loss": 0.2728,
      "step": 15687
    },
    {
      "epoch": 0.07379323969632257,
      "grad_norm": 0.38769325613975525,
      "learning_rate": 0.00018567804841249636,
      "loss": 0.03,
      "step": 15688
    },
    {
      "epoch": 0.07379794349793503,
      "grad_norm": 2.0236849784851074,
      "learning_rate": 0.00018567710543438288,
      "loss": 0.4192,
      "step": 15689
    },
    {
      "epoch": 0.0738026472995475,
      "grad_norm": 0.9609137177467346,
      "learning_rate": 0.0001856761624562694,
      "loss": 0.2498,
      "step": 15690
    },
    {
      "epoch": 0.07380735110115996,
      "grad_norm": 2.070725917816162,
      "learning_rate": 0.00018567521947815592,
      "loss": 0.2806,
      "step": 15691
    },
    {
      "epoch": 0.07381205490277241,
      "grad_norm": 1.545657753944397,
      "learning_rate": 0.00018567427650004244,
      "loss": 0.367,
      "step": 15692
    },
    {
      "epoch": 0.07381675870438488,
      "grad_norm": 2.5813112258911133,
      "learning_rate": 0.00018567333352192898,
      "loss": 0.3052,
      "step": 15693
    },
    {
      "epoch": 0.07382146250599735,
      "grad_norm": 1.7515679597854614,
      "learning_rate": 0.0001856723905438155,
      "loss": 0.2312,
      "step": 15694
    },
    {
      "epoch": 0.07382616630760981,
      "grad_norm": 2.926269769668579,
      "learning_rate": 0.000185671447565702,
      "loss": 0.6093,
      "step": 15695
    },
    {
      "epoch": 0.07383087010922228,
      "grad_norm": 2.615727424621582,
      "learning_rate": 0.0001856705045875885,
      "loss": 0.6064,
      "step": 15696
    },
    {
      "epoch": 0.07383557391083474,
      "grad_norm": 1.001458764076233,
      "learning_rate": 0.00018566956160947506,
      "loss": 0.2833,
      "step": 15697
    },
    {
      "epoch": 0.0738402777124472,
      "grad_norm": 1.0436928272247314,
      "learning_rate": 0.00018566861863136158,
      "loss": 0.1453,
      "step": 15698
    },
    {
      "epoch": 0.07384498151405966,
      "grad_norm": 2.6465721130371094,
      "learning_rate": 0.0001856676756532481,
      "loss": 0.3395,
      "step": 15699
    },
    {
      "epoch": 0.07384968531567213,
      "grad_norm": 1.6030067205429077,
      "learning_rate": 0.00018566673267513461,
      "loss": 0.2292,
      "step": 15700
    },
    {
      "epoch": 0.07385438911728459,
      "grad_norm": 2.210479259490967,
      "learning_rate": 0.00018566578969702113,
      "loss": 0.4341,
      "step": 15701
    },
    {
      "epoch": 0.07385909291889706,
      "grad_norm": 3.7836246490478516,
      "learning_rate": 0.00018566484671890768,
      "loss": 0.3999,
      "step": 15702
    },
    {
      "epoch": 0.07386379672050952,
      "grad_norm": 2.1847002506256104,
      "learning_rate": 0.0001856639037407942,
      "loss": 0.4071,
      "step": 15703
    },
    {
      "epoch": 0.07386850052212197,
      "grad_norm": 3.236158847808838,
      "learning_rate": 0.00018566296076268072,
      "loss": 0.5588,
      "step": 15704
    },
    {
      "epoch": 0.07387320432373444,
      "grad_norm": 0.9374879002571106,
      "learning_rate": 0.00018566201778456723,
      "loss": 0.1219,
      "step": 15705
    },
    {
      "epoch": 0.0738779081253469,
      "grad_norm": 0.8600250482559204,
      "learning_rate": 0.00018566107480645375,
      "loss": 0.029,
      "step": 15706
    },
    {
      "epoch": 0.07388261192695937,
      "grad_norm": 2.929147958755493,
      "learning_rate": 0.00018566013182834027,
      "loss": 0.4585,
      "step": 15707
    },
    {
      "epoch": 0.07388731572857184,
      "grad_norm": 0.8085658550262451,
      "learning_rate": 0.0001856591888502268,
      "loss": 0.0579,
      "step": 15708
    },
    {
      "epoch": 0.07389201953018429,
      "grad_norm": 1.228721261024475,
      "learning_rate": 0.0001856582458721133,
      "loss": 0.1907,
      "step": 15709
    },
    {
      "epoch": 0.07389672333179675,
      "grad_norm": 1.8070441484451294,
      "learning_rate": 0.00018565730289399983,
      "loss": 0.1477,
      "step": 15710
    },
    {
      "epoch": 0.07390142713340922,
      "grad_norm": 1.9862173795700073,
      "learning_rate": 0.00018565635991588637,
      "loss": 0.3455,
      "step": 15711
    },
    {
      "epoch": 0.07390613093502169,
      "grad_norm": 1.4559979438781738,
      "learning_rate": 0.0001856554169377729,
      "loss": 0.1749,
      "step": 15712
    },
    {
      "epoch": 0.07391083473663415,
      "grad_norm": 0.3480260372161865,
      "learning_rate": 0.0001856544739596594,
      "loss": 0.0319,
      "step": 15713
    },
    {
      "epoch": 0.07391553853824662,
      "grad_norm": 1.4270178079605103,
      "learning_rate": 0.00018565353098154593,
      "loss": 0.113,
      "step": 15714
    },
    {
      "epoch": 0.07392024233985907,
      "grad_norm": 2.003969192504883,
      "learning_rate": 0.00018565258800343245,
      "loss": 0.4183,
      "step": 15715
    },
    {
      "epoch": 0.07392494614147153,
      "grad_norm": 0.8665353059768677,
      "learning_rate": 0.00018565164502531897,
      "loss": 0.105,
      "step": 15716
    },
    {
      "epoch": 0.073929649943084,
      "grad_norm": 1.3854814767837524,
      "learning_rate": 0.00018565070204720548,
      "loss": 0.1098,
      "step": 15717
    },
    {
      "epoch": 0.07393435374469647,
      "grad_norm": 4.054560661315918,
      "learning_rate": 0.000185649759069092,
      "loss": 0.4942,
      "step": 15718
    },
    {
      "epoch": 0.07393905754630893,
      "grad_norm": 2.5328097343444824,
      "learning_rate": 0.00018564881609097852,
      "loss": 0.5634,
      "step": 15719
    },
    {
      "epoch": 0.0739437613479214,
      "grad_norm": 1.5809375047683716,
      "learning_rate": 0.00018564787311286507,
      "loss": 0.2067,
      "step": 15720
    },
    {
      "epoch": 0.07394846514953385,
      "grad_norm": 3.3213865756988525,
      "learning_rate": 0.0001856469301347516,
      "loss": 0.405,
      "step": 15721
    },
    {
      "epoch": 0.07395316895114631,
      "grad_norm": 0.2820674180984497,
      "learning_rate": 0.0001856459871566381,
      "loss": 0.0317,
      "step": 15722
    },
    {
      "epoch": 0.07395787275275878,
      "grad_norm": 1.135818362236023,
      "learning_rate": 0.00018564504417852462,
      "loss": 0.1483,
      "step": 15723
    },
    {
      "epoch": 0.07396257655437125,
      "grad_norm": 1.8578747510910034,
      "learning_rate": 0.00018564410120041117,
      "loss": 0.2519,
      "step": 15724
    },
    {
      "epoch": 0.07396728035598371,
      "grad_norm": 0.6099636554718018,
      "learning_rate": 0.0001856431582222977,
      "loss": 0.0988,
      "step": 15725
    },
    {
      "epoch": 0.07397198415759616,
      "grad_norm": 0.6723679900169373,
      "learning_rate": 0.00018564221524418418,
      "loss": 0.0802,
      "step": 15726
    },
    {
      "epoch": 0.07397668795920863,
      "grad_norm": 1.1202534437179565,
      "learning_rate": 0.0001856412722660707,
      "loss": 0.1217,
      "step": 15727
    },
    {
      "epoch": 0.0739813917608211,
      "grad_norm": 1.388335108757019,
      "learning_rate": 0.00018564032928795722,
      "loss": 0.1713,
      "step": 15728
    },
    {
      "epoch": 0.07398609556243356,
      "grad_norm": 2.1779613494873047,
      "learning_rate": 0.00018563938630984376,
      "loss": 0.5146,
      "step": 15729
    },
    {
      "epoch": 0.07399079936404603,
      "grad_norm": 0.25767260789871216,
      "learning_rate": 0.00018563844333173028,
      "loss": 0.0224,
      "step": 15730
    },
    {
      "epoch": 0.07399550316565849,
      "grad_norm": 1.387951374053955,
      "learning_rate": 0.0001856375003536168,
      "loss": 0.179,
      "step": 15731
    },
    {
      "epoch": 0.07400020696727094,
      "grad_norm": 2.296128034591675,
      "learning_rate": 0.00018563655737550332,
      "loss": 0.3325,
      "step": 15732
    },
    {
      "epoch": 0.07400491076888341,
      "grad_norm": 0.4352203607559204,
      "learning_rate": 0.00018563561439738986,
      "loss": 0.0596,
      "step": 15733
    },
    {
      "epoch": 0.07400961457049587,
      "grad_norm": 3.414170265197754,
      "learning_rate": 0.00018563467141927638,
      "loss": 0.6083,
      "step": 15734
    },
    {
      "epoch": 0.07401431837210834,
      "grad_norm": 0.5623006820678711,
      "learning_rate": 0.0001856337284411629,
      "loss": 0.0517,
      "step": 15735
    },
    {
      "epoch": 0.0740190221737208,
      "grad_norm": 4.304982662200928,
      "learning_rate": 0.00018563278546304942,
      "loss": 1.1685,
      "step": 15736
    },
    {
      "epoch": 0.07402372597533327,
      "grad_norm": 1.4687491655349731,
      "learning_rate": 0.0001856318424849359,
      "loss": 0.1558,
      "step": 15737
    },
    {
      "epoch": 0.07402842977694572,
      "grad_norm": 1.4103572368621826,
      "learning_rate": 0.00018563089950682246,
      "loss": 0.2676,
      "step": 15738
    },
    {
      "epoch": 0.07403313357855819,
      "grad_norm": 1.415696382522583,
      "learning_rate": 0.00018562995652870898,
      "loss": 0.1139,
      "step": 15739
    },
    {
      "epoch": 0.07403783738017065,
      "grad_norm": 1.6381890773773193,
      "learning_rate": 0.0001856290135505955,
      "loss": 0.1984,
      "step": 15740
    },
    {
      "epoch": 0.07404254118178312,
      "grad_norm": 0.26196402311325073,
      "learning_rate": 0.00018562807057248201,
      "loss": 0.0132,
      "step": 15741
    },
    {
      "epoch": 0.07404724498339559,
      "grad_norm": 2.132826566696167,
      "learning_rate": 0.00018562712759436853,
      "loss": 0.3446,
      "step": 15742
    },
    {
      "epoch": 0.07405194878500804,
      "grad_norm": 2.092820405960083,
      "learning_rate": 0.00018562618461625508,
      "loss": 0.2001,
      "step": 15743
    },
    {
      "epoch": 0.0740566525866205,
      "grad_norm": 3.5995469093322754,
      "learning_rate": 0.0001856252416381416,
      "loss": 0.7812,
      "step": 15744
    },
    {
      "epoch": 0.07406135638823297,
      "grad_norm": 3.4720346927642822,
      "learning_rate": 0.00018562429866002812,
      "loss": 0.8104,
      "step": 15745
    },
    {
      "epoch": 0.07406606018984543,
      "grad_norm": 0.6268982887268066,
      "learning_rate": 0.00018562335568191463,
      "loss": 0.0483,
      "step": 15746
    },
    {
      "epoch": 0.0740707639914579,
      "grad_norm": 2.043708086013794,
      "learning_rate": 0.00018562241270380115,
      "loss": 0.3836,
      "step": 15747
    },
    {
      "epoch": 0.07407546779307037,
      "grad_norm": 2.4648661613464355,
      "learning_rate": 0.00018562146972568767,
      "loss": 0.3709,
      "step": 15748
    },
    {
      "epoch": 0.07408017159468282,
      "grad_norm": 1.9918917417526245,
      "learning_rate": 0.0001856205267475742,
      "loss": 0.4119,
      "step": 15749
    },
    {
      "epoch": 0.07408487539629528,
      "grad_norm": 2.5818257331848145,
      "learning_rate": 0.0001856195837694607,
      "loss": 0.452,
      "step": 15750
    },
    {
      "epoch": 0.07408957919790775,
      "grad_norm": 2.6357381343841553,
      "learning_rate": 0.00018561864079134723,
      "loss": 0.2055,
      "step": 15751
    },
    {
      "epoch": 0.07409428299952021,
      "grad_norm": 3.6040589809417725,
      "learning_rate": 0.00018561769781323377,
      "loss": 0.4551,
      "step": 15752
    },
    {
      "epoch": 0.07409898680113268,
      "grad_norm": 2.304351806640625,
      "learning_rate": 0.0001856167548351203,
      "loss": 0.1628,
      "step": 15753
    },
    {
      "epoch": 0.07410369060274515,
      "grad_norm": 0.8303496241569519,
      "learning_rate": 0.0001856158118570068,
      "loss": 0.0485,
      "step": 15754
    },
    {
      "epoch": 0.0741083944043576,
      "grad_norm": 1.538323998451233,
      "learning_rate": 0.00018561486887889333,
      "loss": 0.1338,
      "step": 15755
    },
    {
      "epoch": 0.07411309820597006,
      "grad_norm": 1.3627337217330933,
      "learning_rate": 0.00018561392590077987,
      "loss": 0.0657,
      "step": 15756
    },
    {
      "epoch": 0.07411780200758253,
      "grad_norm": 1.2364680767059326,
      "learning_rate": 0.00018561298292266637,
      "loss": 0.1238,
      "step": 15757
    },
    {
      "epoch": 0.074122505809195,
      "grad_norm": 2.090831995010376,
      "learning_rate": 0.00018561203994455288,
      "loss": 0.2772,
      "step": 15758
    },
    {
      "epoch": 0.07412720961080746,
      "grad_norm": 2.2687525749206543,
      "learning_rate": 0.0001856110969664394,
      "loss": 0.4878,
      "step": 15759
    },
    {
      "epoch": 0.07413191341241991,
      "grad_norm": 2.0693464279174805,
      "learning_rate": 0.00018561015398832592,
      "loss": 0.2852,
      "step": 15760
    },
    {
      "epoch": 0.07413661721403238,
      "grad_norm": 9.41467571258545,
      "learning_rate": 0.00018560921101021247,
      "loss": 0.9313,
      "step": 15761
    },
    {
      "epoch": 0.07414132101564484,
      "grad_norm": 6.3413262367248535,
      "learning_rate": 0.000185608268032099,
      "loss": 0.8055,
      "step": 15762
    },
    {
      "epoch": 0.07414602481725731,
      "grad_norm": 4.355077266693115,
      "learning_rate": 0.0001856073250539855,
      "loss": 0.7075,
      "step": 15763
    },
    {
      "epoch": 0.07415072861886977,
      "grad_norm": 1.6122328042984009,
      "learning_rate": 0.00018560638207587202,
      "loss": 0.2283,
      "step": 15764
    },
    {
      "epoch": 0.07415543242048224,
      "grad_norm": 2.658186197280884,
      "learning_rate": 0.00018560543909775857,
      "loss": 0.2902,
      "step": 15765
    },
    {
      "epoch": 0.07416013622209469,
      "grad_norm": 2.8363232612609863,
      "learning_rate": 0.0001856044961196451,
      "loss": 0.417,
      "step": 15766
    },
    {
      "epoch": 0.07416484002370716,
      "grad_norm": 4.545974254608154,
      "learning_rate": 0.0001856035531415316,
      "loss": 0.7368,
      "step": 15767
    },
    {
      "epoch": 0.07416954382531962,
      "grad_norm": 2.9371702671051025,
      "learning_rate": 0.0001856026101634181,
      "loss": 0.4108,
      "step": 15768
    },
    {
      "epoch": 0.07417424762693209,
      "grad_norm": 1.9334384202957153,
      "learning_rate": 0.00018560166718530462,
      "loss": 0.3817,
      "step": 15769
    },
    {
      "epoch": 0.07417895142854455,
      "grad_norm": 3.1907804012298584,
      "learning_rate": 0.00018560072420719116,
      "loss": 0.607,
      "step": 15770
    },
    {
      "epoch": 0.07418365523015702,
      "grad_norm": 6.1888628005981445,
      "learning_rate": 0.00018559978122907768,
      "loss": 0.2984,
      "step": 15771
    },
    {
      "epoch": 0.07418835903176947,
      "grad_norm": 1.5434095859527588,
      "learning_rate": 0.0001855988382509642,
      "loss": 0.3779,
      "step": 15772
    },
    {
      "epoch": 0.07419306283338194,
      "grad_norm": 1.9925687313079834,
      "learning_rate": 0.00018559789527285072,
      "loss": 0.3074,
      "step": 15773
    },
    {
      "epoch": 0.0741977666349944,
      "grad_norm": 1.2050282955169678,
      "learning_rate": 0.00018559695229473726,
      "loss": 0.1305,
      "step": 15774
    },
    {
      "epoch": 0.07420247043660687,
      "grad_norm": 1.5069398880004883,
      "learning_rate": 0.00018559600931662378,
      "loss": 0.2659,
      "step": 15775
    },
    {
      "epoch": 0.07420717423821933,
      "grad_norm": 0.43817585706710815,
      "learning_rate": 0.0001855950663385103,
      "loss": 0.0513,
      "step": 15776
    },
    {
      "epoch": 0.07421187803983179,
      "grad_norm": 1.1128582954406738,
      "learning_rate": 0.00018559412336039682,
      "loss": 0.2238,
      "step": 15777
    },
    {
      "epoch": 0.07421658184144425,
      "grad_norm": 3.1619598865509033,
      "learning_rate": 0.00018559318038228334,
      "loss": 0.7373,
      "step": 15778
    },
    {
      "epoch": 0.07422128564305672,
      "grad_norm": 0.6094844341278076,
      "learning_rate": 0.00018559223740416986,
      "loss": 0.0829,
      "step": 15779
    },
    {
      "epoch": 0.07422598944466918,
      "grad_norm": 0.999969482421875,
      "learning_rate": 0.00018559129442605638,
      "loss": 0.169,
      "step": 15780
    },
    {
      "epoch": 0.07423069324628165,
      "grad_norm": 3.2493884563446045,
      "learning_rate": 0.0001855903514479429,
      "loss": 0.5626,
      "step": 15781
    },
    {
      "epoch": 0.07423539704789411,
      "grad_norm": 2.9406518936157227,
      "learning_rate": 0.00018558940846982941,
      "loss": 0.6146,
      "step": 15782
    },
    {
      "epoch": 0.07424010084950657,
      "grad_norm": 2.4667673110961914,
      "learning_rate": 0.00018558846549171596,
      "loss": 0.5174,
      "step": 15783
    },
    {
      "epoch": 0.07424480465111903,
      "grad_norm": 0.9115803241729736,
      "learning_rate": 0.00018558752251360248,
      "loss": 0.0899,
      "step": 15784
    },
    {
      "epoch": 0.0742495084527315,
      "grad_norm": 1.7948837280273438,
      "learning_rate": 0.000185586579535489,
      "loss": 0.2437,
      "step": 15785
    },
    {
      "epoch": 0.07425421225434396,
      "grad_norm": 3.3695671558380127,
      "learning_rate": 0.00018558563655737552,
      "loss": 0.9222,
      "step": 15786
    },
    {
      "epoch": 0.07425891605595643,
      "grad_norm": 1.3115218877792358,
      "learning_rate": 0.00018558469357926203,
      "loss": 0.3506,
      "step": 15787
    },
    {
      "epoch": 0.0742636198575689,
      "grad_norm": 1.1299903392791748,
      "learning_rate": 0.00018558375060114855,
      "loss": 0.2484,
      "step": 15788
    },
    {
      "epoch": 0.07426832365918135,
      "grad_norm": 0.36185213923454285,
      "learning_rate": 0.00018558280762303507,
      "loss": 0.0309,
      "step": 15789
    },
    {
      "epoch": 0.07427302746079381,
      "grad_norm": 0.9074003100395203,
      "learning_rate": 0.0001855818646449216,
      "loss": 0.1651,
      "step": 15790
    },
    {
      "epoch": 0.07427773126240628,
      "grad_norm": 1.0468577146530151,
      "learning_rate": 0.0001855809216668081,
      "loss": 0.3466,
      "step": 15791
    },
    {
      "epoch": 0.07428243506401874,
      "grad_norm": 0.8836577534675598,
      "learning_rate": 0.00018557997868869463,
      "loss": 0.1655,
      "step": 15792
    },
    {
      "epoch": 0.07428713886563121,
      "grad_norm": 1.8054492473602295,
      "learning_rate": 0.00018557903571058117,
      "loss": 0.2241,
      "step": 15793
    },
    {
      "epoch": 0.07429184266724366,
      "grad_norm": 2.096966028213501,
      "learning_rate": 0.0001855780927324677,
      "loss": 0.1744,
      "step": 15794
    },
    {
      "epoch": 0.07429654646885613,
      "grad_norm": 0.5682700872421265,
      "learning_rate": 0.0001855771497543542,
      "loss": 0.1329,
      "step": 15795
    },
    {
      "epoch": 0.07430125027046859,
      "grad_norm": 1.0589169263839722,
      "learning_rate": 0.00018557620677624073,
      "loss": 0.0959,
      "step": 15796
    },
    {
      "epoch": 0.07430595407208106,
      "grad_norm": 0.8231230974197388,
      "learning_rate": 0.00018557526379812727,
      "loss": 0.1536,
      "step": 15797
    },
    {
      "epoch": 0.07431065787369352,
      "grad_norm": 2.3944616317749023,
      "learning_rate": 0.0001855743208200138,
      "loss": 0.6056,
      "step": 15798
    },
    {
      "epoch": 0.07431536167530599,
      "grad_norm": 1.2050989866256714,
      "learning_rate": 0.00018557337784190028,
      "loss": 0.2989,
      "step": 15799
    },
    {
      "epoch": 0.07432006547691844,
      "grad_norm": 1.093258023262024,
      "learning_rate": 0.0001855724348637868,
      "loss": 0.3625,
      "step": 15800
    },
    {
      "epoch": 0.0743247692785309,
      "grad_norm": 7.200589656829834,
      "learning_rate": 0.00018557149188567332,
      "loss": 0.0978,
      "step": 15801
    },
    {
      "epoch": 0.07432947308014337,
      "grad_norm": 1.6734689474105835,
      "learning_rate": 0.00018557054890755987,
      "loss": 0.1283,
      "step": 15802
    },
    {
      "epoch": 0.07433417688175584,
      "grad_norm": 1.9075738191604614,
      "learning_rate": 0.0001855696059294464,
      "loss": 0.147,
      "step": 15803
    },
    {
      "epoch": 0.0743388806833683,
      "grad_norm": 2.4899351596832275,
      "learning_rate": 0.0001855686629513329,
      "loss": 0.1426,
      "step": 15804
    },
    {
      "epoch": 0.07434358448498077,
      "grad_norm": 2.9708683490753174,
      "learning_rate": 0.00018556771997321942,
      "loss": 0.1654,
      "step": 15805
    },
    {
      "epoch": 0.07434828828659322,
      "grad_norm": 2.1470742225646973,
      "learning_rate": 0.00018556677699510597,
      "loss": 0.2521,
      "step": 15806
    },
    {
      "epoch": 0.07435299208820569,
      "grad_norm": 2.4247124195098877,
      "learning_rate": 0.0001855658340169925,
      "loss": 0.2838,
      "step": 15807
    },
    {
      "epoch": 0.07435769588981815,
      "grad_norm": 1.1893593072891235,
      "learning_rate": 0.000185564891038879,
      "loss": 0.1354,
      "step": 15808
    },
    {
      "epoch": 0.07436239969143062,
      "grad_norm": 0.7969791293144226,
      "learning_rate": 0.00018556394806076553,
      "loss": 0.1176,
      "step": 15809
    },
    {
      "epoch": 0.07436710349304308,
      "grad_norm": 2.7950282096862793,
      "learning_rate": 0.00018556300508265202,
      "loss": 0.4467,
      "step": 15810
    },
    {
      "epoch": 0.07437180729465553,
      "grad_norm": 0.32696107029914856,
      "learning_rate": 0.00018556206210453856,
      "loss": 0.0219,
      "step": 15811
    },
    {
      "epoch": 0.074376511096268,
      "grad_norm": 2.995727777481079,
      "learning_rate": 0.00018556111912642508,
      "loss": 0.6318,
      "step": 15812
    },
    {
      "epoch": 0.07438121489788047,
      "grad_norm": 3.018927574157715,
      "learning_rate": 0.0001855601761483116,
      "loss": 0.3472,
      "step": 15813
    },
    {
      "epoch": 0.07438591869949293,
      "grad_norm": 0.3935372233390808,
      "learning_rate": 0.00018555923317019812,
      "loss": 0.0743,
      "step": 15814
    },
    {
      "epoch": 0.0743906225011054,
      "grad_norm": 1.848083734512329,
      "learning_rate": 0.00018555829019208466,
      "loss": 0.2546,
      "step": 15815
    },
    {
      "epoch": 0.07439532630271786,
      "grad_norm": 0.6138840317726135,
      "learning_rate": 0.00018555734721397118,
      "loss": 0.0629,
      "step": 15816
    },
    {
      "epoch": 0.07440003010433031,
      "grad_norm": 1.4547672271728516,
      "learning_rate": 0.0001855564042358577,
      "loss": 0.181,
      "step": 15817
    },
    {
      "epoch": 0.07440473390594278,
      "grad_norm": 0.2922232449054718,
      "learning_rate": 0.00018555546125774422,
      "loss": 0.0419,
      "step": 15818
    },
    {
      "epoch": 0.07440943770755525,
      "grad_norm": 5.509788513183594,
      "learning_rate": 0.00018555451827963074,
      "loss": 0.136,
      "step": 15819
    },
    {
      "epoch": 0.07441414150916771,
      "grad_norm": 1.548364520072937,
      "learning_rate": 0.00018555357530151726,
      "loss": 0.1292,
      "step": 15820
    },
    {
      "epoch": 0.07441884531078018,
      "grad_norm": 1.6890758275985718,
      "learning_rate": 0.00018555263232340378,
      "loss": 0.1807,
      "step": 15821
    },
    {
      "epoch": 0.07442354911239264,
      "grad_norm": 0.8744797706604004,
      "learning_rate": 0.0001855516893452903,
      "loss": 0.1216,
      "step": 15822
    },
    {
      "epoch": 0.0744282529140051,
      "grad_norm": 1.639212727546692,
      "learning_rate": 0.00018555074636717681,
      "loss": 0.1936,
      "step": 15823
    },
    {
      "epoch": 0.07443295671561756,
      "grad_norm": 1.3656766414642334,
      "learning_rate": 0.00018554980338906336,
      "loss": 0.1482,
      "step": 15824
    },
    {
      "epoch": 0.07443766051723003,
      "grad_norm": 0.9918995499610901,
      "learning_rate": 0.00018554886041094988,
      "loss": 0.1078,
      "step": 15825
    },
    {
      "epoch": 0.07444236431884249,
      "grad_norm": 0.3313581645488739,
      "learning_rate": 0.0001855479174328364,
      "loss": 0.0271,
      "step": 15826
    },
    {
      "epoch": 0.07444706812045496,
      "grad_norm": 1.847399115562439,
      "learning_rate": 0.00018554697445472292,
      "loss": 0.2841,
      "step": 15827
    },
    {
      "epoch": 0.07445177192206741,
      "grad_norm": 5.104203701019287,
      "learning_rate": 0.00018554603147660943,
      "loss": 0.8507,
      "step": 15828
    },
    {
      "epoch": 0.07445647572367987,
      "grad_norm": 1.5838078260421753,
      "learning_rate": 0.00018554508849849598,
      "loss": 0.2516,
      "step": 15829
    },
    {
      "epoch": 0.07446117952529234,
      "grad_norm": 4.7820539474487305,
      "learning_rate": 0.00018554414552038247,
      "loss": 0.6882,
      "step": 15830
    },
    {
      "epoch": 0.0744658833269048,
      "grad_norm": 2.7519845962524414,
      "learning_rate": 0.000185543202542269,
      "loss": 0.3774,
      "step": 15831
    },
    {
      "epoch": 0.07447058712851727,
      "grad_norm": 1.0506232976913452,
      "learning_rate": 0.0001855422595641555,
      "loss": 0.0388,
      "step": 15832
    },
    {
      "epoch": 0.07447529093012974,
      "grad_norm": 0.28797993063926697,
      "learning_rate": 0.00018554131658604205,
      "loss": 0.0205,
      "step": 15833
    },
    {
      "epoch": 0.07447999473174219,
      "grad_norm": 3.40252423286438,
      "learning_rate": 0.00018554037360792857,
      "loss": 0.282,
      "step": 15834
    },
    {
      "epoch": 0.07448469853335465,
      "grad_norm": 1.6453649997711182,
      "learning_rate": 0.0001855394306298151,
      "loss": 0.3036,
      "step": 15835
    },
    {
      "epoch": 0.07448940233496712,
      "grad_norm": 3.8531253337860107,
      "learning_rate": 0.0001855384876517016,
      "loss": 0.5437,
      "step": 15836
    },
    {
      "epoch": 0.07449410613657959,
      "grad_norm": 3.0304689407348633,
      "learning_rate": 0.00018553754467358813,
      "loss": 0.3598,
      "step": 15837
    },
    {
      "epoch": 0.07449880993819205,
      "grad_norm": 1.97907555103302,
      "learning_rate": 0.00018553660169547467,
      "loss": 0.1473,
      "step": 15838
    },
    {
      "epoch": 0.07450351373980452,
      "grad_norm": 1.388107419013977,
      "learning_rate": 0.0001855356587173612,
      "loss": 0.1677,
      "step": 15839
    },
    {
      "epoch": 0.07450821754141697,
      "grad_norm": 1.681929349899292,
      "learning_rate": 0.0001855347157392477,
      "loss": 0.3292,
      "step": 15840
    },
    {
      "epoch": 0.07451292134302943,
      "grad_norm": 1.2026809453964233,
      "learning_rate": 0.0001855337727611342,
      "loss": 0.101,
      "step": 15841
    },
    {
      "epoch": 0.0745176251446419,
      "grad_norm": 0.3957194685935974,
      "learning_rate": 0.00018553282978302072,
      "loss": 0.0265,
      "step": 15842
    },
    {
      "epoch": 0.07452232894625437,
      "grad_norm": 4.553655624389648,
      "learning_rate": 0.00018553188680490727,
      "loss": 0.9093,
      "step": 15843
    },
    {
      "epoch": 0.07452703274786683,
      "grad_norm": 0.5710302591323853,
      "learning_rate": 0.0001855309438267938,
      "loss": 0.0642,
      "step": 15844
    },
    {
      "epoch": 0.07453173654947928,
      "grad_norm": 2.8676581382751465,
      "learning_rate": 0.0001855300008486803,
      "loss": 0.4856,
      "step": 15845
    },
    {
      "epoch": 0.07453644035109175,
      "grad_norm": 1.7342547178268433,
      "learning_rate": 0.00018552905787056682,
      "loss": 0.1864,
      "step": 15846
    },
    {
      "epoch": 0.07454114415270421,
      "grad_norm": 1.6506810188293457,
      "learning_rate": 0.00018552811489245337,
      "loss": 0.1215,
      "step": 15847
    },
    {
      "epoch": 0.07454584795431668,
      "grad_norm": 0.4443305432796478,
      "learning_rate": 0.0001855271719143399,
      "loss": 0.0333,
      "step": 15848
    },
    {
      "epoch": 0.07455055175592915,
      "grad_norm": 1.4417834281921387,
      "learning_rate": 0.0001855262289362264,
      "loss": 0.2619,
      "step": 15849
    },
    {
      "epoch": 0.07455525555754161,
      "grad_norm": 0.9818901419639587,
      "learning_rate": 0.00018552528595811293,
      "loss": 0.1499,
      "step": 15850
    },
    {
      "epoch": 0.07455995935915406,
      "grad_norm": 2.758342981338501,
      "learning_rate": 0.00018552434297999944,
      "loss": 0.2575,
      "step": 15851
    },
    {
      "epoch": 0.07456466316076653,
      "grad_norm": 3.173118829727173,
      "learning_rate": 0.00018552340000188596,
      "loss": 0.251,
      "step": 15852
    },
    {
      "epoch": 0.074569366962379,
      "grad_norm": 0.15312078595161438,
      "learning_rate": 0.00018552245702377248,
      "loss": 0.0142,
      "step": 15853
    },
    {
      "epoch": 0.07457407076399146,
      "grad_norm": 0.8711983561515808,
      "learning_rate": 0.000185521514045659,
      "loss": 0.0575,
      "step": 15854
    },
    {
      "epoch": 0.07457877456560393,
      "grad_norm": 1.535773515701294,
      "learning_rate": 0.00018552057106754552,
      "loss": 0.1131,
      "step": 15855
    },
    {
      "epoch": 0.07458347836721639,
      "grad_norm": 3.5301640033721924,
      "learning_rate": 0.00018551962808943206,
      "loss": 0.3343,
      "step": 15856
    },
    {
      "epoch": 0.07458818216882884,
      "grad_norm": 0.8603851199150085,
      "learning_rate": 0.00018551868511131858,
      "loss": 0.0717,
      "step": 15857
    },
    {
      "epoch": 0.07459288597044131,
      "grad_norm": 1.8364685773849487,
      "learning_rate": 0.0001855177421332051,
      "loss": 0.0965,
      "step": 15858
    },
    {
      "epoch": 0.07459758977205377,
      "grad_norm": 0.8745225667953491,
      "learning_rate": 0.00018551679915509162,
      "loss": 0.0781,
      "step": 15859
    },
    {
      "epoch": 0.07460229357366624,
      "grad_norm": 5.759162425994873,
      "learning_rate": 0.00018551585617697814,
      "loss": 0.5591,
      "step": 15860
    },
    {
      "epoch": 0.0746069973752787,
      "grad_norm": 6.929806709289551,
      "learning_rate": 0.00018551491319886466,
      "loss": 1.2893,
      "step": 15861
    },
    {
      "epoch": 0.07461170117689116,
      "grad_norm": 0.15384340286254883,
      "learning_rate": 0.00018551397022075118,
      "loss": 0.0087,
      "step": 15862
    },
    {
      "epoch": 0.07461640497850362,
      "grad_norm": 3.1761634349823,
      "learning_rate": 0.0001855130272426377,
      "loss": 0.7432,
      "step": 15863
    },
    {
      "epoch": 0.07462110878011609,
      "grad_norm": 3.5495622158050537,
      "learning_rate": 0.00018551208426452421,
      "loss": 0.2559,
      "step": 15864
    },
    {
      "epoch": 0.07462581258172855,
      "grad_norm": 3.20513916015625,
      "learning_rate": 0.00018551114128641076,
      "loss": 0.5805,
      "step": 15865
    },
    {
      "epoch": 0.07463051638334102,
      "grad_norm": 3.6874921321868896,
      "learning_rate": 0.00018551019830829728,
      "loss": 0.3976,
      "step": 15866
    },
    {
      "epoch": 0.07463522018495349,
      "grad_norm": 0.9341010451316833,
      "learning_rate": 0.0001855092553301838,
      "loss": 0.0899,
      "step": 15867
    },
    {
      "epoch": 0.07463992398656594,
      "grad_norm": 2.4362800121307373,
      "learning_rate": 0.00018550831235207032,
      "loss": 0.4046,
      "step": 15868
    },
    {
      "epoch": 0.0746446277881784,
      "grad_norm": 0.8568705320358276,
      "learning_rate": 0.00018550736937395683,
      "loss": 0.0862,
      "step": 15869
    },
    {
      "epoch": 0.07464933158979087,
      "grad_norm": 2.829056978225708,
      "learning_rate": 0.00018550642639584338,
      "loss": 0.2762,
      "step": 15870
    },
    {
      "epoch": 0.07465403539140333,
      "grad_norm": 1.0368373394012451,
      "learning_rate": 0.0001855054834177299,
      "loss": 0.1214,
      "step": 15871
    },
    {
      "epoch": 0.0746587391930158,
      "grad_norm": 0.2735866606235504,
      "learning_rate": 0.0001855045404396164,
      "loss": 0.0264,
      "step": 15872
    },
    {
      "epoch": 0.07466344299462827,
      "grad_norm": 3.5891568660736084,
      "learning_rate": 0.0001855035974615029,
      "loss": 0.5013,
      "step": 15873
    },
    {
      "epoch": 0.07466814679624072,
      "grad_norm": 2.2720906734466553,
      "learning_rate": 0.00018550265448338945,
      "loss": 0.49,
      "step": 15874
    },
    {
      "epoch": 0.07467285059785318,
      "grad_norm": 2.388746500015259,
      "learning_rate": 0.00018550171150527597,
      "loss": 0.1394,
      "step": 15875
    },
    {
      "epoch": 0.07467755439946565,
      "grad_norm": 2.315620183944702,
      "learning_rate": 0.0001855007685271625,
      "loss": 0.5244,
      "step": 15876
    },
    {
      "epoch": 0.07468225820107811,
      "grad_norm": 1.2151556015014648,
      "learning_rate": 0.000185499825549049,
      "loss": 0.0918,
      "step": 15877
    },
    {
      "epoch": 0.07468696200269058,
      "grad_norm": 1.450435757637024,
      "learning_rate": 0.00018549888257093553,
      "loss": 0.1194,
      "step": 15878
    },
    {
      "epoch": 0.07469166580430303,
      "grad_norm": 1.671958565711975,
      "learning_rate": 0.00018549793959282207,
      "loss": 0.2524,
      "step": 15879
    },
    {
      "epoch": 0.0746963696059155,
      "grad_norm": 1.5055267810821533,
      "learning_rate": 0.0001854969966147086,
      "loss": 0.1297,
      "step": 15880
    },
    {
      "epoch": 0.07470107340752796,
      "grad_norm": 0.4512348175048828,
      "learning_rate": 0.0001854960536365951,
      "loss": 0.0502,
      "step": 15881
    },
    {
      "epoch": 0.07470577720914043,
      "grad_norm": 1.7420543432235718,
      "learning_rate": 0.00018549511065848163,
      "loss": 0.1284,
      "step": 15882
    },
    {
      "epoch": 0.0747104810107529,
      "grad_norm": 0.5680060982704163,
      "learning_rate": 0.00018549416768036815,
      "loss": 0.0599,
      "step": 15883
    },
    {
      "epoch": 0.07471518481236536,
      "grad_norm": 0.2826402187347412,
      "learning_rate": 0.00018549322470225467,
      "loss": 0.0199,
      "step": 15884
    },
    {
      "epoch": 0.07471988861397781,
      "grad_norm": 1.6108834743499756,
      "learning_rate": 0.00018549228172414119,
      "loss": 0.2406,
      "step": 15885
    },
    {
      "epoch": 0.07472459241559028,
      "grad_norm": 1.1379241943359375,
      "learning_rate": 0.0001854913387460277,
      "loss": 0.1321,
      "step": 15886
    },
    {
      "epoch": 0.07472929621720274,
      "grad_norm": 0.9858807325363159,
      "learning_rate": 0.00018549039576791422,
      "loss": 0.1089,
      "step": 15887
    },
    {
      "epoch": 0.07473400001881521,
      "grad_norm": 1.2352638244628906,
      "learning_rate": 0.00018548945278980077,
      "loss": 0.1091,
      "step": 15888
    },
    {
      "epoch": 0.07473870382042767,
      "grad_norm": 0.6578578352928162,
      "learning_rate": 0.0001854885098116873,
      "loss": 0.1435,
      "step": 15889
    },
    {
      "epoch": 0.07474340762204014,
      "grad_norm": 0.5601062774658203,
      "learning_rate": 0.0001854875668335738,
      "loss": 0.0837,
      "step": 15890
    },
    {
      "epoch": 0.07474811142365259,
      "grad_norm": 1.8650294542312622,
      "learning_rate": 0.00018548662385546033,
      "loss": 0.2535,
      "step": 15891
    },
    {
      "epoch": 0.07475281522526506,
      "grad_norm": 2.938779592514038,
      "learning_rate": 0.00018548568087734684,
      "loss": 0.7796,
      "step": 15892
    },
    {
      "epoch": 0.07475751902687752,
      "grad_norm": 1.6946483850479126,
      "learning_rate": 0.00018548473789923336,
      "loss": 0.3441,
      "step": 15893
    },
    {
      "epoch": 0.07476222282848999,
      "grad_norm": 6.262720584869385,
      "learning_rate": 0.00018548379492111988,
      "loss": 0.1621,
      "step": 15894
    },
    {
      "epoch": 0.07476692663010245,
      "grad_norm": 3.4965450763702393,
      "learning_rate": 0.0001854828519430064,
      "loss": 1.1323,
      "step": 15895
    },
    {
      "epoch": 0.0747716304317149,
      "grad_norm": 1.117651343345642,
      "learning_rate": 0.00018548190896489292,
      "loss": 0.1347,
      "step": 15896
    },
    {
      "epoch": 0.07477633423332737,
      "grad_norm": 3.1692466735839844,
      "learning_rate": 0.00018548096598677946,
      "loss": 0.4975,
      "step": 15897
    },
    {
      "epoch": 0.07478103803493984,
      "grad_norm": 2.078890800476074,
      "learning_rate": 0.00018548002300866598,
      "loss": 0.4271,
      "step": 15898
    },
    {
      "epoch": 0.0747857418365523,
      "grad_norm": 2.128181219100952,
      "learning_rate": 0.0001854790800305525,
      "loss": 0.2606,
      "step": 15899
    },
    {
      "epoch": 0.07479044563816477,
      "grad_norm": 1.3775144815444946,
      "learning_rate": 0.00018547813705243902,
      "loss": 0.2678,
      "step": 15900
    },
    {
      "epoch": 0.07479514943977723,
      "grad_norm": 0.4202796518802643,
      "learning_rate": 0.00018547719407432554,
      "loss": 0.055,
      "step": 15901
    },
    {
      "epoch": 0.07479985324138969,
      "grad_norm": 2.6643056869506836,
      "learning_rate": 0.00018547625109621208,
      "loss": 0.2568,
      "step": 15902
    },
    {
      "epoch": 0.07480455704300215,
      "grad_norm": 2.5735771656036377,
      "learning_rate": 0.00018547530811809858,
      "loss": 0.2245,
      "step": 15903
    },
    {
      "epoch": 0.07480926084461462,
      "grad_norm": 1.252503752708435,
      "learning_rate": 0.0001854743651399851,
      "loss": 0.0663,
      "step": 15904
    },
    {
      "epoch": 0.07481396464622708,
      "grad_norm": 6.097943305969238,
      "learning_rate": 0.0001854734221618716,
      "loss": 0.5109,
      "step": 15905
    },
    {
      "epoch": 0.07481866844783955,
      "grad_norm": 1.1235716342926025,
      "learning_rate": 0.00018547247918375816,
      "loss": 0.065,
      "step": 15906
    },
    {
      "epoch": 0.07482337224945201,
      "grad_norm": 2.750765800476074,
      "learning_rate": 0.00018547153620564468,
      "loss": 0.2288,
      "step": 15907
    },
    {
      "epoch": 0.07482807605106447,
      "grad_norm": 7.172349452972412,
      "learning_rate": 0.0001854705932275312,
      "loss": 1.1053,
      "step": 15908
    },
    {
      "epoch": 0.07483277985267693,
      "grad_norm": 2.1164679527282715,
      "learning_rate": 0.00018546965024941772,
      "loss": 0.3338,
      "step": 15909
    },
    {
      "epoch": 0.0748374836542894,
      "grad_norm": 1.0331859588623047,
      "learning_rate": 0.00018546870727130423,
      "loss": 0.0908,
      "step": 15910
    },
    {
      "epoch": 0.07484218745590186,
      "grad_norm": 6.398598670959473,
      "learning_rate": 0.00018546776429319078,
      "loss": 0.7392,
      "step": 15911
    },
    {
      "epoch": 0.07484689125751433,
      "grad_norm": 2.1559858322143555,
      "learning_rate": 0.0001854668213150773,
      "loss": 0.1028,
      "step": 15912
    },
    {
      "epoch": 0.07485159505912678,
      "grad_norm": 1.4460586309432983,
      "learning_rate": 0.00018546587833696382,
      "loss": 0.1875,
      "step": 15913
    },
    {
      "epoch": 0.07485629886073925,
      "grad_norm": 2.1583826541900635,
      "learning_rate": 0.00018546493535885034,
      "loss": 0.3283,
      "step": 15914
    },
    {
      "epoch": 0.07486100266235171,
      "grad_norm": 0.8356152772903442,
      "learning_rate": 0.00018546399238073685,
      "loss": 0.1607,
      "step": 15915
    },
    {
      "epoch": 0.07486570646396418,
      "grad_norm": 2.3497676849365234,
      "learning_rate": 0.00018546304940262337,
      "loss": 0.1429,
      "step": 15916
    },
    {
      "epoch": 0.07487041026557664,
      "grad_norm": 0.6027330756187439,
      "learning_rate": 0.0001854621064245099,
      "loss": 0.0365,
      "step": 15917
    },
    {
      "epoch": 0.07487511406718911,
      "grad_norm": 0.24576951563358307,
      "learning_rate": 0.0001854611634463964,
      "loss": 0.0291,
      "step": 15918
    },
    {
      "epoch": 0.07487981786880156,
      "grad_norm": 7.8734517097473145,
      "learning_rate": 0.00018546022046828293,
      "loss": 0.9651,
      "step": 15919
    },
    {
      "epoch": 0.07488452167041403,
      "grad_norm": 1.4125913381576538,
      "learning_rate": 0.00018545927749016947,
      "loss": 0.1457,
      "step": 15920
    },
    {
      "epoch": 0.07488922547202649,
      "grad_norm": 2.212371587753296,
      "learning_rate": 0.000185458334512056,
      "loss": 0.1232,
      "step": 15921
    },
    {
      "epoch": 0.07489392927363896,
      "grad_norm": 2.225771903991699,
      "learning_rate": 0.0001854573915339425,
      "loss": 0.1949,
      "step": 15922
    },
    {
      "epoch": 0.07489863307525142,
      "grad_norm": 1.8474582433700562,
      "learning_rate": 0.00018545644855582903,
      "loss": 0.2135,
      "step": 15923
    },
    {
      "epoch": 0.07490333687686389,
      "grad_norm": 3.7700767517089844,
      "learning_rate": 0.00018545550557771555,
      "loss": 0.3685,
      "step": 15924
    },
    {
      "epoch": 0.07490804067847634,
      "grad_norm": 0.39339685440063477,
      "learning_rate": 0.00018545456259960207,
      "loss": 0.0296,
      "step": 15925
    },
    {
      "epoch": 0.0749127444800888,
      "grad_norm": 1.693010926246643,
      "learning_rate": 0.00018545361962148859,
      "loss": 0.3328,
      "step": 15926
    },
    {
      "epoch": 0.07491744828170127,
      "grad_norm": 1.8629884719848633,
      "learning_rate": 0.0001854526766433751,
      "loss": 0.1429,
      "step": 15927
    },
    {
      "epoch": 0.07492215208331374,
      "grad_norm": 2.395359516143799,
      "learning_rate": 0.00018545173366526162,
      "loss": 0.3214,
      "step": 15928
    },
    {
      "epoch": 0.0749268558849262,
      "grad_norm": 1.164428949356079,
      "learning_rate": 0.00018545079068714817,
      "loss": 0.1883,
      "step": 15929
    },
    {
      "epoch": 0.07493155968653865,
      "grad_norm": 1.2457393407821655,
      "learning_rate": 0.0001854498477090347,
      "loss": 0.2626,
      "step": 15930
    },
    {
      "epoch": 0.07493626348815112,
      "grad_norm": 3.3566648960113525,
      "learning_rate": 0.0001854489047309212,
      "loss": 0.5454,
      "step": 15931
    },
    {
      "epoch": 0.07494096728976359,
      "grad_norm": 3.7491283416748047,
      "learning_rate": 0.00018544796175280773,
      "loss": 0.4496,
      "step": 15932
    },
    {
      "epoch": 0.07494567109137605,
      "grad_norm": 1.1979025602340698,
      "learning_rate": 0.00018544701877469427,
      "loss": 0.0938,
      "step": 15933
    },
    {
      "epoch": 0.07495037489298852,
      "grad_norm": 1.7089083194732666,
      "learning_rate": 0.00018544607579658076,
      "loss": 0.4207,
      "step": 15934
    },
    {
      "epoch": 0.07495507869460098,
      "grad_norm": 2.7778685092926025,
      "learning_rate": 0.00018544513281846728,
      "loss": 0.5502,
      "step": 15935
    },
    {
      "epoch": 0.07495978249621343,
      "grad_norm": 3.1592893600463867,
      "learning_rate": 0.0001854441898403538,
      "loss": 0.3533,
      "step": 15936
    },
    {
      "epoch": 0.0749644862978259,
      "grad_norm": 0.3518669903278351,
      "learning_rate": 0.00018544324686224032,
      "loss": 0.0302,
      "step": 15937
    },
    {
      "epoch": 0.07496919009943837,
      "grad_norm": 0.4311012625694275,
      "learning_rate": 0.00018544230388412686,
      "loss": 0.0733,
      "step": 15938
    },
    {
      "epoch": 0.07497389390105083,
      "grad_norm": 0.529690146446228,
      "learning_rate": 0.00018544136090601338,
      "loss": 0.0576,
      "step": 15939
    },
    {
      "epoch": 0.0749785977026633,
      "grad_norm": 2.342775821685791,
      "learning_rate": 0.0001854404179278999,
      "loss": 0.1504,
      "step": 15940
    },
    {
      "epoch": 0.07498330150427576,
      "grad_norm": 2.4411230087280273,
      "learning_rate": 0.00018543947494978642,
      "loss": 0.1861,
      "step": 15941
    },
    {
      "epoch": 0.07498800530588821,
      "grad_norm": 0.7624654173851013,
      "learning_rate": 0.00018543853197167297,
      "loss": 0.1885,
      "step": 15942
    },
    {
      "epoch": 0.07499270910750068,
      "grad_norm": 0.9777155518531799,
      "learning_rate": 0.00018543758899355948,
      "loss": 0.0524,
      "step": 15943
    },
    {
      "epoch": 0.07499741290911315,
      "grad_norm": 3.1476004123687744,
      "learning_rate": 0.000185436646015446,
      "loss": 0.5836,
      "step": 15944
    },
    {
      "epoch": 0.07500211671072561,
      "grad_norm": 1.4682503938674927,
      "learning_rate": 0.00018543570303733252,
      "loss": 0.1826,
      "step": 15945
    },
    {
      "epoch": 0.07500682051233808,
      "grad_norm": 1.9020848274230957,
      "learning_rate": 0.000185434760059219,
      "loss": 0.2477,
      "step": 15946
    },
    {
      "epoch": 0.07501152431395053,
      "grad_norm": 1.6606658697128296,
      "learning_rate": 0.00018543381708110556,
      "loss": 0.4031,
      "step": 15947
    },
    {
      "epoch": 0.075016228115563,
      "grad_norm": 0.7250465154647827,
      "learning_rate": 0.00018543287410299208,
      "loss": 0.0696,
      "step": 15948
    },
    {
      "epoch": 0.07502093191717546,
      "grad_norm": 1.0006877183914185,
      "learning_rate": 0.0001854319311248786,
      "loss": 0.1451,
      "step": 15949
    },
    {
      "epoch": 0.07502563571878793,
      "grad_norm": 2.038439989089966,
      "learning_rate": 0.00018543098814676512,
      "loss": 0.215,
      "step": 15950
    },
    {
      "epoch": 0.07503033952040039,
      "grad_norm": 2.380786657333374,
      "learning_rate": 0.00018543004516865163,
      "loss": 0.0528,
      "step": 15951
    },
    {
      "epoch": 0.07503504332201286,
      "grad_norm": 5.522821426391602,
      "learning_rate": 0.00018542910219053818,
      "loss": 0.8905,
      "step": 15952
    },
    {
      "epoch": 0.07503974712362531,
      "grad_norm": 1.7121800184249878,
      "learning_rate": 0.0001854281592124247,
      "loss": 0.1327,
      "step": 15953
    },
    {
      "epoch": 0.07504445092523777,
      "grad_norm": 2.428417921066284,
      "learning_rate": 0.00018542721623431122,
      "loss": 0.5341,
      "step": 15954
    },
    {
      "epoch": 0.07504915472685024,
      "grad_norm": 1.9169509410858154,
      "learning_rate": 0.00018542627325619774,
      "loss": 0.1292,
      "step": 15955
    },
    {
      "epoch": 0.0750538585284627,
      "grad_norm": 1.30155611038208,
      "learning_rate": 0.00018542533027808425,
      "loss": 0.0984,
      "step": 15956
    },
    {
      "epoch": 0.07505856233007517,
      "grad_norm": 0.4639574885368347,
      "learning_rate": 0.00018542438729997077,
      "loss": 0.0514,
      "step": 15957
    },
    {
      "epoch": 0.07506326613168764,
      "grad_norm": 1.7953304052352905,
      "learning_rate": 0.0001854234443218573,
      "loss": 0.0623,
      "step": 15958
    },
    {
      "epoch": 0.07506796993330009,
      "grad_norm": 2.684786558151245,
      "learning_rate": 0.0001854225013437438,
      "loss": 0.182,
      "step": 15959
    },
    {
      "epoch": 0.07507267373491255,
      "grad_norm": 5.138287544250488,
      "learning_rate": 0.00018542155836563033,
      "loss": 1.3064,
      "step": 15960
    },
    {
      "epoch": 0.07507737753652502,
      "grad_norm": 0.24496541917324066,
      "learning_rate": 0.00018542061538751687,
      "loss": 0.0224,
      "step": 15961
    },
    {
      "epoch": 0.07508208133813749,
      "grad_norm": 2.4810187816619873,
      "learning_rate": 0.0001854196724094034,
      "loss": 0.1266,
      "step": 15962
    },
    {
      "epoch": 0.07508678513974995,
      "grad_norm": 1.4182608127593994,
      "learning_rate": 0.0001854187294312899,
      "loss": 0.0764,
      "step": 15963
    },
    {
      "epoch": 0.0750914889413624,
      "grad_norm": 8.58159065246582,
      "learning_rate": 0.00018541778645317643,
      "loss": 0.3266,
      "step": 15964
    },
    {
      "epoch": 0.07509619274297487,
      "grad_norm": 2.4399492740631104,
      "learning_rate": 0.00018541684347506295,
      "loss": 0.4454,
      "step": 15965
    },
    {
      "epoch": 0.07510089654458733,
      "grad_norm": 6.339291572570801,
      "learning_rate": 0.00018541590049694947,
      "loss": 0.9717,
      "step": 15966
    },
    {
      "epoch": 0.0751056003461998,
      "grad_norm": 0.11262340843677521,
      "learning_rate": 0.00018541495751883599,
      "loss": 0.007,
      "step": 15967
    },
    {
      "epoch": 0.07511030414781227,
      "grad_norm": 2.3948144912719727,
      "learning_rate": 0.0001854140145407225,
      "loss": 0.2151,
      "step": 15968
    },
    {
      "epoch": 0.07511500794942473,
      "grad_norm": 3.1063222885131836,
      "learning_rate": 0.00018541307156260902,
      "loss": 0.5529,
      "step": 15969
    },
    {
      "epoch": 0.07511971175103718,
      "grad_norm": 3.875019073486328,
      "learning_rate": 0.00018541212858449557,
      "loss": 0.7337,
      "step": 15970
    },
    {
      "epoch": 0.07512441555264965,
      "grad_norm": 4.378746509552002,
      "learning_rate": 0.0001854111856063821,
      "loss": 0.6991,
      "step": 15971
    },
    {
      "epoch": 0.07512911935426211,
      "grad_norm": 3.3149819374084473,
      "learning_rate": 0.0001854102426282686,
      "loss": 0.4465,
      "step": 15972
    },
    {
      "epoch": 0.07513382315587458,
      "grad_norm": 1.9889172315597534,
      "learning_rate": 0.00018540929965015513,
      "loss": 0.1537,
      "step": 15973
    },
    {
      "epoch": 0.07513852695748705,
      "grad_norm": 0.7254939675331116,
      "learning_rate": 0.00018540835667204167,
      "loss": 0.0643,
      "step": 15974
    },
    {
      "epoch": 0.07514323075909951,
      "grad_norm": 1.9288655519485474,
      "learning_rate": 0.0001854074136939282,
      "loss": 0.2022,
      "step": 15975
    },
    {
      "epoch": 0.07514793456071196,
      "grad_norm": 2.7422165870666504,
      "learning_rate": 0.0001854064707158147,
      "loss": 0.2358,
      "step": 15976
    },
    {
      "epoch": 0.07515263836232443,
      "grad_norm": 9.592292785644531,
      "learning_rate": 0.0001854055277377012,
      "loss": 0.2956,
      "step": 15977
    },
    {
      "epoch": 0.0751573421639369,
      "grad_norm": 1.4687976837158203,
      "learning_rate": 0.00018540458475958772,
      "loss": 0.2962,
      "step": 15978
    },
    {
      "epoch": 0.07516204596554936,
      "grad_norm": 0.7569224238395691,
      "learning_rate": 0.00018540364178147426,
      "loss": 0.0664,
      "step": 15979
    },
    {
      "epoch": 0.07516674976716183,
      "grad_norm": 1.3155739307403564,
      "learning_rate": 0.00018540269880336078,
      "loss": 0.1548,
      "step": 15980
    },
    {
      "epoch": 0.07517145356877428,
      "grad_norm": 0.6882657408714294,
      "learning_rate": 0.0001854017558252473,
      "loss": 0.0982,
      "step": 15981
    },
    {
      "epoch": 0.07517615737038674,
      "grad_norm": 0.6692718267440796,
      "learning_rate": 0.00018540081284713382,
      "loss": 0.0634,
      "step": 15982
    },
    {
      "epoch": 0.07518086117199921,
      "grad_norm": 0.9624293446540833,
      "learning_rate": 0.00018539986986902037,
      "loss": 0.0775,
      "step": 15983
    },
    {
      "epoch": 0.07518556497361167,
      "grad_norm": 1.3813036680221558,
      "learning_rate": 0.00018539892689090688,
      "loss": 0.1726,
      "step": 15984
    },
    {
      "epoch": 0.07519026877522414,
      "grad_norm": 4.295394420623779,
      "learning_rate": 0.0001853979839127934,
      "loss": 1.006,
      "step": 15985
    },
    {
      "epoch": 0.0751949725768366,
      "grad_norm": 1.0665576457977295,
      "learning_rate": 0.00018539704093467992,
      "loss": 0.0926,
      "step": 15986
    },
    {
      "epoch": 0.07519967637844906,
      "grad_norm": 2.2775166034698486,
      "learning_rate": 0.00018539609795656644,
      "loss": 0.2536,
      "step": 15987
    },
    {
      "epoch": 0.07520438018006152,
      "grad_norm": 2.6706416606903076,
      "learning_rate": 0.00018539515497845296,
      "loss": 0.6398,
      "step": 15988
    },
    {
      "epoch": 0.07520908398167399,
      "grad_norm": 0.48762306571006775,
      "learning_rate": 0.00018539421200033948,
      "loss": 0.0425,
      "step": 15989
    },
    {
      "epoch": 0.07521378778328645,
      "grad_norm": 3.4828481674194336,
      "learning_rate": 0.000185393269022226,
      "loss": 0.4111,
      "step": 15990
    },
    {
      "epoch": 0.07521849158489892,
      "grad_norm": 1.6570756435394287,
      "learning_rate": 0.00018539232604411252,
      "loss": 0.1685,
      "step": 15991
    },
    {
      "epoch": 0.07522319538651139,
      "grad_norm": 2.8264153003692627,
      "learning_rate": 0.00018539138306599906,
      "loss": 0.6899,
      "step": 15992
    },
    {
      "epoch": 0.07522789918812384,
      "grad_norm": 0.12911081314086914,
      "learning_rate": 0.00018539044008788558,
      "loss": 0.0098,
      "step": 15993
    },
    {
      "epoch": 0.0752326029897363,
      "grad_norm": 0.3127584457397461,
      "learning_rate": 0.0001853894971097721,
      "loss": 0.0283,
      "step": 15994
    },
    {
      "epoch": 0.07523730679134877,
      "grad_norm": 0.655052125453949,
      "learning_rate": 0.00018538855413165862,
      "loss": 0.0619,
      "step": 15995
    },
    {
      "epoch": 0.07524201059296123,
      "grad_norm": 1.4225435256958008,
      "learning_rate": 0.00018538761115354514,
      "loss": 0.1058,
      "step": 15996
    },
    {
      "epoch": 0.0752467143945737,
      "grad_norm": 2.5886940956115723,
      "learning_rate": 0.00018538666817543165,
      "loss": 0.7113,
      "step": 15997
    },
    {
      "epoch": 0.07525141819618615,
      "grad_norm": 0.7662578821182251,
      "learning_rate": 0.00018538572519731817,
      "loss": 0.0965,
      "step": 15998
    },
    {
      "epoch": 0.07525612199779862,
      "grad_norm": 0.5724741816520691,
      "learning_rate": 0.0001853847822192047,
      "loss": 0.0708,
      "step": 15999
    },
    {
      "epoch": 0.07526082579941108,
      "grad_norm": 2.487971782684326,
      "learning_rate": 0.0001853838392410912,
      "loss": 0.5712,
      "step": 16000
    },
    {
      "epoch": 0.07526552960102355,
      "grad_norm": 0.204229474067688,
      "learning_rate": 0.00018538289626297773,
      "loss": 0.0215,
      "step": 16001
    },
    {
      "epoch": 0.07527023340263601,
      "grad_norm": 2.2258260250091553,
      "learning_rate": 0.00018538195328486427,
      "loss": 0.2711,
      "step": 16002
    },
    {
      "epoch": 0.07527493720424848,
      "grad_norm": 2.9156665802001953,
      "learning_rate": 0.0001853810103067508,
      "loss": 0.8061,
      "step": 16003
    },
    {
      "epoch": 0.07527964100586093,
      "grad_norm": 3.923509359359741,
      "learning_rate": 0.0001853800673286373,
      "loss": 0.1847,
      "step": 16004
    },
    {
      "epoch": 0.0752843448074734,
      "grad_norm": 1.2980796098709106,
      "learning_rate": 0.00018537912435052383,
      "loss": 0.1302,
      "step": 16005
    },
    {
      "epoch": 0.07528904860908586,
      "grad_norm": 3.146393060684204,
      "learning_rate": 0.00018537818137241038,
      "loss": 0.374,
      "step": 16006
    },
    {
      "epoch": 0.07529375241069833,
      "grad_norm": 2.6463253498077393,
      "learning_rate": 0.0001853772383942969,
      "loss": 0.4891,
      "step": 16007
    },
    {
      "epoch": 0.0752984562123108,
      "grad_norm": 1.0427420139312744,
      "learning_rate": 0.00018537629541618339,
      "loss": 0.1128,
      "step": 16008
    },
    {
      "epoch": 0.07530316001392326,
      "grad_norm": 1.9366456270217896,
      "learning_rate": 0.0001853753524380699,
      "loss": 0.4144,
      "step": 16009
    },
    {
      "epoch": 0.07530786381553571,
      "grad_norm": 1.3506654500961304,
      "learning_rate": 0.00018537440945995642,
      "loss": 0.0826,
      "step": 16010
    },
    {
      "epoch": 0.07531256761714818,
      "grad_norm": 2.053706169128418,
      "learning_rate": 0.00018537346648184297,
      "loss": 0.2843,
      "step": 16011
    },
    {
      "epoch": 0.07531727141876064,
      "grad_norm": 1.954586386680603,
      "learning_rate": 0.0001853725235037295,
      "loss": 0.2465,
      "step": 16012
    },
    {
      "epoch": 0.07532197522037311,
      "grad_norm": 0.9921872615814209,
      "learning_rate": 0.000185371580525616,
      "loss": 0.0784,
      "step": 16013
    },
    {
      "epoch": 0.07532667902198557,
      "grad_norm": 1.6092551946640015,
      "learning_rate": 0.00018537063754750253,
      "loss": 0.1232,
      "step": 16014
    },
    {
      "epoch": 0.07533138282359803,
      "grad_norm": 2.170090436935425,
      "learning_rate": 0.00018536969456938907,
      "loss": 0.2235,
      "step": 16015
    },
    {
      "epoch": 0.07533608662521049,
      "grad_norm": 1.8834062814712524,
      "learning_rate": 0.0001853687515912756,
      "loss": 0.2428,
      "step": 16016
    },
    {
      "epoch": 0.07534079042682296,
      "grad_norm": 2.0463345050811768,
      "learning_rate": 0.0001853678086131621,
      "loss": 0.1768,
      "step": 16017
    },
    {
      "epoch": 0.07534549422843542,
      "grad_norm": 4.046928405761719,
      "learning_rate": 0.00018536686563504863,
      "loss": 0.6468,
      "step": 16018
    },
    {
      "epoch": 0.07535019803004789,
      "grad_norm": 0.9806931018829346,
      "learning_rate": 0.00018536592265693512,
      "loss": 0.0975,
      "step": 16019
    },
    {
      "epoch": 0.07535490183166035,
      "grad_norm": 3.9588398933410645,
      "learning_rate": 0.00018536497967882166,
      "loss": 1.0048,
      "step": 16020
    },
    {
      "epoch": 0.0753596056332728,
      "grad_norm": 2.248213291168213,
      "learning_rate": 0.00018536403670070818,
      "loss": 0.3226,
      "step": 16021
    },
    {
      "epoch": 0.07536430943488527,
      "grad_norm": 0.2742832601070404,
      "learning_rate": 0.0001853630937225947,
      "loss": 0.0181,
      "step": 16022
    },
    {
      "epoch": 0.07536901323649774,
      "grad_norm": 1.6718058586120605,
      "learning_rate": 0.00018536215074448122,
      "loss": 0.1671,
      "step": 16023
    },
    {
      "epoch": 0.0753737170381102,
      "grad_norm": 0.23410379886627197,
      "learning_rate": 0.00018536120776636777,
      "loss": 0.0277,
      "step": 16024
    },
    {
      "epoch": 0.07537842083972267,
      "grad_norm": 2.1310184001922607,
      "learning_rate": 0.00018536026478825428,
      "loss": 0.4879,
      "step": 16025
    },
    {
      "epoch": 0.07538312464133513,
      "grad_norm": 0.9753848314285278,
      "learning_rate": 0.0001853593218101408,
      "loss": 0.1069,
      "step": 16026
    },
    {
      "epoch": 0.07538782844294759,
      "grad_norm": 2.0204713344573975,
      "learning_rate": 0.00018535837883202732,
      "loss": 0.2171,
      "step": 16027
    },
    {
      "epoch": 0.07539253224456005,
      "grad_norm": 1.309009313583374,
      "learning_rate": 0.00018535743585391384,
      "loss": 0.0996,
      "step": 16028
    },
    {
      "epoch": 0.07539723604617252,
      "grad_norm": 1.2057956457138062,
      "learning_rate": 0.00018535649287580036,
      "loss": 0.1651,
      "step": 16029
    },
    {
      "epoch": 0.07540193984778498,
      "grad_norm": 2.02274751663208,
      "learning_rate": 0.00018535554989768688,
      "loss": 0.1987,
      "step": 16030
    },
    {
      "epoch": 0.07540664364939745,
      "grad_norm": 2.671175241470337,
      "learning_rate": 0.0001853546069195734,
      "loss": 0.4034,
      "step": 16031
    },
    {
      "epoch": 0.0754113474510099,
      "grad_norm": 0.8789969086647034,
      "learning_rate": 0.00018535366394145992,
      "loss": 0.0722,
      "step": 16032
    },
    {
      "epoch": 0.07541605125262237,
      "grad_norm": 1.0847220420837402,
      "learning_rate": 0.00018535272096334646,
      "loss": 0.104,
      "step": 16033
    },
    {
      "epoch": 0.07542075505423483,
      "grad_norm": 0.9279708862304688,
      "learning_rate": 0.00018535177798523298,
      "loss": 0.0784,
      "step": 16034
    },
    {
      "epoch": 0.0754254588558473,
      "grad_norm": 1.414628267288208,
      "learning_rate": 0.0001853508350071195,
      "loss": 0.2572,
      "step": 16035
    },
    {
      "epoch": 0.07543016265745976,
      "grad_norm": 1.8182202577590942,
      "learning_rate": 0.00018534989202900602,
      "loss": 0.2324,
      "step": 16036
    },
    {
      "epoch": 0.07543486645907223,
      "grad_norm": 0.2779049873352051,
      "learning_rate": 0.00018534894905089254,
      "loss": 0.0209,
      "step": 16037
    },
    {
      "epoch": 0.07543957026068468,
      "grad_norm": 2.546877384185791,
      "learning_rate": 0.00018534800607277908,
      "loss": 0.2892,
      "step": 16038
    },
    {
      "epoch": 0.07544427406229715,
      "grad_norm": 1.5115453004837036,
      "learning_rate": 0.00018534706309466557,
      "loss": 0.1626,
      "step": 16039
    },
    {
      "epoch": 0.07544897786390961,
      "grad_norm": 1.7628874778747559,
      "learning_rate": 0.0001853461201165521,
      "loss": 0.4131,
      "step": 16040
    },
    {
      "epoch": 0.07545368166552208,
      "grad_norm": 3.938209295272827,
      "learning_rate": 0.0001853451771384386,
      "loss": 0.4649,
      "step": 16041
    },
    {
      "epoch": 0.07545838546713454,
      "grad_norm": 0.7131192088127136,
      "learning_rate": 0.00018534423416032516,
      "loss": 0.0731,
      "step": 16042
    },
    {
      "epoch": 0.07546308926874701,
      "grad_norm": 3.737351417541504,
      "learning_rate": 0.00018534329118221167,
      "loss": 0.4213,
      "step": 16043
    },
    {
      "epoch": 0.07546779307035946,
      "grad_norm": 5.785372734069824,
      "learning_rate": 0.0001853423482040982,
      "loss": 0.2091,
      "step": 16044
    },
    {
      "epoch": 0.07547249687197193,
      "grad_norm": 0.2148725688457489,
      "learning_rate": 0.0001853414052259847,
      "loss": 0.0174,
      "step": 16045
    },
    {
      "epoch": 0.07547720067358439,
      "grad_norm": 1.6547493934631348,
      "learning_rate": 0.00018534046224787123,
      "loss": 0.1576,
      "step": 16046
    },
    {
      "epoch": 0.07548190447519686,
      "grad_norm": 2.6176161766052246,
      "learning_rate": 0.00018533951926975778,
      "loss": 0.1924,
      "step": 16047
    },
    {
      "epoch": 0.07548660827680932,
      "grad_norm": 0.20111621916294098,
      "learning_rate": 0.0001853385762916443,
      "loss": 0.0121,
      "step": 16048
    },
    {
      "epoch": 0.07549131207842177,
      "grad_norm": 7.448320388793945,
      "learning_rate": 0.0001853376333135308,
      "loss": 1.1644,
      "step": 16049
    },
    {
      "epoch": 0.07549601588003424,
      "grad_norm": 1.3769985437393188,
      "learning_rate": 0.0001853366903354173,
      "loss": 0.2004,
      "step": 16050
    },
    {
      "epoch": 0.0755007196816467,
      "grad_norm": 0.6317768096923828,
      "learning_rate": 0.00018533574735730382,
      "loss": 0.013,
      "step": 16051
    },
    {
      "epoch": 0.07550542348325917,
      "grad_norm": 1.3046460151672363,
      "learning_rate": 0.00018533480437919037,
      "loss": 0.0595,
      "step": 16052
    },
    {
      "epoch": 0.07551012728487164,
      "grad_norm": 4.727104187011719,
      "learning_rate": 0.0001853338614010769,
      "loss": 0.4004,
      "step": 16053
    },
    {
      "epoch": 0.0755148310864841,
      "grad_norm": 1.5964804887771606,
      "learning_rate": 0.0001853329184229634,
      "loss": 0.0989,
      "step": 16054
    },
    {
      "epoch": 0.07551953488809655,
      "grad_norm": 2.8232336044311523,
      "learning_rate": 0.00018533197544484993,
      "loss": 0.353,
      "step": 16055
    },
    {
      "epoch": 0.07552423868970902,
      "grad_norm": 0.7936965823173523,
      "learning_rate": 0.00018533103246673647,
      "loss": 0.0223,
      "step": 16056
    },
    {
      "epoch": 0.07552894249132149,
      "grad_norm": 4.6723127365112305,
      "learning_rate": 0.000185330089488623,
      "loss": 0.9445,
      "step": 16057
    },
    {
      "epoch": 0.07553364629293395,
      "grad_norm": 3.8255279064178467,
      "learning_rate": 0.0001853291465105095,
      "loss": 1.0484,
      "step": 16058
    },
    {
      "epoch": 0.07553835009454642,
      "grad_norm": 1.5912498235702515,
      "learning_rate": 0.00018532820353239603,
      "loss": 0.1674,
      "step": 16059
    },
    {
      "epoch": 0.07554305389615888,
      "grad_norm": 3.076706647872925,
      "learning_rate": 0.00018532726055428255,
      "loss": 0.7267,
      "step": 16060
    },
    {
      "epoch": 0.07554775769777133,
      "grad_norm": 1.9188183546066284,
      "learning_rate": 0.00018532631757616906,
      "loss": 0.1824,
      "step": 16061
    },
    {
      "epoch": 0.0755524614993838,
      "grad_norm": 0.9015560150146484,
      "learning_rate": 0.00018532537459805558,
      "loss": 0.0997,
      "step": 16062
    },
    {
      "epoch": 0.07555716530099627,
      "grad_norm": 2.8299720287323,
      "learning_rate": 0.0001853244316199421,
      "loss": 0.6169,
      "step": 16063
    },
    {
      "epoch": 0.07556186910260873,
      "grad_norm": 2.9219982624053955,
      "learning_rate": 0.00018532348864182862,
      "loss": 0.6307,
      "step": 16064
    },
    {
      "epoch": 0.0755665729042212,
      "grad_norm": 2.648263692855835,
      "learning_rate": 0.00018532254566371517,
      "loss": 0.5883,
      "step": 16065
    },
    {
      "epoch": 0.07557127670583365,
      "grad_norm": 1.91053307056427,
      "learning_rate": 0.00018532160268560168,
      "loss": 0.2954,
      "step": 16066
    },
    {
      "epoch": 0.07557598050744611,
      "grad_norm": 0.878893256187439,
      "learning_rate": 0.0001853206597074882,
      "loss": 0.0659,
      "step": 16067
    },
    {
      "epoch": 0.07558068430905858,
      "grad_norm": 0.8066734075546265,
      "learning_rate": 0.00018531971672937472,
      "loss": 0.0913,
      "step": 16068
    },
    {
      "epoch": 0.07558538811067105,
      "grad_norm": 0.14600814878940582,
      "learning_rate": 0.00018531877375126124,
      "loss": 0.0096,
      "step": 16069
    },
    {
      "epoch": 0.07559009191228351,
      "grad_norm": 1.2233725786209106,
      "learning_rate": 0.00018531783077314776,
      "loss": 0.2936,
      "step": 16070
    },
    {
      "epoch": 0.07559479571389598,
      "grad_norm": 0.2573178708553314,
      "learning_rate": 0.00018531688779503428,
      "loss": 0.0403,
      "step": 16071
    },
    {
      "epoch": 0.07559949951550843,
      "grad_norm": 2.3962855339050293,
      "learning_rate": 0.0001853159448169208,
      "loss": 0.5827,
      "step": 16072
    },
    {
      "epoch": 0.0756042033171209,
      "grad_norm": 2.8968026638031006,
      "learning_rate": 0.00018531500183880732,
      "loss": 0.4897,
      "step": 16073
    },
    {
      "epoch": 0.07560890711873336,
      "grad_norm": 0.351213663816452,
      "learning_rate": 0.00018531405886069386,
      "loss": 0.0369,
      "step": 16074
    },
    {
      "epoch": 0.07561361092034583,
      "grad_norm": 1.171895146369934,
      "learning_rate": 0.00018531311588258038,
      "loss": 0.1881,
      "step": 16075
    },
    {
      "epoch": 0.07561831472195829,
      "grad_norm": 1.7487215995788574,
      "learning_rate": 0.0001853121729044669,
      "loss": 0.2173,
      "step": 16076
    },
    {
      "epoch": 0.07562301852357076,
      "grad_norm": 1.1286909580230713,
      "learning_rate": 0.00018531122992635342,
      "loss": 0.1468,
      "step": 16077
    },
    {
      "epoch": 0.07562772232518321,
      "grad_norm": 1.4343193769454956,
      "learning_rate": 0.00018531028694823994,
      "loss": 0.4414,
      "step": 16078
    },
    {
      "epoch": 0.07563242612679567,
      "grad_norm": 2.143873929977417,
      "learning_rate": 0.00018530934397012648,
      "loss": 0.2901,
      "step": 16079
    },
    {
      "epoch": 0.07563712992840814,
      "grad_norm": 0.5638397336006165,
      "learning_rate": 0.000185308400992013,
      "loss": 0.0518,
      "step": 16080
    },
    {
      "epoch": 0.0756418337300206,
      "grad_norm": 3.320053815841675,
      "learning_rate": 0.0001853074580138995,
      "loss": 0.2178,
      "step": 16081
    },
    {
      "epoch": 0.07564653753163307,
      "grad_norm": 0.700994610786438,
      "learning_rate": 0.000185306515035786,
      "loss": 0.0864,
      "step": 16082
    },
    {
      "epoch": 0.07565124133324552,
      "grad_norm": 0.23706333339214325,
      "learning_rate": 0.00018530557205767256,
      "loss": 0.0236,
      "step": 16083
    },
    {
      "epoch": 0.07565594513485799,
      "grad_norm": 4.4654436111450195,
      "learning_rate": 0.00018530462907955907,
      "loss": 0.2685,
      "step": 16084
    },
    {
      "epoch": 0.07566064893647045,
      "grad_norm": 0.9242954254150391,
      "learning_rate": 0.0001853036861014456,
      "loss": 0.0843,
      "step": 16085
    },
    {
      "epoch": 0.07566535273808292,
      "grad_norm": 3.0072286128997803,
      "learning_rate": 0.0001853027431233321,
      "loss": 0.3231,
      "step": 16086
    },
    {
      "epoch": 0.07567005653969539,
      "grad_norm": 0.7594906091690063,
      "learning_rate": 0.00018530180014521863,
      "loss": 0.1779,
      "step": 16087
    },
    {
      "epoch": 0.07567476034130785,
      "grad_norm": 2.4400634765625,
      "learning_rate": 0.00018530085716710518,
      "loss": 0.6178,
      "step": 16088
    },
    {
      "epoch": 0.0756794641429203,
      "grad_norm": 1.1725629568099976,
      "learning_rate": 0.0001852999141889917,
      "loss": 0.43,
      "step": 16089
    },
    {
      "epoch": 0.07568416794453277,
      "grad_norm": 0.9835768938064575,
      "learning_rate": 0.0001852989712108782,
      "loss": 0.1203,
      "step": 16090
    },
    {
      "epoch": 0.07568887174614523,
      "grad_norm": 0.7641647458076477,
      "learning_rate": 0.00018529802823276473,
      "loss": 0.0997,
      "step": 16091
    },
    {
      "epoch": 0.0756935755477577,
      "grad_norm": 0.7821317315101624,
      "learning_rate": 0.00018529708525465125,
      "loss": 0.1004,
      "step": 16092
    },
    {
      "epoch": 0.07569827934937017,
      "grad_norm": 2.9664533138275146,
      "learning_rate": 0.00018529614227653777,
      "loss": 0.6002,
      "step": 16093
    },
    {
      "epoch": 0.07570298315098263,
      "grad_norm": 1.9211721420288086,
      "learning_rate": 0.0001852951992984243,
      "loss": 0.4056,
      "step": 16094
    },
    {
      "epoch": 0.07570768695259508,
      "grad_norm": 0.405057817697525,
      "learning_rate": 0.0001852942563203108,
      "loss": 0.0456,
      "step": 16095
    },
    {
      "epoch": 0.07571239075420755,
      "grad_norm": 0.7877247929573059,
      "learning_rate": 0.00018529331334219733,
      "loss": 0.2037,
      "step": 16096
    },
    {
      "epoch": 0.07571709455582001,
      "grad_norm": 2.283264398574829,
      "learning_rate": 0.00018529237036408387,
      "loss": 0.5232,
      "step": 16097
    },
    {
      "epoch": 0.07572179835743248,
      "grad_norm": 1.625331997871399,
      "learning_rate": 0.0001852914273859704,
      "loss": 0.2517,
      "step": 16098
    },
    {
      "epoch": 0.07572650215904494,
      "grad_norm": 1.997642159461975,
      "learning_rate": 0.0001852904844078569,
      "loss": 0.415,
      "step": 16099
    },
    {
      "epoch": 0.0757312059606574,
      "grad_norm": 2.593094825744629,
      "learning_rate": 0.00018528954142974343,
      "loss": 0.4651,
      "step": 16100
    },
    {
      "epoch": 0.07573590976226986,
      "grad_norm": 1.0702747106552124,
      "learning_rate": 0.00018528859845162995,
      "loss": 0.0875,
      "step": 16101
    },
    {
      "epoch": 0.07574061356388233,
      "grad_norm": 6.040084362030029,
      "learning_rate": 0.00018528765547351646,
      "loss": 0.5636,
      "step": 16102
    },
    {
      "epoch": 0.0757453173654948,
      "grad_norm": 3.2601161003112793,
      "learning_rate": 0.00018528671249540298,
      "loss": 0.2964,
      "step": 16103
    },
    {
      "epoch": 0.07575002116710726,
      "grad_norm": 1.8933932781219482,
      "learning_rate": 0.0001852857695172895,
      "loss": 0.3114,
      "step": 16104
    },
    {
      "epoch": 0.07575472496871972,
      "grad_norm": 2.9155797958374023,
      "learning_rate": 0.00018528482653917602,
      "loss": 0.6135,
      "step": 16105
    },
    {
      "epoch": 0.07575942877033218,
      "grad_norm": 2.7236969470977783,
      "learning_rate": 0.00018528388356106257,
      "loss": 0.1114,
      "step": 16106
    },
    {
      "epoch": 0.07576413257194464,
      "grad_norm": 0.6969841718673706,
      "learning_rate": 0.00018528294058294908,
      "loss": 0.0929,
      "step": 16107
    },
    {
      "epoch": 0.07576883637355711,
      "grad_norm": 3.6001498699188232,
      "learning_rate": 0.0001852819976048356,
      "loss": 0.3593,
      "step": 16108
    },
    {
      "epoch": 0.07577354017516957,
      "grad_norm": 1.8954386711120605,
      "learning_rate": 0.00018528105462672212,
      "loss": 0.3262,
      "step": 16109
    },
    {
      "epoch": 0.07577824397678204,
      "grad_norm": 1.0400376319885254,
      "learning_rate": 0.00018528011164860864,
      "loss": 0.1038,
      "step": 16110
    },
    {
      "epoch": 0.0757829477783945,
      "grad_norm": 2.753580331802368,
      "learning_rate": 0.00018527916867049519,
      "loss": 0.2144,
      "step": 16111
    },
    {
      "epoch": 0.07578765158000696,
      "grad_norm": 1.8542068004608154,
      "learning_rate": 0.00018527822569238168,
      "loss": 0.3351,
      "step": 16112
    },
    {
      "epoch": 0.07579235538161942,
      "grad_norm": 3.259974241256714,
      "learning_rate": 0.0001852772827142682,
      "loss": 0.5812,
      "step": 16113
    },
    {
      "epoch": 0.07579705918323189,
      "grad_norm": 3.3948144912719727,
      "learning_rate": 0.00018527633973615472,
      "loss": 0.2533,
      "step": 16114
    },
    {
      "epoch": 0.07580176298484435,
      "grad_norm": 3.6066091060638428,
      "learning_rate": 0.00018527539675804126,
      "loss": 0.7737,
      "step": 16115
    },
    {
      "epoch": 0.07580646678645682,
      "grad_norm": 0.9012877941131592,
      "learning_rate": 0.00018527445377992778,
      "loss": 0.1724,
      "step": 16116
    },
    {
      "epoch": 0.07581117058806927,
      "grad_norm": 1.7947982549667358,
      "learning_rate": 0.0001852735108018143,
      "loss": 0.3275,
      "step": 16117
    },
    {
      "epoch": 0.07581587438968174,
      "grad_norm": 1.5960197448730469,
      "learning_rate": 0.00018527256782370082,
      "loss": 0.1987,
      "step": 16118
    },
    {
      "epoch": 0.0758205781912942,
      "grad_norm": 2.2473583221435547,
      "learning_rate": 0.00018527162484558734,
      "loss": 0.6708,
      "step": 16119
    },
    {
      "epoch": 0.07582528199290667,
      "grad_norm": 1.5911325216293335,
      "learning_rate": 0.00018527068186747388,
      "loss": 0.4499,
      "step": 16120
    },
    {
      "epoch": 0.07582998579451913,
      "grad_norm": 1.7388403415679932,
      "learning_rate": 0.0001852697388893604,
      "loss": 0.208,
      "step": 16121
    },
    {
      "epoch": 0.0758346895961316,
      "grad_norm": 2.711941719055176,
      "learning_rate": 0.00018526879591124692,
      "loss": 0.6458,
      "step": 16122
    },
    {
      "epoch": 0.07583939339774405,
      "grad_norm": 1.189649224281311,
      "learning_rate": 0.0001852678529331334,
      "loss": 0.1736,
      "step": 16123
    },
    {
      "epoch": 0.07584409719935652,
      "grad_norm": 1.6479920148849487,
      "learning_rate": 0.00018526690995501996,
      "loss": 0.2313,
      "step": 16124
    },
    {
      "epoch": 0.07584880100096898,
      "grad_norm": 2.7580676078796387,
      "learning_rate": 0.00018526596697690647,
      "loss": 0.4973,
      "step": 16125
    },
    {
      "epoch": 0.07585350480258145,
      "grad_norm": 2.145702362060547,
      "learning_rate": 0.000185265023998793,
      "loss": 0.3051,
      "step": 16126
    },
    {
      "epoch": 0.07585820860419391,
      "grad_norm": 2.887336492538452,
      "learning_rate": 0.0001852640810206795,
      "loss": 0.4388,
      "step": 16127
    },
    {
      "epoch": 0.07586291240580638,
      "grad_norm": 1.6403396129608154,
      "learning_rate": 0.00018526313804256603,
      "loss": 0.2849,
      "step": 16128
    },
    {
      "epoch": 0.07586761620741883,
      "grad_norm": 0.731504499912262,
      "learning_rate": 0.00018526219506445258,
      "loss": 0.1696,
      "step": 16129
    },
    {
      "epoch": 0.0758723200090313,
      "grad_norm": 2.7371621131896973,
      "learning_rate": 0.0001852612520863391,
      "loss": 0.6208,
      "step": 16130
    },
    {
      "epoch": 0.07587702381064376,
      "grad_norm": 0.637510359287262,
      "learning_rate": 0.0001852603091082256,
      "loss": 0.0619,
      "step": 16131
    },
    {
      "epoch": 0.07588172761225623,
      "grad_norm": 0.32720595598220825,
      "learning_rate": 0.00018525936613011213,
      "loss": 0.0323,
      "step": 16132
    },
    {
      "epoch": 0.0758864314138687,
      "grad_norm": 0.7131298780441284,
      "learning_rate": 0.00018525842315199865,
      "loss": 0.0916,
      "step": 16133
    },
    {
      "epoch": 0.07589113521548115,
      "grad_norm": 0.5001333355903625,
      "learning_rate": 0.00018525748017388517,
      "loss": 0.0827,
      "step": 16134
    },
    {
      "epoch": 0.07589583901709361,
      "grad_norm": 2.62906813621521,
      "learning_rate": 0.0001852565371957717,
      "loss": 0.388,
      "step": 16135
    },
    {
      "epoch": 0.07590054281870608,
      "grad_norm": 0.8773118257522583,
      "learning_rate": 0.0001852555942176582,
      "loss": 0.0912,
      "step": 16136
    },
    {
      "epoch": 0.07590524662031854,
      "grad_norm": 2.462902545928955,
      "learning_rate": 0.00018525465123954473,
      "loss": 0.2167,
      "step": 16137
    },
    {
      "epoch": 0.07590995042193101,
      "grad_norm": 0.991882860660553,
      "learning_rate": 0.00018525370826143127,
      "loss": 0.1802,
      "step": 16138
    },
    {
      "epoch": 0.07591465422354347,
      "grad_norm": 2.1405029296875,
      "learning_rate": 0.0001852527652833178,
      "loss": 0.1565,
      "step": 16139
    },
    {
      "epoch": 0.07591935802515593,
      "grad_norm": 1.5552796125411987,
      "learning_rate": 0.0001852518223052043,
      "loss": 0.4978,
      "step": 16140
    },
    {
      "epoch": 0.07592406182676839,
      "grad_norm": 2.9908342361450195,
      "learning_rate": 0.00018525087932709083,
      "loss": 0.9607,
      "step": 16141
    },
    {
      "epoch": 0.07592876562838086,
      "grad_norm": 1.5307432413101196,
      "learning_rate": 0.00018524993634897737,
      "loss": 0.2794,
      "step": 16142
    },
    {
      "epoch": 0.07593346942999332,
      "grad_norm": 1.9975861310958862,
      "learning_rate": 0.00018524899337086386,
      "loss": 0.5379,
      "step": 16143
    },
    {
      "epoch": 0.07593817323160579,
      "grad_norm": 1.2335467338562012,
      "learning_rate": 0.00018524805039275038,
      "loss": 0.1764,
      "step": 16144
    },
    {
      "epoch": 0.07594287703321825,
      "grad_norm": 1.1284383535385132,
      "learning_rate": 0.0001852471074146369,
      "loss": 0.3616,
      "step": 16145
    },
    {
      "epoch": 0.0759475808348307,
      "grad_norm": 0.906636655330658,
      "learning_rate": 0.00018524616443652342,
      "loss": 0.2024,
      "step": 16146
    },
    {
      "epoch": 0.07595228463644317,
      "grad_norm": 1.1463996171951294,
      "learning_rate": 0.00018524522145840997,
      "loss": 0.1457,
      "step": 16147
    },
    {
      "epoch": 0.07595698843805564,
      "grad_norm": 1.7269413471221924,
      "learning_rate": 0.00018524427848029648,
      "loss": 0.4324,
      "step": 16148
    },
    {
      "epoch": 0.0759616922396681,
      "grad_norm": 1.0183625221252441,
      "learning_rate": 0.000185243335502183,
      "loss": 0.2946,
      "step": 16149
    },
    {
      "epoch": 0.07596639604128057,
      "grad_norm": 1.848116397857666,
      "learning_rate": 0.00018524239252406952,
      "loss": 0.2058,
      "step": 16150
    },
    {
      "epoch": 0.07597109984289302,
      "grad_norm": 4.422853469848633,
      "learning_rate": 0.00018524144954595604,
      "loss": 0.2119,
      "step": 16151
    },
    {
      "epoch": 0.07597580364450549,
      "grad_norm": 2.189634084701538,
      "learning_rate": 0.00018524050656784259,
      "loss": 0.2498,
      "step": 16152
    },
    {
      "epoch": 0.07598050744611795,
      "grad_norm": 2.6183102130889893,
      "learning_rate": 0.0001852395635897291,
      "loss": 0.5596,
      "step": 16153
    },
    {
      "epoch": 0.07598521124773042,
      "grad_norm": 0.5630045533180237,
      "learning_rate": 0.0001852386206116156,
      "loss": 0.1127,
      "step": 16154
    },
    {
      "epoch": 0.07598991504934288,
      "grad_norm": 3.8113174438476562,
      "learning_rate": 0.00018523767763350212,
      "loss": 0.5369,
      "step": 16155
    },
    {
      "epoch": 0.07599461885095535,
      "grad_norm": 0.952677309513092,
      "learning_rate": 0.00018523673465538866,
      "loss": 0.0957,
      "step": 16156
    },
    {
      "epoch": 0.0759993226525678,
      "grad_norm": 0.7223628759384155,
      "learning_rate": 0.00018523579167727518,
      "loss": 0.1183,
      "step": 16157
    },
    {
      "epoch": 0.07600402645418027,
      "grad_norm": 2.3026411533355713,
      "learning_rate": 0.0001852348486991617,
      "loss": 0.4164,
      "step": 16158
    },
    {
      "epoch": 0.07600873025579273,
      "grad_norm": 1.223630666732788,
      "learning_rate": 0.00018523390572104822,
      "loss": 0.2187,
      "step": 16159
    },
    {
      "epoch": 0.0760134340574052,
      "grad_norm": 2.267207622528076,
      "learning_rate": 0.00018523296274293474,
      "loss": 0.3679,
      "step": 16160
    },
    {
      "epoch": 0.07601813785901766,
      "grad_norm": 1.9674899578094482,
      "learning_rate": 0.00018523201976482128,
      "loss": 0.2433,
      "step": 16161
    },
    {
      "epoch": 0.07602284166063013,
      "grad_norm": 1.0642108917236328,
      "learning_rate": 0.0001852310767867078,
      "loss": 0.1697,
      "step": 16162
    },
    {
      "epoch": 0.07602754546224258,
      "grad_norm": 3.715306043624878,
      "learning_rate": 0.00018523013380859432,
      "loss": 0.3164,
      "step": 16163
    },
    {
      "epoch": 0.07603224926385505,
      "grad_norm": 1.290902853012085,
      "learning_rate": 0.00018522919083048084,
      "loss": 0.1121,
      "step": 16164
    },
    {
      "epoch": 0.07603695306546751,
      "grad_norm": 1.2574495077133179,
      "learning_rate": 0.00018522824785236736,
      "loss": 0.1516,
      "step": 16165
    },
    {
      "epoch": 0.07604165686707998,
      "grad_norm": 1.386988639831543,
      "learning_rate": 0.00018522730487425387,
      "loss": 0.2178,
      "step": 16166
    },
    {
      "epoch": 0.07604636066869244,
      "grad_norm": 2.8471107482910156,
      "learning_rate": 0.0001852263618961404,
      "loss": 0.2645,
      "step": 16167
    },
    {
      "epoch": 0.0760510644703049,
      "grad_norm": 2.9663960933685303,
      "learning_rate": 0.0001852254189180269,
      "loss": 0.302,
      "step": 16168
    },
    {
      "epoch": 0.07605576827191736,
      "grad_norm": 5.409879207611084,
      "learning_rate": 0.00018522447593991343,
      "loss": 0.6142,
      "step": 16169
    },
    {
      "epoch": 0.07606047207352983,
      "grad_norm": 0.6616231203079224,
      "learning_rate": 0.00018522353296179998,
      "loss": 0.0705,
      "step": 16170
    },
    {
      "epoch": 0.07606517587514229,
      "grad_norm": 4.334823131561279,
      "learning_rate": 0.0001852225899836865,
      "loss": 0.7756,
      "step": 16171
    },
    {
      "epoch": 0.07606987967675476,
      "grad_norm": 4.235592842102051,
      "learning_rate": 0.000185221647005573,
      "loss": 0.5564,
      "step": 16172
    },
    {
      "epoch": 0.07607458347836722,
      "grad_norm": 2.1134610176086426,
      "learning_rate": 0.00018522070402745953,
      "loss": 0.3601,
      "step": 16173
    },
    {
      "epoch": 0.07607928727997967,
      "grad_norm": 0.35816797614097595,
      "learning_rate": 0.00018521976104934605,
      "loss": 0.0478,
      "step": 16174
    },
    {
      "epoch": 0.07608399108159214,
      "grad_norm": 1.3538626432418823,
      "learning_rate": 0.00018521881807123257,
      "loss": 0.2759,
      "step": 16175
    },
    {
      "epoch": 0.0760886948832046,
      "grad_norm": 2.2805440425872803,
      "learning_rate": 0.0001852178750931191,
      "loss": 0.431,
      "step": 16176
    },
    {
      "epoch": 0.07609339868481707,
      "grad_norm": 0.4762600064277649,
      "learning_rate": 0.0001852169321150056,
      "loss": 0.0619,
      "step": 16177
    },
    {
      "epoch": 0.07609810248642954,
      "grad_norm": 1.047369122505188,
      "learning_rate": 0.00018521598913689213,
      "loss": 0.1286,
      "step": 16178
    },
    {
      "epoch": 0.076102806288042,
      "grad_norm": 0.7679150104522705,
      "learning_rate": 0.00018521504615877867,
      "loss": 0.0779,
      "step": 16179
    },
    {
      "epoch": 0.07610751008965445,
      "grad_norm": 4.042922019958496,
      "learning_rate": 0.0001852141031806652,
      "loss": 0.6076,
      "step": 16180
    },
    {
      "epoch": 0.07611221389126692,
      "grad_norm": 2.514603614807129,
      "learning_rate": 0.0001852131602025517,
      "loss": 0.4267,
      "step": 16181
    },
    {
      "epoch": 0.07611691769287939,
      "grad_norm": 4.557170867919922,
      "learning_rate": 0.00018521221722443823,
      "loss": 0.8503,
      "step": 16182
    },
    {
      "epoch": 0.07612162149449185,
      "grad_norm": 1.9467817544937134,
      "learning_rate": 0.00018521127424632477,
      "loss": 0.3888,
      "step": 16183
    },
    {
      "epoch": 0.07612632529610432,
      "grad_norm": 1.8106385469436646,
      "learning_rate": 0.0001852103312682113,
      "loss": 0.2653,
      "step": 16184
    },
    {
      "epoch": 0.07613102909771677,
      "grad_norm": 1.56415593624115,
      "learning_rate": 0.00018520938829009778,
      "loss": 0.2991,
      "step": 16185
    },
    {
      "epoch": 0.07613573289932923,
      "grad_norm": 1.9166814088821411,
      "learning_rate": 0.0001852084453119843,
      "loss": 0.2606,
      "step": 16186
    },
    {
      "epoch": 0.0761404367009417,
      "grad_norm": 1.0367101430892944,
      "learning_rate": 0.00018520750233387082,
      "loss": 0.1612,
      "step": 16187
    },
    {
      "epoch": 0.07614514050255416,
      "grad_norm": 0.733246386051178,
      "learning_rate": 0.00018520655935575737,
      "loss": 0.1077,
      "step": 16188
    },
    {
      "epoch": 0.07614984430416663,
      "grad_norm": 1.1217198371887207,
      "learning_rate": 0.00018520561637764388,
      "loss": 0.1178,
      "step": 16189
    },
    {
      "epoch": 0.0761545481057791,
      "grad_norm": 1.6379011869430542,
      "learning_rate": 0.0001852046733995304,
      "loss": 0.4881,
      "step": 16190
    },
    {
      "epoch": 0.07615925190739155,
      "grad_norm": 0.7711591720581055,
      "learning_rate": 0.00018520373042141692,
      "loss": 0.2083,
      "step": 16191
    },
    {
      "epoch": 0.07616395570900401,
      "grad_norm": 3.7027552127838135,
      "learning_rate": 0.00018520278744330347,
      "loss": 0.6044,
      "step": 16192
    },
    {
      "epoch": 0.07616865951061648,
      "grad_norm": 0.5340336561203003,
      "learning_rate": 0.00018520184446518999,
      "loss": 0.0458,
      "step": 16193
    },
    {
      "epoch": 0.07617336331222894,
      "grad_norm": 3.6347358226776123,
      "learning_rate": 0.0001852009014870765,
      "loss": 0.7682,
      "step": 16194
    },
    {
      "epoch": 0.07617806711384141,
      "grad_norm": 1.781270980834961,
      "learning_rate": 0.00018519995850896302,
      "loss": 0.3614,
      "step": 16195
    },
    {
      "epoch": 0.07618277091545388,
      "grad_norm": 3.026158571243286,
      "learning_rate": 0.00018519901553084954,
      "loss": 0.6458,
      "step": 16196
    },
    {
      "epoch": 0.07618747471706633,
      "grad_norm": 3.984307289123535,
      "learning_rate": 0.00018519807255273606,
      "loss": 0.5229,
      "step": 16197
    },
    {
      "epoch": 0.0761921785186788,
      "grad_norm": 5.416224956512451,
      "learning_rate": 0.00018519712957462258,
      "loss": 0.2628,
      "step": 16198
    },
    {
      "epoch": 0.07619688232029126,
      "grad_norm": 2.6676132678985596,
      "learning_rate": 0.0001851961865965091,
      "loss": 0.4906,
      "step": 16199
    },
    {
      "epoch": 0.07620158612190372,
      "grad_norm": 1.5655295848846436,
      "learning_rate": 0.00018519524361839562,
      "loss": 0.245,
      "step": 16200
    },
    {
      "epoch": 0.07620628992351619,
      "grad_norm": 8.078470230102539,
      "learning_rate": 0.00018519430064028216,
      "loss": 0.6529,
      "step": 16201
    },
    {
      "epoch": 0.07621099372512864,
      "grad_norm": 4.144902229309082,
      "learning_rate": 0.00018519335766216868,
      "loss": 0.7716,
      "step": 16202
    },
    {
      "epoch": 0.07621569752674111,
      "grad_norm": 1.7688885927200317,
      "learning_rate": 0.0001851924146840552,
      "loss": 0.2981,
      "step": 16203
    },
    {
      "epoch": 0.07622040132835357,
      "grad_norm": 3.5766522884368896,
      "learning_rate": 0.00018519147170594172,
      "loss": 0.9233,
      "step": 16204
    },
    {
      "epoch": 0.07622510512996604,
      "grad_norm": 3.3624439239501953,
      "learning_rate": 0.00018519052872782824,
      "loss": 0.6488,
      "step": 16205
    },
    {
      "epoch": 0.0762298089315785,
      "grad_norm": 3.713301420211792,
      "learning_rate": 0.00018518958574971476,
      "loss": 0.5674,
      "step": 16206
    },
    {
      "epoch": 0.07623451273319097,
      "grad_norm": 2.871401309967041,
      "learning_rate": 0.00018518864277160127,
      "loss": 0.3904,
      "step": 16207
    },
    {
      "epoch": 0.07623921653480342,
      "grad_norm": 2.4110660552978516,
      "learning_rate": 0.0001851876997934878,
      "loss": 0.3275,
      "step": 16208
    },
    {
      "epoch": 0.07624392033641589,
      "grad_norm": 1.395446538925171,
      "learning_rate": 0.0001851867568153743,
      "loss": 0.309,
      "step": 16209
    },
    {
      "epoch": 0.07624862413802835,
      "grad_norm": 0.8545651435852051,
      "learning_rate": 0.00018518581383726083,
      "loss": 0.1278,
      "step": 16210
    },
    {
      "epoch": 0.07625332793964082,
      "grad_norm": 2.7168476581573486,
      "learning_rate": 0.00018518487085914738,
      "loss": 0.5508,
      "step": 16211
    },
    {
      "epoch": 0.07625803174125328,
      "grad_norm": 2.0304758548736572,
      "learning_rate": 0.0001851839278810339,
      "loss": 0.3563,
      "step": 16212
    },
    {
      "epoch": 0.07626273554286575,
      "grad_norm": 1.0029336214065552,
      "learning_rate": 0.0001851829849029204,
      "loss": 0.1362,
      "step": 16213
    },
    {
      "epoch": 0.0762674393444782,
      "grad_norm": 2.115114212036133,
      "learning_rate": 0.00018518204192480693,
      "loss": 0.5629,
      "step": 16214
    },
    {
      "epoch": 0.07627214314609067,
      "grad_norm": 1.514413833618164,
      "learning_rate": 0.00018518109894669348,
      "loss": 0.1317,
      "step": 16215
    },
    {
      "epoch": 0.07627684694770313,
      "grad_norm": 3.1185150146484375,
      "learning_rate": 0.00018518015596857997,
      "loss": 0.4684,
      "step": 16216
    },
    {
      "epoch": 0.0762815507493156,
      "grad_norm": 2.4062705039978027,
      "learning_rate": 0.0001851792129904665,
      "loss": 0.5021,
      "step": 16217
    },
    {
      "epoch": 0.07628625455092806,
      "grad_norm": 0.6450438499450684,
      "learning_rate": 0.000185178270012353,
      "loss": 0.081,
      "step": 16218
    },
    {
      "epoch": 0.07629095835254052,
      "grad_norm": 1.3475141525268555,
      "learning_rate": 0.00018517732703423953,
      "loss": 0.3547,
      "step": 16219
    },
    {
      "epoch": 0.07629566215415298,
      "grad_norm": 1.5704928636550903,
      "learning_rate": 0.00018517638405612607,
      "loss": 0.1825,
      "step": 16220
    },
    {
      "epoch": 0.07630036595576545,
      "grad_norm": 2.5468692779541016,
      "learning_rate": 0.0001851754410780126,
      "loss": 0.4499,
      "step": 16221
    },
    {
      "epoch": 0.07630506975737791,
      "grad_norm": 1.0466018915176392,
      "learning_rate": 0.0001851744980998991,
      "loss": 0.1422,
      "step": 16222
    },
    {
      "epoch": 0.07630977355899038,
      "grad_norm": 0.6291792392730713,
      "learning_rate": 0.00018517355512178563,
      "loss": 0.1002,
      "step": 16223
    },
    {
      "epoch": 0.07631447736060284,
      "grad_norm": 0.6804836392402649,
      "learning_rate": 0.00018517261214367217,
      "loss": 0.0894,
      "step": 16224
    },
    {
      "epoch": 0.0763191811622153,
      "grad_norm": 1.8759618997573853,
      "learning_rate": 0.0001851716691655587,
      "loss": 0.1942,
      "step": 16225
    },
    {
      "epoch": 0.07632388496382776,
      "grad_norm": 0.9466244578361511,
      "learning_rate": 0.0001851707261874452,
      "loss": 0.2071,
      "step": 16226
    },
    {
      "epoch": 0.07632858876544023,
      "grad_norm": 3.1143553256988525,
      "learning_rate": 0.00018516978320933173,
      "loss": 0.6738,
      "step": 16227
    },
    {
      "epoch": 0.0763332925670527,
      "grad_norm": 4.4232988357543945,
      "learning_rate": 0.00018516884023121822,
      "loss": 0.6664,
      "step": 16228
    },
    {
      "epoch": 0.07633799636866516,
      "grad_norm": 2.739319086074829,
      "learning_rate": 0.00018516789725310477,
      "loss": 0.7681,
      "step": 16229
    },
    {
      "epoch": 0.07634270017027762,
      "grad_norm": 1.1069620847702026,
      "learning_rate": 0.00018516695427499128,
      "loss": 0.1589,
      "step": 16230
    },
    {
      "epoch": 0.07634740397189008,
      "grad_norm": 2.4960243701934814,
      "learning_rate": 0.0001851660112968778,
      "loss": 0.4102,
      "step": 16231
    },
    {
      "epoch": 0.07635210777350254,
      "grad_norm": 0.7388034462928772,
      "learning_rate": 0.00018516506831876432,
      "loss": 0.0787,
      "step": 16232
    },
    {
      "epoch": 0.07635681157511501,
      "grad_norm": 0.7106949687004089,
      "learning_rate": 0.00018516412534065087,
      "loss": 0.0698,
      "step": 16233
    },
    {
      "epoch": 0.07636151537672747,
      "grad_norm": 1.7174103260040283,
      "learning_rate": 0.00018516318236253739,
      "loss": 0.1817,
      "step": 16234
    },
    {
      "epoch": 0.07636621917833994,
      "grad_norm": 1.0892189741134644,
      "learning_rate": 0.0001851622393844239,
      "loss": 0.1604,
      "step": 16235
    },
    {
      "epoch": 0.07637092297995239,
      "grad_norm": 1.0385321378707886,
      "learning_rate": 0.00018516129640631042,
      "loss": 0.187,
      "step": 16236
    },
    {
      "epoch": 0.07637562678156486,
      "grad_norm": 1.8069863319396973,
      "learning_rate": 0.00018516035342819694,
      "loss": 0.4338,
      "step": 16237
    },
    {
      "epoch": 0.07638033058317732,
      "grad_norm": 1.7545652389526367,
      "learning_rate": 0.00018515941045008346,
      "loss": 0.0908,
      "step": 16238
    },
    {
      "epoch": 0.07638503438478979,
      "grad_norm": 0.17809341847896576,
      "learning_rate": 0.00018515846747196998,
      "loss": 0.0136,
      "step": 16239
    },
    {
      "epoch": 0.07638973818640225,
      "grad_norm": 1.228926181793213,
      "learning_rate": 0.0001851575244938565,
      "loss": 0.2893,
      "step": 16240
    },
    {
      "epoch": 0.07639444198801472,
      "grad_norm": 2.506108283996582,
      "learning_rate": 0.00018515658151574302,
      "loss": 0.3464,
      "step": 16241
    },
    {
      "epoch": 0.07639914578962717,
      "grad_norm": 0.4034161865711212,
      "learning_rate": 0.00018515563853762956,
      "loss": 0.0524,
      "step": 16242
    },
    {
      "epoch": 0.07640384959123964,
      "grad_norm": 0.11380743980407715,
      "learning_rate": 0.00018515469555951608,
      "loss": 0.0086,
      "step": 16243
    },
    {
      "epoch": 0.0764085533928521,
      "grad_norm": 0.5558165907859802,
      "learning_rate": 0.0001851537525814026,
      "loss": 0.0742,
      "step": 16244
    },
    {
      "epoch": 0.07641325719446457,
      "grad_norm": 1.012268304824829,
      "learning_rate": 0.00018515280960328912,
      "loss": 0.1069,
      "step": 16245
    },
    {
      "epoch": 0.07641796099607703,
      "grad_norm": 0.8531525731086731,
      "learning_rate": 0.00018515186662517564,
      "loss": 0.0931,
      "step": 16246
    },
    {
      "epoch": 0.0764226647976895,
      "grad_norm": 1.7723429203033447,
      "learning_rate": 0.00018515092364706216,
      "loss": 0.1518,
      "step": 16247
    },
    {
      "epoch": 0.07642736859930195,
      "grad_norm": 1.0221898555755615,
      "learning_rate": 0.00018514998066894867,
      "loss": 0.1436,
      "step": 16248
    },
    {
      "epoch": 0.07643207240091442,
      "grad_norm": 2.4006688594818115,
      "learning_rate": 0.0001851490376908352,
      "loss": 0.4076,
      "step": 16249
    },
    {
      "epoch": 0.07643677620252688,
      "grad_norm": 2.0524837970733643,
      "learning_rate": 0.0001851480947127217,
      "loss": 0.4425,
      "step": 16250
    },
    {
      "epoch": 0.07644148000413935,
      "grad_norm": 1.163087010383606,
      "learning_rate": 0.00018514715173460826,
      "loss": 0.0807,
      "step": 16251
    },
    {
      "epoch": 0.07644618380575181,
      "grad_norm": 3.1469104290008545,
      "learning_rate": 0.00018514620875649478,
      "loss": 0.4073,
      "step": 16252
    },
    {
      "epoch": 0.07645088760736427,
      "grad_norm": 3.7137746810913086,
      "learning_rate": 0.0001851452657783813,
      "loss": 0.2943,
      "step": 16253
    },
    {
      "epoch": 0.07645559140897673,
      "grad_norm": 4.510809421539307,
      "learning_rate": 0.0001851443228002678,
      "loss": 0.5191,
      "step": 16254
    },
    {
      "epoch": 0.0764602952105892,
      "grad_norm": 3.1316487789154053,
      "learning_rate": 0.00018514337982215433,
      "loss": 0.591,
      "step": 16255
    },
    {
      "epoch": 0.07646499901220166,
      "grad_norm": 0.8872833847999573,
      "learning_rate": 0.00018514243684404088,
      "loss": 0.084,
      "step": 16256
    },
    {
      "epoch": 0.07646970281381413,
      "grad_norm": 1.3350235223770142,
      "learning_rate": 0.0001851414938659274,
      "loss": 0.077,
      "step": 16257
    },
    {
      "epoch": 0.0764744066154266,
      "grad_norm": 0.27615809440612793,
      "learning_rate": 0.00018514055088781391,
      "loss": 0.0175,
      "step": 16258
    },
    {
      "epoch": 0.07647911041703905,
      "grad_norm": 2.0975990295410156,
      "learning_rate": 0.0001851396079097004,
      "loss": 0.1685,
      "step": 16259
    },
    {
      "epoch": 0.07648381421865151,
      "grad_norm": 5.411917686462402,
      "learning_rate": 0.00018513866493158693,
      "loss": 1.1054,
      "step": 16260
    },
    {
      "epoch": 0.07648851802026398,
      "grad_norm": 1.6076252460479736,
      "learning_rate": 0.00018513772195347347,
      "loss": 0.0863,
      "step": 16261
    },
    {
      "epoch": 0.07649322182187644,
      "grad_norm": 2.7699193954467773,
      "learning_rate": 0.00018513677897536,
      "loss": 0.5841,
      "step": 16262
    },
    {
      "epoch": 0.07649792562348891,
      "grad_norm": 1.7502915859222412,
      "learning_rate": 0.0001851358359972465,
      "loss": 0.1371,
      "step": 16263
    },
    {
      "epoch": 0.07650262942510137,
      "grad_norm": 1.5002713203430176,
      "learning_rate": 0.00018513489301913303,
      "loss": 0.197,
      "step": 16264
    },
    {
      "epoch": 0.07650733322671383,
      "grad_norm": 0.4089144170284271,
      "learning_rate": 0.00018513395004101957,
      "loss": 0.0492,
      "step": 16265
    },
    {
      "epoch": 0.07651203702832629,
      "grad_norm": 3.268474578857422,
      "learning_rate": 0.0001851330070629061,
      "loss": 0.3169,
      "step": 16266
    },
    {
      "epoch": 0.07651674082993876,
      "grad_norm": 1.4435521364212036,
      "learning_rate": 0.0001851320640847926,
      "loss": 0.0944,
      "step": 16267
    },
    {
      "epoch": 0.07652144463155122,
      "grad_norm": 1.5727514028549194,
      "learning_rate": 0.00018513112110667913,
      "loss": 0.145,
      "step": 16268
    },
    {
      "epoch": 0.07652614843316369,
      "grad_norm": 5.681776523590088,
      "learning_rate": 0.00018513017812856565,
      "loss": 0.8598,
      "step": 16269
    },
    {
      "epoch": 0.07653085223477614,
      "grad_norm": 2.9212734699249268,
      "learning_rate": 0.00018512923515045217,
      "loss": 0.5519,
      "step": 16270
    },
    {
      "epoch": 0.0765355560363886,
      "grad_norm": 1.874977707862854,
      "learning_rate": 0.00018512829217233868,
      "loss": 0.158,
      "step": 16271
    },
    {
      "epoch": 0.07654025983800107,
      "grad_norm": 1.9369882345199585,
      "learning_rate": 0.0001851273491942252,
      "loss": 0.2142,
      "step": 16272
    },
    {
      "epoch": 0.07654496363961354,
      "grad_norm": 3.257004976272583,
      "learning_rate": 0.00018512640621611172,
      "loss": 0.5402,
      "step": 16273
    },
    {
      "epoch": 0.076549667441226,
      "grad_norm": 4.559799671173096,
      "learning_rate": 0.00018512546323799827,
      "loss": 0.2065,
      "step": 16274
    },
    {
      "epoch": 0.07655437124283847,
      "grad_norm": 2.6071174144744873,
      "learning_rate": 0.00018512452025988479,
      "loss": 0.3582,
      "step": 16275
    },
    {
      "epoch": 0.07655907504445092,
      "grad_norm": 1.7187438011169434,
      "learning_rate": 0.0001851235772817713,
      "loss": 0.1634,
      "step": 16276
    },
    {
      "epoch": 0.07656377884606339,
      "grad_norm": 3.5176353454589844,
      "learning_rate": 0.00018512263430365782,
      "loss": 0.8732,
      "step": 16277
    },
    {
      "epoch": 0.07656848264767585,
      "grad_norm": 1.2726441621780396,
      "learning_rate": 0.00018512169132554434,
      "loss": 0.1612,
      "step": 16278
    },
    {
      "epoch": 0.07657318644928832,
      "grad_norm": 0.319034218788147,
      "learning_rate": 0.00018512074834743086,
      "loss": 0.0255,
      "step": 16279
    },
    {
      "epoch": 0.07657789025090078,
      "grad_norm": 1.994844675064087,
      "learning_rate": 0.00018511980536931738,
      "loss": 0.1894,
      "step": 16280
    },
    {
      "epoch": 0.07658259405251325,
      "grad_norm": 2.9308555126190186,
      "learning_rate": 0.0001851188623912039,
      "loss": 0.6424,
      "step": 16281
    },
    {
      "epoch": 0.0765872978541257,
      "grad_norm": 2.6369338035583496,
      "learning_rate": 0.00018511791941309042,
      "loss": 0.6036,
      "step": 16282
    },
    {
      "epoch": 0.07659200165573816,
      "grad_norm": 2.31988263130188,
      "learning_rate": 0.00018511697643497696,
      "loss": 0.4161,
      "step": 16283
    },
    {
      "epoch": 0.07659670545735063,
      "grad_norm": 1.4715136289596558,
      "learning_rate": 0.00018511603345686348,
      "loss": 0.1418,
      "step": 16284
    },
    {
      "epoch": 0.0766014092589631,
      "grad_norm": 1.572135090827942,
      "learning_rate": 0.00018511509047875,
      "loss": 0.282,
      "step": 16285
    },
    {
      "epoch": 0.07660611306057556,
      "grad_norm": 1.9747822284698486,
      "learning_rate": 0.00018511414750063652,
      "loss": 0.3799,
      "step": 16286
    },
    {
      "epoch": 0.07661081686218801,
      "grad_norm": 1.9290236234664917,
      "learning_rate": 0.00018511320452252304,
      "loss": 0.132,
      "step": 16287
    },
    {
      "epoch": 0.07661552066380048,
      "grad_norm": 1.8464895486831665,
      "learning_rate": 0.00018511226154440958,
      "loss": 0.4457,
      "step": 16288
    },
    {
      "epoch": 0.07662022446541294,
      "grad_norm": 0.572509229183197,
      "learning_rate": 0.0001851113185662961,
      "loss": 0.0711,
      "step": 16289
    },
    {
      "epoch": 0.07662492826702541,
      "grad_norm": 0.3404746353626251,
      "learning_rate": 0.0001851103755881826,
      "loss": 0.0215,
      "step": 16290
    },
    {
      "epoch": 0.07662963206863788,
      "grad_norm": 0.3133486807346344,
      "learning_rate": 0.0001851094326100691,
      "loss": 0.0247,
      "step": 16291
    },
    {
      "epoch": 0.07663433587025034,
      "grad_norm": 2.567309856414795,
      "learning_rate": 0.00018510848963195566,
      "loss": 0.4962,
      "step": 16292
    },
    {
      "epoch": 0.0766390396718628,
      "grad_norm": 2.4118058681488037,
      "learning_rate": 0.00018510754665384218,
      "loss": 0.3084,
      "step": 16293
    },
    {
      "epoch": 0.07664374347347526,
      "grad_norm": 1.084686279296875,
      "learning_rate": 0.0001851066036757287,
      "loss": 0.0916,
      "step": 16294
    },
    {
      "epoch": 0.07664844727508772,
      "grad_norm": 1.0402246713638306,
      "learning_rate": 0.0001851056606976152,
      "loss": 0.1467,
      "step": 16295
    },
    {
      "epoch": 0.07665315107670019,
      "grad_norm": 1.0679447650909424,
      "learning_rate": 0.00018510471771950173,
      "loss": 0.1538,
      "step": 16296
    },
    {
      "epoch": 0.07665785487831266,
      "grad_norm": 1.9030306339263916,
      "learning_rate": 0.00018510377474138828,
      "loss": 0.2282,
      "step": 16297
    },
    {
      "epoch": 0.07666255867992512,
      "grad_norm": 0.6897987723350525,
      "learning_rate": 0.0001851028317632748,
      "loss": 0.105,
      "step": 16298
    },
    {
      "epoch": 0.07666726248153757,
      "grad_norm": 2.035759449005127,
      "learning_rate": 0.00018510188878516131,
      "loss": 0.275,
      "step": 16299
    },
    {
      "epoch": 0.07667196628315004,
      "grad_norm": 1.0820646286010742,
      "learning_rate": 0.00018510094580704783,
      "loss": 0.1628,
      "step": 16300
    },
    {
      "epoch": 0.0766766700847625,
      "grad_norm": 4.5946760177612305,
      "learning_rate": 0.00018510000282893435,
      "loss": 0.3684,
      "step": 16301
    },
    {
      "epoch": 0.07668137388637497,
      "grad_norm": 1.7510088682174683,
      "learning_rate": 0.00018509905985082087,
      "loss": 0.1933,
      "step": 16302
    },
    {
      "epoch": 0.07668607768798744,
      "grad_norm": 2.8183188438415527,
      "learning_rate": 0.0001850981168727074,
      "loss": 0.2958,
      "step": 16303
    },
    {
      "epoch": 0.07669078148959989,
      "grad_norm": 2.4791295528411865,
      "learning_rate": 0.0001850971738945939,
      "loss": 0.3774,
      "step": 16304
    },
    {
      "epoch": 0.07669548529121235,
      "grad_norm": 4.265222549438477,
      "learning_rate": 0.00018509623091648043,
      "loss": 0.6508,
      "step": 16305
    },
    {
      "epoch": 0.07670018909282482,
      "grad_norm": 2.074315071105957,
      "learning_rate": 0.00018509528793836697,
      "loss": 0.2063,
      "step": 16306
    },
    {
      "epoch": 0.07670489289443728,
      "grad_norm": 3.808563232421875,
      "learning_rate": 0.0001850943449602535,
      "loss": 0.5022,
      "step": 16307
    },
    {
      "epoch": 0.07670959669604975,
      "grad_norm": 1.4395742416381836,
      "learning_rate": 0.00018509340198214,
      "loss": 0.1995,
      "step": 16308
    },
    {
      "epoch": 0.07671430049766222,
      "grad_norm": 0.704347550868988,
      "learning_rate": 0.00018509245900402653,
      "loss": 0.0788,
      "step": 16309
    },
    {
      "epoch": 0.07671900429927467,
      "grad_norm": 1.803983449935913,
      "learning_rate": 0.00018509151602591305,
      "loss": 0.1573,
      "step": 16310
    },
    {
      "epoch": 0.07672370810088713,
      "grad_norm": 2.819584846496582,
      "learning_rate": 0.00018509057304779957,
      "loss": 0.4895,
      "step": 16311
    },
    {
      "epoch": 0.0767284119024996,
      "grad_norm": 2.0784504413604736,
      "learning_rate": 0.00018508963006968608,
      "loss": 0.236,
      "step": 16312
    },
    {
      "epoch": 0.07673311570411206,
      "grad_norm": 2.180542469024658,
      "learning_rate": 0.0001850886870915726,
      "loss": 0.1854,
      "step": 16313
    },
    {
      "epoch": 0.07673781950572453,
      "grad_norm": 1.5245661735534668,
      "learning_rate": 0.00018508774411345912,
      "loss": 0.1423,
      "step": 16314
    },
    {
      "epoch": 0.076742523307337,
      "grad_norm": 3.131758213043213,
      "learning_rate": 0.00018508680113534567,
      "loss": 0.3359,
      "step": 16315
    },
    {
      "epoch": 0.07674722710894945,
      "grad_norm": 0.9621034860610962,
      "learning_rate": 0.00018508585815723219,
      "loss": 0.0747,
      "step": 16316
    },
    {
      "epoch": 0.07675193091056191,
      "grad_norm": 1.1236944198608398,
      "learning_rate": 0.0001850849151791187,
      "loss": 0.1002,
      "step": 16317
    },
    {
      "epoch": 0.07675663471217438,
      "grad_norm": 3.815633535385132,
      "learning_rate": 0.00018508397220100522,
      "loss": 0.335,
      "step": 16318
    },
    {
      "epoch": 0.07676133851378684,
      "grad_norm": 1.7850406169891357,
      "learning_rate": 0.00018508302922289174,
      "loss": 0.1862,
      "step": 16319
    },
    {
      "epoch": 0.07676604231539931,
      "grad_norm": 2.564586877822876,
      "learning_rate": 0.0001850820862447783,
      "loss": 0.2743,
      "step": 16320
    },
    {
      "epoch": 0.07677074611701176,
      "grad_norm": 4.493045806884766,
      "learning_rate": 0.00018508114326666478,
      "loss": 0.3336,
      "step": 16321
    },
    {
      "epoch": 0.07677544991862423,
      "grad_norm": 0.10083917528390884,
      "learning_rate": 0.0001850802002885513,
      "loss": 0.0057,
      "step": 16322
    },
    {
      "epoch": 0.0767801537202367,
      "grad_norm": 1.6539137363433838,
      "learning_rate": 0.00018507925731043782,
      "loss": 0.1266,
      "step": 16323
    },
    {
      "epoch": 0.07678485752184916,
      "grad_norm": 0.9964489936828613,
      "learning_rate": 0.00018507831433232436,
      "loss": 0.0893,
      "step": 16324
    },
    {
      "epoch": 0.07678956132346162,
      "grad_norm": 7.353445529937744,
      "learning_rate": 0.00018507737135421088,
      "loss": 0.3737,
      "step": 16325
    },
    {
      "epoch": 0.07679426512507409,
      "grad_norm": 0.3410450220108032,
      "learning_rate": 0.0001850764283760974,
      "loss": 0.0184,
      "step": 16326
    },
    {
      "epoch": 0.07679896892668654,
      "grad_norm": 3.39272403717041,
      "learning_rate": 0.00018507548539798392,
      "loss": 0.1925,
      "step": 16327
    },
    {
      "epoch": 0.07680367272829901,
      "grad_norm": 5.985403537750244,
      "learning_rate": 0.00018507454241987044,
      "loss": 0.8542,
      "step": 16328
    },
    {
      "epoch": 0.07680837652991147,
      "grad_norm": 0.13913705945014954,
      "learning_rate": 0.00018507359944175698,
      "loss": 0.0071,
      "step": 16329
    },
    {
      "epoch": 0.07681308033152394,
      "grad_norm": 2.460132360458374,
      "learning_rate": 0.0001850726564636435,
      "loss": 0.2226,
      "step": 16330
    },
    {
      "epoch": 0.0768177841331364,
      "grad_norm": 0.4467112123966217,
      "learning_rate": 0.00018507171348553002,
      "loss": 0.0638,
      "step": 16331
    },
    {
      "epoch": 0.07682248793474887,
      "grad_norm": 0.209102064371109,
      "learning_rate": 0.0001850707705074165,
      "loss": 0.0119,
      "step": 16332
    },
    {
      "epoch": 0.07682719173636132,
      "grad_norm": 0.837943434715271,
      "learning_rate": 0.00018506982752930306,
      "loss": 0.062,
      "step": 16333
    },
    {
      "epoch": 0.07683189553797379,
      "grad_norm": 1.642683506011963,
      "learning_rate": 0.00018506888455118958,
      "loss": 0.1292,
      "step": 16334
    },
    {
      "epoch": 0.07683659933958625,
      "grad_norm": 2.0402042865753174,
      "learning_rate": 0.0001850679415730761,
      "loss": 0.1479,
      "step": 16335
    },
    {
      "epoch": 0.07684130314119872,
      "grad_norm": 3.589203119277954,
      "learning_rate": 0.0001850669985949626,
      "loss": 0.3675,
      "step": 16336
    },
    {
      "epoch": 0.07684600694281118,
      "grad_norm": 0.11606905609369278,
      "learning_rate": 0.00018506605561684913,
      "loss": 0.0091,
      "step": 16337
    },
    {
      "epoch": 0.07685071074442364,
      "grad_norm": 4.536116123199463,
      "learning_rate": 0.00018506511263873568,
      "loss": 0.6251,
      "step": 16338
    },
    {
      "epoch": 0.0768554145460361,
      "grad_norm": 5.121384143829346,
      "learning_rate": 0.0001850641696606222,
      "loss": 1.0732,
      "step": 16339
    },
    {
      "epoch": 0.07686011834764857,
      "grad_norm": 5.898451805114746,
      "learning_rate": 0.00018506322668250871,
      "loss": 0.6819,
      "step": 16340
    },
    {
      "epoch": 0.07686482214926103,
      "grad_norm": 6.035735607147217,
      "learning_rate": 0.00018506228370439523,
      "loss": 1.1139,
      "step": 16341
    },
    {
      "epoch": 0.0768695259508735,
      "grad_norm": 5.257126808166504,
      "learning_rate": 0.00018506134072628175,
      "loss": 1.2835,
      "step": 16342
    },
    {
      "epoch": 0.07687422975248596,
      "grad_norm": 0.38726896047592163,
      "learning_rate": 0.00018506039774816827,
      "loss": 0.0226,
      "step": 16343
    },
    {
      "epoch": 0.07687893355409842,
      "grad_norm": 3.46211314201355,
      "learning_rate": 0.0001850594547700548,
      "loss": 0.1817,
      "step": 16344
    },
    {
      "epoch": 0.07688363735571088,
      "grad_norm": 1.086617350578308,
      "learning_rate": 0.0001850585117919413,
      "loss": 0.0562,
      "step": 16345
    },
    {
      "epoch": 0.07688834115732335,
      "grad_norm": 1.5076037645339966,
      "learning_rate": 0.00018505756881382783,
      "loss": 0.0888,
      "step": 16346
    },
    {
      "epoch": 0.07689304495893581,
      "grad_norm": 1.2764555215835571,
      "learning_rate": 0.00018505662583571437,
      "loss": 0.042,
      "step": 16347
    },
    {
      "epoch": 0.07689774876054828,
      "grad_norm": 1.6305181980133057,
      "learning_rate": 0.0001850556828576009,
      "loss": 0.1222,
      "step": 16348
    },
    {
      "epoch": 0.07690245256216074,
      "grad_norm": 1.703241229057312,
      "learning_rate": 0.0001850547398794874,
      "loss": 0.3553,
      "step": 16349
    },
    {
      "epoch": 0.0769071563637732,
      "grad_norm": 6.127689838409424,
      "learning_rate": 0.00018505379690137393,
      "loss": 0.3188,
      "step": 16350
    },
    {
      "epoch": 0.07691186016538566,
      "grad_norm": 4.627264022827148,
      "learning_rate": 0.00018505285392326047,
      "loss": 0.2863,
      "step": 16351
    },
    {
      "epoch": 0.07691656396699813,
      "grad_norm": 2.520519733428955,
      "learning_rate": 0.00018505191094514697,
      "loss": 0.4255,
      "step": 16352
    },
    {
      "epoch": 0.0769212677686106,
      "grad_norm": 2.061413049697876,
      "learning_rate": 0.00018505096796703348,
      "loss": 0.1877,
      "step": 16353
    },
    {
      "epoch": 0.07692597157022306,
      "grad_norm": 2.2282638549804688,
      "learning_rate": 0.00018505002498892,
      "loss": 0.1995,
      "step": 16354
    },
    {
      "epoch": 0.07693067537183551,
      "grad_norm": 3.593226671218872,
      "learning_rate": 0.00018504908201080652,
      "loss": 0.4834,
      "step": 16355
    },
    {
      "epoch": 0.07693537917344798,
      "grad_norm": 0.24952958524227142,
      "learning_rate": 0.00018504813903269307,
      "loss": 0.0263,
      "step": 16356
    },
    {
      "epoch": 0.07694008297506044,
      "grad_norm": 1.7825829982757568,
      "learning_rate": 0.00018504719605457959,
      "loss": 0.1815,
      "step": 16357
    },
    {
      "epoch": 0.07694478677667291,
      "grad_norm": 2.1726884841918945,
      "learning_rate": 0.0001850462530764661,
      "loss": 0.1898,
      "step": 16358
    },
    {
      "epoch": 0.07694949057828537,
      "grad_norm": 1.0044212341308594,
      "learning_rate": 0.00018504531009835262,
      "loss": 0.1404,
      "step": 16359
    },
    {
      "epoch": 0.07695419437989784,
      "grad_norm": 2.71476149559021,
      "learning_rate": 0.00018504436712023914,
      "loss": 0.4802,
      "step": 16360
    },
    {
      "epoch": 0.07695889818151029,
      "grad_norm": 2.281934976577759,
      "learning_rate": 0.0001850434241421257,
      "loss": 0.1355,
      "step": 16361
    },
    {
      "epoch": 0.07696360198312276,
      "grad_norm": 2.9231433868408203,
      "learning_rate": 0.0001850424811640122,
      "loss": 0.2217,
      "step": 16362
    },
    {
      "epoch": 0.07696830578473522,
      "grad_norm": 3.437371253967285,
      "learning_rate": 0.0001850415381858987,
      "loss": 0.3836,
      "step": 16363
    },
    {
      "epoch": 0.07697300958634769,
      "grad_norm": 1.1743730306625366,
      "learning_rate": 0.00018504059520778522,
      "loss": 0.1029,
      "step": 16364
    },
    {
      "epoch": 0.07697771338796015,
      "grad_norm": 0.7411070466041565,
      "learning_rate": 0.00018503965222967176,
      "loss": 0.0573,
      "step": 16365
    },
    {
      "epoch": 0.07698241718957262,
      "grad_norm": 4.087386608123779,
      "learning_rate": 0.00018503870925155828,
      "loss": 0.351,
      "step": 16366
    },
    {
      "epoch": 0.07698712099118507,
      "grad_norm": 2.8478000164031982,
      "learning_rate": 0.0001850377662734448,
      "loss": 0.2037,
      "step": 16367
    },
    {
      "epoch": 0.07699182479279754,
      "grad_norm": 3.004305362701416,
      "learning_rate": 0.00018503682329533132,
      "loss": 0.3239,
      "step": 16368
    },
    {
      "epoch": 0.07699652859441,
      "grad_norm": 3.0001919269561768,
      "learning_rate": 0.00018503588031721784,
      "loss": 0.3979,
      "step": 16369
    },
    {
      "epoch": 0.07700123239602247,
      "grad_norm": 0.26386481523513794,
      "learning_rate": 0.00018503493733910438,
      "loss": 0.0249,
      "step": 16370
    },
    {
      "epoch": 0.07700593619763493,
      "grad_norm": 2.314518928527832,
      "learning_rate": 0.0001850339943609909,
      "loss": 0.2299,
      "step": 16371
    },
    {
      "epoch": 0.07701063999924739,
      "grad_norm": 2.0612144470214844,
      "learning_rate": 0.00018503305138287742,
      "loss": 0.2349,
      "step": 16372
    },
    {
      "epoch": 0.07701534380085985,
      "grad_norm": 1.0859941244125366,
      "learning_rate": 0.00018503210840476394,
      "loss": 0.0822,
      "step": 16373
    },
    {
      "epoch": 0.07702004760247232,
      "grad_norm": 1.596587896347046,
      "learning_rate": 0.00018503116542665046,
      "loss": 0.0875,
      "step": 16374
    },
    {
      "epoch": 0.07702475140408478,
      "grad_norm": 1.237106204032898,
      "learning_rate": 0.00018503022244853698,
      "loss": 0.1088,
      "step": 16375
    },
    {
      "epoch": 0.07702945520569725,
      "grad_norm": 0.5919880867004395,
      "learning_rate": 0.0001850292794704235,
      "loss": 0.0397,
      "step": 16376
    },
    {
      "epoch": 0.07703415900730971,
      "grad_norm": 1.7411720752716064,
      "learning_rate": 0.00018502833649231,
      "loss": 0.1286,
      "step": 16377
    },
    {
      "epoch": 0.07703886280892216,
      "grad_norm": 2.1927456855773926,
      "learning_rate": 0.00018502739351419653,
      "loss": 0.3131,
      "step": 16378
    },
    {
      "epoch": 0.07704356661053463,
      "grad_norm": 5.992857933044434,
      "learning_rate": 0.00018502645053608308,
      "loss": 1.0811,
      "step": 16379
    },
    {
      "epoch": 0.0770482704121471,
      "grad_norm": 2.2520370483398438,
      "learning_rate": 0.0001850255075579696,
      "loss": 0.2355,
      "step": 16380
    },
    {
      "epoch": 0.07705297421375956,
      "grad_norm": 2.3241629600524902,
      "learning_rate": 0.00018502456457985611,
      "loss": 0.1269,
      "step": 16381
    },
    {
      "epoch": 0.07705767801537203,
      "grad_norm": 2.180549383163452,
      "learning_rate": 0.00018502362160174263,
      "loss": 0.1561,
      "step": 16382
    },
    {
      "epoch": 0.0770623818169845,
      "grad_norm": 0.4584455192089081,
      "learning_rate": 0.00018502267862362915,
      "loss": 0.0338,
      "step": 16383
    },
    {
      "epoch": 0.07706708561859694,
      "grad_norm": 1.7889742851257324,
      "learning_rate": 0.00018502173564551567,
      "loss": 0.2396,
      "step": 16384
    },
    {
      "epoch": 0.07707178942020941,
      "grad_norm": 3.534578323364258,
      "learning_rate": 0.0001850207926674022,
      "loss": 0.6455,
      "step": 16385
    },
    {
      "epoch": 0.07707649322182188,
      "grad_norm": 1.8217732906341553,
      "learning_rate": 0.0001850198496892887,
      "loss": 0.0904,
      "step": 16386
    },
    {
      "epoch": 0.07708119702343434,
      "grad_norm": 0.8811127543449402,
      "learning_rate": 0.00018501890671117523,
      "loss": 0.067,
      "step": 16387
    },
    {
      "epoch": 0.07708590082504681,
      "grad_norm": 2.8073039054870605,
      "learning_rate": 0.00018501796373306177,
      "loss": 0.4021,
      "step": 16388
    },
    {
      "epoch": 0.07709060462665926,
      "grad_norm": 0.7601862549781799,
      "learning_rate": 0.0001850170207549483,
      "loss": 0.0555,
      "step": 16389
    },
    {
      "epoch": 0.07709530842827172,
      "grad_norm": 1.8448039293289185,
      "learning_rate": 0.0001850160777768348,
      "loss": 0.1489,
      "step": 16390
    },
    {
      "epoch": 0.07710001222988419,
      "grad_norm": 1.5518683195114136,
      "learning_rate": 0.00018501513479872133,
      "loss": 0.1526,
      "step": 16391
    },
    {
      "epoch": 0.07710471603149666,
      "grad_norm": 2.9343972206115723,
      "learning_rate": 0.00018501419182060787,
      "loss": 0.4524,
      "step": 16392
    },
    {
      "epoch": 0.07710941983310912,
      "grad_norm": 2.8278064727783203,
      "learning_rate": 0.0001850132488424944,
      "loss": 0.6658,
      "step": 16393
    },
    {
      "epoch": 0.07711412363472159,
      "grad_norm": 2.092216968536377,
      "learning_rate": 0.00018501230586438088,
      "loss": 0.3185,
      "step": 16394
    },
    {
      "epoch": 0.07711882743633404,
      "grad_norm": 2.5202505588531494,
      "learning_rate": 0.0001850113628862674,
      "loss": 0.3082,
      "step": 16395
    },
    {
      "epoch": 0.0771235312379465,
      "grad_norm": 0.19251972436904907,
      "learning_rate": 0.00018501041990815392,
      "loss": 0.0122,
      "step": 16396
    },
    {
      "epoch": 0.07712823503955897,
      "grad_norm": 1.8789267539978027,
      "learning_rate": 0.00018500947693004047,
      "loss": 0.4783,
      "step": 16397
    },
    {
      "epoch": 0.07713293884117144,
      "grad_norm": 1.8530577421188354,
      "learning_rate": 0.00018500853395192699,
      "loss": 0.208,
      "step": 16398
    },
    {
      "epoch": 0.0771376426427839,
      "grad_norm": 1.6485114097595215,
      "learning_rate": 0.0001850075909738135,
      "loss": 0.0896,
      "step": 16399
    },
    {
      "epoch": 0.07714234644439637,
      "grad_norm": 1.525779128074646,
      "learning_rate": 0.00018500664799570002,
      "loss": 0.1499,
      "step": 16400
    },
    {
      "epoch": 0.07714705024600882,
      "grad_norm": 1.6918158531188965,
      "learning_rate": 0.00018500570501758657,
      "loss": 0.0689,
      "step": 16401
    },
    {
      "epoch": 0.07715175404762128,
      "grad_norm": 0.210439994931221,
      "learning_rate": 0.0001850047620394731,
      "loss": 0.0095,
      "step": 16402
    },
    {
      "epoch": 0.07715645784923375,
      "grad_norm": 0.40335357189178467,
      "learning_rate": 0.0001850038190613596,
      "loss": 0.0328,
      "step": 16403
    },
    {
      "epoch": 0.07716116165084622,
      "grad_norm": 0.5692300200462341,
      "learning_rate": 0.00018500287608324612,
      "loss": 0.0489,
      "step": 16404
    },
    {
      "epoch": 0.07716586545245868,
      "grad_norm": 0.5145652890205383,
      "learning_rate": 0.00018500193310513262,
      "loss": 0.0554,
      "step": 16405
    },
    {
      "epoch": 0.07717056925407113,
      "grad_norm": 1.0591775178909302,
      "learning_rate": 0.00018500099012701916,
      "loss": 0.0668,
      "step": 16406
    },
    {
      "epoch": 0.0771752730556836,
      "grad_norm": 2.884290933609009,
      "learning_rate": 0.00018500004714890568,
      "loss": 0.5038,
      "step": 16407
    },
    {
      "epoch": 0.07717997685729606,
      "grad_norm": 3.7472596168518066,
      "learning_rate": 0.0001849991041707922,
      "loss": 0.6531,
      "step": 16408
    },
    {
      "epoch": 0.07718468065890853,
      "grad_norm": 3.922779083251953,
      "learning_rate": 0.00018499816119267872,
      "loss": 0.5077,
      "step": 16409
    },
    {
      "epoch": 0.077189384460521,
      "grad_norm": 3.1177546977996826,
      "learning_rate": 0.00018499721821456524,
      "loss": 0.3251,
      "step": 16410
    },
    {
      "epoch": 0.07719408826213346,
      "grad_norm": 1.9040230512619019,
      "learning_rate": 0.00018499627523645178,
      "loss": 0.2469,
      "step": 16411
    },
    {
      "epoch": 0.07719879206374591,
      "grad_norm": 3.522392511367798,
      "learning_rate": 0.0001849953322583383,
      "loss": 0.5557,
      "step": 16412
    },
    {
      "epoch": 0.07720349586535838,
      "grad_norm": 2.2629735469818115,
      "learning_rate": 0.00018499438928022482,
      "loss": 0.2156,
      "step": 16413
    },
    {
      "epoch": 0.07720819966697084,
      "grad_norm": 2.0564165115356445,
      "learning_rate": 0.00018499344630211134,
      "loss": 0.1406,
      "step": 16414
    },
    {
      "epoch": 0.07721290346858331,
      "grad_norm": 1.5823049545288086,
      "learning_rate": 0.00018499250332399786,
      "loss": 0.1577,
      "step": 16415
    },
    {
      "epoch": 0.07721760727019578,
      "grad_norm": 4.751852989196777,
      "learning_rate": 0.00018499156034588438,
      "loss": 0.4241,
      "step": 16416
    },
    {
      "epoch": 0.07722231107180824,
      "grad_norm": 4.199569225311279,
      "learning_rate": 0.0001849906173677709,
      "loss": 0.6933,
      "step": 16417
    },
    {
      "epoch": 0.0772270148734207,
      "grad_norm": 2.4697458744049072,
      "learning_rate": 0.0001849896743896574,
      "loss": 0.3765,
      "step": 16418
    },
    {
      "epoch": 0.07723171867503316,
      "grad_norm": 1.8076590299606323,
      "learning_rate": 0.00018498873141154393,
      "loss": 0.1623,
      "step": 16419
    },
    {
      "epoch": 0.07723642247664562,
      "grad_norm": 2.4296956062316895,
      "learning_rate": 0.00018498778843343048,
      "loss": 0.1593,
      "step": 16420
    },
    {
      "epoch": 0.07724112627825809,
      "grad_norm": 1.9174188375473022,
      "learning_rate": 0.000184986845455317,
      "loss": 0.2035,
      "step": 16421
    },
    {
      "epoch": 0.07724583007987056,
      "grad_norm": 3.4341132640838623,
      "learning_rate": 0.00018498590247720351,
      "loss": 0.3137,
      "step": 16422
    },
    {
      "epoch": 0.07725053388148301,
      "grad_norm": 1.8585742712020874,
      "learning_rate": 0.00018498495949909003,
      "loss": 0.4014,
      "step": 16423
    },
    {
      "epoch": 0.07725523768309547,
      "grad_norm": 1.1602452993392944,
      "learning_rate": 0.00018498401652097658,
      "loss": 0.13,
      "step": 16424
    },
    {
      "epoch": 0.07725994148470794,
      "grad_norm": 4.0586256980896,
      "learning_rate": 0.00018498307354286307,
      "loss": 0.7078,
      "step": 16425
    },
    {
      "epoch": 0.0772646452863204,
      "grad_norm": 2.263984441757202,
      "learning_rate": 0.0001849821305647496,
      "loss": 0.2808,
      "step": 16426
    },
    {
      "epoch": 0.07726934908793287,
      "grad_norm": 1.5010318756103516,
      "learning_rate": 0.0001849811875866361,
      "loss": 0.1212,
      "step": 16427
    },
    {
      "epoch": 0.07727405288954534,
      "grad_norm": 1.352555751800537,
      "learning_rate": 0.00018498024460852263,
      "loss": 0.1247,
      "step": 16428
    },
    {
      "epoch": 0.07727875669115779,
      "grad_norm": 1.2733154296875,
      "learning_rate": 0.00018497930163040917,
      "loss": 0.0749,
      "step": 16429
    },
    {
      "epoch": 0.07728346049277025,
      "grad_norm": 0.707062840461731,
      "learning_rate": 0.0001849783586522957,
      "loss": 0.0889,
      "step": 16430
    },
    {
      "epoch": 0.07728816429438272,
      "grad_norm": 3.563950300216675,
      "learning_rate": 0.0001849774156741822,
      "loss": 0.399,
      "step": 16431
    },
    {
      "epoch": 0.07729286809599518,
      "grad_norm": 0.7733553647994995,
      "learning_rate": 0.00018497647269606873,
      "loss": 0.0648,
      "step": 16432
    },
    {
      "epoch": 0.07729757189760765,
      "grad_norm": 2.293738603591919,
      "learning_rate": 0.00018497552971795527,
      "loss": 0.1901,
      "step": 16433
    },
    {
      "epoch": 0.07730227569922012,
      "grad_norm": 0.4166654050350189,
      "learning_rate": 0.0001849745867398418,
      "loss": 0.0276,
      "step": 16434
    },
    {
      "epoch": 0.07730697950083257,
      "grad_norm": 0.3585410714149475,
      "learning_rate": 0.0001849736437617283,
      "loss": 0.0406,
      "step": 16435
    },
    {
      "epoch": 0.07731168330244503,
      "grad_norm": 1.416287899017334,
      "learning_rate": 0.0001849727007836148,
      "loss": 0.0674,
      "step": 16436
    },
    {
      "epoch": 0.0773163871040575,
      "grad_norm": 1.6387126445770264,
      "learning_rate": 0.00018497175780550132,
      "loss": 0.1535,
      "step": 16437
    },
    {
      "epoch": 0.07732109090566996,
      "grad_norm": 3.2230794429779053,
      "learning_rate": 0.00018497081482738787,
      "loss": 0.5081,
      "step": 16438
    },
    {
      "epoch": 0.07732579470728243,
      "grad_norm": 3.2986083030700684,
      "learning_rate": 0.00018496987184927439,
      "loss": 0.5362,
      "step": 16439
    },
    {
      "epoch": 0.07733049850889488,
      "grad_norm": 5.7366485595703125,
      "learning_rate": 0.0001849689288711609,
      "loss": 0.954,
      "step": 16440
    },
    {
      "epoch": 0.07733520231050735,
      "grad_norm": 3.2897229194641113,
      "learning_rate": 0.00018496798589304742,
      "loss": 0.1481,
      "step": 16441
    },
    {
      "epoch": 0.07733990611211981,
      "grad_norm": 3.69545578956604,
      "learning_rate": 0.00018496704291493397,
      "loss": 0.2747,
      "step": 16442
    },
    {
      "epoch": 0.07734460991373228,
      "grad_norm": 0.974886953830719,
      "learning_rate": 0.0001849660999368205,
      "loss": 0.0776,
      "step": 16443
    },
    {
      "epoch": 0.07734931371534474,
      "grad_norm": 1.0613579750061035,
      "learning_rate": 0.000184965156958707,
      "loss": 0.0327,
      "step": 16444
    },
    {
      "epoch": 0.07735401751695721,
      "grad_norm": 2.418379783630371,
      "learning_rate": 0.00018496421398059352,
      "loss": 0.2547,
      "step": 16445
    },
    {
      "epoch": 0.07735872131856966,
      "grad_norm": 1.1620687246322632,
      "learning_rate": 0.00018496327100248004,
      "loss": 0.06,
      "step": 16446
    },
    {
      "epoch": 0.07736342512018213,
      "grad_norm": 3.2372231483459473,
      "learning_rate": 0.00018496232802436656,
      "loss": 0.6241,
      "step": 16447
    },
    {
      "epoch": 0.0773681289217946,
      "grad_norm": 1.777747631072998,
      "learning_rate": 0.00018496138504625308,
      "loss": 0.4611,
      "step": 16448
    },
    {
      "epoch": 0.07737283272340706,
      "grad_norm": 4.028436183929443,
      "learning_rate": 0.0001849604420681396,
      "loss": 0.3029,
      "step": 16449
    },
    {
      "epoch": 0.07737753652501952,
      "grad_norm": 0.8915127515792847,
      "learning_rate": 0.00018495949909002612,
      "loss": 0.0554,
      "step": 16450
    },
    {
      "epoch": 0.07738224032663199,
      "grad_norm": 3.6914560794830322,
      "learning_rate": 0.00018495855611191266,
      "loss": 0.3132,
      "step": 16451
    },
    {
      "epoch": 0.07738694412824444,
      "grad_norm": 3.883033514022827,
      "learning_rate": 0.00018495761313379918,
      "loss": 0.3502,
      "step": 16452
    },
    {
      "epoch": 0.07739164792985691,
      "grad_norm": 0.7837774157524109,
      "learning_rate": 0.0001849566701556857,
      "loss": 0.0489,
      "step": 16453
    },
    {
      "epoch": 0.07739635173146937,
      "grad_norm": 1.748738408088684,
      "learning_rate": 0.00018495572717757222,
      "loss": 0.1328,
      "step": 16454
    },
    {
      "epoch": 0.07740105553308184,
      "grad_norm": 0.6123125553131104,
      "learning_rate": 0.00018495478419945874,
      "loss": 0.02,
      "step": 16455
    },
    {
      "epoch": 0.0774057593346943,
      "grad_norm": 3.0728352069854736,
      "learning_rate": 0.00018495384122134526,
      "loss": 0.6154,
      "step": 16456
    },
    {
      "epoch": 0.07741046313630676,
      "grad_norm": 3.0970818996429443,
      "learning_rate": 0.00018495289824323178,
      "loss": 0.6667,
      "step": 16457
    },
    {
      "epoch": 0.07741516693791922,
      "grad_norm": 1.4707679748535156,
      "learning_rate": 0.0001849519552651183,
      "loss": 0.1872,
      "step": 16458
    },
    {
      "epoch": 0.07741987073953169,
      "grad_norm": 4.753581523895264,
      "learning_rate": 0.0001849510122870048,
      "loss": 0.3878,
      "step": 16459
    },
    {
      "epoch": 0.07742457454114415,
      "grad_norm": 0.36740824580192566,
      "learning_rate": 0.00018495006930889136,
      "loss": 0.0393,
      "step": 16460
    },
    {
      "epoch": 0.07742927834275662,
      "grad_norm": 3.2259719371795654,
      "learning_rate": 0.00018494912633077788,
      "loss": 0.3577,
      "step": 16461
    },
    {
      "epoch": 0.07743398214436908,
      "grad_norm": 4.399287700653076,
      "learning_rate": 0.0001849481833526644,
      "loss": 0.9919,
      "step": 16462
    },
    {
      "epoch": 0.07743868594598154,
      "grad_norm": 4.75516414642334,
      "learning_rate": 0.00018494724037455091,
      "loss": 0.8094,
      "step": 16463
    },
    {
      "epoch": 0.077443389747594,
      "grad_norm": 1.2560689449310303,
      "learning_rate": 0.00018494629739643743,
      "loss": 0.1273,
      "step": 16464
    },
    {
      "epoch": 0.07744809354920647,
      "grad_norm": 0.6763251423835754,
      "learning_rate": 0.00018494535441832398,
      "loss": 0.0376,
      "step": 16465
    },
    {
      "epoch": 0.07745279735081893,
      "grad_norm": 3.952100992202759,
      "learning_rate": 0.0001849444114402105,
      "loss": 0.6224,
      "step": 16466
    },
    {
      "epoch": 0.0774575011524314,
      "grad_norm": 1.6250001192092896,
      "learning_rate": 0.000184943468462097,
      "loss": 0.1227,
      "step": 16467
    },
    {
      "epoch": 0.07746220495404386,
      "grad_norm": 1.1266539096832275,
      "learning_rate": 0.0001849425254839835,
      "loss": 0.0686,
      "step": 16468
    },
    {
      "epoch": 0.07746690875565632,
      "grad_norm": 1.750011920928955,
      "learning_rate": 0.00018494158250587003,
      "loss": 0.3204,
      "step": 16469
    },
    {
      "epoch": 0.07747161255726878,
      "grad_norm": 1.536686897277832,
      "learning_rate": 0.00018494063952775657,
      "loss": 0.2,
      "step": 16470
    },
    {
      "epoch": 0.07747631635888125,
      "grad_norm": 3.358576774597168,
      "learning_rate": 0.0001849396965496431,
      "loss": 0.6765,
      "step": 16471
    },
    {
      "epoch": 0.07748102016049371,
      "grad_norm": 0.03136865794658661,
      "learning_rate": 0.0001849387535715296,
      "loss": 0.0018,
      "step": 16472
    },
    {
      "epoch": 0.07748572396210618,
      "grad_norm": 2.107323408126831,
      "learning_rate": 0.00018493781059341613,
      "loss": 0.3357,
      "step": 16473
    },
    {
      "epoch": 0.07749042776371863,
      "grad_norm": 2.550961971282959,
      "learning_rate": 0.00018493686761530267,
      "loss": 0.4353,
      "step": 16474
    },
    {
      "epoch": 0.0774951315653311,
      "grad_norm": 2.275520086288452,
      "learning_rate": 0.0001849359246371892,
      "loss": 0.2675,
      "step": 16475
    },
    {
      "epoch": 0.07749983536694356,
      "grad_norm": 3.1945960521698,
      "learning_rate": 0.0001849349816590757,
      "loss": 0.7261,
      "step": 16476
    },
    {
      "epoch": 0.07750453916855603,
      "grad_norm": 1.2790460586547852,
      "learning_rate": 0.00018493403868096223,
      "loss": 0.1546,
      "step": 16477
    },
    {
      "epoch": 0.0775092429701685,
      "grad_norm": 3.854677438735962,
      "learning_rate": 0.00018493309570284875,
      "loss": 0.4763,
      "step": 16478
    },
    {
      "epoch": 0.07751394677178096,
      "grad_norm": 1.8794164657592773,
      "learning_rate": 0.00018493215272473527,
      "loss": 0.3302,
      "step": 16479
    },
    {
      "epoch": 0.07751865057339341,
      "grad_norm": 1.1529992818832397,
      "learning_rate": 0.00018493120974662179,
      "loss": 0.2034,
      "step": 16480
    },
    {
      "epoch": 0.07752335437500588,
      "grad_norm": 0.9389674663543701,
      "learning_rate": 0.0001849302667685083,
      "loss": 0.0343,
      "step": 16481
    },
    {
      "epoch": 0.07752805817661834,
      "grad_norm": 1.8410773277282715,
      "learning_rate": 0.00018492932379039482,
      "loss": 0.1987,
      "step": 16482
    },
    {
      "epoch": 0.07753276197823081,
      "grad_norm": 0.6286216974258423,
      "learning_rate": 0.00018492838081228137,
      "loss": 0.0577,
      "step": 16483
    },
    {
      "epoch": 0.07753746577984327,
      "grad_norm": 2.4565629959106445,
      "learning_rate": 0.0001849274378341679,
      "loss": 0.4192,
      "step": 16484
    },
    {
      "epoch": 0.07754216958145574,
      "grad_norm": 4.324024677276611,
      "learning_rate": 0.0001849264948560544,
      "loss": 0.4373,
      "step": 16485
    },
    {
      "epoch": 0.07754687338306819,
      "grad_norm": 0.6547783017158508,
      "learning_rate": 0.00018492555187794092,
      "loss": 0.0397,
      "step": 16486
    },
    {
      "epoch": 0.07755157718468066,
      "grad_norm": 1.5729373693466187,
      "learning_rate": 0.00018492460889982744,
      "loss": 0.2697,
      "step": 16487
    },
    {
      "epoch": 0.07755628098629312,
      "grad_norm": 0.2714606821537018,
      "learning_rate": 0.00018492366592171396,
      "loss": 0.0167,
      "step": 16488
    },
    {
      "epoch": 0.07756098478790559,
      "grad_norm": 0.38215962052345276,
      "learning_rate": 0.00018492272294360048,
      "loss": 0.0528,
      "step": 16489
    },
    {
      "epoch": 0.07756568858951805,
      "grad_norm": 2.746887683868408,
      "learning_rate": 0.000184921779965487,
      "loss": 0.3336,
      "step": 16490
    },
    {
      "epoch": 0.0775703923911305,
      "grad_norm": 1.7834992408752441,
      "learning_rate": 0.00018492083698737352,
      "loss": 0.1937,
      "step": 16491
    },
    {
      "epoch": 0.07757509619274297,
      "grad_norm": 3.208519458770752,
      "learning_rate": 0.00018491989400926006,
      "loss": 0.555,
      "step": 16492
    },
    {
      "epoch": 0.07757979999435544,
      "grad_norm": 3.5264182090759277,
      "learning_rate": 0.00018491895103114658,
      "loss": 0.8822,
      "step": 16493
    },
    {
      "epoch": 0.0775845037959679,
      "grad_norm": 2.280897617340088,
      "learning_rate": 0.0001849180080530331,
      "loss": 0.1782,
      "step": 16494
    },
    {
      "epoch": 0.07758920759758037,
      "grad_norm": 0.9076535105705261,
      "learning_rate": 0.00018491706507491962,
      "loss": 0.0826,
      "step": 16495
    },
    {
      "epoch": 0.07759391139919283,
      "grad_norm": 0.5274502038955688,
      "learning_rate": 0.00018491612209680614,
      "loss": 0.0402,
      "step": 16496
    },
    {
      "epoch": 0.07759861520080528,
      "grad_norm": 1.1297286748886108,
      "learning_rate": 0.00018491517911869268,
      "loss": 0.1522,
      "step": 16497
    },
    {
      "epoch": 0.07760331900241775,
      "grad_norm": 1.1250910758972168,
      "learning_rate": 0.00018491423614057918,
      "loss": 0.2754,
      "step": 16498
    },
    {
      "epoch": 0.07760802280403022,
      "grad_norm": 3.0926156044006348,
      "learning_rate": 0.0001849132931624657,
      "loss": 0.5997,
      "step": 16499
    },
    {
      "epoch": 0.07761272660564268,
      "grad_norm": 1.7506883144378662,
      "learning_rate": 0.0001849123501843522,
      "loss": 0.2017,
      "step": 16500
    },
    {
      "epoch": 0.07761743040725515,
      "grad_norm": 0.9543281197547913,
      "learning_rate": 0.00018491140720623876,
      "loss": 0.0777,
      "step": 16501
    },
    {
      "epoch": 0.07762213420886761,
      "grad_norm": 1.7225176095962524,
      "learning_rate": 0.00018491046422812528,
      "loss": 0.1901,
      "step": 16502
    },
    {
      "epoch": 0.07762683801048006,
      "grad_norm": 3.265491485595703,
      "learning_rate": 0.0001849095212500118,
      "loss": 0.7049,
      "step": 16503
    },
    {
      "epoch": 0.07763154181209253,
      "grad_norm": 3.6472184658050537,
      "learning_rate": 0.00018490857827189831,
      "loss": 0.5319,
      "step": 16504
    },
    {
      "epoch": 0.077636245613705,
      "grad_norm": 0.1803407371044159,
      "learning_rate": 0.00018490763529378483,
      "loss": 0.0194,
      "step": 16505
    },
    {
      "epoch": 0.07764094941531746,
      "grad_norm": 2.617995500564575,
      "learning_rate": 0.00018490669231567138,
      "loss": 0.2718,
      "step": 16506
    },
    {
      "epoch": 0.07764565321692993,
      "grad_norm": 0.9850163459777832,
      "learning_rate": 0.0001849057493375579,
      "loss": 0.0646,
      "step": 16507
    },
    {
      "epoch": 0.07765035701854238,
      "grad_norm": 2.4239706993103027,
      "learning_rate": 0.00018490480635944442,
      "loss": 0.3757,
      "step": 16508
    },
    {
      "epoch": 0.07765506082015484,
      "grad_norm": 2.9892032146453857,
      "learning_rate": 0.00018490386338133094,
      "loss": 0.3382,
      "step": 16509
    },
    {
      "epoch": 0.07765976462176731,
      "grad_norm": 0.4046936333179474,
      "learning_rate": 0.00018490292040321745,
      "loss": 0.0344,
      "step": 16510
    },
    {
      "epoch": 0.07766446842337978,
      "grad_norm": 2.8544974327087402,
      "learning_rate": 0.00018490197742510397,
      "loss": 0.3943,
      "step": 16511
    },
    {
      "epoch": 0.07766917222499224,
      "grad_norm": 3.3339340686798096,
      "learning_rate": 0.0001849010344469905,
      "loss": 0.3656,
      "step": 16512
    },
    {
      "epoch": 0.07767387602660471,
      "grad_norm": 1.6268713474273682,
      "learning_rate": 0.000184900091468877,
      "loss": 0.1834,
      "step": 16513
    },
    {
      "epoch": 0.07767857982821716,
      "grad_norm": 4.110107421875,
      "learning_rate": 0.00018489914849076353,
      "loss": 0.6055,
      "step": 16514
    },
    {
      "epoch": 0.07768328362982962,
      "grad_norm": 0.925880491733551,
      "learning_rate": 0.00018489820551265007,
      "loss": 0.1188,
      "step": 16515
    },
    {
      "epoch": 0.07768798743144209,
      "grad_norm": 2.3192150592803955,
      "learning_rate": 0.0001848972625345366,
      "loss": 0.3435,
      "step": 16516
    },
    {
      "epoch": 0.07769269123305456,
      "grad_norm": 1.8968974351882935,
      "learning_rate": 0.0001848963195564231,
      "loss": 0.2064,
      "step": 16517
    },
    {
      "epoch": 0.07769739503466702,
      "grad_norm": 1.6778582334518433,
      "learning_rate": 0.00018489537657830963,
      "loss": 0.1836,
      "step": 16518
    },
    {
      "epoch": 0.07770209883627949,
      "grad_norm": 1.1117229461669922,
      "learning_rate": 0.00018489443360019615,
      "loss": 0.13,
      "step": 16519
    },
    {
      "epoch": 0.07770680263789194,
      "grad_norm": 1.6756986379623413,
      "learning_rate": 0.00018489349062208267,
      "loss": 0.1471,
      "step": 16520
    },
    {
      "epoch": 0.0777115064395044,
      "grad_norm": 2.665733814239502,
      "learning_rate": 0.00018489254764396919,
      "loss": 0.3423,
      "step": 16521
    },
    {
      "epoch": 0.07771621024111687,
      "grad_norm": 1.4343056678771973,
      "learning_rate": 0.0001848916046658557,
      "loss": 0.1775,
      "step": 16522
    },
    {
      "epoch": 0.07772091404272934,
      "grad_norm": 0.2541426420211792,
      "learning_rate": 0.00018489066168774222,
      "loss": 0.0204,
      "step": 16523
    },
    {
      "epoch": 0.0777256178443418,
      "grad_norm": 0.8237268328666687,
      "learning_rate": 0.00018488971870962877,
      "loss": 0.0845,
      "step": 16524
    },
    {
      "epoch": 0.07773032164595425,
      "grad_norm": 1.8789784908294678,
      "learning_rate": 0.0001848887757315153,
      "loss": 0.3124,
      "step": 16525
    },
    {
      "epoch": 0.07773502544756672,
      "grad_norm": 1.5693143606185913,
      "learning_rate": 0.0001848878327534018,
      "loss": 0.1678,
      "step": 16526
    },
    {
      "epoch": 0.07773972924917918,
      "grad_norm": 7.2257795333862305,
      "learning_rate": 0.00018488688977528832,
      "loss": 0.4897,
      "step": 16527
    },
    {
      "epoch": 0.07774443305079165,
      "grad_norm": 3.6195068359375,
      "learning_rate": 0.00018488594679717484,
      "loss": 0.703,
      "step": 16528
    },
    {
      "epoch": 0.07774913685240412,
      "grad_norm": 0.4697229266166687,
      "learning_rate": 0.00018488500381906136,
      "loss": 0.0587,
      "step": 16529
    },
    {
      "epoch": 0.07775384065401658,
      "grad_norm": 0.35762810707092285,
      "learning_rate": 0.00018488406084094788,
      "loss": 0.0524,
      "step": 16530
    },
    {
      "epoch": 0.07775854445562903,
      "grad_norm": 3.0867788791656494,
      "learning_rate": 0.0001848831178628344,
      "loss": 0.4921,
      "step": 16531
    },
    {
      "epoch": 0.0777632482572415,
      "grad_norm": 1.5006392002105713,
      "learning_rate": 0.00018488217488472092,
      "loss": 0.158,
      "step": 16532
    },
    {
      "epoch": 0.07776795205885396,
      "grad_norm": 2.050579309463501,
      "learning_rate": 0.00018488123190660746,
      "loss": 0.1869,
      "step": 16533
    },
    {
      "epoch": 0.07777265586046643,
      "grad_norm": 0.8133442401885986,
      "learning_rate": 0.00018488028892849398,
      "loss": 0.0682,
      "step": 16534
    },
    {
      "epoch": 0.0777773596620789,
      "grad_norm": 1.9246840476989746,
      "learning_rate": 0.0001848793459503805,
      "loss": 0.1965,
      "step": 16535
    },
    {
      "epoch": 0.07778206346369136,
      "grad_norm": 3.037081003189087,
      "learning_rate": 0.00018487840297226702,
      "loss": 0.3531,
      "step": 16536
    },
    {
      "epoch": 0.07778676726530381,
      "grad_norm": 0.25194796919822693,
      "learning_rate": 0.00018487745999415354,
      "loss": 0.0168,
      "step": 16537
    },
    {
      "epoch": 0.07779147106691628,
      "grad_norm": 0.8205241560935974,
      "learning_rate": 0.00018487651701604008,
      "loss": 0.0741,
      "step": 16538
    },
    {
      "epoch": 0.07779617486852874,
      "grad_norm": 2.9082703590393066,
      "learning_rate": 0.0001848755740379266,
      "loss": 0.4211,
      "step": 16539
    },
    {
      "epoch": 0.07780087867014121,
      "grad_norm": 2.192612886428833,
      "learning_rate": 0.00018487463105981312,
      "loss": 0.4303,
      "step": 16540
    },
    {
      "epoch": 0.07780558247175368,
      "grad_norm": 2.580702066421509,
      "learning_rate": 0.0001848736880816996,
      "loss": 0.2527,
      "step": 16541
    },
    {
      "epoch": 0.07781028627336613,
      "grad_norm": 5.264968395233154,
      "learning_rate": 0.00018487274510358616,
      "loss": 0.5224,
      "step": 16542
    },
    {
      "epoch": 0.0778149900749786,
      "grad_norm": 1.9571800231933594,
      "learning_rate": 0.00018487180212547268,
      "loss": 0.1251,
      "step": 16543
    },
    {
      "epoch": 0.07781969387659106,
      "grad_norm": 5.2816162109375,
      "learning_rate": 0.0001848708591473592,
      "loss": 0.8223,
      "step": 16544
    },
    {
      "epoch": 0.07782439767820352,
      "grad_norm": 1.588273525238037,
      "learning_rate": 0.00018486991616924571,
      "loss": 0.1845,
      "step": 16545
    },
    {
      "epoch": 0.07782910147981599,
      "grad_norm": 2.5386903285980225,
      "learning_rate": 0.00018486897319113223,
      "loss": 0.3335,
      "step": 16546
    },
    {
      "epoch": 0.07783380528142846,
      "grad_norm": 0.5869877338409424,
      "learning_rate": 0.00018486803021301878,
      "loss": 0.0452,
      "step": 16547
    },
    {
      "epoch": 0.07783850908304091,
      "grad_norm": 2.529020071029663,
      "learning_rate": 0.0001848670872349053,
      "loss": 0.5308,
      "step": 16548
    },
    {
      "epoch": 0.07784321288465337,
      "grad_norm": 0.27595043182373047,
      "learning_rate": 0.00018486614425679182,
      "loss": 0.0219,
      "step": 16549
    },
    {
      "epoch": 0.07784791668626584,
      "grad_norm": 2.269324779510498,
      "learning_rate": 0.00018486520127867834,
      "loss": 0.3832,
      "step": 16550
    },
    {
      "epoch": 0.0778526204878783,
      "grad_norm": 2.698150873184204,
      "learning_rate": 0.00018486425830056485,
      "loss": 0.51,
      "step": 16551
    },
    {
      "epoch": 0.07785732428949077,
      "grad_norm": 3.4017882347106934,
      "learning_rate": 0.00018486331532245137,
      "loss": 0.2148,
      "step": 16552
    },
    {
      "epoch": 0.07786202809110324,
      "grad_norm": 0.9291208982467651,
      "learning_rate": 0.0001848623723443379,
      "loss": 0.0486,
      "step": 16553
    },
    {
      "epoch": 0.07786673189271569,
      "grad_norm": 0.9720607399940491,
      "learning_rate": 0.0001848614293662244,
      "loss": 0.0619,
      "step": 16554
    },
    {
      "epoch": 0.07787143569432815,
      "grad_norm": 1.9059089422225952,
      "learning_rate": 0.00018486048638811093,
      "loss": 0.2608,
      "step": 16555
    },
    {
      "epoch": 0.07787613949594062,
      "grad_norm": 3.4813241958618164,
      "learning_rate": 0.00018485954340999747,
      "loss": 0.0735,
      "step": 16556
    },
    {
      "epoch": 0.07788084329755308,
      "grad_norm": 2.839216709136963,
      "learning_rate": 0.000184858600431884,
      "loss": 0.4309,
      "step": 16557
    },
    {
      "epoch": 0.07788554709916555,
      "grad_norm": 0.8451497554779053,
      "learning_rate": 0.0001848576574537705,
      "loss": 0.0995,
      "step": 16558
    },
    {
      "epoch": 0.077890250900778,
      "grad_norm": 6.252568244934082,
      "learning_rate": 0.00018485671447565703,
      "loss": 0.3408,
      "step": 16559
    },
    {
      "epoch": 0.07789495470239047,
      "grad_norm": 2.089200258255005,
      "learning_rate": 0.00018485577149754355,
      "loss": 0.1846,
      "step": 16560
    },
    {
      "epoch": 0.07789965850400293,
      "grad_norm": 2.783092975616455,
      "learning_rate": 0.00018485482851943007,
      "loss": 0.2746,
      "step": 16561
    },
    {
      "epoch": 0.0779043623056154,
      "grad_norm": 0.7385149598121643,
      "learning_rate": 0.00018485388554131659,
      "loss": 0.0597,
      "step": 16562
    },
    {
      "epoch": 0.07790906610722786,
      "grad_norm": 7.310883045196533,
      "learning_rate": 0.0001848529425632031,
      "loss": 0.6914,
      "step": 16563
    },
    {
      "epoch": 0.07791376990884033,
      "grad_norm": 3.759402275085449,
      "learning_rate": 0.00018485199958508962,
      "loss": 0.2115,
      "step": 16564
    },
    {
      "epoch": 0.07791847371045278,
      "grad_norm": 2.0411536693573,
      "learning_rate": 0.00018485105660697617,
      "loss": 0.2085,
      "step": 16565
    },
    {
      "epoch": 0.07792317751206525,
      "grad_norm": 1.2180248498916626,
      "learning_rate": 0.0001848501136288627,
      "loss": 0.0854,
      "step": 16566
    },
    {
      "epoch": 0.07792788131367771,
      "grad_norm": 0.9573163986206055,
      "learning_rate": 0.0001848491706507492,
      "loss": 0.077,
      "step": 16567
    },
    {
      "epoch": 0.07793258511529018,
      "grad_norm": 2.1913211345672607,
      "learning_rate": 0.00018484822767263572,
      "loss": 0.2098,
      "step": 16568
    },
    {
      "epoch": 0.07793728891690264,
      "grad_norm": 3.2697103023529053,
      "learning_rate": 0.00018484728469452224,
      "loss": 0.5242,
      "step": 16569
    },
    {
      "epoch": 0.07794199271851511,
      "grad_norm": 1.5901857614517212,
      "learning_rate": 0.0001848463417164088,
      "loss": 0.1994,
      "step": 16570
    },
    {
      "epoch": 0.07794669652012756,
      "grad_norm": 1.095307469367981,
      "learning_rate": 0.0001848453987382953,
      "loss": 0.1506,
      "step": 16571
    },
    {
      "epoch": 0.07795140032174003,
      "grad_norm": 3.145876169204712,
      "learning_rate": 0.0001848444557601818,
      "loss": 0.4531,
      "step": 16572
    },
    {
      "epoch": 0.0779561041233525,
      "grad_norm": 0.4897612929344177,
      "learning_rate": 0.00018484351278206832,
      "loss": 0.0373,
      "step": 16573
    },
    {
      "epoch": 0.07796080792496496,
      "grad_norm": 1.3155564069747925,
      "learning_rate": 0.00018484256980395486,
      "loss": 0.1085,
      "step": 16574
    },
    {
      "epoch": 0.07796551172657742,
      "grad_norm": 2.377378463745117,
      "learning_rate": 0.00018484162682584138,
      "loss": 0.1031,
      "step": 16575
    },
    {
      "epoch": 0.07797021552818988,
      "grad_norm": 0.1046500951051712,
      "learning_rate": 0.0001848406838477279,
      "loss": 0.0067,
      "step": 16576
    },
    {
      "epoch": 0.07797491932980234,
      "grad_norm": 2.031527042388916,
      "learning_rate": 0.00018483974086961442,
      "loss": 0.2023,
      "step": 16577
    },
    {
      "epoch": 0.07797962313141481,
      "grad_norm": 2.8179261684417725,
      "learning_rate": 0.00018483879789150094,
      "loss": 0.5583,
      "step": 16578
    },
    {
      "epoch": 0.07798432693302727,
      "grad_norm": 2.101726531982422,
      "learning_rate": 0.00018483785491338748,
      "loss": 0.2071,
      "step": 16579
    },
    {
      "epoch": 0.07798903073463974,
      "grad_norm": 1.972521185874939,
      "learning_rate": 0.000184836911935274,
      "loss": 0.2194,
      "step": 16580
    },
    {
      "epoch": 0.0779937345362522,
      "grad_norm": 2.138460159301758,
      "learning_rate": 0.00018483596895716052,
      "loss": 0.3096,
      "step": 16581
    },
    {
      "epoch": 0.07799843833786466,
      "grad_norm": 3.258164405822754,
      "learning_rate": 0.00018483502597904704,
      "loss": 0.3563,
      "step": 16582
    },
    {
      "epoch": 0.07800314213947712,
      "grad_norm": 1.4825111627578735,
      "learning_rate": 0.00018483408300093356,
      "loss": 0.1893,
      "step": 16583
    },
    {
      "epoch": 0.07800784594108959,
      "grad_norm": 2.722817897796631,
      "learning_rate": 0.00018483314002282008,
      "loss": 0.6779,
      "step": 16584
    },
    {
      "epoch": 0.07801254974270205,
      "grad_norm": 1.5743268728256226,
      "learning_rate": 0.0001848321970447066,
      "loss": 0.1725,
      "step": 16585
    },
    {
      "epoch": 0.07801725354431452,
      "grad_norm": 1.4787378311157227,
      "learning_rate": 0.00018483125406659311,
      "loss": 0.1623,
      "step": 16586
    },
    {
      "epoch": 0.07802195734592698,
      "grad_norm": 2.155728340148926,
      "learning_rate": 0.00018483031108847963,
      "loss": 0.3255,
      "step": 16587
    },
    {
      "epoch": 0.07802666114753944,
      "grad_norm": 0.7540409564971924,
      "learning_rate": 0.00018482936811036618,
      "loss": 0.0717,
      "step": 16588
    },
    {
      "epoch": 0.0780313649491519,
      "grad_norm": 0.5938631296157837,
      "learning_rate": 0.0001848284251322527,
      "loss": 0.0786,
      "step": 16589
    },
    {
      "epoch": 0.07803606875076437,
      "grad_norm": 1.5072604417800903,
      "learning_rate": 0.00018482748215413922,
      "loss": 0.3398,
      "step": 16590
    },
    {
      "epoch": 0.07804077255237683,
      "grad_norm": 4.543464183807373,
      "learning_rate": 0.00018482653917602573,
      "loss": 0.3645,
      "step": 16591
    },
    {
      "epoch": 0.0780454763539893,
      "grad_norm": 3.3734352588653564,
      "learning_rate": 0.00018482559619791225,
      "loss": 0.6543,
      "step": 16592
    },
    {
      "epoch": 0.07805018015560175,
      "grad_norm": 2.601938009262085,
      "learning_rate": 0.00018482465321979877,
      "loss": 0.7901,
      "step": 16593
    },
    {
      "epoch": 0.07805488395721422,
      "grad_norm": 1.2336475849151611,
      "learning_rate": 0.0001848237102416853,
      "loss": 0.1012,
      "step": 16594
    },
    {
      "epoch": 0.07805958775882668,
      "grad_norm": 2.4317381381988525,
      "learning_rate": 0.0001848227672635718,
      "loss": 0.3928,
      "step": 16595
    },
    {
      "epoch": 0.07806429156043915,
      "grad_norm": 2.8332409858703613,
      "learning_rate": 0.00018482182428545833,
      "loss": 0.3976,
      "step": 16596
    },
    {
      "epoch": 0.07806899536205161,
      "grad_norm": 0.8746253848075867,
      "learning_rate": 0.00018482088130734487,
      "loss": 0.0836,
      "step": 16597
    },
    {
      "epoch": 0.07807369916366408,
      "grad_norm": 1.290971040725708,
      "learning_rate": 0.0001848199383292314,
      "loss": 0.3032,
      "step": 16598
    },
    {
      "epoch": 0.07807840296527653,
      "grad_norm": 1.2222098112106323,
      "learning_rate": 0.0001848189953511179,
      "loss": 0.2496,
      "step": 16599
    },
    {
      "epoch": 0.078083106766889,
      "grad_norm": 1.387001395225525,
      "learning_rate": 0.00018481805237300443,
      "loss": 0.2574,
      "step": 16600
    },
    {
      "epoch": 0.07808781056850146,
      "grad_norm": 2.1127071380615234,
      "learning_rate": 0.00018481710939489098,
      "loss": 0.2349,
      "step": 16601
    },
    {
      "epoch": 0.07809251437011393,
      "grad_norm": 3.2318155765533447,
      "learning_rate": 0.0001848161664167775,
      "loss": 0.1657,
      "step": 16602
    },
    {
      "epoch": 0.07809721817172639,
      "grad_norm": 1.824247121810913,
      "learning_rate": 0.00018481522343866399,
      "loss": 0.1775,
      "step": 16603
    },
    {
      "epoch": 0.07810192197333886,
      "grad_norm": 2.431647539138794,
      "learning_rate": 0.0001848142804605505,
      "loss": 0.3152,
      "step": 16604
    },
    {
      "epoch": 0.07810662577495131,
      "grad_norm": 2.59200382232666,
      "learning_rate": 0.00018481333748243702,
      "loss": 0.3096,
      "step": 16605
    },
    {
      "epoch": 0.07811132957656378,
      "grad_norm": 0.9932643175125122,
      "learning_rate": 0.00018481239450432357,
      "loss": 0.092,
      "step": 16606
    },
    {
      "epoch": 0.07811603337817624,
      "grad_norm": 1.9231235980987549,
      "learning_rate": 0.0001848114515262101,
      "loss": 0.3274,
      "step": 16607
    },
    {
      "epoch": 0.07812073717978871,
      "grad_norm": 2.090637683868408,
      "learning_rate": 0.0001848105085480966,
      "loss": 0.3709,
      "step": 16608
    },
    {
      "epoch": 0.07812544098140117,
      "grad_norm": 2.9463510513305664,
      "learning_rate": 0.00018480956556998312,
      "loss": 0.4236,
      "step": 16609
    },
    {
      "epoch": 0.07813014478301362,
      "grad_norm": 3.409106731414795,
      "learning_rate": 0.00018480862259186967,
      "loss": 0.4631,
      "step": 16610
    },
    {
      "epoch": 0.07813484858462609,
      "grad_norm": 0.6030338406562805,
      "learning_rate": 0.0001848076796137562,
      "loss": 0.0532,
      "step": 16611
    },
    {
      "epoch": 0.07813955238623856,
      "grad_norm": 2.0425477027893066,
      "learning_rate": 0.0001848067366356427,
      "loss": 0.3808,
      "step": 16612
    },
    {
      "epoch": 0.07814425618785102,
      "grad_norm": 2.4080560207366943,
      "learning_rate": 0.00018480579365752923,
      "loss": 0.3338,
      "step": 16613
    },
    {
      "epoch": 0.07814895998946349,
      "grad_norm": 3.136202812194824,
      "learning_rate": 0.00018480485067941572,
      "loss": 0.5886,
      "step": 16614
    },
    {
      "epoch": 0.07815366379107595,
      "grad_norm": 1.502638816833496,
      "learning_rate": 0.00018480390770130226,
      "loss": 0.1431,
      "step": 16615
    },
    {
      "epoch": 0.0781583675926884,
      "grad_norm": 2.9117982387542725,
      "learning_rate": 0.00018480296472318878,
      "loss": 0.4231,
      "step": 16616
    },
    {
      "epoch": 0.07816307139430087,
      "grad_norm": 0.5750096440315247,
      "learning_rate": 0.0001848020217450753,
      "loss": 0.0639,
      "step": 16617
    },
    {
      "epoch": 0.07816777519591334,
      "grad_norm": 1.5612657070159912,
      "learning_rate": 0.00018480107876696182,
      "loss": 0.2468,
      "step": 16618
    },
    {
      "epoch": 0.0781724789975258,
      "grad_norm": 3.2284820079803467,
      "learning_rate": 0.00018480013578884834,
      "loss": 0.3898,
      "step": 16619
    },
    {
      "epoch": 0.07817718279913827,
      "grad_norm": 0.6253347396850586,
      "learning_rate": 0.00018479919281073488,
      "loss": 0.056,
      "step": 16620
    },
    {
      "epoch": 0.07818188660075073,
      "grad_norm": 2.604764938354492,
      "learning_rate": 0.0001847982498326214,
      "loss": 0.3114,
      "step": 16621
    },
    {
      "epoch": 0.07818659040236318,
      "grad_norm": 2.903089761734009,
      "learning_rate": 0.00018479730685450792,
      "loss": 0.324,
      "step": 16622
    },
    {
      "epoch": 0.07819129420397565,
      "grad_norm": 1.5281708240509033,
      "learning_rate": 0.00018479636387639444,
      "loss": 0.1019,
      "step": 16623
    },
    {
      "epoch": 0.07819599800558812,
      "grad_norm": 1.4917635917663574,
      "learning_rate": 0.00018479542089828096,
      "loss": 0.1874,
      "step": 16624
    },
    {
      "epoch": 0.07820070180720058,
      "grad_norm": 0.8798100352287292,
      "learning_rate": 0.00018479447792016748,
      "loss": 0.1079,
      "step": 16625
    },
    {
      "epoch": 0.07820540560881305,
      "grad_norm": 2.305048704147339,
      "learning_rate": 0.000184793534942054,
      "loss": 0.2901,
      "step": 16626
    },
    {
      "epoch": 0.0782101094104255,
      "grad_norm": 2.179456949234009,
      "learning_rate": 0.00018479259196394051,
      "loss": 0.2,
      "step": 16627
    },
    {
      "epoch": 0.07821481321203796,
      "grad_norm": 1.927922010421753,
      "learning_rate": 0.00018479164898582703,
      "loss": 0.2678,
      "step": 16628
    },
    {
      "epoch": 0.07821951701365043,
      "grad_norm": 3.234410524368286,
      "learning_rate": 0.00018479070600771358,
      "loss": 0.5024,
      "step": 16629
    },
    {
      "epoch": 0.0782242208152629,
      "grad_norm": 0.6372267603874207,
      "learning_rate": 0.0001847897630296001,
      "loss": 0.0517,
      "step": 16630
    },
    {
      "epoch": 0.07822892461687536,
      "grad_norm": 0.5303030610084534,
      "learning_rate": 0.00018478882005148662,
      "loss": 0.0547,
      "step": 16631
    },
    {
      "epoch": 0.07823362841848783,
      "grad_norm": 0.98411625623703,
      "learning_rate": 0.00018478787707337313,
      "loss": 0.1327,
      "step": 16632
    },
    {
      "epoch": 0.07823833222010028,
      "grad_norm": 2.844317674636841,
      "learning_rate": 0.00018478693409525968,
      "loss": 0.3366,
      "step": 16633
    },
    {
      "epoch": 0.07824303602171274,
      "grad_norm": 2.2524468898773193,
      "learning_rate": 0.00018478599111714617,
      "loss": 0.3123,
      "step": 16634
    },
    {
      "epoch": 0.07824773982332521,
      "grad_norm": 0.903968334197998,
      "learning_rate": 0.0001847850481390327,
      "loss": 0.0757,
      "step": 16635
    },
    {
      "epoch": 0.07825244362493768,
      "grad_norm": 1.4287419319152832,
      "learning_rate": 0.0001847841051609192,
      "loss": 0.239,
      "step": 16636
    },
    {
      "epoch": 0.07825714742655014,
      "grad_norm": 0.26384806632995605,
      "learning_rate": 0.00018478316218280573,
      "loss": 0.021,
      "step": 16637
    },
    {
      "epoch": 0.07826185122816261,
      "grad_norm": 0.5168705582618713,
      "learning_rate": 0.00018478221920469227,
      "loss": 0.0375,
      "step": 16638
    },
    {
      "epoch": 0.07826655502977506,
      "grad_norm": 3.257905960083008,
      "learning_rate": 0.0001847812762265788,
      "loss": 0.8238,
      "step": 16639
    },
    {
      "epoch": 0.07827125883138752,
      "grad_norm": 1.9550669193267822,
      "learning_rate": 0.0001847803332484653,
      "loss": 0.4549,
      "step": 16640
    },
    {
      "epoch": 0.07827596263299999,
      "grad_norm": 0.4769198000431061,
      "learning_rate": 0.00018477939027035183,
      "loss": 0.0341,
      "step": 16641
    },
    {
      "epoch": 0.07828066643461246,
      "grad_norm": 0.822872519493103,
      "learning_rate": 0.00018477844729223838,
      "loss": 0.0657,
      "step": 16642
    },
    {
      "epoch": 0.07828537023622492,
      "grad_norm": 1.6514673233032227,
      "learning_rate": 0.0001847775043141249,
      "loss": 0.2219,
      "step": 16643
    },
    {
      "epoch": 0.07829007403783737,
      "grad_norm": 2.380223512649536,
      "learning_rate": 0.0001847765613360114,
      "loss": 0.3176,
      "step": 16644
    },
    {
      "epoch": 0.07829477783944984,
      "grad_norm": 4.205966949462891,
      "learning_rate": 0.0001847756183578979,
      "loss": 0.889,
      "step": 16645
    },
    {
      "epoch": 0.0782994816410623,
      "grad_norm": 5.96252965927124,
      "learning_rate": 0.00018477467537978442,
      "loss": 0.5961,
      "step": 16646
    },
    {
      "epoch": 0.07830418544267477,
      "grad_norm": 2.4818243980407715,
      "learning_rate": 0.00018477373240167097,
      "loss": 0.3417,
      "step": 16647
    },
    {
      "epoch": 0.07830888924428724,
      "grad_norm": 0.8849684000015259,
      "learning_rate": 0.0001847727894235575,
      "loss": 0.1036,
      "step": 16648
    },
    {
      "epoch": 0.0783135930458997,
      "grad_norm": 1.9815452098846436,
      "learning_rate": 0.000184771846445444,
      "loss": 0.2026,
      "step": 16649
    },
    {
      "epoch": 0.07831829684751215,
      "grad_norm": 0.4757303297519684,
      "learning_rate": 0.00018477090346733052,
      "loss": 0.079,
      "step": 16650
    },
    {
      "epoch": 0.07832300064912462,
      "grad_norm": 2.6182861328125,
      "learning_rate": 0.00018476996048921707,
      "loss": 0.2307,
      "step": 16651
    },
    {
      "epoch": 0.07832770445073708,
      "grad_norm": 2.0800158977508545,
      "learning_rate": 0.0001847690175111036,
      "loss": 0.0918,
      "step": 16652
    },
    {
      "epoch": 0.07833240825234955,
      "grad_norm": 3.7735211849212646,
      "learning_rate": 0.0001847680745329901,
      "loss": 0.8081,
      "step": 16653
    },
    {
      "epoch": 0.07833711205396202,
      "grad_norm": 0.09792827814817429,
      "learning_rate": 0.00018476713155487663,
      "loss": 0.0071,
      "step": 16654
    },
    {
      "epoch": 0.07834181585557448,
      "grad_norm": 1.5796971321105957,
      "learning_rate": 0.00018476618857676315,
      "loss": 0.2863,
      "step": 16655
    },
    {
      "epoch": 0.07834651965718693,
      "grad_norm": 1.8195037841796875,
      "learning_rate": 0.00018476524559864966,
      "loss": 0.128,
      "step": 16656
    },
    {
      "epoch": 0.0783512234587994,
      "grad_norm": 5.84525728225708,
      "learning_rate": 0.00018476430262053618,
      "loss": 0.5285,
      "step": 16657
    },
    {
      "epoch": 0.07835592726041186,
      "grad_norm": 1.5463534593582153,
      "learning_rate": 0.0001847633596424227,
      "loss": 0.0821,
      "step": 16658
    },
    {
      "epoch": 0.07836063106202433,
      "grad_norm": 1.5999919176101685,
      "learning_rate": 0.00018476241666430922,
      "loss": 0.161,
      "step": 16659
    },
    {
      "epoch": 0.0783653348636368,
      "grad_norm": 3.983551025390625,
      "learning_rate": 0.00018476147368619577,
      "loss": 0.4057,
      "step": 16660
    },
    {
      "epoch": 0.07837003866524925,
      "grad_norm": 4.422049045562744,
      "learning_rate": 0.00018476053070808228,
      "loss": 0.8806,
      "step": 16661
    },
    {
      "epoch": 0.07837474246686171,
      "grad_norm": 0.9753056764602661,
      "learning_rate": 0.0001847595877299688,
      "loss": 0.113,
      "step": 16662
    },
    {
      "epoch": 0.07837944626847418,
      "grad_norm": 3.033702850341797,
      "learning_rate": 0.00018475864475185532,
      "loss": 0.1923,
      "step": 16663
    },
    {
      "epoch": 0.07838415007008664,
      "grad_norm": 1.359723687171936,
      "learning_rate": 0.00018475770177374184,
      "loss": 0.1461,
      "step": 16664
    },
    {
      "epoch": 0.07838885387169911,
      "grad_norm": 0.5795301198959351,
      "learning_rate": 0.00018475675879562836,
      "loss": 0.0404,
      "step": 16665
    },
    {
      "epoch": 0.07839355767331158,
      "grad_norm": 2.896117925643921,
      "learning_rate": 0.00018475581581751488,
      "loss": 0.3959,
      "step": 16666
    },
    {
      "epoch": 0.07839826147492403,
      "grad_norm": 1.9799716472625732,
      "learning_rate": 0.0001847548728394014,
      "loss": 0.1634,
      "step": 16667
    },
    {
      "epoch": 0.0784029652765365,
      "grad_norm": 3.2523722648620605,
      "learning_rate": 0.00018475392986128791,
      "loss": 0.46,
      "step": 16668
    },
    {
      "epoch": 0.07840766907814896,
      "grad_norm": 1.9273884296417236,
      "learning_rate": 0.00018475298688317446,
      "loss": 0.2899,
      "step": 16669
    },
    {
      "epoch": 0.07841237287976142,
      "grad_norm": 1.7382512092590332,
      "learning_rate": 0.00018475204390506098,
      "loss": 0.1584,
      "step": 16670
    },
    {
      "epoch": 0.07841707668137389,
      "grad_norm": 2.1483545303344727,
      "learning_rate": 0.0001847511009269475,
      "loss": 0.3564,
      "step": 16671
    },
    {
      "epoch": 0.07842178048298636,
      "grad_norm": 1.7922766208648682,
      "learning_rate": 0.00018475015794883402,
      "loss": 0.2981,
      "step": 16672
    },
    {
      "epoch": 0.07842648428459881,
      "grad_norm": 2.4070496559143066,
      "learning_rate": 0.00018474921497072053,
      "loss": 0.3516,
      "step": 16673
    },
    {
      "epoch": 0.07843118808621127,
      "grad_norm": 0.43069395422935486,
      "learning_rate": 0.00018474827199260708,
      "loss": 0.0384,
      "step": 16674
    },
    {
      "epoch": 0.07843589188782374,
      "grad_norm": 1.9952301979064941,
      "learning_rate": 0.0001847473290144936,
      "loss": 0.0723,
      "step": 16675
    },
    {
      "epoch": 0.0784405956894362,
      "grad_norm": 3.5429160594940186,
      "learning_rate": 0.0001847463860363801,
      "loss": 0.3386,
      "step": 16676
    },
    {
      "epoch": 0.07844529949104867,
      "grad_norm": 3.8523576259613037,
      "learning_rate": 0.0001847454430582666,
      "loss": 0.8455,
      "step": 16677
    },
    {
      "epoch": 0.07845000329266112,
      "grad_norm": 2.4104409217834473,
      "learning_rate": 0.00018474450008015313,
      "loss": 0.4978,
      "step": 16678
    },
    {
      "epoch": 0.07845470709427359,
      "grad_norm": 2.22314715385437,
      "learning_rate": 0.00018474355710203967,
      "loss": 0.4892,
      "step": 16679
    },
    {
      "epoch": 0.07845941089588605,
      "grad_norm": 0.8885960578918457,
      "learning_rate": 0.0001847426141239262,
      "loss": 0.146,
      "step": 16680
    },
    {
      "epoch": 0.07846411469749852,
      "grad_norm": 2.8903558254241943,
      "learning_rate": 0.0001847416711458127,
      "loss": 0.3217,
      "step": 16681
    },
    {
      "epoch": 0.07846881849911098,
      "grad_norm": 1.8393640518188477,
      "learning_rate": 0.00018474072816769923,
      "loss": 0.2976,
      "step": 16682
    },
    {
      "epoch": 0.07847352230072345,
      "grad_norm": 1.6300286054611206,
      "learning_rate": 0.00018473978518958578,
      "loss": 0.2423,
      "step": 16683
    },
    {
      "epoch": 0.0784782261023359,
      "grad_norm": 0.5897801518440247,
      "learning_rate": 0.0001847388422114723,
      "loss": 0.0644,
      "step": 16684
    },
    {
      "epoch": 0.07848292990394837,
      "grad_norm": 1.5717053413391113,
      "learning_rate": 0.0001847378992333588,
      "loss": 0.3639,
      "step": 16685
    },
    {
      "epoch": 0.07848763370556083,
      "grad_norm": 0.615104615688324,
      "learning_rate": 0.00018473695625524533,
      "loss": 0.0731,
      "step": 16686
    },
    {
      "epoch": 0.0784923375071733,
      "grad_norm": 1.3820240497589111,
      "learning_rate": 0.00018473601327713182,
      "loss": 0.2646,
      "step": 16687
    },
    {
      "epoch": 0.07849704130878576,
      "grad_norm": 0.5642204284667969,
      "learning_rate": 0.00018473507029901837,
      "loss": 0.0768,
      "step": 16688
    },
    {
      "epoch": 0.07850174511039823,
      "grad_norm": 10.308915138244629,
      "learning_rate": 0.0001847341273209049,
      "loss": 0.7099,
      "step": 16689
    },
    {
      "epoch": 0.07850644891201068,
      "grad_norm": 0.9952518343925476,
      "learning_rate": 0.0001847331843427914,
      "loss": 0.1732,
      "step": 16690
    },
    {
      "epoch": 0.07851115271362315,
      "grad_norm": 2.8476457595825195,
      "learning_rate": 0.00018473224136467792,
      "loss": 0.2542,
      "step": 16691
    },
    {
      "epoch": 0.07851585651523561,
      "grad_norm": 3.5622105598449707,
      "learning_rate": 0.00018473129838656447,
      "loss": 0.4284,
      "step": 16692
    },
    {
      "epoch": 0.07852056031684808,
      "grad_norm": 0.7160733342170715,
      "learning_rate": 0.000184730355408451,
      "loss": 0.1176,
      "step": 16693
    },
    {
      "epoch": 0.07852526411846054,
      "grad_norm": 0.4533717632293701,
      "learning_rate": 0.0001847294124303375,
      "loss": 0.0517,
      "step": 16694
    },
    {
      "epoch": 0.078529967920073,
      "grad_norm": 2.123183012008667,
      "learning_rate": 0.00018472846945222403,
      "loss": 0.4332,
      "step": 16695
    },
    {
      "epoch": 0.07853467172168546,
      "grad_norm": 1.5998069047927856,
      "learning_rate": 0.00018472752647411055,
      "loss": 0.1594,
      "step": 16696
    },
    {
      "epoch": 0.07853937552329793,
      "grad_norm": 1.947072982788086,
      "learning_rate": 0.00018472658349599706,
      "loss": 0.2565,
      "step": 16697
    },
    {
      "epoch": 0.07854407932491039,
      "grad_norm": 0.44827550649642944,
      "learning_rate": 0.00018472564051788358,
      "loss": 0.0626,
      "step": 16698
    },
    {
      "epoch": 0.07854878312652286,
      "grad_norm": 2.058833122253418,
      "learning_rate": 0.0001847246975397701,
      "loss": 0.3457,
      "step": 16699
    },
    {
      "epoch": 0.07855348692813532,
      "grad_norm": 3.9696130752563477,
      "learning_rate": 0.00018472375456165662,
      "loss": 0.674,
      "step": 16700
    },
    {
      "epoch": 0.07855819072974778,
      "grad_norm": 3.455475091934204,
      "learning_rate": 0.00018472281158354317,
      "loss": 0.4577,
      "step": 16701
    },
    {
      "epoch": 0.07856289453136024,
      "grad_norm": 1.773624062538147,
      "learning_rate": 0.00018472186860542968,
      "loss": 0.1488,
      "step": 16702
    },
    {
      "epoch": 0.07856759833297271,
      "grad_norm": 1.1719762086868286,
      "learning_rate": 0.0001847209256273162,
      "loss": 0.0804,
      "step": 16703
    },
    {
      "epoch": 0.07857230213458517,
      "grad_norm": 2.4381725788116455,
      "learning_rate": 0.00018471998264920272,
      "loss": 0.3184,
      "step": 16704
    },
    {
      "epoch": 0.07857700593619764,
      "grad_norm": 3.251220226287842,
      "learning_rate": 0.00018471903967108924,
      "loss": 0.27,
      "step": 16705
    },
    {
      "epoch": 0.0785817097378101,
      "grad_norm": 0.2094847559928894,
      "learning_rate": 0.00018471809669297579,
      "loss": 0.0221,
      "step": 16706
    },
    {
      "epoch": 0.07858641353942256,
      "grad_norm": 0.9484827518463135,
      "learning_rate": 0.00018471715371486228,
      "loss": 0.0837,
      "step": 16707
    },
    {
      "epoch": 0.07859111734103502,
      "grad_norm": 1.3435304164886475,
      "learning_rate": 0.0001847162107367488,
      "loss": 0.141,
      "step": 16708
    },
    {
      "epoch": 0.07859582114264749,
      "grad_norm": 3.0368709564208984,
      "learning_rate": 0.00018471526775863531,
      "loss": 0.1573,
      "step": 16709
    },
    {
      "epoch": 0.07860052494425995,
      "grad_norm": 4.91094970703125,
      "learning_rate": 0.00018471432478052186,
      "loss": 1.1664,
      "step": 16710
    },
    {
      "epoch": 0.07860522874587242,
      "grad_norm": 4.435929775238037,
      "learning_rate": 0.00018471338180240838,
      "loss": 0.6358,
      "step": 16711
    },
    {
      "epoch": 0.07860993254748487,
      "grad_norm": 1.79281747341156,
      "learning_rate": 0.0001847124388242949,
      "loss": 0.1731,
      "step": 16712
    },
    {
      "epoch": 0.07861463634909734,
      "grad_norm": 2.760978937149048,
      "learning_rate": 0.00018471149584618142,
      "loss": 0.6732,
      "step": 16713
    },
    {
      "epoch": 0.0786193401507098,
      "grad_norm": 0.5408245325088501,
      "learning_rate": 0.00018471055286806793,
      "loss": 0.0534,
      "step": 16714
    },
    {
      "epoch": 0.07862404395232227,
      "grad_norm": 1.7806024551391602,
      "learning_rate": 0.00018470960988995448,
      "loss": 0.1787,
      "step": 16715
    },
    {
      "epoch": 0.07862874775393473,
      "grad_norm": 2.284532308578491,
      "learning_rate": 0.000184708666911841,
      "loss": 0.32,
      "step": 16716
    },
    {
      "epoch": 0.0786334515555472,
      "grad_norm": 3.9915990829467773,
      "learning_rate": 0.00018470772393372752,
      "loss": 0.7728,
      "step": 16717
    },
    {
      "epoch": 0.07863815535715965,
      "grad_norm": 4.385408878326416,
      "learning_rate": 0.000184706780955614,
      "loss": 0.8871,
      "step": 16718
    },
    {
      "epoch": 0.07864285915877212,
      "grad_norm": 2.471045732498169,
      "learning_rate": 0.00018470583797750056,
      "loss": 0.3394,
      "step": 16719
    },
    {
      "epoch": 0.07864756296038458,
      "grad_norm": 1.6640739440917969,
      "learning_rate": 0.00018470489499938707,
      "loss": 0.1857,
      "step": 16720
    },
    {
      "epoch": 0.07865226676199705,
      "grad_norm": 2.0539441108703613,
      "learning_rate": 0.0001847039520212736,
      "loss": 0.253,
      "step": 16721
    },
    {
      "epoch": 0.07865697056360951,
      "grad_norm": 5.5112528800964355,
      "learning_rate": 0.0001847030090431601,
      "loss": 0.8132,
      "step": 16722
    },
    {
      "epoch": 0.07866167436522198,
      "grad_norm": 2.6357505321502686,
      "learning_rate": 0.00018470206606504663,
      "loss": 0.2333,
      "step": 16723
    },
    {
      "epoch": 0.07866637816683443,
      "grad_norm": 2.5177407264709473,
      "learning_rate": 0.00018470112308693318,
      "loss": 0.4428,
      "step": 16724
    },
    {
      "epoch": 0.0786710819684469,
      "grad_norm": 7.028380393981934,
      "learning_rate": 0.0001847001801088197,
      "loss": 0.3504,
      "step": 16725
    },
    {
      "epoch": 0.07867578577005936,
      "grad_norm": 1.6990816593170166,
      "learning_rate": 0.0001846992371307062,
      "loss": 0.2138,
      "step": 16726
    },
    {
      "epoch": 0.07868048957167183,
      "grad_norm": 2.8241260051727295,
      "learning_rate": 0.00018469829415259273,
      "loss": 0.5489,
      "step": 16727
    },
    {
      "epoch": 0.07868519337328429,
      "grad_norm": 0.7386712431907654,
      "learning_rate": 0.00018469735117447925,
      "loss": 0.1011,
      "step": 16728
    },
    {
      "epoch": 0.07868989717489674,
      "grad_norm": 0.9696153402328491,
      "learning_rate": 0.00018469640819636577,
      "loss": 0.1067,
      "step": 16729
    },
    {
      "epoch": 0.07869460097650921,
      "grad_norm": 0.8815981149673462,
      "learning_rate": 0.0001846954652182523,
      "loss": 0.1079,
      "step": 16730
    },
    {
      "epoch": 0.07869930477812168,
      "grad_norm": 0.7312243580818176,
      "learning_rate": 0.0001846945222401388,
      "loss": 0.0827,
      "step": 16731
    },
    {
      "epoch": 0.07870400857973414,
      "grad_norm": 2.534810781478882,
      "learning_rate": 0.00018469357926202532,
      "loss": 0.2821,
      "step": 16732
    },
    {
      "epoch": 0.07870871238134661,
      "grad_norm": 1.8483697175979614,
      "learning_rate": 0.00018469263628391187,
      "loss": 0.1778,
      "step": 16733
    },
    {
      "epoch": 0.07871341618295907,
      "grad_norm": 2.445281982421875,
      "learning_rate": 0.0001846916933057984,
      "loss": 0.4522,
      "step": 16734
    },
    {
      "epoch": 0.07871811998457152,
      "grad_norm": 0.37338075041770935,
      "learning_rate": 0.0001846907503276849,
      "loss": 0.0268,
      "step": 16735
    },
    {
      "epoch": 0.07872282378618399,
      "grad_norm": 4.450231075286865,
      "learning_rate": 0.00018468980734957143,
      "loss": 0.5881,
      "step": 16736
    },
    {
      "epoch": 0.07872752758779646,
      "grad_norm": 0.34062108397483826,
      "learning_rate": 0.00018468886437145795,
      "loss": 0.0595,
      "step": 16737
    },
    {
      "epoch": 0.07873223138940892,
      "grad_norm": 0.5397908687591553,
      "learning_rate": 0.00018468792139334446,
      "loss": 0.0326,
      "step": 16738
    },
    {
      "epoch": 0.07873693519102139,
      "grad_norm": 1.0820690393447876,
      "learning_rate": 0.00018468697841523098,
      "loss": 0.119,
      "step": 16739
    },
    {
      "epoch": 0.07874163899263385,
      "grad_norm": 1.6250958442687988,
      "learning_rate": 0.0001846860354371175,
      "loss": 0.1761,
      "step": 16740
    },
    {
      "epoch": 0.0787463427942463,
      "grad_norm": 0.4893614947795868,
      "learning_rate": 0.00018468509245900402,
      "loss": 0.0399,
      "step": 16741
    },
    {
      "epoch": 0.07875104659585877,
      "grad_norm": 1.9714828729629517,
      "learning_rate": 0.00018468414948089057,
      "loss": 0.1858,
      "step": 16742
    },
    {
      "epoch": 0.07875575039747124,
      "grad_norm": 1.2758493423461914,
      "learning_rate": 0.00018468320650277708,
      "loss": 0.0786,
      "step": 16743
    },
    {
      "epoch": 0.0787604541990837,
      "grad_norm": 2.1413261890411377,
      "learning_rate": 0.0001846822635246636,
      "loss": 0.2051,
      "step": 16744
    },
    {
      "epoch": 0.07876515800069617,
      "grad_norm": 1.420606255531311,
      "learning_rate": 0.00018468132054655012,
      "loss": 0.2448,
      "step": 16745
    },
    {
      "epoch": 0.07876986180230863,
      "grad_norm": 0.3723023533821106,
      "learning_rate": 0.00018468037756843664,
      "loss": 0.0309,
      "step": 16746
    },
    {
      "epoch": 0.07877456560392108,
      "grad_norm": 1.4542779922485352,
      "learning_rate": 0.00018467943459032319,
      "loss": 0.1943,
      "step": 16747
    },
    {
      "epoch": 0.07877926940553355,
      "grad_norm": 2.066978693008423,
      "learning_rate": 0.0001846784916122097,
      "loss": 0.5292,
      "step": 16748
    },
    {
      "epoch": 0.07878397320714602,
      "grad_norm": 2.809053897857666,
      "learning_rate": 0.0001846775486340962,
      "loss": 0.4576,
      "step": 16749
    },
    {
      "epoch": 0.07878867700875848,
      "grad_norm": 1.8873611688613892,
      "learning_rate": 0.00018467660565598271,
      "loss": 0.3238,
      "step": 16750
    },
    {
      "epoch": 0.07879338081037095,
      "grad_norm": 2.4189841747283936,
      "learning_rate": 0.00018467566267786926,
      "loss": 0.0892,
      "step": 16751
    },
    {
      "epoch": 0.0787980846119834,
      "grad_norm": 2.3323447704315186,
      "learning_rate": 0.00018467471969975578,
      "loss": 0.105,
      "step": 16752
    },
    {
      "epoch": 0.07880278841359586,
      "grad_norm": 0.9150843620300293,
      "learning_rate": 0.0001846737767216423,
      "loss": 0.0799,
      "step": 16753
    },
    {
      "epoch": 0.07880749221520833,
      "grad_norm": 6.734920978546143,
      "learning_rate": 0.00018467283374352882,
      "loss": 1.0328,
      "step": 16754
    },
    {
      "epoch": 0.0788121960168208,
      "grad_norm": 5.513527870178223,
      "learning_rate": 0.00018467189076541533,
      "loss": 0.4471,
      "step": 16755
    },
    {
      "epoch": 0.07881689981843326,
      "grad_norm": 1.2061071395874023,
      "learning_rate": 0.00018467094778730188,
      "loss": 0.0676,
      "step": 16756
    },
    {
      "epoch": 0.07882160362004573,
      "grad_norm": 1.7666378021240234,
      "learning_rate": 0.0001846700048091884,
      "loss": 0.1748,
      "step": 16757
    },
    {
      "epoch": 0.07882630742165818,
      "grad_norm": 0.764094889163971,
      "learning_rate": 0.00018466906183107492,
      "loss": 0.0465,
      "step": 16758
    },
    {
      "epoch": 0.07883101122327064,
      "grad_norm": 0.8389994502067566,
      "learning_rate": 0.00018466811885296144,
      "loss": 0.0561,
      "step": 16759
    },
    {
      "epoch": 0.07883571502488311,
      "grad_norm": 0.4922902584075928,
      "learning_rate": 0.00018466717587484796,
      "loss": 0.0325,
      "step": 16760
    },
    {
      "epoch": 0.07884041882649558,
      "grad_norm": 1.7416400909423828,
      "learning_rate": 0.00018466623289673447,
      "loss": 0.0773,
      "step": 16761
    },
    {
      "epoch": 0.07884512262810804,
      "grad_norm": 1.5479344129562378,
      "learning_rate": 0.000184665289918621,
      "loss": 0.1142,
      "step": 16762
    },
    {
      "epoch": 0.07884982642972051,
      "grad_norm": 3.398669958114624,
      "learning_rate": 0.0001846643469405075,
      "loss": 0.2009,
      "step": 16763
    },
    {
      "epoch": 0.07885453023133296,
      "grad_norm": 2.4668452739715576,
      "learning_rate": 0.00018466340396239403,
      "loss": 0.4195,
      "step": 16764
    },
    {
      "epoch": 0.07885923403294542,
      "grad_norm": 4.139854907989502,
      "learning_rate": 0.00018466246098428058,
      "loss": 0.7686,
      "step": 16765
    },
    {
      "epoch": 0.07886393783455789,
      "grad_norm": 0.7477960586547852,
      "learning_rate": 0.0001846615180061671,
      "loss": 0.0353,
      "step": 16766
    },
    {
      "epoch": 0.07886864163617036,
      "grad_norm": 1.0827577114105225,
      "learning_rate": 0.0001846605750280536,
      "loss": 0.0931,
      "step": 16767
    },
    {
      "epoch": 0.07887334543778282,
      "grad_norm": 3.7138211727142334,
      "learning_rate": 0.00018465963204994013,
      "loss": 0.6675,
      "step": 16768
    },
    {
      "epoch": 0.07887804923939527,
      "grad_norm": 3.150996208190918,
      "learning_rate": 0.00018465868907182665,
      "loss": 0.3138,
      "step": 16769
    },
    {
      "epoch": 0.07888275304100774,
      "grad_norm": 2.421452522277832,
      "learning_rate": 0.00018465774609371317,
      "loss": 0.3752,
      "step": 16770
    },
    {
      "epoch": 0.0788874568426202,
      "grad_norm": 1.9820159673690796,
      "learning_rate": 0.0001846568031155997,
      "loss": 0.1899,
      "step": 16771
    },
    {
      "epoch": 0.07889216064423267,
      "grad_norm": 0.6117421388626099,
      "learning_rate": 0.0001846558601374862,
      "loss": 0.0584,
      "step": 16772
    },
    {
      "epoch": 0.07889686444584514,
      "grad_norm": 0.4828221797943115,
      "learning_rate": 0.00018465491715937272,
      "loss": 0.0371,
      "step": 16773
    },
    {
      "epoch": 0.0789015682474576,
      "grad_norm": 2.911189079284668,
      "learning_rate": 0.00018465397418125927,
      "loss": 0.3812,
      "step": 16774
    },
    {
      "epoch": 0.07890627204907005,
      "grad_norm": 5.0276923179626465,
      "learning_rate": 0.0001846530312031458,
      "loss": 0.8835,
      "step": 16775
    },
    {
      "epoch": 0.07891097585068252,
      "grad_norm": 3.7314162254333496,
      "learning_rate": 0.0001846520882250323,
      "loss": 0.1739,
      "step": 16776
    },
    {
      "epoch": 0.07891567965229498,
      "grad_norm": 1.3321257829666138,
      "learning_rate": 0.00018465114524691883,
      "loss": 0.1715,
      "step": 16777
    },
    {
      "epoch": 0.07892038345390745,
      "grad_norm": 2.4229300022125244,
      "learning_rate": 0.00018465020226880534,
      "loss": 0.4535,
      "step": 16778
    },
    {
      "epoch": 0.07892508725551992,
      "grad_norm": 0.13999107480049133,
      "learning_rate": 0.0001846492592906919,
      "loss": 0.0101,
      "step": 16779
    },
    {
      "epoch": 0.07892979105713238,
      "grad_norm": 0.595615029335022,
      "learning_rate": 0.00018464831631257838,
      "loss": 0.0379,
      "step": 16780
    },
    {
      "epoch": 0.07893449485874483,
      "grad_norm": 2.183128595352173,
      "learning_rate": 0.0001846473733344649,
      "loss": 0.165,
      "step": 16781
    },
    {
      "epoch": 0.0789391986603573,
      "grad_norm": 1.9002177715301514,
      "learning_rate": 0.00018464643035635142,
      "loss": 0.1863,
      "step": 16782
    },
    {
      "epoch": 0.07894390246196976,
      "grad_norm": 2.531345844268799,
      "learning_rate": 0.00018464548737823797,
      "loss": 0.6437,
      "step": 16783
    },
    {
      "epoch": 0.07894860626358223,
      "grad_norm": 1.806435465812683,
      "learning_rate": 0.00018464454440012448,
      "loss": 0.1096,
      "step": 16784
    },
    {
      "epoch": 0.0789533100651947,
      "grad_norm": 3.124835729598999,
      "learning_rate": 0.000184643601422011,
      "loss": 0.4371,
      "step": 16785
    },
    {
      "epoch": 0.07895801386680715,
      "grad_norm": 3.0842928886413574,
      "learning_rate": 0.00018464265844389752,
      "loss": 0.3733,
      "step": 16786
    },
    {
      "epoch": 0.07896271766841961,
      "grad_norm": 0.8278597593307495,
      "learning_rate": 0.00018464171546578404,
      "loss": 0.0673,
      "step": 16787
    },
    {
      "epoch": 0.07896742147003208,
      "grad_norm": 4.201355934143066,
      "learning_rate": 0.00018464077248767059,
      "loss": 0.6424,
      "step": 16788
    },
    {
      "epoch": 0.07897212527164454,
      "grad_norm": 3.779709577560425,
      "learning_rate": 0.0001846398295095571,
      "loss": 0.4057,
      "step": 16789
    },
    {
      "epoch": 0.07897682907325701,
      "grad_norm": 1.5564894676208496,
      "learning_rate": 0.00018463888653144362,
      "loss": 0.2128,
      "step": 16790
    },
    {
      "epoch": 0.07898153287486948,
      "grad_norm": 2.0394623279571533,
      "learning_rate": 0.00018463794355333014,
      "loss": 0.4714,
      "step": 16791
    },
    {
      "epoch": 0.07898623667648193,
      "grad_norm": 2.471034526824951,
      "learning_rate": 0.00018463700057521666,
      "loss": 0.2584,
      "step": 16792
    },
    {
      "epoch": 0.07899094047809439,
      "grad_norm": 3.06455397605896,
      "learning_rate": 0.00018463605759710318,
      "loss": 0.7697,
      "step": 16793
    },
    {
      "epoch": 0.07899564427970686,
      "grad_norm": 1.8200057744979858,
      "learning_rate": 0.0001846351146189897,
      "loss": 0.3488,
      "step": 16794
    },
    {
      "epoch": 0.07900034808131932,
      "grad_norm": 0.8806877732276917,
      "learning_rate": 0.00018463417164087622,
      "loss": 0.1129,
      "step": 16795
    },
    {
      "epoch": 0.07900505188293179,
      "grad_norm": 3.859362840652466,
      "learning_rate": 0.00018463322866276273,
      "loss": 0.5496,
      "step": 16796
    },
    {
      "epoch": 0.07900975568454426,
      "grad_norm": 1.5127149820327759,
      "learning_rate": 0.00018463228568464928,
      "loss": 0.1459,
      "step": 16797
    },
    {
      "epoch": 0.07901445948615671,
      "grad_norm": 0.6735188961029053,
      "learning_rate": 0.0001846313427065358,
      "loss": 0.0611,
      "step": 16798
    },
    {
      "epoch": 0.07901916328776917,
      "grad_norm": 1.0806478261947632,
      "learning_rate": 0.00018463039972842232,
      "loss": 0.1535,
      "step": 16799
    },
    {
      "epoch": 0.07902386708938164,
      "grad_norm": 0.4610731303691864,
      "learning_rate": 0.00018462945675030884,
      "loss": 0.0562,
      "step": 16800
    },
    {
      "epoch": 0.0790285708909941,
      "grad_norm": 0.7366060614585876,
      "learning_rate": 0.00018462851377219536,
      "loss": 0.0391,
      "step": 16801
    },
    {
      "epoch": 0.07903327469260657,
      "grad_norm": 2.376452922821045,
      "learning_rate": 0.00018462757079408187,
      "loss": 0.1094,
      "step": 16802
    },
    {
      "epoch": 0.07903797849421902,
      "grad_norm": 3.8612451553344727,
      "learning_rate": 0.0001846266278159684,
      "loss": 0.1956,
      "step": 16803
    },
    {
      "epoch": 0.07904268229583149,
      "grad_norm": 0.5406075716018677,
      "learning_rate": 0.0001846256848378549,
      "loss": 0.0723,
      "step": 16804
    },
    {
      "epoch": 0.07904738609744395,
      "grad_norm": 2.0622756481170654,
      "learning_rate": 0.00018462474185974143,
      "loss": 0.2424,
      "step": 16805
    },
    {
      "epoch": 0.07905208989905642,
      "grad_norm": 2.0845632553100586,
      "learning_rate": 0.00018462379888162798,
      "loss": 0.3125,
      "step": 16806
    },
    {
      "epoch": 0.07905679370066888,
      "grad_norm": 0.6810484528541565,
      "learning_rate": 0.0001846228559035145,
      "loss": 0.0663,
      "step": 16807
    },
    {
      "epoch": 0.07906149750228135,
      "grad_norm": 2.9857442378997803,
      "learning_rate": 0.000184621912925401,
      "loss": 0.3381,
      "step": 16808
    },
    {
      "epoch": 0.0790662013038938,
      "grad_norm": 3.1843888759613037,
      "learning_rate": 0.00018462096994728753,
      "loss": 0.3663,
      "step": 16809
    },
    {
      "epoch": 0.07907090510550627,
      "grad_norm": 4.043195724487305,
      "learning_rate": 0.00018462002696917408,
      "loss": 0.5083,
      "step": 16810
    },
    {
      "epoch": 0.07907560890711873,
      "grad_norm": 0.03032434545457363,
      "learning_rate": 0.00018461908399106057,
      "loss": 0.0016,
      "step": 16811
    },
    {
      "epoch": 0.0790803127087312,
      "grad_norm": 3.288332223892212,
      "learning_rate": 0.0001846181410129471,
      "loss": 0.7479,
      "step": 16812
    },
    {
      "epoch": 0.07908501651034366,
      "grad_norm": 2.5809874534606934,
      "learning_rate": 0.0001846171980348336,
      "loss": 0.0985,
      "step": 16813
    },
    {
      "epoch": 0.07908972031195613,
      "grad_norm": 0.25172606110572815,
      "learning_rate": 0.00018461625505672012,
      "loss": 0.0213,
      "step": 16814
    },
    {
      "epoch": 0.07909442411356858,
      "grad_norm": 2.299346446990967,
      "learning_rate": 0.00018461531207860667,
      "loss": 0.21,
      "step": 16815
    },
    {
      "epoch": 0.07909912791518105,
      "grad_norm": 0.9266145825386047,
      "learning_rate": 0.0001846143691004932,
      "loss": 0.066,
      "step": 16816
    },
    {
      "epoch": 0.07910383171679351,
      "grad_norm": 2.6304397583007812,
      "learning_rate": 0.0001846134261223797,
      "loss": 0.3472,
      "step": 16817
    },
    {
      "epoch": 0.07910853551840598,
      "grad_norm": 4.808404922485352,
      "learning_rate": 0.00018461248314426623,
      "loss": 0.9986,
      "step": 16818
    },
    {
      "epoch": 0.07911323932001844,
      "grad_norm": 1.0009318590164185,
      "learning_rate": 0.00018461154016615277,
      "loss": 0.0935,
      "step": 16819
    },
    {
      "epoch": 0.0791179431216309,
      "grad_norm": 0.4959584176540375,
      "learning_rate": 0.0001846105971880393,
      "loss": 0.0256,
      "step": 16820
    },
    {
      "epoch": 0.07912264692324336,
      "grad_norm": 1.2720993757247925,
      "learning_rate": 0.0001846096542099258,
      "loss": 0.0692,
      "step": 16821
    },
    {
      "epoch": 0.07912735072485583,
      "grad_norm": 0.9147387146949768,
      "learning_rate": 0.00018460871123181233,
      "loss": 0.0683,
      "step": 16822
    },
    {
      "epoch": 0.07913205452646829,
      "grad_norm": 4.289098262786865,
      "learning_rate": 0.00018460776825369882,
      "loss": 0.7019,
      "step": 16823
    },
    {
      "epoch": 0.07913675832808076,
      "grad_norm": 0.24416613578796387,
      "learning_rate": 0.00018460682527558537,
      "loss": 0.0164,
      "step": 16824
    },
    {
      "epoch": 0.07914146212969322,
      "grad_norm": 0.5278670191764832,
      "learning_rate": 0.00018460588229747188,
      "loss": 0.0197,
      "step": 16825
    },
    {
      "epoch": 0.07914616593130568,
      "grad_norm": 1.2643669843673706,
      "learning_rate": 0.0001846049393193584,
      "loss": 0.0589,
      "step": 16826
    },
    {
      "epoch": 0.07915086973291814,
      "grad_norm": 2.346569538116455,
      "learning_rate": 0.00018460399634124492,
      "loss": 0.2377,
      "step": 16827
    },
    {
      "epoch": 0.07915557353453061,
      "grad_norm": 6.150073528289795,
      "learning_rate": 0.00018460305336313144,
      "loss": 1.1363,
      "step": 16828
    },
    {
      "epoch": 0.07916027733614307,
      "grad_norm": 1.9849801063537598,
      "learning_rate": 0.00018460211038501799,
      "loss": 0.1651,
      "step": 16829
    },
    {
      "epoch": 0.07916498113775554,
      "grad_norm": 0.6822956800460815,
      "learning_rate": 0.0001846011674069045,
      "loss": 0.0498,
      "step": 16830
    },
    {
      "epoch": 0.079169684939368,
      "grad_norm": 5.162160873413086,
      "learning_rate": 0.00018460022442879102,
      "loss": 1.0394,
      "step": 16831
    },
    {
      "epoch": 0.07917438874098046,
      "grad_norm": 3.101015090942383,
      "learning_rate": 0.00018459928145067754,
      "loss": 0.4369,
      "step": 16832
    },
    {
      "epoch": 0.07917909254259292,
      "grad_norm": 0.7567636966705322,
      "learning_rate": 0.00018459833847256406,
      "loss": 0.041,
      "step": 16833
    },
    {
      "epoch": 0.07918379634420539,
      "grad_norm": 1.8738648891448975,
      "learning_rate": 0.00018459739549445058,
      "loss": 0.1898,
      "step": 16834
    },
    {
      "epoch": 0.07918850014581785,
      "grad_norm": 3.246084213256836,
      "learning_rate": 0.0001845964525163371,
      "loss": 0.2422,
      "step": 16835
    },
    {
      "epoch": 0.07919320394743032,
      "grad_norm": 0.2908651530742645,
      "learning_rate": 0.00018459550953822362,
      "loss": 0.0147,
      "step": 16836
    },
    {
      "epoch": 0.07919790774904277,
      "grad_norm": 2.4523677825927734,
      "learning_rate": 0.00018459456656011013,
      "loss": 0.4793,
      "step": 16837
    },
    {
      "epoch": 0.07920261155065524,
      "grad_norm": 1.3616366386413574,
      "learning_rate": 0.00018459362358199668,
      "loss": 0.099,
      "step": 16838
    },
    {
      "epoch": 0.0792073153522677,
      "grad_norm": 3.0909769535064697,
      "learning_rate": 0.0001845926806038832,
      "loss": 0.2871,
      "step": 16839
    },
    {
      "epoch": 0.07921201915388017,
      "grad_norm": 0.4504660964012146,
      "learning_rate": 0.00018459173762576972,
      "loss": 0.0309,
      "step": 16840
    },
    {
      "epoch": 0.07921672295549263,
      "grad_norm": 0.10776987671852112,
      "learning_rate": 0.00018459079464765624,
      "loss": 0.0064,
      "step": 16841
    },
    {
      "epoch": 0.0792214267571051,
      "grad_norm": 3.951503276824951,
      "learning_rate": 0.00018458985166954276,
      "loss": 0.9846,
      "step": 16842
    },
    {
      "epoch": 0.07922613055871755,
      "grad_norm": 4.207304000854492,
      "learning_rate": 0.00018458890869142927,
      "loss": 0.8561,
      "step": 16843
    },
    {
      "epoch": 0.07923083436033002,
      "grad_norm": 2.9292774200439453,
      "learning_rate": 0.0001845879657133158,
      "loss": 0.4857,
      "step": 16844
    },
    {
      "epoch": 0.07923553816194248,
      "grad_norm": 3.1085238456726074,
      "learning_rate": 0.0001845870227352023,
      "loss": 0.529,
      "step": 16845
    },
    {
      "epoch": 0.07924024196355495,
      "grad_norm": 1.1130361557006836,
      "learning_rate": 0.00018458607975708883,
      "loss": 0.0983,
      "step": 16846
    },
    {
      "epoch": 0.07924494576516741,
      "grad_norm": 3.807305335998535,
      "learning_rate": 0.00018458513677897538,
      "loss": 0.7375,
      "step": 16847
    },
    {
      "epoch": 0.07924964956677988,
      "grad_norm": 0.7740168571472168,
      "learning_rate": 0.0001845841938008619,
      "loss": 0.1342,
      "step": 16848
    },
    {
      "epoch": 0.07925435336839233,
      "grad_norm": 1.240705966949463,
      "learning_rate": 0.0001845832508227484,
      "loss": 0.1556,
      "step": 16849
    },
    {
      "epoch": 0.0792590571700048,
      "grad_norm": 1.995764970779419,
      "learning_rate": 0.00018458230784463493,
      "loss": 0.3373,
      "step": 16850
    },
    {
      "epoch": 0.07926376097161726,
      "grad_norm": 4.520312786102295,
      "learning_rate": 0.00018458136486652148,
      "loss": 0.5557,
      "step": 16851
    },
    {
      "epoch": 0.07926846477322973,
      "grad_norm": 8.040007591247559,
      "learning_rate": 0.000184580421888408,
      "loss": 0.9018,
      "step": 16852
    },
    {
      "epoch": 0.07927316857484219,
      "grad_norm": 3.5964672565460205,
      "learning_rate": 0.00018457947891029451,
      "loss": 0.7928,
      "step": 16853
    },
    {
      "epoch": 0.07927787237645464,
      "grad_norm": 0.3703358471393585,
      "learning_rate": 0.000184578535932181,
      "loss": 0.0166,
      "step": 16854
    },
    {
      "epoch": 0.07928257617806711,
      "grad_norm": 4.78465461730957,
      "learning_rate": 0.00018457759295406752,
      "loss": 0.83,
      "step": 16855
    },
    {
      "epoch": 0.07928727997967958,
      "grad_norm": 1.3884661197662354,
      "learning_rate": 0.00018457664997595407,
      "loss": 0.1435,
      "step": 16856
    },
    {
      "epoch": 0.07929198378129204,
      "grad_norm": 1.7363128662109375,
      "learning_rate": 0.0001845757069978406,
      "loss": 0.1361,
      "step": 16857
    },
    {
      "epoch": 0.07929668758290451,
      "grad_norm": 0.8782510161399841,
      "learning_rate": 0.0001845747640197271,
      "loss": 0.0691,
      "step": 16858
    },
    {
      "epoch": 0.07930139138451697,
      "grad_norm": 3.001669406890869,
      "learning_rate": 0.00018457382104161363,
      "loss": 0.3979,
      "step": 16859
    },
    {
      "epoch": 0.07930609518612942,
      "grad_norm": 0.23043541610240936,
      "learning_rate": 0.00018457287806350017,
      "loss": 0.0325,
      "step": 16860
    },
    {
      "epoch": 0.07931079898774189,
      "grad_norm": 3.732365131378174,
      "learning_rate": 0.0001845719350853867,
      "loss": 0.2907,
      "step": 16861
    },
    {
      "epoch": 0.07931550278935436,
      "grad_norm": 1.5536565780639648,
      "learning_rate": 0.0001845709921072732,
      "loss": 0.1677,
      "step": 16862
    },
    {
      "epoch": 0.07932020659096682,
      "grad_norm": 1.2228903770446777,
      "learning_rate": 0.00018457004912915973,
      "loss": 0.1248,
      "step": 16863
    },
    {
      "epoch": 0.07932491039257929,
      "grad_norm": 0.6991478800773621,
      "learning_rate": 0.00018456910615104625,
      "loss": 0.065,
      "step": 16864
    },
    {
      "epoch": 0.07932961419419175,
      "grad_norm": 2.1285154819488525,
      "learning_rate": 0.00018456816317293277,
      "loss": 0.2893,
      "step": 16865
    },
    {
      "epoch": 0.0793343179958042,
      "grad_norm": 4.322099685668945,
      "learning_rate": 0.00018456722019481928,
      "loss": 0.9413,
      "step": 16866
    },
    {
      "epoch": 0.07933902179741667,
      "grad_norm": 2.9778246879577637,
      "learning_rate": 0.0001845662772167058,
      "loss": 0.5012,
      "step": 16867
    },
    {
      "epoch": 0.07934372559902914,
      "grad_norm": 2.3636555671691895,
      "learning_rate": 0.00018456533423859232,
      "loss": 0.407,
      "step": 16868
    },
    {
      "epoch": 0.0793484294006416,
      "grad_norm": 2.2972195148468018,
      "learning_rate": 0.00018456439126047887,
      "loss": 0.3027,
      "step": 16869
    },
    {
      "epoch": 0.07935313320225407,
      "grad_norm": 0.6302855014801025,
      "learning_rate": 0.00018456344828236539,
      "loss": 0.0499,
      "step": 16870
    },
    {
      "epoch": 0.07935783700386652,
      "grad_norm": 2.152338743209839,
      "learning_rate": 0.0001845625053042519,
      "loss": 0.2241,
      "step": 16871
    },
    {
      "epoch": 0.07936254080547898,
      "grad_norm": 2.866253614425659,
      "learning_rate": 0.00018456156232613842,
      "loss": 0.6278,
      "step": 16872
    },
    {
      "epoch": 0.07936724460709145,
      "grad_norm": 1.042148232460022,
      "learning_rate": 0.00018456061934802494,
      "loss": 0.1079,
      "step": 16873
    },
    {
      "epoch": 0.07937194840870392,
      "grad_norm": 4.338201522827148,
      "learning_rate": 0.00018455967636991146,
      "loss": 0.2368,
      "step": 16874
    },
    {
      "epoch": 0.07937665221031638,
      "grad_norm": 2.531114101409912,
      "learning_rate": 0.00018455873339179798,
      "loss": 0.2573,
      "step": 16875
    },
    {
      "epoch": 0.07938135601192885,
      "grad_norm": 2.237173557281494,
      "learning_rate": 0.0001845577904136845,
      "loss": 0.2451,
      "step": 16876
    },
    {
      "epoch": 0.0793860598135413,
      "grad_norm": 3.9422342777252197,
      "learning_rate": 0.00018455684743557102,
      "loss": 0.6817,
      "step": 16877
    },
    {
      "epoch": 0.07939076361515376,
      "grad_norm": 2.266498565673828,
      "learning_rate": 0.00018455590445745753,
      "loss": 0.2394,
      "step": 16878
    },
    {
      "epoch": 0.07939546741676623,
      "grad_norm": 0.31852126121520996,
      "learning_rate": 0.00018455496147934408,
      "loss": 0.0358,
      "step": 16879
    },
    {
      "epoch": 0.0794001712183787,
      "grad_norm": 1.5486928224563599,
      "learning_rate": 0.0001845540185012306,
      "loss": 0.2508,
      "step": 16880
    },
    {
      "epoch": 0.07940487501999116,
      "grad_norm": 2.558009147644043,
      "learning_rate": 0.00018455307552311712,
      "loss": 0.3525,
      "step": 16881
    },
    {
      "epoch": 0.07940957882160363,
      "grad_norm": 1.1069303750991821,
      "learning_rate": 0.00018455213254500364,
      "loss": 0.1013,
      "step": 16882
    },
    {
      "epoch": 0.07941428262321608,
      "grad_norm": 2.470484972000122,
      "learning_rate": 0.00018455118956689018,
      "loss": 0.4095,
      "step": 16883
    },
    {
      "epoch": 0.07941898642482854,
      "grad_norm": 0.579246461391449,
      "learning_rate": 0.0001845502465887767,
      "loss": 0.0863,
      "step": 16884
    },
    {
      "epoch": 0.07942369022644101,
      "grad_norm": 1.9004039764404297,
      "learning_rate": 0.0001845493036106632,
      "loss": 0.2461,
      "step": 16885
    },
    {
      "epoch": 0.07942839402805348,
      "grad_norm": 3.1320717334747314,
      "learning_rate": 0.0001845483606325497,
      "loss": 0.5208,
      "step": 16886
    },
    {
      "epoch": 0.07943309782966594,
      "grad_norm": 0.7149237394332886,
      "learning_rate": 0.00018454741765443623,
      "loss": 0.0591,
      "step": 16887
    },
    {
      "epoch": 0.07943780163127839,
      "grad_norm": 3.937544822692871,
      "learning_rate": 0.00018454647467632278,
      "loss": 0.6997,
      "step": 16888
    },
    {
      "epoch": 0.07944250543289086,
      "grad_norm": 1.1987953186035156,
      "learning_rate": 0.0001845455316982093,
      "loss": 0.1531,
      "step": 16889
    },
    {
      "epoch": 0.07944720923450332,
      "grad_norm": 0.7524529695510864,
      "learning_rate": 0.0001845445887200958,
      "loss": 0.0745,
      "step": 16890
    },
    {
      "epoch": 0.07945191303611579,
      "grad_norm": 0.25996077060699463,
      "learning_rate": 0.00018454364574198233,
      "loss": 0.0194,
      "step": 16891
    },
    {
      "epoch": 0.07945661683772826,
      "grad_norm": 2.370277166366577,
      "learning_rate": 0.00018454270276386888,
      "loss": 0.318,
      "step": 16892
    },
    {
      "epoch": 0.07946132063934072,
      "grad_norm": 1.765273094177246,
      "learning_rate": 0.0001845417597857554,
      "loss": 0.2669,
      "step": 16893
    },
    {
      "epoch": 0.07946602444095317,
      "grad_norm": 5.929781436920166,
      "learning_rate": 0.00018454081680764191,
      "loss": 0.5995,
      "step": 16894
    },
    {
      "epoch": 0.07947072824256564,
      "grad_norm": 1.330604910850525,
      "learning_rate": 0.00018453987382952843,
      "loss": 0.1333,
      "step": 16895
    },
    {
      "epoch": 0.0794754320441781,
      "grad_norm": 0.6978998184204102,
      "learning_rate": 0.00018453893085141492,
      "loss": 0.073,
      "step": 16896
    },
    {
      "epoch": 0.07948013584579057,
      "grad_norm": 0.18575875461101532,
      "learning_rate": 0.00018453798787330147,
      "loss": 0.0108,
      "step": 16897
    },
    {
      "epoch": 0.07948483964740304,
      "grad_norm": 4.9961347579956055,
      "learning_rate": 0.000184537044895188,
      "loss": 0.6601,
      "step": 16898
    },
    {
      "epoch": 0.0794895434490155,
      "grad_norm": 1.467592477798462,
      "learning_rate": 0.0001845361019170745,
      "loss": 0.1506,
      "step": 16899
    },
    {
      "epoch": 0.07949424725062795,
      "grad_norm": 2.1369550228118896,
      "learning_rate": 0.00018453515893896103,
      "loss": 0.3099,
      "step": 16900
    },
    {
      "epoch": 0.07949895105224042,
      "grad_norm": 1.714137077331543,
      "learning_rate": 0.00018453421596084757,
      "loss": 0.1806,
      "step": 16901
    },
    {
      "epoch": 0.07950365485385288,
      "grad_norm": 1.674411416053772,
      "learning_rate": 0.0001845332729827341,
      "loss": 0.1242,
      "step": 16902
    },
    {
      "epoch": 0.07950835865546535,
      "grad_norm": 2.7145142555236816,
      "learning_rate": 0.0001845323300046206,
      "loss": 0.55,
      "step": 16903
    },
    {
      "epoch": 0.07951306245707782,
      "grad_norm": 2.9020559787750244,
      "learning_rate": 0.00018453138702650713,
      "loss": 0.5046,
      "step": 16904
    },
    {
      "epoch": 0.07951776625869027,
      "grad_norm": 2.4210965633392334,
      "learning_rate": 0.00018453044404839365,
      "loss": 0.47,
      "step": 16905
    },
    {
      "epoch": 0.07952247006030273,
      "grad_norm": 4.035975933074951,
      "learning_rate": 0.00018452950107028017,
      "loss": 0.3701,
      "step": 16906
    },
    {
      "epoch": 0.0795271738619152,
      "grad_norm": 0.32906609773635864,
      "learning_rate": 0.00018452855809216668,
      "loss": 0.0293,
      "step": 16907
    },
    {
      "epoch": 0.07953187766352766,
      "grad_norm": 1.1292239427566528,
      "learning_rate": 0.0001845276151140532,
      "loss": 0.0756,
      "step": 16908
    },
    {
      "epoch": 0.07953658146514013,
      "grad_norm": 2.244288682937622,
      "learning_rate": 0.00018452667213593972,
      "loss": 0.4277,
      "step": 16909
    },
    {
      "epoch": 0.0795412852667526,
      "grad_norm": 0.47078603506088257,
      "learning_rate": 0.00018452572915782627,
      "loss": 0.0457,
      "step": 16910
    },
    {
      "epoch": 0.07954598906836505,
      "grad_norm": 0.19807440042495728,
      "learning_rate": 0.00018452478617971279,
      "loss": 0.011,
      "step": 16911
    },
    {
      "epoch": 0.07955069286997751,
      "grad_norm": 2.2735812664031982,
      "learning_rate": 0.0001845238432015993,
      "loss": 0.3077,
      "step": 16912
    },
    {
      "epoch": 0.07955539667158998,
      "grad_norm": 2.6082444190979004,
      "learning_rate": 0.00018452290022348582,
      "loss": 0.5161,
      "step": 16913
    },
    {
      "epoch": 0.07956010047320244,
      "grad_norm": 2.230483055114746,
      "learning_rate": 0.00018452195724537234,
      "loss": 0.2559,
      "step": 16914
    },
    {
      "epoch": 0.07956480427481491,
      "grad_norm": 3.6133971214294434,
      "learning_rate": 0.0001845210142672589,
      "loss": 0.5077,
      "step": 16915
    },
    {
      "epoch": 0.07956950807642738,
      "grad_norm": 0.8305376768112183,
      "learning_rate": 0.00018452007128914538,
      "loss": 0.0885,
      "step": 16916
    },
    {
      "epoch": 0.07957421187803983,
      "grad_norm": 1.2096800804138184,
      "learning_rate": 0.0001845191283110319,
      "loss": 0.0181,
      "step": 16917
    },
    {
      "epoch": 0.07957891567965229,
      "grad_norm": 8.230460166931152,
      "learning_rate": 0.00018451818533291842,
      "loss": 0.5344,
      "step": 16918
    },
    {
      "epoch": 0.07958361948126476,
      "grad_norm": 0.8697593808174133,
      "learning_rate": 0.00018451724235480496,
      "loss": 0.1341,
      "step": 16919
    },
    {
      "epoch": 0.07958832328287722,
      "grad_norm": 2.328582763671875,
      "learning_rate": 0.00018451629937669148,
      "loss": 0.1909,
      "step": 16920
    },
    {
      "epoch": 0.07959302708448969,
      "grad_norm": 0.8189014196395874,
      "learning_rate": 0.000184515356398578,
      "loss": 0.133,
      "step": 16921
    },
    {
      "epoch": 0.07959773088610214,
      "grad_norm": 0.47930485010147095,
      "learning_rate": 0.00018451441342046452,
      "loss": 0.0521,
      "step": 16922
    },
    {
      "epoch": 0.07960243468771461,
      "grad_norm": 2.3213813304901123,
      "learning_rate": 0.00018451347044235104,
      "loss": 0.0921,
      "step": 16923
    },
    {
      "epoch": 0.07960713848932707,
      "grad_norm": 2.324927568435669,
      "learning_rate": 0.00018451252746423758,
      "loss": 0.3339,
      "step": 16924
    },
    {
      "epoch": 0.07961184229093954,
      "grad_norm": 5.862944602966309,
      "learning_rate": 0.0001845115844861241,
      "loss": 0.304,
      "step": 16925
    },
    {
      "epoch": 0.079616546092552,
      "grad_norm": 1.171095371246338,
      "learning_rate": 0.00018451064150801062,
      "loss": 0.0832,
      "step": 16926
    },
    {
      "epoch": 0.07962124989416447,
      "grad_norm": 1.2707085609436035,
      "learning_rate": 0.0001845096985298971,
      "loss": 0.0806,
      "step": 16927
    },
    {
      "epoch": 0.07962595369577692,
      "grad_norm": 3.630418300628662,
      "learning_rate": 0.00018450875555178366,
      "loss": 0.3182,
      "step": 16928
    },
    {
      "epoch": 0.07963065749738939,
      "grad_norm": 1.3228654861450195,
      "learning_rate": 0.00018450781257367018,
      "loss": 0.1197,
      "step": 16929
    },
    {
      "epoch": 0.07963536129900185,
      "grad_norm": 0.614855945110321,
      "learning_rate": 0.0001845068695955567,
      "loss": 0.0522,
      "step": 16930
    },
    {
      "epoch": 0.07964006510061432,
      "grad_norm": 3.075249195098877,
      "learning_rate": 0.0001845059266174432,
      "loss": 0.3853,
      "step": 16931
    },
    {
      "epoch": 0.07964476890222678,
      "grad_norm": 4.235781669616699,
      "learning_rate": 0.00018450498363932973,
      "loss": 0.6051,
      "step": 16932
    },
    {
      "epoch": 0.07964947270383925,
      "grad_norm": 3.128164529800415,
      "learning_rate": 0.00018450404066121628,
      "loss": 0.6101,
      "step": 16933
    },
    {
      "epoch": 0.0796541765054517,
      "grad_norm": 3.750864267349243,
      "learning_rate": 0.0001845030976831028,
      "loss": 0.5445,
      "step": 16934
    },
    {
      "epoch": 0.07965888030706417,
      "grad_norm": 2.0401768684387207,
      "learning_rate": 0.00018450215470498931,
      "loss": 0.321,
      "step": 16935
    },
    {
      "epoch": 0.07966358410867663,
      "grad_norm": 1.2649681568145752,
      "learning_rate": 0.00018450121172687583,
      "loss": 0.1387,
      "step": 16936
    },
    {
      "epoch": 0.0796682879102891,
      "grad_norm": 5.873511791229248,
      "learning_rate": 0.00018450026874876235,
      "loss": 0.5966,
      "step": 16937
    },
    {
      "epoch": 0.07967299171190156,
      "grad_norm": 2.54380464553833,
      "learning_rate": 0.00018449932577064887,
      "loss": 0.1447,
      "step": 16938
    },
    {
      "epoch": 0.07967769551351402,
      "grad_norm": 0.6142094731330872,
      "learning_rate": 0.0001844983827925354,
      "loss": 0.0342,
      "step": 16939
    },
    {
      "epoch": 0.07968239931512648,
      "grad_norm": 5.247790813446045,
      "learning_rate": 0.0001844974398144219,
      "loss": 0.4821,
      "step": 16940
    },
    {
      "epoch": 0.07968710311673895,
      "grad_norm": 2.9812700748443604,
      "learning_rate": 0.00018449649683630843,
      "loss": 0.2772,
      "step": 16941
    },
    {
      "epoch": 0.07969180691835141,
      "grad_norm": 1.4327752590179443,
      "learning_rate": 0.00018449555385819497,
      "loss": 0.1009,
      "step": 16942
    },
    {
      "epoch": 0.07969651071996388,
      "grad_norm": 2.4653451442718506,
      "learning_rate": 0.0001844946108800815,
      "loss": 0.3712,
      "step": 16943
    },
    {
      "epoch": 0.07970121452157634,
      "grad_norm": 2.832191228866577,
      "learning_rate": 0.000184493667901968,
      "loss": 0.4893,
      "step": 16944
    },
    {
      "epoch": 0.0797059183231888,
      "grad_norm": 2.296487331390381,
      "learning_rate": 0.00018449272492385453,
      "loss": 0.1998,
      "step": 16945
    },
    {
      "epoch": 0.07971062212480126,
      "grad_norm": 2.2851297855377197,
      "learning_rate": 0.00018449178194574105,
      "loss": 0.2305,
      "step": 16946
    },
    {
      "epoch": 0.07971532592641373,
      "grad_norm": 2.9475417137145996,
      "learning_rate": 0.00018449083896762757,
      "loss": 0.2446,
      "step": 16947
    },
    {
      "epoch": 0.07972002972802619,
      "grad_norm": 4.706790924072266,
      "learning_rate": 0.00018448989598951408,
      "loss": 0.9943,
      "step": 16948
    },
    {
      "epoch": 0.07972473352963866,
      "grad_norm": 2.34515118598938,
      "learning_rate": 0.0001844889530114006,
      "loss": 0.3662,
      "step": 16949
    },
    {
      "epoch": 0.07972943733125112,
      "grad_norm": 2.9860684871673584,
      "learning_rate": 0.00018448801003328712,
      "loss": 0.227,
      "step": 16950
    },
    {
      "epoch": 0.07973414113286358,
      "grad_norm": 1.4384698867797852,
      "learning_rate": 0.00018448706705517367,
      "loss": 0.1035,
      "step": 16951
    },
    {
      "epoch": 0.07973884493447604,
      "grad_norm": 0.769366443157196,
      "learning_rate": 0.00018448612407706019,
      "loss": 0.0416,
      "step": 16952
    },
    {
      "epoch": 0.07974354873608851,
      "grad_norm": 3.0466208457946777,
      "learning_rate": 0.0001844851810989467,
      "loss": 0.4359,
      "step": 16953
    },
    {
      "epoch": 0.07974825253770097,
      "grad_norm": 5.483771800994873,
      "learning_rate": 0.00018448423812083322,
      "loss": 0.8037,
      "step": 16954
    },
    {
      "epoch": 0.07975295633931344,
      "grad_norm": 0.3717278838157654,
      "learning_rate": 0.00018448329514271974,
      "loss": 0.0181,
      "step": 16955
    },
    {
      "epoch": 0.07975766014092589,
      "grad_norm": 0.7866017818450928,
      "learning_rate": 0.0001844823521646063,
      "loss": 0.0618,
      "step": 16956
    },
    {
      "epoch": 0.07976236394253836,
      "grad_norm": 3.4037506580352783,
      "learning_rate": 0.0001844814091864928,
      "loss": 0.7438,
      "step": 16957
    },
    {
      "epoch": 0.07976706774415082,
      "grad_norm": 3.101628065109253,
      "learning_rate": 0.0001844804662083793,
      "loss": 0.1522,
      "step": 16958
    },
    {
      "epoch": 0.07977177154576329,
      "grad_norm": 2.8318374156951904,
      "learning_rate": 0.00018447952323026582,
      "loss": 0.2437,
      "step": 16959
    },
    {
      "epoch": 0.07977647534737575,
      "grad_norm": 1.4252392053604126,
      "learning_rate": 0.00018447858025215236,
      "loss": 0.0604,
      "step": 16960
    },
    {
      "epoch": 0.07978117914898822,
      "grad_norm": 3.1749401092529297,
      "learning_rate": 0.00018447763727403888,
      "loss": 0.2304,
      "step": 16961
    },
    {
      "epoch": 0.07978588295060067,
      "grad_norm": 3.731411933898926,
      "learning_rate": 0.0001844766942959254,
      "loss": 0.6461,
      "step": 16962
    },
    {
      "epoch": 0.07979058675221314,
      "grad_norm": 3.6522645950317383,
      "learning_rate": 0.00018447575131781192,
      "loss": 1.1286,
      "step": 16963
    },
    {
      "epoch": 0.0797952905538256,
      "grad_norm": 0.35718345642089844,
      "learning_rate": 0.00018447480833969844,
      "loss": 0.0289,
      "step": 16964
    },
    {
      "epoch": 0.07979999435543807,
      "grad_norm": 3.411513090133667,
      "learning_rate": 0.00018447386536158498,
      "loss": 0.3721,
      "step": 16965
    },
    {
      "epoch": 0.07980469815705053,
      "grad_norm": 3.842285633087158,
      "learning_rate": 0.0001844729223834715,
      "loss": 0.2721,
      "step": 16966
    },
    {
      "epoch": 0.079809401958663,
      "grad_norm": 1.210979700088501,
      "learning_rate": 0.00018447197940535802,
      "loss": 0.0896,
      "step": 16967
    },
    {
      "epoch": 0.07981410576027545,
      "grad_norm": 3.1435089111328125,
      "learning_rate": 0.00018447103642724454,
      "loss": 0.4528,
      "step": 16968
    },
    {
      "epoch": 0.07981880956188792,
      "grad_norm": 0.6848874092102051,
      "learning_rate": 0.00018447009344913106,
      "loss": 0.066,
      "step": 16969
    },
    {
      "epoch": 0.07982351336350038,
      "grad_norm": 1.6844160556793213,
      "learning_rate": 0.00018446915047101758,
      "loss": 0.1614,
      "step": 16970
    },
    {
      "epoch": 0.07982821716511285,
      "grad_norm": 3.853538990020752,
      "learning_rate": 0.0001844682074929041,
      "loss": 0.5149,
      "step": 16971
    },
    {
      "epoch": 0.07983292096672531,
      "grad_norm": 1.950063943862915,
      "learning_rate": 0.0001844672645147906,
      "loss": 0.2226,
      "step": 16972
    },
    {
      "epoch": 0.07983762476833776,
      "grad_norm": 1.770129919052124,
      "learning_rate": 0.00018446632153667713,
      "loss": 0.3152,
      "step": 16973
    },
    {
      "epoch": 0.07984232856995023,
      "grad_norm": 0.031005611643195152,
      "learning_rate": 0.00018446537855856368,
      "loss": 0.0015,
      "step": 16974
    },
    {
      "epoch": 0.0798470323715627,
      "grad_norm": 2.086621046066284,
      "learning_rate": 0.0001844644355804502,
      "loss": 0.1857,
      "step": 16975
    },
    {
      "epoch": 0.07985173617317516,
      "grad_norm": 2.4122111797332764,
      "learning_rate": 0.00018446349260233671,
      "loss": 0.3693,
      "step": 16976
    },
    {
      "epoch": 0.07985643997478763,
      "grad_norm": 0.6066890358924866,
      "learning_rate": 0.00018446254962422323,
      "loss": 0.0551,
      "step": 16977
    },
    {
      "epoch": 0.07986114377640009,
      "grad_norm": 2.567265033721924,
      "learning_rate": 0.00018446160664610975,
      "loss": 0.3173,
      "step": 16978
    },
    {
      "epoch": 0.07986584757801254,
      "grad_norm": 1.251288890838623,
      "learning_rate": 0.00018446066366799627,
      "loss": 0.1171,
      "step": 16979
    },
    {
      "epoch": 0.07987055137962501,
      "grad_norm": 0.9643645286560059,
      "learning_rate": 0.0001844597206898828,
      "loss": 0.1341,
      "step": 16980
    },
    {
      "epoch": 0.07987525518123748,
      "grad_norm": 1.120120644569397,
      "learning_rate": 0.0001844587777117693,
      "loss": 0.1411,
      "step": 16981
    },
    {
      "epoch": 0.07987995898284994,
      "grad_norm": 3.644251823425293,
      "learning_rate": 0.00018445783473365583,
      "loss": 0.627,
      "step": 16982
    },
    {
      "epoch": 0.0798846627844624,
      "grad_norm": 0.4732533097267151,
      "learning_rate": 0.00018445689175554237,
      "loss": 0.0203,
      "step": 16983
    },
    {
      "epoch": 0.07988936658607487,
      "grad_norm": 1.869461178779602,
      "learning_rate": 0.0001844559487774289,
      "loss": 0.1824,
      "step": 16984
    },
    {
      "epoch": 0.07989407038768732,
      "grad_norm": 0.619580090045929,
      "learning_rate": 0.0001844550057993154,
      "loss": 0.0493,
      "step": 16985
    },
    {
      "epoch": 0.07989877418929979,
      "grad_norm": 1.1070003509521484,
      "learning_rate": 0.00018445406282120193,
      "loss": 0.1086,
      "step": 16986
    },
    {
      "epoch": 0.07990347799091226,
      "grad_norm": 1.54008948802948,
      "learning_rate": 0.00018445311984308845,
      "loss": 0.1855,
      "step": 16987
    },
    {
      "epoch": 0.07990818179252472,
      "grad_norm": 1.7776206731796265,
      "learning_rate": 0.000184452176864975,
      "loss": 0.2002,
      "step": 16988
    },
    {
      "epoch": 0.07991288559413719,
      "grad_norm": 4.900177955627441,
      "learning_rate": 0.00018445123388686148,
      "loss": 1.2227,
      "step": 16989
    },
    {
      "epoch": 0.07991758939574964,
      "grad_norm": 2.502857208251953,
      "learning_rate": 0.000184450290908748,
      "loss": 0.5509,
      "step": 16990
    },
    {
      "epoch": 0.0799222931973621,
      "grad_norm": 0.6179544925689697,
      "learning_rate": 0.00018444934793063452,
      "loss": 0.0396,
      "step": 16991
    },
    {
      "epoch": 0.07992699699897457,
      "grad_norm": 0.3553517460823059,
      "learning_rate": 0.00018444840495252107,
      "loss": 0.0295,
      "step": 16992
    },
    {
      "epoch": 0.07993170080058704,
      "grad_norm": 1.8503267765045166,
      "learning_rate": 0.00018444746197440759,
      "loss": 0.2171,
      "step": 16993
    },
    {
      "epoch": 0.0799364046021995,
      "grad_norm": 2.3820199966430664,
      "learning_rate": 0.0001844465189962941,
      "loss": 0.5081,
      "step": 16994
    },
    {
      "epoch": 0.07994110840381197,
      "grad_norm": 2.111980438232422,
      "learning_rate": 0.00018444557601818062,
      "loss": 0.4165,
      "step": 16995
    },
    {
      "epoch": 0.07994581220542442,
      "grad_norm": 1.8687986135482788,
      "learning_rate": 0.00018444463304006714,
      "loss": 0.3736,
      "step": 16996
    },
    {
      "epoch": 0.07995051600703688,
      "grad_norm": 1.8448301553726196,
      "learning_rate": 0.0001844436900619537,
      "loss": 0.2535,
      "step": 16997
    },
    {
      "epoch": 0.07995521980864935,
      "grad_norm": 1.270713210105896,
      "learning_rate": 0.0001844427470838402,
      "loss": 0.1991,
      "step": 16998
    },
    {
      "epoch": 0.07995992361026182,
      "grad_norm": 0.6930466890335083,
      "learning_rate": 0.00018444180410572672,
      "loss": 0.109,
      "step": 16999
    },
    {
      "epoch": 0.07996462741187428,
      "grad_norm": 1.4338812828063965,
      "learning_rate": 0.00018444086112761322,
      "loss": 0.1404,
      "step": 17000
    },
    {
      "epoch": 0.07996933121348675,
      "grad_norm": 1.7145836353302002,
      "learning_rate": 0.00018443991814949976,
      "loss": 0.1194,
      "step": 17001
    },
    {
      "epoch": 0.0799740350150992,
      "grad_norm": 0.4665570557117462,
      "learning_rate": 0.00018443897517138628,
      "loss": 0.0457,
      "step": 17002
    },
    {
      "epoch": 0.07997873881671166,
      "grad_norm": 2.0462119579315186,
      "learning_rate": 0.0001844380321932728,
      "loss": 0.169,
      "step": 17003
    },
    {
      "epoch": 0.07998344261832413,
      "grad_norm": 2.3225436210632324,
      "learning_rate": 0.00018443708921515932,
      "loss": 0.3514,
      "step": 17004
    },
    {
      "epoch": 0.0799881464199366,
      "grad_norm": 1.9746454954147339,
      "learning_rate": 0.00018443614623704584,
      "loss": 0.2429,
      "step": 17005
    },
    {
      "epoch": 0.07999285022154906,
      "grad_norm": 0.9516522884368896,
      "learning_rate": 0.00018443520325893238,
      "loss": 0.0542,
      "step": 17006
    },
    {
      "epoch": 0.07999755402316151,
      "grad_norm": 4.0022196769714355,
      "learning_rate": 0.0001844342602808189,
      "loss": 1.1651,
      "step": 17007
    },
    {
      "epoch": 0.08000225782477398,
      "grad_norm": 2.1268351078033447,
      "learning_rate": 0.00018443331730270542,
      "loss": 0.1738,
      "step": 17008
    },
    {
      "epoch": 0.08000696162638644,
      "grad_norm": 3.9206178188323975,
      "learning_rate": 0.00018443237432459194,
      "loss": 0.3458,
      "step": 17009
    },
    {
      "epoch": 0.08001166542799891,
      "grad_norm": 0.17632873356342316,
      "learning_rate": 0.00018443143134647846,
      "loss": 0.018,
      "step": 17010
    },
    {
      "epoch": 0.08001636922961138,
      "grad_norm": 1.9288315773010254,
      "learning_rate": 0.00018443048836836498,
      "loss": 0.1414,
      "step": 17011
    },
    {
      "epoch": 0.08002107303122384,
      "grad_norm": 3.1127007007598877,
      "learning_rate": 0.0001844295453902515,
      "loss": 0.5152,
      "step": 17012
    },
    {
      "epoch": 0.08002577683283629,
      "grad_norm": 1.4088010787963867,
      "learning_rate": 0.000184428602412138,
      "loss": 0.1499,
      "step": 17013
    },
    {
      "epoch": 0.08003048063444876,
      "grad_norm": 3.735776901245117,
      "learning_rate": 0.00018442765943402453,
      "loss": 0.5577,
      "step": 17014
    },
    {
      "epoch": 0.08003518443606122,
      "grad_norm": 0.2665276825428009,
      "learning_rate": 0.00018442671645591108,
      "loss": 0.0191,
      "step": 17015
    },
    {
      "epoch": 0.08003988823767369,
      "grad_norm": 1.845147967338562,
      "learning_rate": 0.0001844257734777976,
      "loss": 0.1835,
      "step": 17016
    },
    {
      "epoch": 0.08004459203928616,
      "grad_norm": 2.1737029552459717,
      "learning_rate": 0.00018442483049968411,
      "loss": 0.2475,
      "step": 17017
    },
    {
      "epoch": 0.08004929584089862,
      "grad_norm": 2.823075294494629,
      "learning_rate": 0.00018442388752157063,
      "loss": 0.3975,
      "step": 17018
    },
    {
      "epoch": 0.08005399964251107,
      "grad_norm": 3.65815806388855,
      "learning_rate": 0.00018442294454345718,
      "loss": 1.3104,
      "step": 17019
    },
    {
      "epoch": 0.08005870344412354,
      "grad_norm": 3.8599419593811035,
      "learning_rate": 0.00018442200156534367,
      "loss": 0.4154,
      "step": 17020
    },
    {
      "epoch": 0.080063407245736,
      "grad_norm": 2.078798770904541,
      "learning_rate": 0.0001844210585872302,
      "loss": 0.3226,
      "step": 17021
    },
    {
      "epoch": 0.08006811104734847,
      "grad_norm": 11.639981269836426,
      "learning_rate": 0.0001844201156091167,
      "loss": 0.464,
      "step": 17022
    },
    {
      "epoch": 0.08007281484896094,
      "grad_norm": 2.2169864177703857,
      "learning_rate": 0.00018441917263100323,
      "loss": 0.2581,
      "step": 17023
    },
    {
      "epoch": 0.08007751865057339,
      "grad_norm": 0.4199828505516052,
      "learning_rate": 0.00018441822965288977,
      "loss": 0.0314,
      "step": 17024
    },
    {
      "epoch": 0.08008222245218585,
      "grad_norm": 0.4479508399963379,
      "learning_rate": 0.0001844172866747763,
      "loss": 0.0337,
      "step": 17025
    },
    {
      "epoch": 0.08008692625379832,
      "grad_norm": 1.2572619915008545,
      "learning_rate": 0.0001844163436966628,
      "loss": 0.0816,
      "step": 17026
    },
    {
      "epoch": 0.08009163005541078,
      "grad_norm": 2.306248188018799,
      "learning_rate": 0.00018441540071854933,
      "loss": 0.4535,
      "step": 17027
    },
    {
      "epoch": 0.08009633385702325,
      "grad_norm": 1.3013637065887451,
      "learning_rate": 0.00018441445774043587,
      "loss": 0.1335,
      "step": 17028
    },
    {
      "epoch": 0.08010103765863572,
      "grad_norm": 0.4461527168750763,
      "learning_rate": 0.0001844135147623224,
      "loss": 0.022,
      "step": 17029
    },
    {
      "epoch": 0.08010574146024817,
      "grad_norm": 2.9599926471710205,
      "learning_rate": 0.0001844125717842089,
      "loss": 0.717,
      "step": 17030
    },
    {
      "epoch": 0.08011044526186063,
      "grad_norm": 1.522857427597046,
      "learning_rate": 0.0001844116288060954,
      "loss": 0.2087,
      "step": 17031
    },
    {
      "epoch": 0.0801151490634731,
      "grad_norm": 1.9900293350219727,
      "learning_rate": 0.00018441068582798192,
      "loss": 0.2574,
      "step": 17032
    },
    {
      "epoch": 0.08011985286508556,
      "grad_norm": 0.9089001417160034,
      "learning_rate": 0.00018440974284986847,
      "loss": 0.1378,
      "step": 17033
    },
    {
      "epoch": 0.08012455666669803,
      "grad_norm": 2.137734889984131,
      "learning_rate": 0.00018440879987175499,
      "loss": 0.4886,
      "step": 17034
    },
    {
      "epoch": 0.0801292604683105,
      "grad_norm": 3.3106255531311035,
      "learning_rate": 0.0001844078568936415,
      "loss": 0.4634,
      "step": 17035
    },
    {
      "epoch": 0.08013396426992295,
      "grad_norm": 0.5908012986183167,
      "learning_rate": 0.00018440691391552802,
      "loss": 0.1018,
      "step": 17036
    },
    {
      "epoch": 0.08013866807153541,
      "grad_norm": 0.6228542327880859,
      "learning_rate": 0.00018440597093741454,
      "loss": 0.0801,
      "step": 17037
    },
    {
      "epoch": 0.08014337187314788,
      "grad_norm": 0.6542430520057678,
      "learning_rate": 0.0001844050279593011,
      "loss": 0.0606,
      "step": 17038
    },
    {
      "epoch": 0.08014807567476034,
      "grad_norm": 2.5193300247192383,
      "learning_rate": 0.0001844040849811876,
      "loss": 0.3672,
      "step": 17039
    },
    {
      "epoch": 0.08015277947637281,
      "grad_norm": 3.9231889247894287,
      "learning_rate": 0.00018440314200307412,
      "loss": 0.2983,
      "step": 17040
    },
    {
      "epoch": 0.08015748327798526,
      "grad_norm": 0.660579264163971,
      "learning_rate": 0.00018440219902496064,
      "loss": 0.0704,
      "step": 17041
    },
    {
      "epoch": 0.08016218707959773,
      "grad_norm": 2.445112705230713,
      "learning_rate": 0.00018440125604684716,
      "loss": 0.368,
      "step": 17042
    },
    {
      "epoch": 0.08016689088121019,
      "grad_norm": 1.4164584875106812,
      "learning_rate": 0.00018440031306873368,
      "loss": 0.1088,
      "step": 17043
    },
    {
      "epoch": 0.08017159468282266,
      "grad_norm": 2.1177725791931152,
      "learning_rate": 0.0001843993700906202,
      "loss": 0.2249,
      "step": 17044
    },
    {
      "epoch": 0.08017629848443512,
      "grad_norm": 3.2486650943756104,
      "learning_rate": 0.00018439842711250672,
      "loss": 0.4278,
      "step": 17045
    },
    {
      "epoch": 0.08018100228604759,
      "grad_norm": 1.0961731672286987,
      "learning_rate": 0.00018439748413439324,
      "loss": 0.0855,
      "step": 17046
    },
    {
      "epoch": 0.08018570608766004,
      "grad_norm": 3.3906755447387695,
      "learning_rate": 0.00018439654115627978,
      "loss": 0.6184,
      "step": 17047
    },
    {
      "epoch": 0.08019040988927251,
      "grad_norm": 3.5575485229492188,
      "learning_rate": 0.0001843955981781663,
      "loss": 0.4917,
      "step": 17048
    },
    {
      "epoch": 0.08019511369088497,
      "grad_norm": 2.8246240615844727,
      "learning_rate": 0.00018439465520005282,
      "loss": 0.3171,
      "step": 17049
    },
    {
      "epoch": 0.08019981749249744,
      "grad_norm": 3.7782092094421387,
      "learning_rate": 0.00018439371222193934,
      "loss": 0.7574,
      "step": 17050
    },
    {
      "epoch": 0.0802045212941099,
      "grad_norm": 1.469412088394165,
      "learning_rate": 0.00018439276924382586,
      "loss": 0.1425,
      "step": 17051
    },
    {
      "epoch": 0.08020922509572237,
      "grad_norm": 1.0909749269485474,
      "learning_rate": 0.00018439182626571238,
      "loss": 0.0607,
      "step": 17052
    },
    {
      "epoch": 0.08021392889733482,
      "grad_norm": 2.780939817428589,
      "learning_rate": 0.0001843908832875989,
      "loss": 0.2591,
      "step": 17053
    },
    {
      "epoch": 0.08021863269894729,
      "grad_norm": 2.31684947013855,
      "learning_rate": 0.0001843899403094854,
      "loss": 0.1564,
      "step": 17054
    },
    {
      "epoch": 0.08022333650055975,
      "grad_norm": 2.0137369632720947,
      "learning_rate": 0.00018438899733137193,
      "loss": 0.3154,
      "step": 17055
    },
    {
      "epoch": 0.08022804030217222,
      "grad_norm": 6.2281413078308105,
      "learning_rate": 0.00018438805435325848,
      "loss": 1.0409,
      "step": 17056
    },
    {
      "epoch": 0.08023274410378468,
      "grad_norm": 1.2228306531906128,
      "learning_rate": 0.000184387111375145,
      "loss": 0.1146,
      "step": 17057
    },
    {
      "epoch": 0.08023744790539714,
      "grad_norm": 1.8993914127349854,
      "learning_rate": 0.00018438616839703151,
      "loss": 0.1396,
      "step": 17058
    },
    {
      "epoch": 0.0802421517070096,
      "grad_norm": 3.16304349899292,
      "learning_rate": 0.00018438522541891803,
      "loss": 0.412,
      "step": 17059
    },
    {
      "epoch": 0.08024685550862207,
      "grad_norm": 3.869081974029541,
      "learning_rate": 0.00018438428244080458,
      "loss": 0.398,
      "step": 17060
    },
    {
      "epoch": 0.08025155931023453,
      "grad_norm": 1.9242480993270874,
      "learning_rate": 0.0001843833394626911,
      "loss": 0.2447,
      "step": 17061
    },
    {
      "epoch": 0.080256263111847,
      "grad_norm": 2.8618667125701904,
      "learning_rate": 0.0001843823964845776,
      "loss": 0.275,
      "step": 17062
    },
    {
      "epoch": 0.08026096691345946,
      "grad_norm": 3.3108155727386475,
      "learning_rate": 0.0001843814535064641,
      "loss": 0.5902,
      "step": 17063
    },
    {
      "epoch": 0.08026567071507192,
      "grad_norm": 4.0307207107543945,
      "learning_rate": 0.00018438051052835063,
      "loss": 0.7185,
      "step": 17064
    },
    {
      "epoch": 0.08027037451668438,
      "grad_norm": 1.9089124202728271,
      "learning_rate": 0.00018437956755023717,
      "loss": 0.1482,
      "step": 17065
    },
    {
      "epoch": 0.08027507831829685,
      "grad_norm": 2.745126724243164,
      "learning_rate": 0.0001843786245721237,
      "loss": 0.3559,
      "step": 17066
    },
    {
      "epoch": 0.08027978211990931,
      "grad_norm": 3.056875228881836,
      "learning_rate": 0.0001843776815940102,
      "loss": 0.3781,
      "step": 17067
    },
    {
      "epoch": 0.08028448592152178,
      "grad_norm": 2.0188965797424316,
      "learning_rate": 0.00018437673861589673,
      "loss": 0.2709,
      "step": 17068
    },
    {
      "epoch": 0.08028918972313424,
      "grad_norm": 0.559200644493103,
      "learning_rate": 0.00018437579563778327,
      "loss": 0.0672,
      "step": 17069
    },
    {
      "epoch": 0.0802938935247467,
      "grad_norm": 2.0011556148529053,
      "learning_rate": 0.0001843748526596698,
      "loss": 0.1907,
      "step": 17070
    },
    {
      "epoch": 0.08029859732635916,
      "grad_norm": 2.356077194213867,
      "learning_rate": 0.0001843739096815563,
      "loss": 0.3669,
      "step": 17071
    },
    {
      "epoch": 0.08030330112797163,
      "grad_norm": 5.604734897613525,
      "learning_rate": 0.00018437296670344283,
      "loss": 0.4822,
      "step": 17072
    },
    {
      "epoch": 0.08030800492958409,
      "grad_norm": 3.1420063972473145,
      "learning_rate": 0.00018437202372532935,
      "loss": 0.5253,
      "step": 17073
    },
    {
      "epoch": 0.08031270873119656,
      "grad_norm": 2.8856303691864014,
      "learning_rate": 0.00018437108074721587,
      "loss": 0.2966,
      "step": 17074
    },
    {
      "epoch": 0.08031741253280901,
      "grad_norm": 1.8379813432693481,
      "learning_rate": 0.00018437013776910239,
      "loss": 0.2498,
      "step": 17075
    },
    {
      "epoch": 0.08032211633442148,
      "grad_norm": 0.6904669404029846,
      "learning_rate": 0.0001843691947909889,
      "loss": 0.0819,
      "step": 17076
    },
    {
      "epoch": 0.08032682013603394,
      "grad_norm": 0.8544715642929077,
      "learning_rate": 0.00018436825181287542,
      "loss": 0.1313,
      "step": 17077
    },
    {
      "epoch": 0.0803315239376464,
      "grad_norm": 2.027973175048828,
      "learning_rate": 0.00018436730883476197,
      "loss": 0.4436,
      "step": 17078
    },
    {
      "epoch": 0.08033622773925887,
      "grad_norm": 0.8844742774963379,
      "learning_rate": 0.0001843663658566485,
      "loss": 0.0917,
      "step": 17079
    },
    {
      "epoch": 0.08034093154087134,
      "grad_norm": 2.069084405899048,
      "learning_rate": 0.000184365422878535,
      "loss": 0.2476,
      "step": 17080
    },
    {
      "epoch": 0.08034563534248379,
      "grad_norm": 2.884671688079834,
      "learning_rate": 0.00018436447990042152,
      "loss": 0.2484,
      "step": 17081
    },
    {
      "epoch": 0.08035033914409626,
      "grad_norm": 1.0669786930084229,
      "learning_rate": 0.00018436353692230804,
      "loss": 0.1372,
      "step": 17082
    },
    {
      "epoch": 0.08035504294570872,
      "grad_norm": 1.8298304080963135,
      "learning_rate": 0.00018436259394419456,
      "loss": 0.2877,
      "step": 17083
    },
    {
      "epoch": 0.08035974674732119,
      "grad_norm": 0.5405109524726868,
      "learning_rate": 0.00018436165096608108,
      "loss": 0.0326,
      "step": 17084
    },
    {
      "epoch": 0.08036445054893365,
      "grad_norm": 0.4778216779232025,
      "learning_rate": 0.0001843607079879676,
      "loss": 0.0338,
      "step": 17085
    },
    {
      "epoch": 0.08036915435054612,
      "grad_norm": 5.3667778968811035,
      "learning_rate": 0.00018435976500985412,
      "loss": 0.4472,
      "step": 17086
    },
    {
      "epoch": 0.08037385815215857,
      "grad_norm": 1.889936089515686,
      "learning_rate": 0.00018435882203174064,
      "loss": 0.2579,
      "step": 17087
    },
    {
      "epoch": 0.08037856195377104,
      "grad_norm": 3.8826048374176025,
      "learning_rate": 0.00018435787905362718,
      "loss": 0.6615,
      "step": 17088
    },
    {
      "epoch": 0.0803832657553835,
      "grad_norm": 1.6654956340789795,
      "learning_rate": 0.0001843569360755137,
      "loss": 0.1558,
      "step": 17089
    },
    {
      "epoch": 0.08038796955699597,
      "grad_norm": 3.5770533084869385,
      "learning_rate": 0.00018435599309740022,
      "loss": 0.4132,
      "step": 17090
    },
    {
      "epoch": 0.08039267335860843,
      "grad_norm": 1.2366245985031128,
      "learning_rate": 0.00018435505011928674,
      "loss": 0.1391,
      "step": 17091
    },
    {
      "epoch": 0.08039737716022088,
      "grad_norm": 2.2338128089904785,
      "learning_rate": 0.00018435410714117328,
      "loss": 0.5537,
      "step": 17092
    },
    {
      "epoch": 0.08040208096183335,
      "grad_norm": 2.4468932151794434,
      "learning_rate": 0.00018435316416305978,
      "loss": 0.5124,
      "step": 17093
    },
    {
      "epoch": 0.08040678476344582,
      "grad_norm": 1.8942304849624634,
      "learning_rate": 0.0001843522211849463,
      "loss": 0.2682,
      "step": 17094
    },
    {
      "epoch": 0.08041148856505828,
      "grad_norm": 1.534234642982483,
      "learning_rate": 0.0001843512782068328,
      "loss": 0.3024,
      "step": 17095
    },
    {
      "epoch": 0.08041619236667075,
      "grad_norm": 1.348705530166626,
      "learning_rate": 0.00018435033522871933,
      "loss": 0.2274,
      "step": 17096
    },
    {
      "epoch": 0.08042089616828321,
      "grad_norm": 0.15605929493904114,
      "learning_rate": 0.00018434939225060588,
      "loss": 0.0112,
      "step": 17097
    },
    {
      "epoch": 0.08042559996989566,
      "grad_norm": 0.5265796184539795,
      "learning_rate": 0.0001843484492724924,
      "loss": 0.047,
      "step": 17098
    },
    {
      "epoch": 0.08043030377150813,
      "grad_norm": 2.3798303604125977,
      "learning_rate": 0.00018434750629437891,
      "loss": 0.6506,
      "step": 17099
    },
    {
      "epoch": 0.0804350075731206,
      "grad_norm": 5.369828224182129,
      "learning_rate": 0.00018434656331626543,
      "loss": 0.5007,
      "step": 17100
    },
    {
      "epoch": 0.08043971137473306,
      "grad_norm": 1.6185109615325928,
      "learning_rate": 0.00018434562033815198,
      "loss": 0.1324,
      "step": 17101
    },
    {
      "epoch": 0.08044441517634553,
      "grad_norm": 0.9310922622680664,
      "learning_rate": 0.0001843446773600385,
      "loss": 0.0708,
      "step": 17102
    },
    {
      "epoch": 0.08044911897795799,
      "grad_norm": 2.058513641357422,
      "learning_rate": 0.00018434373438192502,
      "loss": 0.1593,
      "step": 17103
    },
    {
      "epoch": 0.08045382277957044,
      "grad_norm": 2.1476895809173584,
      "learning_rate": 0.00018434279140381153,
      "loss": 0.1965,
      "step": 17104
    },
    {
      "epoch": 0.08045852658118291,
      "grad_norm": 2.835753917694092,
      "learning_rate": 0.00018434184842569803,
      "loss": 0.3392,
      "step": 17105
    },
    {
      "epoch": 0.08046323038279538,
      "grad_norm": 0.30131596326828003,
      "learning_rate": 0.00018434090544758457,
      "loss": 0.0496,
      "step": 17106
    },
    {
      "epoch": 0.08046793418440784,
      "grad_norm": 1.9806550741195679,
      "learning_rate": 0.0001843399624694711,
      "loss": 0.1863,
      "step": 17107
    },
    {
      "epoch": 0.0804726379860203,
      "grad_norm": 1.0215851068496704,
      "learning_rate": 0.0001843390194913576,
      "loss": 0.0688,
      "step": 17108
    },
    {
      "epoch": 0.08047734178763276,
      "grad_norm": 1.4289100170135498,
      "learning_rate": 0.00018433807651324413,
      "loss": 0.1205,
      "step": 17109
    },
    {
      "epoch": 0.08048204558924522,
      "grad_norm": 3.41098690032959,
      "learning_rate": 0.00018433713353513067,
      "loss": 0.3099,
      "step": 17110
    },
    {
      "epoch": 0.08048674939085769,
      "grad_norm": 3.9602317810058594,
      "learning_rate": 0.0001843361905570172,
      "loss": 0.4834,
      "step": 17111
    },
    {
      "epoch": 0.08049145319247016,
      "grad_norm": 4.232567310333252,
      "learning_rate": 0.0001843352475789037,
      "loss": 0.3395,
      "step": 17112
    },
    {
      "epoch": 0.08049615699408262,
      "grad_norm": 11.276799201965332,
      "learning_rate": 0.00018433430460079023,
      "loss": 0.5947,
      "step": 17113
    },
    {
      "epoch": 0.08050086079569509,
      "grad_norm": 0.16037963330745697,
      "learning_rate": 0.00018433336162267675,
      "loss": 0.0135,
      "step": 17114
    },
    {
      "epoch": 0.08050556459730754,
      "grad_norm": 1.3979806900024414,
      "learning_rate": 0.00018433241864456327,
      "loss": 0.111,
      "step": 17115
    },
    {
      "epoch": 0.08051026839892,
      "grad_norm": 1.3508886098861694,
      "learning_rate": 0.00018433147566644979,
      "loss": 0.1012,
      "step": 17116
    },
    {
      "epoch": 0.08051497220053247,
      "grad_norm": 0.4413391947746277,
      "learning_rate": 0.0001843305326883363,
      "loss": 0.039,
      "step": 17117
    },
    {
      "epoch": 0.08051967600214494,
      "grad_norm": 2.8552160263061523,
      "learning_rate": 0.00018432958971022282,
      "loss": 0.6497,
      "step": 17118
    },
    {
      "epoch": 0.0805243798037574,
      "grad_norm": 2.7764358520507812,
      "learning_rate": 0.00018432864673210937,
      "loss": 0.4397,
      "step": 17119
    },
    {
      "epoch": 0.08052908360536987,
      "grad_norm": 23.699892044067383,
      "learning_rate": 0.0001843277037539959,
      "loss": 0.4778,
      "step": 17120
    },
    {
      "epoch": 0.08053378740698232,
      "grad_norm": 1.5086054801940918,
      "learning_rate": 0.0001843267607758824,
      "loss": 0.2105,
      "step": 17121
    },
    {
      "epoch": 0.08053849120859478,
      "grad_norm": 1.2625820636749268,
      "learning_rate": 0.00018432581779776892,
      "loss": 0.0992,
      "step": 17122
    },
    {
      "epoch": 0.08054319501020725,
      "grad_norm": 2.656952142715454,
      "learning_rate": 0.00018432487481965544,
      "loss": 0.3008,
      "step": 17123
    },
    {
      "epoch": 0.08054789881181972,
      "grad_norm": 0.1969812512397766,
      "learning_rate": 0.00018432393184154196,
      "loss": 0.0141,
      "step": 17124
    },
    {
      "epoch": 0.08055260261343218,
      "grad_norm": 1.62095308303833,
      "learning_rate": 0.00018432298886342848,
      "loss": 0.1481,
      "step": 17125
    },
    {
      "epoch": 0.08055730641504463,
      "grad_norm": 2.9118525981903076,
      "learning_rate": 0.000184322045885315,
      "loss": 0.2004,
      "step": 17126
    },
    {
      "epoch": 0.0805620102166571,
      "grad_norm": 2.037424087524414,
      "learning_rate": 0.00018432110290720152,
      "loss": 0.1693,
      "step": 17127
    },
    {
      "epoch": 0.08056671401826956,
      "grad_norm": 2.968143939971924,
      "learning_rate": 0.00018432015992908806,
      "loss": 0.3859,
      "step": 17128
    },
    {
      "epoch": 0.08057141781988203,
      "grad_norm": 0.5404237508773804,
      "learning_rate": 0.00018431921695097458,
      "loss": 0.0505,
      "step": 17129
    },
    {
      "epoch": 0.0805761216214945,
      "grad_norm": 3.46590256690979,
      "learning_rate": 0.0001843182739728611,
      "loss": 0.7412,
      "step": 17130
    },
    {
      "epoch": 0.08058082542310696,
      "grad_norm": 3.730339288711548,
      "learning_rate": 0.00018431733099474762,
      "loss": 0.4837,
      "step": 17131
    },
    {
      "epoch": 0.08058552922471941,
      "grad_norm": 3.3204264640808105,
      "learning_rate": 0.00018431638801663414,
      "loss": 0.212,
      "step": 17132
    },
    {
      "epoch": 0.08059023302633188,
      "grad_norm": 1.3507319688796997,
      "learning_rate": 0.00018431544503852068,
      "loss": 0.3022,
      "step": 17133
    },
    {
      "epoch": 0.08059493682794434,
      "grad_norm": 1.9572242498397827,
      "learning_rate": 0.0001843145020604072,
      "loss": 0.198,
      "step": 17134
    },
    {
      "epoch": 0.08059964062955681,
      "grad_norm": 0.5474327802658081,
      "learning_rate": 0.00018431355908229372,
      "loss": 0.0489,
      "step": 17135
    },
    {
      "epoch": 0.08060434443116928,
      "grad_norm": 1.8960485458374023,
      "learning_rate": 0.0001843126161041802,
      "loss": 0.186,
      "step": 17136
    },
    {
      "epoch": 0.08060904823278174,
      "grad_norm": 3.2332301139831543,
      "learning_rate": 0.00018431167312606673,
      "loss": 0.6309,
      "step": 17137
    },
    {
      "epoch": 0.08061375203439419,
      "grad_norm": 1.4746142625808716,
      "learning_rate": 0.00018431073014795328,
      "loss": 0.2805,
      "step": 17138
    },
    {
      "epoch": 0.08061845583600666,
      "grad_norm": 0.2940719723701477,
      "learning_rate": 0.0001843097871698398,
      "loss": 0.0543,
      "step": 17139
    },
    {
      "epoch": 0.08062315963761912,
      "grad_norm": 1.3991619348526,
      "learning_rate": 0.00018430884419172631,
      "loss": 0.1566,
      "step": 17140
    },
    {
      "epoch": 0.08062786343923159,
      "grad_norm": 2.3218538761138916,
      "learning_rate": 0.00018430790121361283,
      "loss": 0.3267,
      "step": 17141
    },
    {
      "epoch": 0.08063256724084406,
      "grad_norm": 2.256300687789917,
      "learning_rate": 0.00018430695823549938,
      "loss": 0.0992,
      "step": 17142
    },
    {
      "epoch": 0.08063727104245651,
      "grad_norm": 1.658759593963623,
      "learning_rate": 0.0001843060152573859,
      "loss": 0.4028,
      "step": 17143
    },
    {
      "epoch": 0.08064197484406897,
      "grad_norm": 3.758697271347046,
      "learning_rate": 0.00018430507227927242,
      "loss": 0.6454,
      "step": 17144
    },
    {
      "epoch": 0.08064667864568144,
      "grad_norm": 0.7776514291763306,
      "learning_rate": 0.00018430412930115893,
      "loss": 0.089,
      "step": 17145
    },
    {
      "epoch": 0.0806513824472939,
      "grad_norm": 0.9788488745689392,
      "learning_rate": 0.00018430318632304545,
      "loss": 0.0973,
      "step": 17146
    },
    {
      "epoch": 0.08065608624890637,
      "grad_norm": 2.3346357345581055,
      "learning_rate": 0.00018430224334493197,
      "loss": 0.6349,
      "step": 17147
    },
    {
      "epoch": 0.08066079005051884,
      "grad_norm": 2.8601911067962646,
      "learning_rate": 0.0001843013003668185,
      "loss": 0.7785,
      "step": 17148
    },
    {
      "epoch": 0.08066549385213129,
      "grad_norm": 1.5447944402694702,
      "learning_rate": 0.000184300357388705,
      "loss": 0.2603,
      "step": 17149
    },
    {
      "epoch": 0.08067019765374375,
      "grad_norm": 1.334416151046753,
      "learning_rate": 0.00018429941441059153,
      "loss": 0.2738,
      "step": 17150
    },
    {
      "epoch": 0.08067490145535622,
      "grad_norm": 9.91614818572998,
      "learning_rate": 0.00018429847143247807,
      "loss": 0.2473,
      "step": 17151
    },
    {
      "epoch": 0.08067960525696868,
      "grad_norm": 0.8686787486076355,
      "learning_rate": 0.0001842975284543646,
      "loss": 0.0577,
      "step": 17152
    },
    {
      "epoch": 0.08068430905858115,
      "grad_norm": 7.602344512939453,
      "learning_rate": 0.0001842965854762511,
      "loss": 0.4357,
      "step": 17153
    },
    {
      "epoch": 0.08068901286019362,
      "grad_norm": 2.484952211380005,
      "learning_rate": 0.00018429564249813763,
      "loss": 0.2801,
      "step": 17154
    },
    {
      "epoch": 0.08069371666180607,
      "grad_norm": 0.29574036598205566,
      "learning_rate": 0.00018429469952002415,
      "loss": 0.0444,
      "step": 17155
    },
    {
      "epoch": 0.08069842046341853,
      "grad_norm": 3.4660937786102295,
      "learning_rate": 0.00018429375654191067,
      "loss": 0.9226,
      "step": 17156
    },
    {
      "epoch": 0.080703124265031,
      "grad_norm": 2.0947532653808594,
      "learning_rate": 0.00018429281356379719,
      "loss": 0.5964,
      "step": 17157
    },
    {
      "epoch": 0.08070782806664346,
      "grad_norm": 1.2771151065826416,
      "learning_rate": 0.0001842918705856837,
      "loss": 0.2263,
      "step": 17158
    },
    {
      "epoch": 0.08071253186825593,
      "grad_norm": 0.8222874999046326,
      "learning_rate": 0.00018429092760757022,
      "loss": 0.0997,
      "step": 17159
    },
    {
      "epoch": 0.08071723566986838,
      "grad_norm": 1.796820044517517,
      "learning_rate": 0.00018428998462945677,
      "loss": 0.2624,
      "step": 17160
    },
    {
      "epoch": 0.08072193947148085,
      "grad_norm": 11.3533353805542,
      "learning_rate": 0.0001842890416513433,
      "loss": 0.2794,
      "step": 17161
    },
    {
      "epoch": 0.08072664327309331,
      "grad_norm": 1.0387893915176392,
      "learning_rate": 0.0001842880986732298,
      "loss": 0.0776,
      "step": 17162
    },
    {
      "epoch": 0.08073134707470578,
      "grad_norm": 0.12859442830085754,
      "learning_rate": 0.00018428715569511632,
      "loss": 0.0098,
      "step": 17163
    },
    {
      "epoch": 0.08073605087631824,
      "grad_norm": 1.2406389713287354,
      "learning_rate": 0.00018428621271700284,
      "loss": 0.1225,
      "step": 17164
    },
    {
      "epoch": 0.08074075467793071,
      "grad_norm": 1.4792523384094238,
      "learning_rate": 0.0001842852697388894,
      "loss": 0.153,
      "step": 17165
    },
    {
      "epoch": 0.08074545847954316,
      "grad_norm": 1.4226785898208618,
      "learning_rate": 0.0001842843267607759,
      "loss": 0.1395,
      "step": 17166
    },
    {
      "epoch": 0.08075016228115563,
      "grad_norm": 1.525881290435791,
      "learning_rate": 0.0001842833837826624,
      "loss": 0.2184,
      "step": 17167
    },
    {
      "epoch": 0.08075486608276809,
      "grad_norm": 1.5064952373504639,
      "learning_rate": 0.00018428244080454892,
      "loss": 0.1521,
      "step": 17168
    },
    {
      "epoch": 0.08075956988438056,
      "grad_norm": 3.196929693222046,
      "learning_rate": 0.00018428149782643546,
      "loss": 0.623,
      "step": 17169
    },
    {
      "epoch": 0.08076427368599302,
      "grad_norm": 1.9772273302078247,
      "learning_rate": 0.00018428055484832198,
      "loss": 0.4782,
      "step": 17170
    },
    {
      "epoch": 0.08076897748760549,
      "grad_norm": 2.2567086219787598,
      "learning_rate": 0.0001842796118702085,
      "loss": 0.2854,
      "step": 17171
    },
    {
      "epoch": 0.08077368128921794,
      "grad_norm": 4.156668663024902,
      "learning_rate": 0.00018427866889209502,
      "loss": 0.753,
      "step": 17172
    },
    {
      "epoch": 0.0807783850908304,
      "grad_norm": 2.3064422607421875,
      "learning_rate": 0.00018427772591398154,
      "loss": 0.2255,
      "step": 17173
    },
    {
      "epoch": 0.08078308889244287,
      "grad_norm": 0.5940534472465515,
      "learning_rate": 0.00018427678293586808,
      "loss": 0.09,
      "step": 17174
    },
    {
      "epoch": 0.08078779269405534,
      "grad_norm": 0.6350789666175842,
      "learning_rate": 0.0001842758399577546,
      "loss": 0.0603,
      "step": 17175
    },
    {
      "epoch": 0.0807924964956678,
      "grad_norm": 0.7402081489562988,
      "learning_rate": 0.00018427489697964112,
      "loss": 0.0847,
      "step": 17176
    },
    {
      "epoch": 0.08079720029728026,
      "grad_norm": 3.3689630031585693,
      "learning_rate": 0.00018427395400152764,
      "loss": 0.2488,
      "step": 17177
    },
    {
      "epoch": 0.08080190409889272,
      "grad_norm": 2.4545207023620605,
      "learning_rate": 0.00018427301102341416,
      "loss": 0.4801,
      "step": 17178
    },
    {
      "epoch": 0.08080660790050519,
      "grad_norm": 2.210836887359619,
      "learning_rate": 0.00018427206804530068,
      "loss": 0.2754,
      "step": 17179
    },
    {
      "epoch": 0.08081131170211765,
      "grad_norm": 2.805217981338501,
      "learning_rate": 0.0001842711250671872,
      "loss": 0.5608,
      "step": 17180
    },
    {
      "epoch": 0.08081601550373012,
      "grad_norm": 2.53774094581604,
      "learning_rate": 0.00018427018208907371,
      "loss": 0.3021,
      "step": 17181
    },
    {
      "epoch": 0.08082071930534258,
      "grad_norm": 1.9204611778259277,
      "learning_rate": 0.00018426923911096023,
      "loss": 0.1772,
      "step": 17182
    },
    {
      "epoch": 0.08082542310695504,
      "grad_norm": 1.710180401802063,
      "learning_rate": 0.00018426829613284678,
      "loss": 0.3442,
      "step": 17183
    },
    {
      "epoch": 0.0808301269085675,
      "grad_norm": 2.263042688369751,
      "learning_rate": 0.0001842673531547333,
      "loss": 0.4391,
      "step": 17184
    },
    {
      "epoch": 0.08083483071017997,
      "grad_norm": 0.5533117055892944,
      "learning_rate": 0.00018426641017661982,
      "loss": 0.0536,
      "step": 17185
    },
    {
      "epoch": 0.08083953451179243,
      "grad_norm": 0.7231817245483398,
      "learning_rate": 0.00018426546719850633,
      "loss": 0.1045,
      "step": 17186
    },
    {
      "epoch": 0.0808442383134049,
      "grad_norm": 2.3243374824523926,
      "learning_rate": 0.00018426452422039285,
      "loss": 0.6638,
      "step": 17187
    },
    {
      "epoch": 0.08084894211501736,
      "grad_norm": 2.468900203704834,
      "learning_rate": 0.00018426358124227937,
      "loss": 0.2224,
      "step": 17188
    },
    {
      "epoch": 0.08085364591662982,
      "grad_norm": 1.7410575151443481,
      "learning_rate": 0.0001842626382641659,
      "loss": 0.3945,
      "step": 17189
    },
    {
      "epoch": 0.08085834971824228,
      "grad_norm": 0.6158996820449829,
      "learning_rate": 0.0001842616952860524,
      "loss": 0.076,
      "step": 17190
    },
    {
      "epoch": 0.08086305351985475,
      "grad_norm": 0.7259823679924011,
      "learning_rate": 0.00018426075230793893,
      "loss": 0.0933,
      "step": 17191
    },
    {
      "epoch": 0.08086775732146721,
      "grad_norm": 1.5735108852386475,
      "learning_rate": 0.00018425980932982547,
      "loss": 0.5156,
      "step": 17192
    },
    {
      "epoch": 0.08087246112307968,
      "grad_norm": 0.42072904109954834,
      "learning_rate": 0.000184258866351712,
      "loss": 0.0456,
      "step": 17193
    },
    {
      "epoch": 0.08087716492469213,
      "grad_norm": 2.5036613941192627,
      "learning_rate": 0.0001842579233735985,
      "loss": 0.4225,
      "step": 17194
    },
    {
      "epoch": 0.0808818687263046,
      "grad_norm": 2.1172099113464355,
      "learning_rate": 0.00018425698039548503,
      "loss": 0.2191,
      "step": 17195
    },
    {
      "epoch": 0.08088657252791706,
      "grad_norm": 1.064052939414978,
      "learning_rate": 0.00018425603741737155,
      "loss": 0.1223,
      "step": 17196
    },
    {
      "epoch": 0.08089127632952953,
      "grad_norm": 2.1901965141296387,
      "learning_rate": 0.0001842550944392581,
      "loss": 0.3741,
      "step": 17197
    },
    {
      "epoch": 0.08089598013114199,
      "grad_norm": 1.4431509971618652,
      "learning_rate": 0.00018425415146114459,
      "loss": 0.2806,
      "step": 17198
    },
    {
      "epoch": 0.08090068393275446,
      "grad_norm": 3.018235683441162,
      "learning_rate": 0.0001842532084830311,
      "loss": 0.8755,
      "step": 17199
    },
    {
      "epoch": 0.08090538773436691,
      "grad_norm": 1.3976528644561768,
      "learning_rate": 0.00018425226550491762,
      "loss": 0.2036,
      "step": 17200
    },
    {
      "epoch": 0.08091009153597938,
      "grad_norm": 1.4418028593063354,
      "learning_rate": 0.00018425132252680417,
      "loss": 0.1088,
      "step": 17201
    },
    {
      "epoch": 0.08091479533759184,
      "grad_norm": 2.132854461669922,
      "learning_rate": 0.0001842503795486907,
      "loss": 0.1947,
      "step": 17202
    },
    {
      "epoch": 0.0809194991392043,
      "grad_norm": 1.4989036321640015,
      "learning_rate": 0.0001842494365705772,
      "loss": 0.1335,
      "step": 17203
    },
    {
      "epoch": 0.08092420294081677,
      "grad_norm": 1.3212751150131226,
      "learning_rate": 0.00018424849359246372,
      "loss": 0.1775,
      "step": 17204
    },
    {
      "epoch": 0.08092890674242924,
      "grad_norm": 2.7827138900756836,
      "learning_rate": 0.00018424755061435024,
      "loss": 0.1605,
      "step": 17205
    },
    {
      "epoch": 0.08093361054404169,
      "grad_norm": 1.3481656312942505,
      "learning_rate": 0.0001842466076362368,
      "loss": 0.0834,
      "step": 17206
    },
    {
      "epoch": 0.08093831434565416,
      "grad_norm": 2.0527095794677734,
      "learning_rate": 0.0001842456646581233,
      "loss": 0.1683,
      "step": 17207
    },
    {
      "epoch": 0.08094301814726662,
      "grad_norm": 1.6556522846221924,
      "learning_rate": 0.00018424472168000983,
      "loss": 0.178,
      "step": 17208
    },
    {
      "epoch": 0.08094772194887909,
      "grad_norm": 0.41987717151641846,
      "learning_rate": 0.00018424377870189632,
      "loss": 0.0195,
      "step": 17209
    },
    {
      "epoch": 0.08095242575049155,
      "grad_norm": 2.625882863998413,
      "learning_rate": 0.00018424283572378286,
      "loss": 0.4885,
      "step": 17210
    },
    {
      "epoch": 0.080957129552104,
      "grad_norm": 1.8292862176895142,
      "learning_rate": 0.00018424189274566938,
      "loss": 0.3836,
      "step": 17211
    },
    {
      "epoch": 0.08096183335371647,
      "grad_norm": 3.4573066234588623,
      "learning_rate": 0.0001842409497675559,
      "loss": 0.426,
      "step": 17212
    },
    {
      "epoch": 0.08096653715532894,
      "grad_norm": 3.217432737350464,
      "learning_rate": 0.00018424000678944242,
      "loss": 0.3929,
      "step": 17213
    },
    {
      "epoch": 0.0809712409569414,
      "grad_norm": 3.8764140605926514,
      "learning_rate": 0.00018423906381132894,
      "loss": 0.5886,
      "step": 17214
    },
    {
      "epoch": 0.08097594475855387,
      "grad_norm": 0.7846987247467041,
      "learning_rate": 0.00018423812083321548,
      "loss": 0.0729,
      "step": 17215
    },
    {
      "epoch": 0.08098064856016633,
      "grad_norm": 3.0078554153442383,
      "learning_rate": 0.000184237177855102,
      "loss": 0.6185,
      "step": 17216
    },
    {
      "epoch": 0.08098535236177878,
      "grad_norm": 4.139603614807129,
      "learning_rate": 0.00018423623487698852,
      "loss": 0.8901,
      "step": 17217
    },
    {
      "epoch": 0.08099005616339125,
      "grad_norm": 2.197456121444702,
      "learning_rate": 0.00018423529189887504,
      "loss": 0.1686,
      "step": 17218
    },
    {
      "epoch": 0.08099475996500372,
      "grad_norm": 0.34403106570243835,
      "learning_rate": 0.00018423434892076156,
      "loss": 0.0343,
      "step": 17219
    },
    {
      "epoch": 0.08099946376661618,
      "grad_norm": 1.4443440437316895,
      "learning_rate": 0.00018423340594264808,
      "loss": 0.2602,
      "step": 17220
    },
    {
      "epoch": 0.08100416756822865,
      "grad_norm": 1.8456623554229736,
      "learning_rate": 0.0001842324629645346,
      "loss": 0.2919,
      "step": 17221
    },
    {
      "epoch": 0.08100887136984111,
      "grad_norm": 1.7565377950668335,
      "learning_rate": 0.00018423151998642111,
      "loss": 0.466,
      "step": 17222
    },
    {
      "epoch": 0.08101357517145356,
      "grad_norm": 1.4000352621078491,
      "learning_rate": 0.00018423057700830763,
      "loss": 0.3288,
      "step": 17223
    },
    {
      "epoch": 0.08101827897306603,
      "grad_norm": 2.890901565551758,
      "learning_rate": 0.00018422963403019418,
      "loss": 0.4497,
      "step": 17224
    },
    {
      "epoch": 0.0810229827746785,
      "grad_norm": 2.3363749980926514,
      "learning_rate": 0.0001842286910520807,
      "loss": 0.7291,
      "step": 17225
    },
    {
      "epoch": 0.08102768657629096,
      "grad_norm": 0.8132007122039795,
      "learning_rate": 0.00018422774807396722,
      "loss": 0.1056,
      "step": 17226
    },
    {
      "epoch": 0.08103239037790343,
      "grad_norm": 2.3797271251678467,
      "learning_rate": 0.00018422680509585373,
      "loss": 0.4335,
      "step": 17227
    },
    {
      "epoch": 0.08103709417951588,
      "grad_norm": 1.683292269706726,
      "learning_rate": 0.00018422586211774025,
      "loss": 0.3924,
      "step": 17228
    },
    {
      "epoch": 0.08104179798112834,
      "grad_norm": 3.4079582691192627,
      "learning_rate": 0.00018422491913962677,
      "loss": 0.3588,
      "step": 17229
    },
    {
      "epoch": 0.08104650178274081,
      "grad_norm": 2.3238048553466797,
      "learning_rate": 0.0001842239761615133,
      "loss": 0.4993,
      "step": 17230
    },
    {
      "epoch": 0.08105120558435328,
      "grad_norm": 1.5071536302566528,
      "learning_rate": 0.0001842230331833998,
      "loss": 0.2613,
      "step": 17231
    },
    {
      "epoch": 0.08105590938596574,
      "grad_norm": 0.9918982982635498,
      "learning_rate": 0.00018422209020528633,
      "loss": 0.1437,
      "step": 17232
    },
    {
      "epoch": 0.0810606131875782,
      "grad_norm": 0.8703712224960327,
      "learning_rate": 0.00018422114722717287,
      "loss": 0.1192,
      "step": 17233
    },
    {
      "epoch": 0.08106531698919066,
      "grad_norm": 0.8737093210220337,
      "learning_rate": 0.0001842202042490594,
      "loss": 0.2144,
      "step": 17234
    },
    {
      "epoch": 0.08107002079080312,
      "grad_norm": 1.3911277055740356,
      "learning_rate": 0.0001842192612709459,
      "loss": 0.2227,
      "step": 17235
    },
    {
      "epoch": 0.08107472459241559,
      "grad_norm": 1.1865047216415405,
      "learning_rate": 0.00018421831829283243,
      "loss": 0.1824,
      "step": 17236
    },
    {
      "epoch": 0.08107942839402806,
      "grad_norm": 1.0246849060058594,
      "learning_rate": 0.00018421737531471898,
      "loss": 0.2183,
      "step": 17237
    },
    {
      "epoch": 0.08108413219564052,
      "grad_norm": 2.024228811264038,
      "learning_rate": 0.0001842164323366055,
      "loss": 0.5321,
      "step": 17238
    },
    {
      "epoch": 0.08108883599725299,
      "grad_norm": 2.2558019161224365,
      "learning_rate": 0.000184215489358492,
      "loss": 0.4934,
      "step": 17239
    },
    {
      "epoch": 0.08109353979886544,
      "grad_norm": 0.7949623465538025,
      "learning_rate": 0.0001842145463803785,
      "loss": 0.1912,
      "step": 17240
    },
    {
      "epoch": 0.0810982436004779,
      "grad_norm": 0.9051221609115601,
      "learning_rate": 0.00018421360340226502,
      "loss": 0.3062,
      "step": 17241
    },
    {
      "epoch": 0.08110294740209037,
      "grad_norm": 1.7319254875183105,
      "learning_rate": 0.00018421266042415157,
      "loss": 0.3647,
      "step": 17242
    },
    {
      "epoch": 0.08110765120370284,
      "grad_norm": 0.6309303045272827,
      "learning_rate": 0.0001842117174460381,
      "loss": 0.1334,
      "step": 17243
    },
    {
      "epoch": 0.0811123550053153,
      "grad_norm": 1.1349915266036987,
      "learning_rate": 0.0001842107744679246,
      "loss": 0.3457,
      "step": 17244
    },
    {
      "epoch": 0.08111705880692775,
      "grad_norm": 1.5987436771392822,
      "learning_rate": 0.00018420983148981112,
      "loss": 0.4252,
      "step": 17245
    },
    {
      "epoch": 0.08112176260854022,
      "grad_norm": 1.0050681829452515,
      "learning_rate": 0.00018420888851169764,
      "loss": 0.32,
      "step": 17246
    },
    {
      "epoch": 0.08112646641015268,
      "grad_norm": 1.2449281215667725,
      "learning_rate": 0.0001842079455335842,
      "loss": 0.1527,
      "step": 17247
    },
    {
      "epoch": 0.08113117021176515,
      "grad_norm": 1.2192175388336182,
      "learning_rate": 0.0001842070025554707,
      "loss": 0.1798,
      "step": 17248
    },
    {
      "epoch": 0.08113587401337762,
      "grad_norm": 0.6758894920349121,
      "learning_rate": 0.00018420605957735723,
      "loss": 0.1192,
      "step": 17249
    },
    {
      "epoch": 0.08114057781499008,
      "grad_norm": 0.4657156765460968,
      "learning_rate": 0.00018420511659924374,
      "loss": 0.0788,
      "step": 17250
    },
    {
      "epoch": 0.08114528161660253,
      "grad_norm": 2.33957576751709,
      "learning_rate": 0.00018420417362113026,
      "loss": 0.1493,
      "step": 17251
    },
    {
      "epoch": 0.081149985418215,
      "grad_norm": 3.382072687149048,
      "learning_rate": 0.00018420323064301678,
      "loss": 0.3098,
      "step": 17252
    },
    {
      "epoch": 0.08115468921982746,
      "grad_norm": 3.0740387439727783,
      "learning_rate": 0.0001842022876649033,
      "loss": 0.3493,
      "step": 17253
    },
    {
      "epoch": 0.08115939302143993,
      "grad_norm": 3.6889729499816895,
      "learning_rate": 0.00018420134468678982,
      "loss": 0.54,
      "step": 17254
    },
    {
      "epoch": 0.0811640968230524,
      "grad_norm": 6.202322006225586,
      "learning_rate": 0.00018420040170867634,
      "loss": 1.0208,
      "step": 17255
    },
    {
      "epoch": 0.08116880062466486,
      "grad_norm": 3.611098527908325,
      "learning_rate": 0.00018419945873056288,
      "loss": 0.5896,
      "step": 17256
    },
    {
      "epoch": 0.08117350442627731,
      "grad_norm": 0.8372430205345154,
      "learning_rate": 0.0001841985157524494,
      "loss": 0.0893,
      "step": 17257
    },
    {
      "epoch": 0.08117820822788978,
      "grad_norm": 1.523884892463684,
      "learning_rate": 0.00018419757277433592,
      "loss": 0.2271,
      "step": 17258
    },
    {
      "epoch": 0.08118291202950224,
      "grad_norm": 2.726547956466675,
      "learning_rate": 0.00018419662979622244,
      "loss": 0.5568,
      "step": 17259
    },
    {
      "epoch": 0.08118761583111471,
      "grad_norm": 1.5283886194229126,
      "learning_rate": 0.00018419568681810896,
      "loss": 0.2102,
      "step": 17260
    },
    {
      "epoch": 0.08119231963272718,
      "grad_norm": 3.6028335094451904,
      "learning_rate": 0.00018419474383999548,
      "loss": 0.5041,
      "step": 17261
    },
    {
      "epoch": 0.08119702343433963,
      "grad_norm": 1.7251451015472412,
      "learning_rate": 0.000184193800861882,
      "loss": 0.3011,
      "step": 17262
    },
    {
      "epoch": 0.08120172723595209,
      "grad_norm": 1.2937175035476685,
      "learning_rate": 0.00018419285788376851,
      "loss": 0.1293,
      "step": 17263
    },
    {
      "epoch": 0.08120643103756456,
      "grad_norm": 1.9262346029281616,
      "learning_rate": 0.00018419191490565503,
      "loss": 0.1622,
      "step": 17264
    },
    {
      "epoch": 0.08121113483917702,
      "grad_norm": 0.8264157772064209,
      "learning_rate": 0.00018419097192754158,
      "loss": 0.128,
      "step": 17265
    },
    {
      "epoch": 0.08121583864078949,
      "grad_norm": 1.1733512878417969,
      "learning_rate": 0.0001841900289494281,
      "loss": 0.1504,
      "step": 17266
    },
    {
      "epoch": 0.08122054244240196,
      "grad_norm": 1.4469013214111328,
      "learning_rate": 0.00018418908597131462,
      "loss": 0.1389,
      "step": 17267
    },
    {
      "epoch": 0.0812252462440144,
      "grad_norm": 0.9200544953346252,
      "learning_rate": 0.00018418814299320113,
      "loss": 0.1397,
      "step": 17268
    },
    {
      "epoch": 0.08122995004562687,
      "grad_norm": 0.8748123049736023,
      "learning_rate": 0.00018418720001508768,
      "loss": 0.1476,
      "step": 17269
    },
    {
      "epoch": 0.08123465384723934,
      "grad_norm": 1.6470324993133545,
      "learning_rate": 0.0001841862570369742,
      "loss": 0.1702,
      "step": 17270
    },
    {
      "epoch": 0.0812393576488518,
      "grad_norm": 2.3171160221099854,
      "learning_rate": 0.0001841853140588607,
      "loss": 0.2334,
      "step": 17271
    },
    {
      "epoch": 0.08124406145046427,
      "grad_norm": 0.6335151195526123,
      "learning_rate": 0.0001841843710807472,
      "loss": 0.0622,
      "step": 17272
    },
    {
      "epoch": 0.08124876525207674,
      "grad_norm": 0.36994099617004395,
      "learning_rate": 0.00018418342810263373,
      "loss": 0.0405,
      "step": 17273
    },
    {
      "epoch": 0.08125346905368919,
      "grad_norm": 3.387795925140381,
      "learning_rate": 0.00018418248512452027,
      "loss": 0.3654,
      "step": 17274
    },
    {
      "epoch": 0.08125817285530165,
      "grad_norm": 1.4813629388809204,
      "learning_rate": 0.0001841815421464068,
      "loss": 0.342,
      "step": 17275
    },
    {
      "epoch": 0.08126287665691412,
      "grad_norm": 0.4469907581806183,
      "learning_rate": 0.0001841805991682933,
      "loss": 0.0427,
      "step": 17276
    },
    {
      "epoch": 0.08126758045852658,
      "grad_norm": 2.983898162841797,
      "learning_rate": 0.00018417965619017983,
      "loss": 0.3704,
      "step": 17277
    },
    {
      "epoch": 0.08127228426013905,
      "grad_norm": 3.050625801086426,
      "learning_rate": 0.00018417871321206638,
      "loss": 0.2579,
      "step": 17278
    },
    {
      "epoch": 0.0812769880617515,
      "grad_norm": 4.03198766708374,
      "learning_rate": 0.0001841777702339529,
      "loss": 0.9116,
      "step": 17279
    },
    {
      "epoch": 0.08128169186336397,
      "grad_norm": 1.0189554691314697,
      "learning_rate": 0.0001841768272558394,
      "loss": 0.1059,
      "step": 17280
    },
    {
      "epoch": 0.08128639566497643,
      "grad_norm": 0.3851886987686157,
      "learning_rate": 0.00018417588427772593,
      "loss": 0.0381,
      "step": 17281
    },
    {
      "epoch": 0.0812910994665889,
      "grad_norm": 0.881857693195343,
      "learning_rate": 0.00018417494129961242,
      "loss": 0.0609,
      "step": 17282
    },
    {
      "epoch": 0.08129580326820136,
      "grad_norm": 3.132219076156616,
      "learning_rate": 0.00018417399832149897,
      "loss": 0.607,
      "step": 17283
    },
    {
      "epoch": 0.08130050706981383,
      "grad_norm": 0.5287221074104309,
      "learning_rate": 0.0001841730553433855,
      "loss": 0.049,
      "step": 17284
    },
    {
      "epoch": 0.08130521087142628,
      "grad_norm": 0.34928056597709656,
      "learning_rate": 0.000184172112365272,
      "loss": 0.0456,
      "step": 17285
    },
    {
      "epoch": 0.08130991467303875,
      "grad_norm": 0.22162963449954987,
      "learning_rate": 0.00018417116938715852,
      "loss": 0.0178,
      "step": 17286
    },
    {
      "epoch": 0.08131461847465121,
      "grad_norm": 0.43667563796043396,
      "learning_rate": 0.00018417022640904507,
      "loss": 0.0493,
      "step": 17287
    },
    {
      "epoch": 0.08131932227626368,
      "grad_norm": 2.527439594268799,
      "learning_rate": 0.0001841692834309316,
      "loss": 0.6313,
      "step": 17288
    },
    {
      "epoch": 0.08132402607787614,
      "grad_norm": 3.175480604171753,
      "learning_rate": 0.0001841683404528181,
      "loss": 0.3362,
      "step": 17289
    },
    {
      "epoch": 0.08132872987948861,
      "grad_norm": 2.843463659286499,
      "learning_rate": 0.00018416739747470463,
      "loss": 0.3452,
      "step": 17290
    },
    {
      "epoch": 0.08133343368110106,
      "grad_norm": 2.379767656326294,
      "learning_rate": 0.00018416645449659114,
      "loss": 0.2347,
      "step": 17291
    },
    {
      "epoch": 0.08133813748271353,
      "grad_norm": 1.6616759300231934,
      "learning_rate": 0.00018416551151847766,
      "loss": 0.3087,
      "step": 17292
    },
    {
      "epoch": 0.08134284128432599,
      "grad_norm": 0.42372456192970276,
      "learning_rate": 0.00018416456854036418,
      "loss": 0.0297,
      "step": 17293
    },
    {
      "epoch": 0.08134754508593846,
      "grad_norm": 0.6520522832870483,
      "learning_rate": 0.0001841636255622507,
      "loss": 0.0851,
      "step": 17294
    },
    {
      "epoch": 0.08135224888755092,
      "grad_norm": 1.1650378704071045,
      "learning_rate": 0.00018416268258413722,
      "loss": 0.0945,
      "step": 17295
    },
    {
      "epoch": 0.08135695268916338,
      "grad_norm": 2.8495681285858154,
      "learning_rate": 0.00018416173960602374,
      "loss": 0.3579,
      "step": 17296
    },
    {
      "epoch": 0.08136165649077584,
      "grad_norm": 0.5190766453742981,
      "learning_rate": 0.00018416079662791028,
      "loss": 0.0554,
      "step": 17297
    },
    {
      "epoch": 0.0813663602923883,
      "grad_norm": 1.2564373016357422,
      "learning_rate": 0.0001841598536497968,
      "loss": 0.129,
      "step": 17298
    },
    {
      "epoch": 0.08137106409400077,
      "grad_norm": 1.261841893196106,
      "learning_rate": 0.00018415891067168332,
      "loss": 0.098,
      "step": 17299
    },
    {
      "epoch": 0.08137576789561324,
      "grad_norm": 1.1230522394180298,
      "learning_rate": 0.00018415796769356984,
      "loss": 0.0865,
      "step": 17300
    },
    {
      "epoch": 0.0813804716972257,
      "grad_norm": 5.823319911956787,
      "learning_rate": 0.00018415702471545639,
      "loss": 0.7313,
      "step": 17301
    },
    {
      "epoch": 0.08138517549883816,
      "grad_norm": 0.198314368724823,
      "learning_rate": 0.00018415608173734288,
      "loss": 0.0066,
      "step": 17302
    },
    {
      "epoch": 0.08138987930045062,
      "grad_norm": 2.8662970066070557,
      "learning_rate": 0.0001841551387592294,
      "loss": 0.367,
      "step": 17303
    },
    {
      "epoch": 0.08139458310206309,
      "grad_norm": 0.6902220249176025,
      "learning_rate": 0.00018415419578111591,
      "loss": 0.0568,
      "step": 17304
    },
    {
      "epoch": 0.08139928690367555,
      "grad_norm": 2.2270944118499756,
      "learning_rate": 0.00018415325280300243,
      "loss": 0.4749,
      "step": 17305
    },
    {
      "epoch": 0.08140399070528802,
      "grad_norm": 0.7458634972572327,
      "learning_rate": 0.00018415230982488898,
      "loss": 0.0634,
      "step": 17306
    },
    {
      "epoch": 0.08140869450690048,
      "grad_norm": 0.38045281171798706,
      "learning_rate": 0.0001841513668467755,
      "loss": 0.0284,
      "step": 17307
    },
    {
      "epoch": 0.08141339830851294,
      "grad_norm": 3.785844087600708,
      "learning_rate": 0.00018415042386866202,
      "loss": 0.242,
      "step": 17308
    },
    {
      "epoch": 0.0814181021101254,
      "grad_norm": 2.863227367401123,
      "learning_rate": 0.00018414948089054853,
      "loss": 0.2421,
      "step": 17309
    },
    {
      "epoch": 0.08142280591173787,
      "grad_norm": 4.416896343231201,
      "learning_rate": 0.00018414853791243508,
      "loss": 0.5376,
      "step": 17310
    },
    {
      "epoch": 0.08142750971335033,
      "grad_norm": 4.609310626983643,
      "learning_rate": 0.0001841475949343216,
      "loss": 0.4824,
      "step": 17311
    },
    {
      "epoch": 0.0814322135149628,
      "grad_norm": 5.8542938232421875,
      "learning_rate": 0.00018414665195620812,
      "loss": 0.4485,
      "step": 17312
    },
    {
      "epoch": 0.08143691731657525,
      "grad_norm": 3.9350624084472656,
      "learning_rate": 0.0001841457089780946,
      "loss": 0.6028,
      "step": 17313
    },
    {
      "epoch": 0.08144162111818772,
      "grad_norm": 22.161502838134766,
      "learning_rate": 0.00018414476599998113,
      "loss": 0.4341,
      "step": 17314
    },
    {
      "epoch": 0.08144632491980018,
      "grad_norm": 10.223113059997559,
      "learning_rate": 0.00018414382302186767,
      "loss": 0.306,
      "step": 17315
    },
    {
      "epoch": 0.08145102872141265,
      "grad_norm": 3.019493341445923,
      "learning_rate": 0.0001841428800437542,
      "loss": 0.1322,
      "step": 17316
    },
    {
      "epoch": 0.08145573252302511,
      "grad_norm": 3.2164785861968994,
      "learning_rate": 0.0001841419370656407,
      "loss": 0.5129,
      "step": 17317
    },
    {
      "epoch": 0.08146043632463758,
      "grad_norm": 2.306736946105957,
      "learning_rate": 0.00018414099408752723,
      "loss": 0.1842,
      "step": 17318
    },
    {
      "epoch": 0.08146514012625003,
      "grad_norm": 3.8196041584014893,
      "learning_rate": 0.00018414005110941378,
      "loss": 0.9324,
      "step": 17319
    },
    {
      "epoch": 0.0814698439278625,
      "grad_norm": 2.6076743602752686,
      "learning_rate": 0.0001841391081313003,
      "loss": 0.2982,
      "step": 17320
    },
    {
      "epoch": 0.08147454772947496,
      "grad_norm": 2.803917646408081,
      "learning_rate": 0.0001841381651531868,
      "loss": 0.1757,
      "step": 17321
    },
    {
      "epoch": 0.08147925153108743,
      "grad_norm": 5.457267761230469,
      "learning_rate": 0.00018413722217507333,
      "loss": 0.1073,
      "step": 17322
    },
    {
      "epoch": 0.08148395533269989,
      "grad_norm": 3.649724006652832,
      "learning_rate": 0.00018413627919695985,
      "loss": 0.5059,
      "step": 17323
    },
    {
      "epoch": 0.08148865913431236,
      "grad_norm": 0.2677081227302551,
      "learning_rate": 0.00018413533621884637,
      "loss": 0.0457,
      "step": 17324
    },
    {
      "epoch": 0.08149336293592481,
      "grad_norm": 3.2493598461151123,
      "learning_rate": 0.0001841343932407329,
      "loss": 0.5363,
      "step": 17325
    },
    {
      "epoch": 0.08149806673753728,
      "grad_norm": 2.248867988586426,
      "learning_rate": 0.0001841334502626194,
      "loss": 0.213,
      "step": 17326
    },
    {
      "epoch": 0.08150277053914974,
      "grad_norm": 1.6154600381851196,
      "learning_rate": 0.00018413250728450592,
      "loss": 0.1169,
      "step": 17327
    },
    {
      "epoch": 0.0815074743407622,
      "grad_norm": 2.1749491691589355,
      "learning_rate": 0.00018413156430639247,
      "loss": 0.146,
      "step": 17328
    },
    {
      "epoch": 0.08151217814237467,
      "grad_norm": 3.0561013221740723,
      "learning_rate": 0.000184130621328279,
      "loss": 0.3103,
      "step": 17329
    },
    {
      "epoch": 0.08151688194398712,
      "grad_norm": 3.452378988265991,
      "learning_rate": 0.0001841296783501655,
      "loss": 0.4517,
      "step": 17330
    },
    {
      "epoch": 0.08152158574559959,
      "grad_norm": 2.0154519081115723,
      "learning_rate": 0.00018412873537205203,
      "loss": 0.4172,
      "step": 17331
    },
    {
      "epoch": 0.08152628954721206,
      "grad_norm": 1.8824421167373657,
      "learning_rate": 0.00018412779239393854,
      "loss": 0.1727,
      "step": 17332
    },
    {
      "epoch": 0.08153099334882452,
      "grad_norm": 0.812136173248291,
      "learning_rate": 0.00018412684941582506,
      "loss": 0.0953,
      "step": 17333
    },
    {
      "epoch": 0.08153569715043699,
      "grad_norm": 1.2713326215744019,
      "learning_rate": 0.00018412590643771158,
      "loss": 0.1195,
      "step": 17334
    },
    {
      "epoch": 0.08154040095204945,
      "grad_norm": 1.8626946210861206,
      "learning_rate": 0.0001841249634595981,
      "loss": 0.1943,
      "step": 17335
    },
    {
      "epoch": 0.0815451047536619,
      "grad_norm": 3.236044406890869,
      "learning_rate": 0.00018412402048148462,
      "loss": 0.4252,
      "step": 17336
    },
    {
      "epoch": 0.08154980855527437,
      "grad_norm": 2.9459354877471924,
      "learning_rate": 0.00018412307750337116,
      "loss": 0.5845,
      "step": 17337
    },
    {
      "epoch": 0.08155451235688684,
      "grad_norm": 2.0297327041625977,
      "learning_rate": 0.00018412213452525768,
      "loss": 0.4745,
      "step": 17338
    },
    {
      "epoch": 0.0815592161584993,
      "grad_norm": 2.3361258506774902,
      "learning_rate": 0.0001841211915471442,
      "loss": 0.3465,
      "step": 17339
    },
    {
      "epoch": 0.08156391996011177,
      "grad_norm": 1.4639384746551514,
      "learning_rate": 0.00018412024856903072,
      "loss": 0.2215,
      "step": 17340
    },
    {
      "epoch": 0.08156862376172423,
      "grad_norm": 1.651284098625183,
      "learning_rate": 0.00018411930559091724,
      "loss": 0.2174,
      "step": 17341
    },
    {
      "epoch": 0.08157332756333668,
      "grad_norm": 0.9967559576034546,
      "learning_rate": 0.00018411836261280379,
      "loss": 0.0876,
      "step": 17342
    },
    {
      "epoch": 0.08157803136494915,
      "grad_norm": 0.15874885022640228,
      "learning_rate": 0.0001841174196346903,
      "loss": 0.0129,
      "step": 17343
    },
    {
      "epoch": 0.08158273516656162,
      "grad_norm": 1.3536688089370728,
      "learning_rate": 0.0001841164766565768,
      "loss": 0.2793,
      "step": 17344
    },
    {
      "epoch": 0.08158743896817408,
      "grad_norm": 3.1505746841430664,
      "learning_rate": 0.00018411553367846331,
      "loss": 0.7999,
      "step": 17345
    },
    {
      "epoch": 0.08159214276978655,
      "grad_norm": 2.864346981048584,
      "learning_rate": 0.00018411459070034983,
      "loss": 0.1848,
      "step": 17346
    },
    {
      "epoch": 0.081596846571399,
      "grad_norm": 2.7082200050354004,
      "learning_rate": 0.00018411364772223638,
      "loss": 0.6527,
      "step": 17347
    },
    {
      "epoch": 0.08160155037301146,
      "grad_norm": 1.7575408220291138,
      "learning_rate": 0.0001841127047441229,
      "loss": 0.4892,
      "step": 17348
    },
    {
      "epoch": 0.08160625417462393,
      "grad_norm": 1.580003261566162,
      "learning_rate": 0.00018411176176600942,
      "loss": 0.111,
      "step": 17349
    },
    {
      "epoch": 0.0816109579762364,
      "grad_norm": 2.659085512161255,
      "learning_rate": 0.00018411081878789593,
      "loss": 0.605,
      "step": 17350
    },
    {
      "epoch": 0.08161566177784886,
      "grad_norm": 2.9632983207702637,
      "learning_rate": 0.00018410987580978248,
      "loss": 0.2387,
      "step": 17351
    },
    {
      "epoch": 0.08162036557946133,
      "grad_norm": 1.3340392112731934,
      "learning_rate": 0.000184108932831669,
      "loss": 0.1678,
      "step": 17352
    },
    {
      "epoch": 0.08162506938107378,
      "grad_norm": 0.7045742273330688,
      "learning_rate": 0.00018410798985355552,
      "loss": 0.0694,
      "step": 17353
    },
    {
      "epoch": 0.08162977318268624,
      "grad_norm": 6.2022294998168945,
      "learning_rate": 0.00018410704687544204,
      "loss": 0.1982,
      "step": 17354
    },
    {
      "epoch": 0.08163447698429871,
      "grad_norm": 3.2505383491516113,
      "learning_rate": 0.00018410610389732855,
      "loss": 0.3503,
      "step": 17355
    },
    {
      "epoch": 0.08163918078591118,
      "grad_norm": 1.9402309656143188,
      "learning_rate": 0.00018410516091921507,
      "loss": 0.2063,
      "step": 17356
    },
    {
      "epoch": 0.08164388458752364,
      "grad_norm": 5.0021185874938965,
      "learning_rate": 0.0001841042179411016,
      "loss": 0.8915,
      "step": 17357
    },
    {
      "epoch": 0.0816485883891361,
      "grad_norm": 3.2596216201782227,
      "learning_rate": 0.0001841032749629881,
      "loss": 0.3456,
      "step": 17358
    },
    {
      "epoch": 0.08165329219074856,
      "grad_norm": 2.2716667652130127,
      "learning_rate": 0.00018410233198487463,
      "loss": 0.2292,
      "step": 17359
    },
    {
      "epoch": 0.08165799599236102,
      "grad_norm": 2.501277208328247,
      "learning_rate": 0.00018410138900676118,
      "loss": 0.25,
      "step": 17360
    },
    {
      "epoch": 0.08166269979397349,
      "grad_norm": 3.2104477882385254,
      "learning_rate": 0.0001841004460286477,
      "loss": 0.3236,
      "step": 17361
    },
    {
      "epoch": 0.08166740359558596,
      "grad_norm": 3.5144550800323486,
      "learning_rate": 0.0001840995030505342,
      "loss": 0.4519,
      "step": 17362
    },
    {
      "epoch": 0.08167210739719842,
      "grad_norm": 1.7864835262298584,
      "learning_rate": 0.00018409856007242073,
      "loss": 0.2809,
      "step": 17363
    },
    {
      "epoch": 0.08167681119881087,
      "grad_norm": 6.925131320953369,
      "learning_rate": 0.00018409761709430725,
      "loss": 0.4293,
      "step": 17364
    },
    {
      "epoch": 0.08168151500042334,
      "grad_norm": 1.9357233047485352,
      "learning_rate": 0.00018409667411619377,
      "loss": 0.2579,
      "step": 17365
    },
    {
      "epoch": 0.0816862188020358,
      "grad_norm": 1.5018168687820435,
      "learning_rate": 0.0001840957311380803,
      "loss": 0.1539,
      "step": 17366
    },
    {
      "epoch": 0.08169092260364827,
      "grad_norm": 1.489913821220398,
      "learning_rate": 0.0001840947881599668,
      "loss": 0.1979,
      "step": 17367
    },
    {
      "epoch": 0.08169562640526074,
      "grad_norm": 1.51046884059906,
      "learning_rate": 0.00018409384518185332,
      "loss": 0.1281,
      "step": 17368
    },
    {
      "epoch": 0.0817003302068732,
      "grad_norm": 2.9955270290374756,
      "learning_rate": 0.00018409290220373987,
      "loss": 0.6342,
      "step": 17369
    },
    {
      "epoch": 0.08170503400848565,
      "grad_norm": 1.2564711570739746,
      "learning_rate": 0.0001840919592256264,
      "loss": 0.1239,
      "step": 17370
    },
    {
      "epoch": 0.08170973781009812,
      "grad_norm": 3.832855701446533,
      "learning_rate": 0.0001840910162475129,
      "loss": 0.9424,
      "step": 17371
    },
    {
      "epoch": 0.08171444161171058,
      "grad_norm": 0.5730242133140564,
      "learning_rate": 0.00018409007326939943,
      "loss": 0.0692,
      "step": 17372
    },
    {
      "epoch": 0.08171914541332305,
      "grad_norm": 1.2373607158660889,
      "learning_rate": 0.00018408913029128594,
      "loss": 0.2069,
      "step": 17373
    },
    {
      "epoch": 0.08172384921493552,
      "grad_norm": 2.607438087463379,
      "learning_rate": 0.0001840881873131725,
      "loss": 0.5452,
      "step": 17374
    },
    {
      "epoch": 0.08172855301654798,
      "grad_norm": 1.4508769512176514,
      "learning_rate": 0.00018408724433505898,
      "loss": 0.2169,
      "step": 17375
    },
    {
      "epoch": 0.08173325681816043,
      "grad_norm": 2.248119831085205,
      "learning_rate": 0.0001840863013569455,
      "loss": 0.2289,
      "step": 17376
    },
    {
      "epoch": 0.0817379606197729,
      "grad_norm": 1.0428969860076904,
      "learning_rate": 0.00018408535837883202,
      "loss": 0.1698,
      "step": 17377
    },
    {
      "epoch": 0.08174266442138536,
      "grad_norm": 1.1915541887283325,
      "learning_rate": 0.00018408441540071856,
      "loss": 0.1291,
      "step": 17378
    },
    {
      "epoch": 0.08174736822299783,
      "grad_norm": 1.2058852910995483,
      "learning_rate": 0.00018408347242260508,
      "loss": 0.1543,
      "step": 17379
    },
    {
      "epoch": 0.0817520720246103,
      "grad_norm": 2.2437500953674316,
      "learning_rate": 0.0001840825294444916,
      "loss": 0.4538,
      "step": 17380
    },
    {
      "epoch": 0.08175677582622275,
      "grad_norm": 2.1553802490234375,
      "learning_rate": 0.00018408158646637812,
      "loss": 0.3229,
      "step": 17381
    },
    {
      "epoch": 0.08176147962783521,
      "grad_norm": 0.9532349109649658,
      "learning_rate": 0.00018408064348826464,
      "loss": 0.1189,
      "step": 17382
    },
    {
      "epoch": 0.08176618342944768,
      "grad_norm": 1.612398624420166,
      "learning_rate": 0.00018407970051015119,
      "loss": 0.2896,
      "step": 17383
    },
    {
      "epoch": 0.08177088723106014,
      "grad_norm": 2.2246181964874268,
      "learning_rate": 0.0001840787575320377,
      "loss": 0.3087,
      "step": 17384
    },
    {
      "epoch": 0.08177559103267261,
      "grad_norm": 2.161452054977417,
      "learning_rate": 0.00018407781455392422,
      "loss": 0.2944,
      "step": 17385
    },
    {
      "epoch": 0.08178029483428507,
      "grad_norm": 3.1855647563934326,
      "learning_rate": 0.00018407687157581074,
      "loss": 0.3518,
      "step": 17386
    },
    {
      "epoch": 0.08178499863589753,
      "grad_norm": 1.2627363204956055,
      "learning_rate": 0.00018407592859769726,
      "loss": 0.3345,
      "step": 17387
    },
    {
      "epoch": 0.08178970243750999,
      "grad_norm": 1.0498905181884766,
      "learning_rate": 0.00018407498561958378,
      "loss": 0.1198,
      "step": 17388
    },
    {
      "epoch": 0.08179440623912246,
      "grad_norm": 2.06428861618042,
      "learning_rate": 0.0001840740426414703,
      "loss": 0.5558,
      "step": 17389
    },
    {
      "epoch": 0.08179911004073492,
      "grad_norm": 1.2210441827774048,
      "learning_rate": 0.00018407309966335682,
      "loss": 0.2185,
      "step": 17390
    },
    {
      "epoch": 0.08180381384234739,
      "grad_norm": 1.4220815896987915,
      "learning_rate": 0.00018407215668524333,
      "loss": 0.3899,
      "step": 17391
    },
    {
      "epoch": 0.08180851764395985,
      "grad_norm": 1.7719968557357788,
      "learning_rate": 0.00018407121370712988,
      "loss": 0.2565,
      "step": 17392
    },
    {
      "epoch": 0.0818132214455723,
      "grad_norm": 1.9289991855621338,
      "learning_rate": 0.0001840702707290164,
      "loss": 0.5183,
      "step": 17393
    },
    {
      "epoch": 0.08181792524718477,
      "grad_norm": 1.004780888557434,
      "learning_rate": 0.00018406932775090292,
      "loss": 0.1218,
      "step": 17394
    },
    {
      "epoch": 0.08182262904879724,
      "grad_norm": 0.6062656044960022,
      "learning_rate": 0.00018406838477278944,
      "loss": 0.0744,
      "step": 17395
    },
    {
      "epoch": 0.0818273328504097,
      "grad_norm": 1.2554340362548828,
      "learning_rate": 0.00018406744179467595,
      "loss": 0.1673,
      "step": 17396
    },
    {
      "epoch": 0.08183203665202217,
      "grad_norm": 1.1154338121414185,
      "learning_rate": 0.00018406649881656247,
      "loss": 0.1731,
      "step": 17397
    },
    {
      "epoch": 0.08183674045363462,
      "grad_norm": 0.3100505471229553,
      "learning_rate": 0.000184065555838449,
      "loss": 0.0439,
      "step": 17398
    },
    {
      "epoch": 0.08184144425524709,
      "grad_norm": 0.7436621785163879,
      "learning_rate": 0.0001840646128603355,
      "loss": 0.1321,
      "step": 17399
    },
    {
      "epoch": 0.08184614805685955,
      "grad_norm": 0.4201804995536804,
      "learning_rate": 0.00018406366988222203,
      "loss": 0.0563,
      "step": 17400
    },
    {
      "epoch": 0.08185085185847202,
      "grad_norm": 2.747628688812256,
      "learning_rate": 0.00018406272690410857,
      "loss": 0.3062,
      "step": 17401
    },
    {
      "epoch": 0.08185555566008448,
      "grad_norm": 2.3356804847717285,
      "learning_rate": 0.0001840617839259951,
      "loss": 0.4948,
      "step": 17402
    },
    {
      "epoch": 0.08186025946169695,
      "grad_norm": 0.3556780219078064,
      "learning_rate": 0.0001840608409478816,
      "loss": 0.0327,
      "step": 17403
    },
    {
      "epoch": 0.0818649632633094,
      "grad_norm": 2.4027159214019775,
      "learning_rate": 0.00018405989796976813,
      "loss": 0.2738,
      "step": 17404
    },
    {
      "epoch": 0.08186966706492187,
      "grad_norm": 2.051971197128296,
      "learning_rate": 0.00018405895499165465,
      "loss": 0.1117,
      "step": 17405
    },
    {
      "epoch": 0.08187437086653433,
      "grad_norm": 1.7646801471710205,
      "learning_rate": 0.00018405801201354117,
      "loss": 0.1429,
      "step": 17406
    },
    {
      "epoch": 0.0818790746681468,
      "grad_norm": 2.268085241317749,
      "learning_rate": 0.0001840570690354277,
      "loss": 0.4807,
      "step": 17407
    },
    {
      "epoch": 0.08188377846975926,
      "grad_norm": 3.0252368450164795,
      "learning_rate": 0.0001840561260573142,
      "loss": 0.3476,
      "step": 17408
    },
    {
      "epoch": 0.08188848227137173,
      "grad_norm": 0.24096843600273132,
      "learning_rate": 0.00018405518307920072,
      "loss": 0.0149,
      "step": 17409
    },
    {
      "epoch": 0.08189318607298418,
      "grad_norm": 2.893688678741455,
      "learning_rate": 0.00018405424010108727,
      "loss": 0.5825,
      "step": 17410
    },
    {
      "epoch": 0.08189788987459665,
      "grad_norm": 2.075504779815674,
      "learning_rate": 0.0001840532971229738,
      "loss": 0.1501,
      "step": 17411
    },
    {
      "epoch": 0.08190259367620911,
      "grad_norm": 3.690584421157837,
      "learning_rate": 0.0001840523541448603,
      "loss": 0.6165,
      "step": 17412
    },
    {
      "epoch": 0.08190729747782158,
      "grad_norm": 4.518378257751465,
      "learning_rate": 0.00018405141116674683,
      "loss": 0.8262,
      "step": 17413
    },
    {
      "epoch": 0.08191200127943404,
      "grad_norm": 2.9092183113098145,
      "learning_rate": 0.00018405046818863334,
      "loss": 0.1598,
      "step": 17414
    },
    {
      "epoch": 0.0819167050810465,
      "grad_norm": 2.1243908405303955,
      "learning_rate": 0.0001840495252105199,
      "loss": 0.1166,
      "step": 17415
    },
    {
      "epoch": 0.08192140888265896,
      "grad_norm": 1.8316316604614258,
      "learning_rate": 0.0001840485822324064,
      "loss": 0.1993,
      "step": 17416
    },
    {
      "epoch": 0.08192611268427143,
      "grad_norm": 2.5326945781707764,
      "learning_rate": 0.00018404763925429293,
      "loss": 0.3139,
      "step": 17417
    },
    {
      "epoch": 0.08193081648588389,
      "grad_norm": 2.1883695125579834,
      "learning_rate": 0.00018404669627617942,
      "loss": 0.3408,
      "step": 17418
    },
    {
      "epoch": 0.08193552028749636,
      "grad_norm": 4.376508712768555,
      "learning_rate": 0.00018404575329806596,
      "loss": 1.0656,
      "step": 17419
    },
    {
      "epoch": 0.08194022408910882,
      "grad_norm": 2.077012777328491,
      "learning_rate": 0.00018404481031995248,
      "loss": 0.0922,
      "step": 17420
    },
    {
      "epoch": 0.08194492789072128,
      "grad_norm": 3.07667875289917,
      "learning_rate": 0.000184043867341839,
      "loss": 0.3486,
      "step": 17421
    },
    {
      "epoch": 0.08194963169233374,
      "grad_norm": 1.3111733198165894,
      "learning_rate": 0.00018404292436372552,
      "loss": 0.1195,
      "step": 17422
    },
    {
      "epoch": 0.0819543354939462,
      "grad_norm": 1.751014232635498,
      "learning_rate": 0.00018404198138561204,
      "loss": 0.1619,
      "step": 17423
    },
    {
      "epoch": 0.08195903929555867,
      "grad_norm": 0.09003009647130966,
      "learning_rate": 0.00018404103840749859,
      "loss": 0.0072,
      "step": 17424
    },
    {
      "epoch": 0.08196374309717114,
      "grad_norm": 3.6880781650543213,
      "learning_rate": 0.0001840400954293851,
      "loss": 0.3144,
      "step": 17425
    },
    {
      "epoch": 0.0819684468987836,
      "grad_norm": 0.460409551858902,
      "learning_rate": 0.00018403915245127162,
      "loss": 0.0567,
      "step": 17426
    },
    {
      "epoch": 0.08197315070039606,
      "grad_norm": 0.5252286195755005,
      "learning_rate": 0.00018403820947315814,
      "loss": 0.06,
      "step": 17427
    },
    {
      "epoch": 0.08197785450200852,
      "grad_norm": 2.900507926940918,
      "learning_rate": 0.00018403726649504466,
      "loss": 0.3532,
      "step": 17428
    },
    {
      "epoch": 0.08198255830362099,
      "grad_norm": 0.7020460367202759,
      "learning_rate": 0.00018403632351693118,
      "loss": 0.0664,
      "step": 17429
    },
    {
      "epoch": 0.08198726210523345,
      "grad_norm": 1.8455291986465454,
      "learning_rate": 0.0001840353805388177,
      "loss": 0.3875,
      "step": 17430
    },
    {
      "epoch": 0.08199196590684592,
      "grad_norm": 2.36678409576416,
      "learning_rate": 0.00018403443756070422,
      "loss": 0.4434,
      "step": 17431
    },
    {
      "epoch": 0.08199666970845837,
      "grad_norm": 1.2029449939727783,
      "learning_rate": 0.00018403349458259073,
      "loss": 0.2328,
      "step": 17432
    },
    {
      "epoch": 0.08200137351007084,
      "grad_norm": 1.045637607574463,
      "learning_rate": 0.00018403255160447728,
      "loss": 0.1407,
      "step": 17433
    },
    {
      "epoch": 0.0820060773116833,
      "grad_norm": 1.0202479362487793,
      "learning_rate": 0.0001840316086263638,
      "loss": 0.1324,
      "step": 17434
    },
    {
      "epoch": 0.08201078111329577,
      "grad_norm": 1.1565285921096802,
      "learning_rate": 0.00018403066564825032,
      "loss": 0.1905,
      "step": 17435
    },
    {
      "epoch": 0.08201548491490823,
      "grad_norm": 1.2388851642608643,
      "learning_rate": 0.00018402972267013684,
      "loss": 0.1627,
      "step": 17436
    },
    {
      "epoch": 0.0820201887165207,
      "grad_norm": 1.4521198272705078,
      "learning_rate": 0.00018402877969202335,
      "loss": 0.1478,
      "step": 17437
    },
    {
      "epoch": 0.08202489251813315,
      "grad_norm": 0.7989686131477356,
      "learning_rate": 0.00018402783671390987,
      "loss": 0.0631,
      "step": 17438
    },
    {
      "epoch": 0.08202959631974562,
      "grad_norm": 0.8573902249336243,
      "learning_rate": 0.0001840268937357964,
      "loss": 0.0748,
      "step": 17439
    },
    {
      "epoch": 0.08203430012135808,
      "grad_norm": 2.166630983352661,
      "learning_rate": 0.0001840259507576829,
      "loss": 0.3894,
      "step": 17440
    },
    {
      "epoch": 0.08203900392297055,
      "grad_norm": 2.077951431274414,
      "learning_rate": 0.00018402500777956943,
      "loss": 0.2206,
      "step": 17441
    },
    {
      "epoch": 0.08204370772458301,
      "grad_norm": 1.3800619840621948,
      "learning_rate": 0.00018402406480145597,
      "loss": 0.305,
      "step": 17442
    },
    {
      "epoch": 0.08204841152619548,
      "grad_norm": 3.021311044692993,
      "learning_rate": 0.0001840231218233425,
      "loss": 0.3253,
      "step": 17443
    },
    {
      "epoch": 0.08205311532780793,
      "grad_norm": 2.7233521938323975,
      "learning_rate": 0.000184022178845229,
      "loss": 0.2417,
      "step": 17444
    },
    {
      "epoch": 0.0820578191294204,
      "grad_norm": 0.3114626109600067,
      "learning_rate": 0.00018402123586711553,
      "loss": 0.0258,
      "step": 17445
    },
    {
      "epoch": 0.08206252293103286,
      "grad_norm": 1.1758716106414795,
      "learning_rate": 0.00018402029288900208,
      "loss": 0.0962,
      "step": 17446
    },
    {
      "epoch": 0.08206722673264533,
      "grad_norm": 1.3008719682693481,
      "learning_rate": 0.0001840193499108886,
      "loss": 0.3976,
      "step": 17447
    },
    {
      "epoch": 0.08207193053425779,
      "grad_norm": 1.6849911212921143,
      "learning_rate": 0.0001840184069327751,
      "loss": 0.1924,
      "step": 17448
    },
    {
      "epoch": 0.08207663433587024,
      "grad_norm": 3.1951041221618652,
      "learning_rate": 0.0001840174639546616,
      "loss": 1.0041,
      "step": 17449
    },
    {
      "epoch": 0.08208133813748271,
      "grad_norm": 1.197108268737793,
      "learning_rate": 0.00018401652097654812,
      "loss": 0.1303,
      "step": 17450
    },
    {
      "epoch": 0.08208604193909518,
      "grad_norm": 1.1918708086013794,
      "learning_rate": 0.00018401557799843467,
      "loss": 0.1035,
      "step": 17451
    },
    {
      "epoch": 0.08209074574070764,
      "grad_norm": 3.3480656147003174,
      "learning_rate": 0.0001840146350203212,
      "loss": 0.418,
      "step": 17452
    },
    {
      "epoch": 0.0820954495423201,
      "grad_norm": 0.5828223824501038,
      "learning_rate": 0.0001840136920422077,
      "loss": 0.033,
      "step": 17453
    },
    {
      "epoch": 0.08210015334393257,
      "grad_norm": 0.17618612945079803,
      "learning_rate": 0.00018401274906409423,
      "loss": 0.0112,
      "step": 17454
    },
    {
      "epoch": 0.08210485714554502,
      "grad_norm": 0.26602646708488464,
      "learning_rate": 0.00018401180608598074,
      "loss": 0.0105,
      "step": 17455
    },
    {
      "epoch": 0.08210956094715749,
      "grad_norm": 1.685045838356018,
      "learning_rate": 0.0001840108631078673,
      "loss": 0.1702,
      "step": 17456
    },
    {
      "epoch": 0.08211426474876996,
      "grad_norm": 7.04102087020874,
      "learning_rate": 0.0001840099201297538,
      "loss": 0.5334,
      "step": 17457
    },
    {
      "epoch": 0.08211896855038242,
      "grad_norm": 4.439507007598877,
      "learning_rate": 0.00018400897715164033,
      "loss": 1.2777,
      "step": 17458
    },
    {
      "epoch": 0.08212367235199489,
      "grad_norm": 2.250955581665039,
      "learning_rate": 0.00018400803417352685,
      "loss": 0.1964,
      "step": 17459
    },
    {
      "epoch": 0.08212837615360735,
      "grad_norm": 3.192716598510742,
      "learning_rate": 0.00018400709119541336,
      "loss": 0.3758,
      "step": 17460
    },
    {
      "epoch": 0.0821330799552198,
      "grad_norm": 2.3037161827087402,
      "learning_rate": 0.00018400614821729988,
      "loss": 0.4248,
      "step": 17461
    },
    {
      "epoch": 0.08213778375683227,
      "grad_norm": 2.1906960010528564,
      "learning_rate": 0.0001840052052391864,
      "loss": 0.1856,
      "step": 17462
    },
    {
      "epoch": 0.08214248755844474,
      "grad_norm": 1.305400013923645,
      "learning_rate": 0.00018400426226107292,
      "loss": 0.1317,
      "step": 17463
    },
    {
      "epoch": 0.0821471913600572,
      "grad_norm": 2.217170000076294,
      "learning_rate": 0.00018400331928295944,
      "loss": 0.2823,
      "step": 17464
    },
    {
      "epoch": 0.08215189516166967,
      "grad_norm": 3.3509933948516846,
      "learning_rate": 0.00018400237630484599,
      "loss": 0.8496,
      "step": 17465
    },
    {
      "epoch": 0.08215659896328212,
      "grad_norm": 0.5144814252853394,
      "learning_rate": 0.0001840014333267325,
      "loss": 0.0584,
      "step": 17466
    },
    {
      "epoch": 0.08216130276489458,
      "grad_norm": 2.211909770965576,
      "learning_rate": 0.00018400049034861902,
      "loss": 0.2282,
      "step": 17467
    },
    {
      "epoch": 0.08216600656650705,
      "grad_norm": 2.9897148609161377,
      "learning_rate": 0.00018399954737050554,
      "loss": 0.3104,
      "step": 17468
    },
    {
      "epoch": 0.08217071036811952,
      "grad_norm": 2.0014331340789795,
      "learning_rate": 0.00018399860439239206,
      "loss": 0.2426,
      "step": 17469
    },
    {
      "epoch": 0.08217541416973198,
      "grad_norm": 3.1932077407836914,
      "learning_rate": 0.00018399766141427858,
      "loss": 0.427,
      "step": 17470
    },
    {
      "epoch": 0.08218011797134445,
      "grad_norm": 0.6652907133102417,
      "learning_rate": 0.0001839967184361651,
      "loss": 0.0705,
      "step": 17471
    },
    {
      "epoch": 0.0821848217729569,
      "grad_norm": 0.4245174825191498,
      "learning_rate": 0.00018399577545805162,
      "loss": 0.0347,
      "step": 17472
    },
    {
      "epoch": 0.08218952557456936,
      "grad_norm": 1.1751314401626587,
      "learning_rate": 0.00018399483247993813,
      "loss": 0.0658,
      "step": 17473
    },
    {
      "epoch": 0.08219422937618183,
      "grad_norm": 0.6770316958427429,
      "learning_rate": 0.00018399388950182468,
      "loss": 0.0604,
      "step": 17474
    },
    {
      "epoch": 0.0821989331777943,
      "grad_norm": 1.6063618659973145,
      "learning_rate": 0.0001839929465237112,
      "loss": 0.2018,
      "step": 17475
    },
    {
      "epoch": 0.08220363697940676,
      "grad_norm": 1.1930029392242432,
      "learning_rate": 0.00018399200354559772,
      "loss": 0.1229,
      "step": 17476
    },
    {
      "epoch": 0.08220834078101923,
      "grad_norm": 1.781494379043579,
      "learning_rate": 0.00018399106056748424,
      "loss": 0.1943,
      "step": 17477
    },
    {
      "epoch": 0.08221304458263168,
      "grad_norm": 1.4460947513580322,
      "learning_rate": 0.00018399011758937078,
      "loss": 0.113,
      "step": 17478
    },
    {
      "epoch": 0.08221774838424414,
      "grad_norm": 2.465707778930664,
      "learning_rate": 0.00018398917461125727,
      "loss": 0.2713,
      "step": 17479
    },
    {
      "epoch": 0.08222245218585661,
      "grad_norm": 0.8676059246063232,
      "learning_rate": 0.0001839882316331438,
      "loss": 0.1196,
      "step": 17480
    },
    {
      "epoch": 0.08222715598746907,
      "grad_norm": 1.6875849962234497,
      "learning_rate": 0.0001839872886550303,
      "loss": 0.1567,
      "step": 17481
    },
    {
      "epoch": 0.08223185978908154,
      "grad_norm": 2.589583158493042,
      "learning_rate": 0.00018398634567691683,
      "loss": 0.4183,
      "step": 17482
    },
    {
      "epoch": 0.08223656359069399,
      "grad_norm": 0.2172805368900299,
      "learning_rate": 0.00018398540269880337,
      "loss": 0.0185,
      "step": 17483
    },
    {
      "epoch": 0.08224126739230646,
      "grad_norm": 3.248516082763672,
      "learning_rate": 0.0001839844597206899,
      "loss": 0.3188,
      "step": 17484
    },
    {
      "epoch": 0.08224597119391892,
      "grad_norm": 2.6363892555236816,
      "learning_rate": 0.0001839835167425764,
      "loss": 0.1796,
      "step": 17485
    },
    {
      "epoch": 0.08225067499553139,
      "grad_norm": 4.965702533721924,
      "learning_rate": 0.00018398257376446293,
      "loss": 0.9942,
      "step": 17486
    },
    {
      "epoch": 0.08225537879714385,
      "grad_norm": 0.15082982182502747,
      "learning_rate": 0.00018398163078634948,
      "loss": 0.0116,
      "step": 17487
    },
    {
      "epoch": 0.08226008259875632,
      "grad_norm": 3.5478923320770264,
      "learning_rate": 0.000183980687808236,
      "loss": 1.0237,
      "step": 17488
    },
    {
      "epoch": 0.08226478640036877,
      "grad_norm": 1.8228919506072998,
      "learning_rate": 0.00018397974483012251,
      "loss": 0.4504,
      "step": 17489
    },
    {
      "epoch": 0.08226949020198124,
      "grad_norm": 3.9099490642547607,
      "learning_rate": 0.00018397880185200903,
      "loss": 0.1518,
      "step": 17490
    },
    {
      "epoch": 0.0822741940035937,
      "grad_norm": 0.5699781775474548,
      "learning_rate": 0.00018397785887389552,
      "loss": 0.0504,
      "step": 17491
    },
    {
      "epoch": 0.08227889780520617,
      "grad_norm": 2.345745801925659,
      "learning_rate": 0.00018397691589578207,
      "loss": 0.2558,
      "step": 17492
    },
    {
      "epoch": 0.08228360160681863,
      "grad_norm": 2.527473211288452,
      "learning_rate": 0.0001839759729176686,
      "loss": 0.1838,
      "step": 17493
    },
    {
      "epoch": 0.0822883054084311,
      "grad_norm": 1.324052333831787,
      "learning_rate": 0.0001839750299395551,
      "loss": 0.2742,
      "step": 17494
    },
    {
      "epoch": 0.08229300921004355,
      "grad_norm": 0.3602825701236725,
      "learning_rate": 0.00018397408696144163,
      "loss": 0.0413,
      "step": 17495
    },
    {
      "epoch": 0.08229771301165602,
      "grad_norm": 2.1001157760620117,
      "learning_rate": 0.00018397314398332817,
      "loss": 0.1594,
      "step": 17496
    },
    {
      "epoch": 0.08230241681326848,
      "grad_norm": 6.573227405548096,
      "learning_rate": 0.0001839722010052147,
      "loss": 0.3558,
      "step": 17497
    },
    {
      "epoch": 0.08230712061488095,
      "grad_norm": 2.886399745941162,
      "learning_rate": 0.0001839712580271012,
      "loss": 0.4055,
      "step": 17498
    },
    {
      "epoch": 0.08231182441649341,
      "grad_norm": 1.3999648094177246,
      "learning_rate": 0.00018397031504898773,
      "loss": 0.2749,
      "step": 17499
    },
    {
      "epoch": 0.08231652821810587,
      "grad_norm": 2.720919132232666,
      "learning_rate": 0.00018396937207087425,
      "loss": 0.7316,
      "step": 17500
    }
  ],
  "logging_steps": 1,
  "max_steps": 212594,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0429524824626299e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
