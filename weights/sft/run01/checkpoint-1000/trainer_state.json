{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.004703801612463193,
  "eval_steps": 42519,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.7038016124631924e-06,
      "grad_norm": 3.8041722774505615,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.7375,
      "step": 1
    },
    {
      "epoch": 9.407603224926385e-06,
      "grad_norm": 4.354788303375244,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.5438,
      "step": 2
    },
    {
      "epoch": 1.4111404837389578e-05,
      "grad_norm": 6.505493640899658,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6008,
      "step": 3
    },
    {
      "epoch": 1.881520644985277e-05,
      "grad_norm": 4.024658203125,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.5406,
      "step": 4
    },
    {
      "epoch": 2.3519008062315965e-05,
      "grad_norm": 0.6205708980560303,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0346,
      "step": 5
    },
    {
      "epoch": 2.8222809674779156e-05,
      "grad_norm": 2.1557469367980957,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.2234,
      "step": 6
    },
    {
      "epoch": 3.292661128724235e-05,
      "grad_norm": 2.4203062057495117,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.1888,
      "step": 7
    },
    {
      "epoch": 3.763041289970554e-05,
      "grad_norm": 4.038275718688965,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.5306,
      "step": 8
    },
    {
      "epoch": 4.2334214512168735e-05,
      "grad_norm": 5.567150592803955,
      "learning_rate": 3.6e-06,
      "loss": 0.8904,
      "step": 9
    },
    {
      "epoch": 4.703801612463193e-05,
      "grad_norm": 2.0147883892059326,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.2581,
      "step": 10
    },
    {
      "epoch": 5.174181773709512e-05,
      "grad_norm": 2.8323683738708496,
      "learning_rate": 4.4e-06,
      "loss": 0.1862,
      "step": 11
    },
    {
      "epoch": 5.644561934955831e-05,
      "grad_norm": 1.6947802305221558,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.1079,
      "step": 12
    },
    {
      "epoch": 6.11494209620215e-05,
      "grad_norm": 4.809198379516602,
      "learning_rate": 5.2e-06,
      "loss": 0.6683,
      "step": 13
    },
    {
      "epoch": 6.58532225744847e-05,
      "grad_norm": 5.638553142547607,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.8748,
      "step": 14
    },
    {
      "epoch": 7.055702418694789e-05,
      "grad_norm": 1.4174034595489502,
      "learning_rate": 6e-06,
      "loss": 0.1869,
      "step": 15
    },
    {
      "epoch": 7.526082579941108e-05,
      "grad_norm": 7.593025207519531,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.7924,
      "step": 16
    },
    {
      "epoch": 7.996462741187428e-05,
      "grad_norm": 7.029094696044922,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.8385,
      "step": 17
    },
    {
      "epoch": 8.466842902433747e-05,
      "grad_norm": 4.548736095428467,
      "learning_rate": 7.2e-06,
      "loss": 0.7518,
      "step": 18
    },
    {
      "epoch": 8.937223063680066e-05,
      "grad_norm": 2.9964427947998047,
      "learning_rate": 7.6e-06,
      "loss": 0.6038,
      "step": 19
    },
    {
      "epoch": 9.407603224926386e-05,
      "grad_norm": 7.868130207061768,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.625,
      "step": 20
    },
    {
      "epoch": 9.877983386172705e-05,
      "grad_norm": 2.2138407230377197,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.1034,
      "step": 21
    },
    {
      "epoch": 0.00010348363547419024,
      "grad_norm": 2.5939178466796875,
      "learning_rate": 8.8e-06,
      "loss": 0.3425,
      "step": 22
    },
    {
      "epoch": 0.00010818743708665344,
      "grad_norm": 1.3374449014663696,
      "learning_rate": 9.2e-06,
      "loss": 0.076,
      "step": 23
    },
    {
      "epoch": 0.00011289123869911663,
      "grad_norm": 2.0057127475738525,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.3457,
      "step": 24
    },
    {
      "epoch": 0.00011759504031157981,
      "grad_norm": 2.428433418273926,
      "learning_rate": 1e-05,
      "loss": 0.2852,
      "step": 25
    },
    {
      "epoch": 0.000122298841924043,
      "grad_norm": 4.798130512237549,
      "learning_rate": 1.04e-05,
      "loss": 0.55,
      "step": 26
    },
    {
      "epoch": 0.0001270026435365062,
      "grad_norm": 1.618207335472107,
      "learning_rate": 1.08e-05,
      "loss": 0.219,
      "step": 27
    },
    {
      "epoch": 0.0001317064451489694,
      "grad_norm": 1.1659034490585327,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.053,
      "step": 28
    },
    {
      "epoch": 0.00013641024676143258,
      "grad_norm": 2.4309582710266113,
      "learning_rate": 1.16e-05,
      "loss": 0.346,
      "step": 29
    },
    {
      "epoch": 0.00014111404837389578,
      "grad_norm": 0.9599677324295044,
      "learning_rate": 1.2e-05,
      "loss": 0.0486,
      "step": 30
    },
    {
      "epoch": 0.00014581784998635898,
      "grad_norm": 4.665159225463867,
      "learning_rate": 1.24e-05,
      "loss": 0.4113,
      "step": 31
    },
    {
      "epoch": 0.00015052165159882216,
      "grad_norm": 5.331118106842041,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.9317,
      "step": 32
    },
    {
      "epoch": 0.00015522545321128536,
      "grad_norm": 1.710689663887024,
      "learning_rate": 1.32e-05,
      "loss": 0.2711,
      "step": 33
    },
    {
      "epoch": 0.00015992925482374856,
      "grad_norm": 2.14216947555542,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.1654,
      "step": 34
    },
    {
      "epoch": 0.00016463305643621174,
      "grad_norm": 4.485231399536133,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5813,
      "step": 35
    },
    {
      "epoch": 0.00016933685804867494,
      "grad_norm": 1.9404183626174927,
      "learning_rate": 1.44e-05,
      "loss": 0.3459,
      "step": 36
    },
    {
      "epoch": 0.00017404065966113814,
      "grad_norm": 1.6494685411453247,
      "learning_rate": 1.48e-05,
      "loss": 0.2261,
      "step": 37
    },
    {
      "epoch": 0.00017874446127360131,
      "grad_norm": 2.5063393115997314,
      "learning_rate": 1.52e-05,
      "loss": 0.2159,
      "step": 38
    },
    {
      "epoch": 0.00018344826288606452,
      "grad_norm": 1.0466309785842896,
      "learning_rate": 1.56e-05,
      "loss": 0.0309,
      "step": 39
    },
    {
      "epoch": 0.00018815206449852772,
      "grad_norm": 1.0847481489181519,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1703,
      "step": 40
    },
    {
      "epoch": 0.0001928558661109909,
      "grad_norm": 3.547720193862915,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.119,
      "step": 41
    },
    {
      "epoch": 0.0001975596677234541,
      "grad_norm": 1.8616689443588257,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2958,
      "step": 42
    },
    {
      "epoch": 0.0002022634693359173,
      "grad_norm": 5.395574569702148,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.3685,
      "step": 43
    },
    {
      "epoch": 0.00020696727094838047,
      "grad_norm": 2.6937150955200195,
      "learning_rate": 1.76e-05,
      "loss": 0.1679,
      "step": 44
    },
    {
      "epoch": 0.00021167107256084367,
      "grad_norm": 2.65310001373291,
      "learning_rate": 1.8e-05,
      "loss": 0.1311,
      "step": 45
    },
    {
      "epoch": 0.00021637487417330687,
      "grad_norm": 3.083963632583618,
      "learning_rate": 1.84e-05,
      "loss": 0.3188,
      "step": 46
    },
    {
      "epoch": 0.00022107867578577005,
      "grad_norm": 2.4717369079589844,
      "learning_rate": 1.88e-05,
      "loss": 0.075,
      "step": 47
    },
    {
      "epoch": 0.00022578247739823325,
      "grad_norm": 2.2474513053894043,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.2237,
      "step": 48
    },
    {
      "epoch": 0.00023048627901069645,
      "grad_norm": 2.5590081214904785,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.1322,
      "step": 49
    },
    {
      "epoch": 0.00023519008062315963,
      "grad_norm": 3.6861019134521484,
      "learning_rate": 2e-05,
      "loss": 0.253,
      "step": 50
    },
    {
      "epoch": 0.00023989388223562283,
      "grad_norm": 5.394125461578369,
      "learning_rate": 2.04e-05,
      "loss": 0.1632,
      "step": 51
    },
    {
      "epoch": 0.000244597683848086,
      "grad_norm": 2.0115935802459717,
      "learning_rate": 2.08e-05,
      "loss": 0.1024,
      "step": 52
    },
    {
      "epoch": 0.0002493014854605492,
      "grad_norm": 3.8001034259796143,
      "learning_rate": 2.12e-05,
      "loss": 0.3323,
      "step": 53
    },
    {
      "epoch": 0.0002540052870730124,
      "grad_norm": 5.794704914093018,
      "learning_rate": 2.16e-05,
      "loss": 0.1992,
      "step": 54
    },
    {
      "epoch": 0.0002587090886854756,
      "grad_norm": 10.996417999267578,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.933,
      "step": 55
    },
    {
      "epoch": 0.0002634128902979388,
      "grad_norm": 10.299786567687988,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.9505,
      "step": 56
    },
    {
      "epoch": 0.000268116691910402,
      "grad_norm": 12.242238998413086,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.0156,
      "step": 57
    },
    {
      "epoch": 0.00027282049352286516,
      "grad_norm": 2.7500998973846436,
      "learning_rate": 2.32e-05,
      "loss": 0.271,
      "step": 58
    },
    {
      "epoch": 0.00027752429513532836,
      "grad_norm": 2.10526442527771,
      "learning_rate": 2.36e-05,
      "loss": 0.089,
      "step": 59
    },
    {
      "epoch": 0.00028222809674779156,
      "grad_norm": 12.716087341308594,
      "learning_rate": 2.4e-05,
      "loss": 0.8743,
      "step": 60
    },
    {
      "epoch": 0.00028693189836025477,
      "grad_norm": 9.265603065490723,
      "learning_rate": 2.44e-05,
      "loss": 0.5664,
      "step": 61
    },
    {
      "epoch": 0.00029163569997271797,
      "grad_norm": 13.476297378540039,
      "learning_rate": 2.48e-05,
      "loss": 0.6459,
      "step": 62
    },
    {
      "epoch": 0.00029633950158518117,
      "grad_norm": 2.3783257007598877,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0892,
      "step": 63
    },
    {
      "epoch": 0.0003010433031976443,
      "grad_norm": 12.59670639038086,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.4919,
      "step": 64
    },
    {
      "epoch": 0.0003057471048101075,
      "grad_norm": 9.37648868560791,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4956,
      "step": 65
    },
    {
      "epoch": 0.0003104509064225707,
      "grad_norm": 3.158296585083008,
      "learning_rate": 2.64e-05,
      "loss": 0.1308,
      "step": 66
    },
    {
      "epoch": 0.0003151547080350339,
      "grad_norm": 8.29719352722168,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.2386,
      "step": 67
    },
    {
      "epoch": 0.0003198585096474971,
      "grad_norm": 4.998426914215088,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.2414,
      "step": 68
    },
    {
      "epoch": 0.0003245623112599603,
      "grad_norm": 3.8499088287353516,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.1199,
      "step": 69
    },
    {
      "epoch": 0.00032926611287242347,
      "grad_norm": 7.454540729522705,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.451,
      "step": 70
    },
    {
      "epoch": 0.0003339699144848867,
      "grad_norm": 4.821732997894287,
      "learning_rate": 2.84e-05,
      "loss": 0.2166,
      "step": 71
    },
    {
      "epoch": 0.0003386737160973499,
      "grad_norm": 2.726874589920044,
      "learning_rate": 2.88e-05,
      "loss": 0.0991,
      "step": 72
    },
    {
      "epoch": 0.0003433775177098131,
      "grad_norm": 13.291203498840332,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.9011,
      "step": 73
    },
    {
      "epoch": 0.0003480813193222763,
      "grad_norm": 8.086989402770996,
      "learning_rate": 2.96e-05,
      "loss": 0.3664,
      "step": 74
    },
    {
      "epoch": 0.0003527851209347395,
      "grad_norm": 14.738718032836914,
      "learning_rate": 3e-05,
      "loss": 0.6584,
      "step": 75
    },
    {
      "epoch": 0.00035748892254720263,
      "grad_norm": 10.834789276123047,
      "learning_rate": 3.04e-05,
      "loss": 0.5116,
      "step": 76
    },
    {
      "epoch": 0.00036219272415966583,
      "grad_norm": 2.7025794982910156,
      "learning_rate": 3.08e-05,
      "loss": 0.1983,
      "step": 77
    },
    {
      "epoch": 0.00036689652577212903,
      "grad_norm": 2.4473915100097656,
      "learning_rate": 3.12e-05,
      "loss": 0.1744,
      "step": 78
    },
    {
      "epoch": 0.00037160032738459223,
      "grad_norm": 6.345713138580322,
      "learning_rate": 3.16e-05,
      "loss": 0.229,
      "step": 79
    },
    {
      "epoch": 0.00037630412899705544,
      "grad_norm": 10.160054206848145,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4571,
      "step": 80
    },
    {
      "epoch": 0.00038100793060951864,
      "grad_norm": 2.898986339569092,
      "learning_rate": 3.24e-05,
      "loss": 0.0716,
      "step": 81
    },
    {
      "epoch": 0.0003857117322219818,
      "grad_norm": 5.519388198852539,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.1854,
      "step": 82
    },
    {
      "epoch": 0.000390415533834445,
      "grad_norm": 4.479554653167725,
      "learning_rate": 3.32e-05,
      "loss": 0.2935,
      "step": 83
    },
    {
      "epoch": 0.0003951193354469082,
      "grad_norm": 10.21839714050293,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.4533,
      "step": 84
    },
    {
      "epoch": 0.0003998231370593714,
      "grad_norm": 1.8370931148529053,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0839,
      "step": 85
    },
    {
      "epoch": 0.0004045269386718346,
      "grad_norm": 6.385246276855469,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.306,
      "step": 86
    },
    {
      "epoch": 0.0004092307402842978,
      "grad_norm": 9.136309623718262,
      "learning_rate": 3.48e-05,
      "loss": 0.3864,
      "step": 87
    },
    {
      "epoch": 0.00041393454189676094,
      "grad_norm": 6.78175163269043,
      "learning_rate": 3.52e-05,
      "loss": 0.2843,
      "step": 88
    },
    {
      "epoch": 0.00041863834350922414,
      "grad_norm": 1.9543720483779907,
      "learning_rate": 3.56e-05,
      "loss": 0.0363,
      "step": 89
    },
    {
      "epoch": 0.00042334214512168735,
      "grad_norm": 4.420919895172119,
      "learning_rate": 3.6e-05,
      "loss": 0.2107,
      "step": 90
    },
    {
      "epoch": 0.00042804594673415055,
      "grad_norm": 5.062385559082031,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.1204,
      "step": 91
    },
    {
      "epoch": 0.00043274974834661375,
      "grad_norm": 4.584104537963867,
      "learning_rate": 3.68e-05,
      "loss": 0.5167,
      "step": 92
    },
    {
      "epoch": 0.00043745354995907695,
      "grad_norm": 11.308633804321289,
      "learning_rate": 3.72e-05,
      "loss": 0.5346,
      "step": 93
    },
    {
      "epoch": 0.0004421573515715401,
      "grad_norm": 9.862894058227539,
      "learning_rate": 3.76e-05,
      "loss": 0.7457,
      "step": 94
    },
    {
      "epoch": 0.0004468611531840033,
      "grad_norm": 33.0572509765625,
      "learning_rate": 3.8e-05,
      "loss": 0.7491,
      "step": 95
    },
    {
      "epoch": 0.0004515649547964665,
      "grad_norm": 2.078057289123535,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0662,
      "step": 96
    },
    {
      "epoch": 0.0004562687564089297,
      "grad_norm": 9.548422813415527,
      "learning_rate": 3.88e-05,
      "loss": 1.217,
      "step": 97
    },
    {
      "epoch": 0.0004609725580213929,
      "grad_norm": 3.2684428691864014,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.1829,
      "step": 98
    },
    {
      "epoch": 0.0004656763596338561,
      "grad_norm": 13.482787132263184,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.3653,
      "step": 99
    },
    {
      "epoch": 0.00047038016124631925,
      "grad_norm": 2.9813427925109863,
      "learning_rate": 4e-05,
      "loss": 0.1295,
      "step": 100
    },
    {
      "epoch": 0.00047508396285878246,
      "grad_norm": 12.643174171447754,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.6676,
      "step": 101
    },
    {
      "epoch": 0.00047978776447124566,
      "grad_norm": 18.31464195251465,
      "learning_rate": 4.08e-05,
      "loss": 1.6749,
      "step": 102
    },
    {
      "epoch": 0.00048449156608370886,
      "grad_norm": 5.860870361328125,
      "learning_rate": 4.12e-05,
      "loss": 0.1404,
      "step": 103
    },
    {
      "epoch": 0.000489195367696172,
      "grad_norm": 7.620203495025635,
      "learning_rate": 4.16e-05,
      "loss": 0.3323,
      "step": 104
    },
    {
      "epoch": 0.0004938991693086353,
      "grad_norm": 4.882566928863525,
      "learning_rate": 4.2e-05,
      "loss": 0.1472,
      "step": 105
    },
    {
      "epoch": 0.0004986029709210984,
      "grad_norm": 7.276484489440918,
      "learning_rate": 4.24e-05,
      "loss": 0.1996,
      "step": 106
    },
    {
      "epoch": 0.0005033067725335617,
      "grad_norm": 9.876720428466797,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.7294,
      "step": 107
    },
    {
      "epoch": 0.0005080105741460248,
      "grad_norm": 8.190587043762207,
      "learning_rate": 4.32e-05,
      "loss": 0.4564,
      "step": 108
    },
    {
      "epoch": 0.000512714375758488,
      "grad_norm": 12.742801666259766,
      "learning_rate": 4.36e-05,
      "loss": 1.1462,
      "step": 109
    },
    {
      "epoch": 0.0005174181773709512,
      "grad_norm": 4.173835277557373,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.1501,
      "step": 110
    },
    {
      "epoch": 0.0005221219789834144,
      "grad_norm": 2.1551454067230225,
      "learning_rate": 4.44e-05,
      "loss": 0.0968,
      "step": 111
    },
    {
      "epoch": 0.0005268257805958776,
      "grad_norm": 9.060416221618652,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.9367,
      "step": 112
    },
    {
      "epoch": 0.0005315295822083408,
      "grad_norm": 1.3614718914031982,
      "learning_rate": 4.52e-05,
      "loss": 0.0317,
      "step": 113
    },
    {
      "epoch": 0.000536233383820804,
      "grad_norm": 7.594012260437012,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.8011,
      "step": 114
    },
    {
      "epoch": 0.0005409371854332672,
      "grad_norm": 8.204193115234375,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.4185,
      "step": 115
    },
    {
      "epoch": 0.0005456409870457303,
      "grad_norm": 1.4568963050842285,
      "learning_rate": 4.64e-05,
      "loss": 0.0542,
      "step": 116
    },
    {
      "epoch": 0.0005503447886581936,
      "grad_norm": 9.194090843200684,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.469,
      "step": 117
    },
    {
      "epoch": 0.0005550485902706567,
      "grad_norm": 2.9352447986602783,
      "learning_rate": 4.72e-05,
      "loss": 0.2225,
      "step": 118
    },
    {
      "epoch": 0.00055975239188312,
      "grad_norm": 18.89095115661621,
      "learning_rate": 4.76e-05,
      "loss": 0.8045,
      "step": 119
    },
    {
      "epoch": 0.0005644561934955831,
      "grad_norm": 1.3745579719543457,
      "learning_rate": 4.8e-05,
      "loss": 0.0519,
      "step": 120
    },
    {
      "epoch": 0.0005691599951080463,
      "grad_norm": 6.6944475173950195,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.4802,
      "step": 121
    },
    {
      "epoch": 0.0005738637967205095,
      "grad_norm": 3.0428519248962402,
      "learning_rate": 4.88e-05,
      "loss": 0.1497,
      "step": 122
    },
    {
      "epoch": 0.0005785675983329727,
      "grad_norm": 5.592650413513184,
      "learning_rate": 4.92e-05,
      "loss": 0.5897,
      "step": 123
    },
    {
      "epoch": 0.0005832713999454359,
      "grad_norm": 2.0244908332824707,
      "learning_rate": 4.96e-05,
      "loss": 0.0951,
      "step": 124
    },
    {
      "epoch": 0.0005879752015578991,
      "grad_norm": 4.579395771026611,
      "learning_rate": 5e-05,
      "loss": 0.2466,
      "step": 125
    },
    {
      "epoch": 0.0005926790031703623,
      "grad_norm": 7.484107971191406,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 0.4343,
      "step": 126
    },
    {
      "epoch": 0.0005973828047828255,
      "grad_norm": 3.6764602661132812,
      "learning_rate": 5.08e-05,
      "loss": 0.2355,
      "step": 127
    },
    {
      "epoch": 0.0006020866063952886,
      "grad_norm": 2.7435081005096436,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 0.11,
      "step": 128
    },
    {
      "epoch": 0.0006067904080077519,
      "grad_norm": 2.7252235412597656,
      "learning_rate": 5.16e-05,
      "loss": 0.1106,
      "step": 129
    },
    {
      "epoch": 0.000611494209620215,
      "grad_norm": 5.371652126312256,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.262,
      "step": 130
    },
    {
      "epoch": 0.0006161980112326783,
      "grad_norm": 5.665862560272217,
      "learning_rate": 5.2400000000000007e-05,
      "loss": 0.4071,
      "step": 131
    },
    {
      "epoch": 0.0006209018128451414,
      "grad_norm": 6.609068870544434,
      "learning_rate": 5.28e-05,
      "loss": 0.5057,
      "step": 132
    },
    {
      "epoch": 0.0006256056144576046,
      "grad_norm": 1.3313148021697998,
      "learning_rate": 5.3200000000000006e-05,
      "loss": 0.0579,
      "step": 133
    },
    {
      "epoch": 0.0006303094160700678,
      "grad_norm": 3.316894769668579,
      "learning_rate": 5.360000000000001e-05,
      "loss": 0.1622,
      "step": 134
    },
    {
      "epoch": 0.000635013217682531,
      "grad_norm": 4.961429595947266,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.3672,
      "step": 135
    },
    {
      "epoch": 0.0006397170192949942,
      "grad_norm": 5.017652988433838,
      "learning_rate": 5.440000000000001e-05,
      "loss": 0.3129,
      "step": 136
    },
    {
      "epoch": 0.0006444208209074574,
      "grad_norm": 8.691753387451172,
      "learning_rate": 5.4800000000000004e-05,
      "loss": 0.5734,
      "step": 137
    },
    {
      "epoch": 0.0006491246225199207,
      "grad_norm": 0.6221335530281067,
      "learning_rate": 5.520000000000001e-05,
      "loss": 0.0149,
      "step": 138
    },
    {
      "epoch": 0.0006538284241323838,
      "grad_norm": 1.8843748569488525,
      "learning_rate": 5.560000000000001e-05,
      "loss": 0.1999,
      "step": 139
    },
    {
      "epoch": 0.0006585322257448469,
      "grad_norm": 9.793206214904785,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.6915,
      "step": 140
    },
    {
      "epoch": 0.0006632360273573102,
      "grad_norm": 9.097029685974121,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 0.8844,
      "step": 141
    },
    {
      "epoch": 0.0006679398289697733,
      "grad_norm": 5.145073413848877,
      "learning_rate": 5.68e-05,
      "loss": 0.3095,
      "step": 142
    },
    {
      "epoch": 0.0006726436305822366,
      "grad_norm": 5.8812785148620605,
      "learning_rate": 5.72e-05,
      "loss": 0.4756,
      "step": 143
    },
    {
      "epoch": 0.0006773474321946998,
      "grad_norm": 7.443398952484131,
      "learning_rate": 5.76e-05,
      "loss": 0.4334,
      "step": 144
    },
    {
      "epoch": 0.0006820512338071629,
      "grad_norm": 11.967938423156738,
      "learning_rate": 5.8e-05,
      "loss": 0.7469,
      "step": 145
    },
    {
      "epoch": 0.0006867550354196262,
      "grad_norm": 1.9351905584335327,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 0.0628,
      "step": 146
    },
    {
      "epoch": 0.0006914588370320893,
      "grad_norm": 3.3493361473083496,
      "learning_rate": 5.88e-05,
      "loss": 0.1065,
      "step": 147
    },
    {
      "epoch": 0.0006961626386445526,
      "grad_norm": 6.269193172454834,
      "learning_rate": 5.92e-05,
      "loss": 0.4698,
      "step": 148
    },
    {
      "epoch": 0.0007008664402570157,
      "grad_norm": 0.9900681972503662,
      "learning_rate": 5.96e-05,
      "loss": 0.0214,
      "step": 149
    },
    {
      "epoch": 0.000705570241869479,
      "grad_norm": 3.5989089012145996,
      "learning_rate": 6e-05,
      "loss": 0.3623,
      "step": 150
    },
    {
      "epoch": 0.0007102740434819421,
      "grad_norm": 1.4215503931045532,
      "learning_rate": 6.04e-05,
      "loss": 0.1001,
      "step": 151
    },
    {
      "epoch": 0.0007149778450944053,
      "grad_norm": 2.1982524394989014,
      "learning_rate": 6.08e-05,
      "loss": 0.0498,
      "step": 152
    },
    {
      "epoch": 0.0007196816467068685,
      "grad_norm": 4.4305267333984375,
      "learning_rate": 6.12e-05,
      "loss": 0.3794,
      "step": 153
    },
    {
      "epoch": 0.0007243854483193317,
      "grad_norm": 1.8252030611038208,
      "learning_rate": 6.16e-05,
      "loss": 0.1198,
      "step": 154
    },
    {
      "epoch": 0.0007290892499317949,
      "grad_norm": 6.528279781341553,
      "learning_rate": 6.2e-05,
      "loss": 0.2826,
      "step": 155
    },
    {
      "epoch": 0.0007337930515442581,
      "grad_norm": 3.325836658477783,
      "learning_rate": 6.24e-05,
      "loss": 0.0713,
      "step": 156
    },
    {
      "epoch": 0.0007384968531567212,
      "grad_norm": 2.212897539138794,
      "learning_rate": 6.280000000000001e-05,
      "loss": 0.1107,
      "step": 157
    },
    {
      "epoch": 0.0007432006547691845,
      "grad_norm": 12.83935260772705,
      "learning_rate": 6.32e-05,
      "loss": 0.7381,
      "step": 158
    },
    {
      "epoch": 0.0007479044563816476,
      "grad_norm": 6.565043926239014,
      "learning_rate": 6.36e-05,
      "loss": 0.28,
      "step": 159
    },
    {
      "epoch": 0.0007526082579941109,
      "grad_norm": 7.796690940856934,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.957,
      "step": 160
    },
    {
      "epoch": 0.000757312059606574,
      "grad_norm": 1.417212963104248,
      "learning_rate": 6.440000000000001e-05,
      "loss": 0.0339,
      "step": 161
    },
    {
      "epoch": 0.0007620158612190373,
      "grad_norm": 8.943127632141113,
      "learning_rate": 6.48e-05,
      "loss": 0.8647,
      "step": 162
    },
    {
      "epoch": 0.0007667196628315004,
      "grad_norm": 4.862338542938232,
      "learning_rate": 6.52e-05,
      "loss": 0.2068,
      "step": 163
    },
    {
      "epoch": 0.0007714234644439636,
      "grad_norm": 11.924006462097168,
      "learning_rate": 6.560000000000001e-05,
      "loss": 0.9752,
      "step": 164
    },
    {
      "epoch": 0.0007761272660564268,
      "grad_norm": 9.445218086242676,
      "learning_rate": 6.6e-05,
      "loss": 0.7182,
      "step": 165
    },
    {
      "epoch": 0.00078083106766889,
      "grad_norm": 13.994558334350586,
      "learning_rate": 6.64e-05,
      "loss": 0.857,
      "step": 166
    },
    {
      "epoch": 0.0007855348692813532,
      "grad_norm": 0.8699049949645996,
      "learning_rate": 6.680000000000001e-05,
      "loss": 0.0215,
      "step": 167
    },
    {
      "epoch": 0.0007902386708938164,
      "grad_norm": 1.8705347776412964,
      "learning_rate": 6.720000000000001e-05,
      "loss": 0.0448,
      "step": 168
    },
    {
      "epoch": 0.0007949424725062795,
      "grad_norm": 0.3770587742328644,
      "learning_rate": 6.76e-05,
      "loss": 0.009,
      "step": 169
    },
    {
      "epoch": 0.0007996462741187428,
      "grad_norm": 0.5122890472412109,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0142,
      "step": 170
    },
    {
      "epoch": 0.0008043500757312059,
      "grad_norm": 0.4145972430706024,
      "learning_rate": 6.840000000000001e-05,
      "loss": 0.0102,
      "step": 171
    },
    {
      "epoch": 0.0008090538773436692,
      "grad_norm": 3.248856782913208,
      "learning_rate": 6.879999999999999e-05,
      "loss": 0.1887,
      "step": 172
    },
    {
      "epoch": 0.0008137576789561323,
      "grad_norm": 4.414053440093994,
      "learning_rate": 6.92e-05,
      "loss": 0.4014,
      "step": 173
    },
    {
      "epoch": 0.0008184614805685956,
      "grad_norm": 5.474277019500732,
      "learning_rate": 6.96e-05,
      "loss": 0.418,
      "step": 174
    },
    {
      "epoch": 0.0008231652821810587,
      "grad_norm": 4.56879997253418,
      "learning_rate": 7e-05,
      "loss": 0.3402,
      "step": 175
    },
    {
      "epoch": 0.0008278690837935219,
      "grad_norm": 9.654291152954102,
      "learning_rate": 7.04e-05,
      "loss": 1.128,
      "step": 176
    },
    {
      "epoch": 0.0008325728854059851,
      "grad_norm": 5.972589015960693,
      "learning_rate": 7.08e-05,
      "loss": 0.4881,
      "step": 177
    },
    {
      "epoch": 0.0008372766870184483,
      "grad_norm": 10.788414001464844,
      "learning_rate": 7.12e-05,
      "loss": 0.3418,
      "step": 178
    },
    {
      "epoch": 0.0008419804886309115,
      "grad_norm": 6.574192523956299,
      "learning_rate": 7.16e-05,
      "loss": 0.7221,
      "step": 179
    },
    {
      "epoch": 0.0008466842902433747,
      "grad_norm": 9.723762512207031,
      "learning_rate": 7.2e-05,
      "loss": 0.9245,
      "step": 180
    },
    {
      "epoch": 0.0008513880918558378,
      "grad_norm": 1.9738357067108154,
      "learning_rate": 7.24e-05,
      "loss": 0.1339,
      "step": 181
    },
    {
      "epoch": 0.0008560918934683011,
      "grad_norm": 6.301769733428955,
      "learning_rate": 7.280000000000001e-05,
      "loss": 0.377,
      "step": 182
    },
    {
      "epoch": 0.0008607956950807642,
      "grad_norm": 2.496238946914673,
      "learning_rate": 7.32e-05,
      "loss": 0.2578,
      "step": 183
    },
    {
      "epoch": 0.0008654994966932275,
      "grad_norm": 0.6077069044113159,
      "learning_rate": 7.36e-05,
      "loss": 0.0142,
      "step": 184
    },
    {
      "epoch": 0.0008702032983056906,
      "grad_norm": 5.066396236419678,
      "learning_rate": 7.4e-05,
      "loss": 0.5662,
      "step": 185
    },
    {
      "epoch": 0.0008749070999181539,
      "grad_norm": 5.250504016876221,
      "learning_rate": 7.44e-05,
      "loss": 0.3519,
      "step": 186
    },
    {
      "epoch": 0.000879610901530617,
      "grad_norm": 1.6067606210708618,
      "learning_rate": 7.48e-05,
      "loss": 0.2353,
      "step": 187
    },
    {
      "epoch": 0.0008843147031430802,
      "grad_norm": 4.391499042510986,
      "learning_rate": 7.52e-05,
      "loss": 0.2789,
      "step": 188
    },
    {
      "epoch": 0.0008890185047555435,
      "grad_norm": 4.714088439941406,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.3386,
      "step": 189
    },
    {
      "epoch": 0.0008937223063680066,
      "grad_norm": 1.2048958539962769,
      "learning_rate": 7.6e-05,
      "loss": 0.0683,
      "step": 190
    },
    {
      "epoch": 0.0008984261079804699,
      "grad_norm": 2.281769275665283,
      "learning_rate": 7.64e-05,
      "loss": 0.2103,
      "step": 191
    },
    {
      "epoch": 0.000903129909592933,
      "grad_norm": 2.5437893867492676,
      "learning_rate": 7.680000000000001e-05,
      "loss": 0.3243,
      "step": 192
    },
    {
      "epoch": 0.0009078337112053962,
      "grad_norm": 4.540826320648193,
      "learning_rate": 7.72e-05,
      "loss": 0.3501,
      "step": 193
    },
    {
      "epoch": 0.0009125375128178594,
      "grad_norm": 5.794620037078857,
      "learning_rate": 7.76e-05,
      "loss": 0.469,
      "step": 194
    },
    {
      "epoch": 0.0009172413144303226,
      "grad_norm": 4.361606121063232,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.4278,
      "step": 195
    },
    {
      "epoch": 0.0009219451160427858,
      "grad_norm": 4.752676963806152,
      "learning_rate": 7.840000000000001e-05,
      "loss": 0.2079,
      "step": 196
    },
    {
      "epoch": 0.000926648917655249,
      "grad_norm": 6.17271614074707,
      "learning_rate": 7.88e-05,
      "loss": 0.2917,
      "step": 197
    },
    {
      "epoch": 0.0009313527192677122,
      "grad_norm": 9.199727058410645,
      "learning_rate": 7.920000000000001e-05,
      "loss": 0.5121,
      "step": 198
    },
    {
      "epoch": 0.0009360565208801754,
      "grad_norm": 7.618838787078857,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.4281,
      "step": 199
    },
    {
      "epoch": 0.0009407603224926385,
      "grad_norm": 3.7657148838043213,
      "learning_rate": 8e-05,
      "loss": 0.1815,
      "step": 200
    },
    {
      "epoch": 0.0009454641241051018,
      "grad_norm": 6.839115142822266,
      "learning_rate": 8.04e-05,
      "loss": 0.3815,
      "step": 201
    },
    {
      "epoch": 0.0009501679257175649,
      "grad_norm": 14.295804023742676,
      "learning_rate": 8.080000000000001e-05,
      "loss": 0.2907,
      "step": 202
    },
    {
      "epoch": 0.0009548717273300282,
      "grad_norm": 3.368785858154297,
      "learning_rate": 8.120000000000001e-05,
      "loss": 0.2574,
      "step": 203
    },
    {
      "epoch": 0.0009595755289424913,
      "grad_norm": 2.79160475730896,
      "learning_rate": 8.16e-05,
      "loss": 0.1702,
      "step": 204
    },
    {
      "epoch": 0.0009642793305549545,
      "grad_norm": 6.467050552368164,
      "learning_rate": 8.2e-05,
      "loss": 0.3644,
      "step": 205
    },
    {
      "epoch": 0.0009689831321674177,
      "grad_norm": 2.370896339416504,
      "learning_rate": 8.24e-05,
      "loss": 0.2172,
      "step": 206
    },
    {
      "epoch": 0.0009736869337798809,
      "grad_norm": 3.2137718200683594,
      "learning_rate": 8.28e-05,
      "loss": 0.1011,
      "step": 207
    },
    {
      "epoch": 0.000978390735392344,
      "grad_norm": 1.7555370330810547,
      "learning_rate": 8.32e-05,
      "loss": 0.1656,
      "step": 208
    },
    {
      "epoch": 0.0009830945370048073,
      "grad_norm": 2.7426974773406982,
      "learning_rate": 8.36e-05,
      "loss": 0.273,
      "step": 209
    },
    {
      "epoch": 0.0009877983386172705,
      "grad_norm": 2.8553502559661865,
      "learning_rate": 8.4e-05,
      "loss": 0.1907,
      "step": 210
    },
    {
      "epoch": 0.0009925021402297336,
      "grad_norm": 1.90424382686615,
      "learning_rate": 8.44e-05,
      "loss": 0.107,
      "step": 211
    },
    {
      "epoch": 0.0009972059418421968,
      "grad_norm": 5.73706579208374,
      "learning_rate": 8.48e-05,
      "loss": 0.3738,
      "step": 212
    },
    {
      "epoch": 0.00100190974345466,
      "grad_norm": 2.8048489093780518,
      "learning_rate": 8.52e-05,
      "loss": 0.4472,
      "step": 213
    },
    {
      "epoch": 0.0010066135450671233,
      "grad_norm": 1.6752967834472656,
      "learning_rate": 8.560000000000001e-05,
      "loss": 0.0435,
      "step": 214
    },
    {
      "epoch": 0.0010113173466795864,
      "grad_norm": 2.529334306716919,
      "learning_rate": 8.6e-05,
      "loss": 0.0836,
      "step": 215
    },
    {
      "epoch": 0.0010160211482920496,
      "grad_norm": 4.091884613037109,
      "learning_rate": 8.64e-05,
      "loss": 0.4066,
      "step": 216
    },
    {
      "epoch": 0.0010207249499045129,
      "grad_norm": 2.7089121341705322,
      "learning_rate": 8.680000000000001e-05,
      "loss": 0.3219,
      "step": 217
    },
    {
      "epoch": 0.001025428751516976,
      "grad_norm": 1.5982224941253662,
      "learning_rate": 8.72e-05,
      "loss": 0.0786,
      "step": 218
    },
    {
      "epoch": 0.0010301325531294392,
      "grad_norm": 4.552698135375977,
      "learning_rate": 8.76e-05,
      "loss": 0.3143,
      "step": 219
    },
    {
      "epoch": 0.0010348363547419024,
      "grad_norm": 2.8155717849731445,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.1612,
      "step": 220
    },
    {
      "epoch": 0.0010395401563543657,
      "grad_norm": 1.075063943862915,
      "learning_rate": 8.840000000000001e-05,
      "loss": 0.0651,
      "step": 221
    },
    {
      "epoch": 0.0010442439579668287,
      "grad_norm": 5.97920036315918,
      "learning_rate": 8.88e-05,
      "loss": 0.3641,
      "step": 222
    },
    {
      "epoch": 0.001048947759579292,
      "grad_norm": 3.6123642921447754,
      "learning_rate": 8.92e-05,
      "loss": 0.1178,
      "step": 223
    },
    {
      "epoch": 0.0010536515611917552,
      "grad_norm": 5.538098335266113,
      "learning_rate": 8.960000000000001e-05,
      "loss": 0.2042,
      "step": 224
    },
    {
      "epoch": 0.0010583553628042183,
      "grad_norm": 6.724918842315674,
      "learning_rate": 9e-05,
      "loss": 0.5351,
      "step": 225
    },
    {
      "epoch": 0.0010630591644166815,
      "grad_norm": 2.9325978755950928,
      "learning_rate": 9.04e-05,
      "loss": 0.105,
      "step": 226
    },
    {
      "epoch": 0.0010677629660291448,
      "grad_norm": 9.571220397949219,
      "learning_rate": 9.080000000000001e-05,
      "loss": 0.8588,
      "step": 227
    },
    {
      "epoch": 0.001072466767641608,
      "grad_norm": 6.101480484008789,
      "learning_rate": 9.120000000000001e-05,
      "loss": 0.6106,
      "step": 228
    },
    {
      "epoch": 0.001077170569254071,
      "grad_norm": 10.276091575622559,
      "learning_rate": 9.16e-05,
      "loss": 0.7576,
      "step": 229
    },
    {
      "epoch": 0.0010818743708665343,
      "grad_norm": 1.127669095993042,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.0297,
      "step": 230
    },
    {
      "epoch": 0.0010865781724789976,
      "grad_norm": 2.7628986835479736,
      "learning_rate": 9.240000000000001e-05,
      "loss": 0.1571,
      "step": 231
    },
    {
      "epoch": 0.0010912819740914606,
      "grad_norm": 14.119653701782227,
      "learning_rate": 9.28e-05,
      "loss": 1.9333,
      "step": 232
    },
    {
      "epoch": 0.001095985775703924,
      "grad_norm": 13.460287094116211,
      "learning_rate": 9.320000000000002e-05,
      "loss": 1.212,
      "step": 233
    },
    {
      "epoch": 0.0011006895773163872,
      "grad_norm": 4.873778343200684,
      "learning_rate": 9.360000000000001e-05,
      "loss": 0.3513,
      "step": 234
    },
    {
      "epoch": 0.0011053933789288502,
      "grad_norm": 3.878368616104126,
      "learning_rate": 9.4e-05,
      "loss": 0.2724,
      "step": 235
    },
    {
      "epoch": 0.0011100971805413134,
      "grad_norm": 3.217156410217285,
      "learning_rate": 9.44e-05,
      "loss": 0.2127,
      "step": 236
    },
    {
      "epoch": 0.0011148009821537767,
      "grad_norm": 6.07342004776001,
      "learning_rate": 9.48e-05,
      "loss": 0.2077,
      "step": 237
    },
    {
      "epoch": 0.00111950478376624,
      "grad_norm": 1.9611207246780396,
      "learning_rate": 9.52e-05,
      "loss": 0.1384,
      "step": 238
    },
    {
      "epoch": 0.001124208585378703,
      "grad_norm": 3.120346784591675,
      "learning_rate": 9.56e-05,
      "loss": 0.3784,
      "step": 239
    },
    {
      "epoch": 0.0011289123869911663,
      "grad_norm": 5.3007493019104,
      "learning_rate": 9.6e-05,
      "loss": 0.8169,
      "step": 240
    },
    {
      "epoch": 0.0011336161886036295,
      "grad_norm": 0.5454070568084717,
      "learning_rate": 9.64e-05,
      "loss": 0.0179,
      "step": 241
    },
    {
      "epoch": 0.0011383199902160925,
      "grad_norm": 13.921612739562988,
      "learning_rate": 9.680000000000001e-05,
      "loss": 0.5207,
      "step": 242
    },
    {
      "epoch": 0.0011430237918285558,
      "grad_norm": 3.9617912769317627,
      "learning_rate": 9.72e-05,
      "loss": 0.1699,
      "step": 243
    },
    {
      "epoch": 0.001147727593441019,
      "grad_norm": 2.149620532989502,
      "learning_rate": 9.76e-05,
      "loss": 0.1292,
      "step": 244
    },
    {
      "epoch": 0.0011524313950534823,
      "grad_norm": 5.69052791595459,
      "learning_rate": 9.8e-05,
      "loss": 0.5643,
      "step": 245
    },
    {
      "epoch": 0.0011571351966659454,
      "grad_norm": 2.9579975605010986,
      "learning_rate": 9.84e-05,
      "loss": 0.2219,
      "step": 246
    },
    {
      "epoch": 0.0011618389982784086,
      "grad_norm": 2.4756321907043457,
      "learning_rate": 9.88e-05,
      "loss": 0.2636,
      "step": 247
    },
    {
      "epoch": 0.0011665427998908719,
      "grad_norm": 3.988203287124634,
      "learning_rate": 9.92e-05,
      "loss": 0.491,
      "step": 248
    },
    {
      "epoch": 0.001171246601503335,
      "grad_norm": 1.099124789237976,
      "learning_rate": 9.960000000000001e-05,
      "loss": 0.0622,
      "step": 249
    },
    {
      "epoch": 0.0011759504031157982,
      "grad_norm": 4.605346202850342,
      "learning_rate": 0.0001,
      "loss": 0.2227,
      "step": 250
    },
    {
      "epoch": 0.0011806542047282614,
      "grad_norm": 0.6628114581108093,
      "learning_rate": 0.0001004,
      "loss": 0.0529,
      "step": 251
    },
    {
      "epoch": 0.0011853580063407247,
      "grad_norm": 4.205711364746094,
      "learning_rate": 0.00010080000000000001,
      "loss": 0.2924,
      "step": 252
    },
    {
      "epoch": 0.0011900618079531877,
      "grad_norm": 3.8498294353485107,
      "learning_rate": 0.00010120000000000001,
      "loss": 0.2712,
      "step": 253
    },
    {
      "epoch": 0.001194765609565651,
      "grad_norm": 9.447155952453613,
      "learning_rate": 0.0001016,
      "loss": 0.9995,
      "step": 254
    },
    {
      "epoch": 0.0011994694111781142,
      "grad_norm": 3.0022261142730713,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.3952,
      "step": 255
    },
    {
      "epoch": 0.0012041732127905773,
      "grad_norm": 6.012879371643066,
      "learning_rate": 0.00010240000000000001,
      "loss": 1.2967,
      "step": 256
    },
    {
      "epoch": 0.0012088770144030405,
      "grad_norm": 2.265723466873169,
      "learning_rate": 0.0001028,
      "loss": 0.1677,
      "step": 257
    },
    {
      "epoch": 0.0012135808160155038,
      "grad_norm": 1.6405222415924072,
      "learning_rate": 0.0001032,
      "loss": 0.2574,
      "step": 258
    },
    {
      "epoch": 0.0012182846176279668,
      "grad_norm": 2.443418025970459,
      "learning_rate": 0.00010360000000000001,
      "loss": 0.1905,
      "step": 259
    },
    {
      "epoch": 0.00122298841924043,
      "grad_norm": 4.350813865661621,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.267,
      "step": 260
    },
    {
      "epoch": 0.0012276922208528933,
      "grad_norm": 1.9887791872024536,
      "learning_rate": 0.0001044,
      "loss": 0.1648,
      "step": 261
    },
    {
      "epoch": 0.0012323960224653566,
      "grad_norm": 1.3817839622497559,
      "learning_rate": 0.00010480000000000001,
      "loss": 0.0726,
      "step": 262
    },
    {
      "epoch": 0.0012370998240778196,
      "grad_norm": 3.970262050628662,
      "learning_rate": 0.00010520000000000001,
      "loss": 0.2769,
      "step": 263
    },
    {
      "epoch": 0.0012418036256902829,
      "grad_norm": 13.086316108703613,
      "learning_rate": 0.0001056,
      "loss": 1.1528,
      "step": 264
    },
    {
      "epoch": 0.0012465074273027461,
      "grad_norm": 8.955090522766113,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.5816,
      "step": 265
    },
    {
      "epoch": 0.0012512112289152092,
      "grad_norm": 7.970674991607666,
      "learning_rate": 0.00010640000000000001,
      "loss": 0.371,
      "step": 266
    },
    {
      "epoch": 0.0012559150305276724,
      "grad_norm": 5.113158226013184,
      "learning_rate": 0.00010680000000000001,
      "loss": 0.6101,
      "step": 267
    },
    {
      "epoch": 0.0012606188321401357,
      "grad_norm": 6.568826675415039,
      "learning_rate": 0.00010720000000000002,
      "loss": 0.3781,
      "step": 268
    },
    {
      "epoch": 0.001265322633752599,
      "grad_norm": 6.245065689086914,
      "learning_rate": 0.00010760000000000001,
      "loss": 0.3579,
      "step": 269
    },
    {
      "epoch": 0.001270026435365062,
      "grad_norm": 2.730668306350708,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.1458,
      "step": 270
    },
    {
      "epoch": 0.0012747302369775252,
      "grad_norm": 2.522618532180786,
      "learning_rate": 0.00010840000000000002,
      "loss": 0.2655,
      "step": 271
    },
    {
      "epoch": 0.0012794340385899885,
      "grad_norm": 3.7141852378845215,
      "learning_rate": 0.00010880000000000002,
      "loss": 0.4087,
      "step": 272
    },
    {
      "epoch": 0.0012841378402024515,
      "grad_norm": 5.851397514343262,
      "learning_rate": 0.00010920000000000001,
      "loss": 0.363,
      "step": 273
    },
    {
      "epoch": 0.0012888416418149148,
      "grad_norm": 4.247869491577148,
      "learning_rate": 0.00010960000000000001,
      "loss": 0.2285,
      "step": 274
    },
    {
      "epoch": 0.001293545443427378,
      "grad_norm": 2.001089572906494,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.1478,
      "step": 275
    },
    {
      "epoch": 0.0012982492450398413,
      "grad_norm": 0.5621914267539978,
      "learning_rate": 0.00011040000000000001,
      "loss": 0.0185,
      "step": 276
    },
    {
      "epoch": 0.0013029530466523043,
      "grad_norm": 2.1327598094940186,
      "learning_rate": 0.00011080000000000001,
      "loss": 0.1601,
      "step": 277
    },
    {
      "epoch": 0.0013076568482647676,
      "grad_norm": 3.787323236465454,
      "learning_rate": 0.00011120000000000002,
      "loss": 0.2316,
      "step": 278
    },
    {
      "epoch": 0.0013123606498772309,
      "grad_norm": 7.50827169418335,
      "learning_rate": 0.00011160000000000002,
      "loss": 0.7375,
      "step": 279
    },
    {
      "epoch": 0.0013170644514896939,
      "grad_norm": 2.4351136684417725,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.1164,
      "step": 280
    },
    {
      "epoch": 0.0013217682531021571,
      "grad_norm": 6.235156536102295,
      "learning_rate": 0.00011240000000000002,
      "loss": 0.6748,
      "step": 281
    },
    {
      "epoch": 0.0013264720547146204,
      "grad_norm": 6.498373985290527,
      "learning_rate": 0.00011279999999999999,
      "loss": 0.3911,
      "step": 282
    },
    {
      "epoch": 0.0013311758563270834,
      "grad_norm": 1.9898738861083984,
      "learning_rate": 0.0001132,
      "loss": 0.1857,
      "step": 283
    },
    {
      "epoch": 0.0013358796579395467,
      "grad_norm": 5.861527442932129,
      "learning_rate": 0.0001136,
      "loss": 0.7763,
      "step": 284
    },
    {
      "epoch": 0.00134058345955201,
      "grad_norm": 4.748760223388672,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.483,
      "step": 285
    },
    {
      "epoch": 0.0013452872611644732,
      "grad_norm": 0.20194639265537262,
      "learning_rate": 0.0001144,
      "loss": 0.0063,
      "step": 286
    },
    {
      "epoch": 0.0013499910627769362,
      "grad_norm": 5.861070156097412,
      "learning_rate": 0.0001148,
      "loss": 0.8052,
      "step": 287
    },
    {
      "epoch": 0.0013546948643893995,
      "grad_norm": 4.721028804779053,
      "learning_rate": 0.0001152,
      "loss": 0.7506,
      "step": 288
    },
    {
      "epoch": 0.0013593986660018628,
      "grad_norm": 0.6236014366149902,
      "learning_rate": 0.00011559999999999999,
      "loss": 0.0229,
      "step": 289
    },
    {
      "epoch": 0.0013641024676143258,
      "grad_norm": 4.265346050262451,
      "learning_rate": 0.000116,
      "loss": 0.4207,
      "step": 290
    },
    {
      "epoch": 0.001368806269226789,
      "grad_norm": 1.9097594022750854,
      "learning_rate": 0.0001164,
      "loss": 0.1345,
      "step": 291
    },
    {
      "epoch": 0.0013735100708392523,
      "grad_norm": 7.239140510559082,
      "learning_rate": 0.00011679999999999999,
      "loss": 0.6882,
      "step": 292
    },
    {
      "epoch": 0.0013782138724517156,
      "grad_norm": 3.469158887863159,
      "learning_rate": 0.0001172,
      "loss": 0.537,
      "step": 293
    },
    {
      "epoch": 0.0013829176740641786,
      "grad_norm": 6.196928024291992,
      "learning_rate": 0.0001176,
      "loss": 0.652,
      "step": 294
    },
    {
      "epoch": 0.0013876214756766419,
      "grad_norm": 5.817218780517578,
      "learning_rate": 0.000118,
      "loss": 0.5883,
      "step": 295
    },
    {
      "epoch": 0.0013923252772891051,
      "grad_norm": 0.9719970226287842,
      "learning_rate": 0.0001184,
      "loss": 0.0995,
      "step": 296
    },
    {
      "epoch": 0.0013970290789015682,
      "grad_norm": 1.451691746711731,
      "learning_rate": 0.0001188,
      "loss": 0.0822,
      "step": 297
    },
    {
      "epoch": 0.0014017328805140314,
      "grad_norm": 4.7092790603637695,
      "learning_rate": 0.0001192,
      "loss": 0.6913,
      "step": 298
    },
    {
      "epoch": 0.0014064366821264947,
      "grad_norm": 2.3695647716522217,
      "learning_rate": 0.00011960000000000001,
      "loss": 0.1064,
      "step": 299
    },
    {
      "epoch": 0.001411140483738958,
      "grad_norm": 1.1952271461486816,
      "learning_rate": 0.00012,
      "loss": 0.2535,
      "step": 300
    },
    {
      "epoch": 0.001415844285351421,
      "grad_norm": 5.761165142059326,
      "learning_rate": 0.0001204,
      "loss": 0.4578,
      "step": 301
    },
    {
      "epoch": 0.0014205480869638842,
      "grad_norm": 9.61013126373291,
      "learning_rate": 0.0001208,
      "loss": 0.5669,
      "step": 302
    },
    {
      "epoch": 0.0014252518885763475,
      "grad_norm": 4.267979621887207,
      "learning_rate": 0.0001212,
      "loss": 0.4776,
      "step": 303
    },
    {
      "epoch": 0.0014299556901888105,
      "grad_norm": 3.4100570678710938,
      "learning_rate": 0.0001216,
      "loss": 0.3825,
      "step": 304
    },
    {
      "epoch": 0.0014346594918012738,
      "grad_norm": 2.896878719329834,
      "learning_rate": 0.000122,
      "loss": 0.479,
      "step": 305
    },
    {
      "epoch": 0.001439363293413737,
      "grad_norm": 7.535650730133057,
      "learning_rate": 0.0001224,
      "loss": 0.988,
      "step": 306
    },
    {
      "epoch": 0.0014440670950262,
      "grad_norm": 7.952488899230957,
      "learning_rate": 0.0001228,
      "loss": 1.0579,
      "step": 307
    },
    {
      "epoch": 0.0014487708966386633,
      "grad_norm": 1.957253336906433,
      "learning_rate": 0.0001232,
      "loss": 0.2634,
      "step": 308
    },
    {
      "epoch": 0.0014534746982511266,
      "grad_norm": 4.205195426940918,
      "learning_rate": 0.0001236,
      "loss": 0.5535,
      "step": 309
    },
    {
      "epoch": 0.0014581784998635898,
      "grad_norm": 2.924506425857544,
      "learning_rate": 0.000124,
      "loss": 0.4655,
      "step": 310
    },
    {
      "epoch": 0.0014628823014760529,
      "grad_norm": 3.6749260425567627,
      "learning_rate": 0.00012440000000000002,
      "loss": 0.5132,
      "step": 311
    },
    {
      "epoch": 0.0014675861030885161,
      "grad_norm": 2.4244179725646973,
      "learning_rate": 0.0001248,
      "loss": 0.4429,
      "step": 312
    },
    {
      "epoch": 0.0014722899047009794,
      "grad_norm": 2.774231195449829,
      "learning_rate": 0.0001252,
      "loss": 0.3405,
      "step": 313
    },
    {
      "epoch": 0.0014769937063134424,
      "grad_norm": 2.4018137454986572,
      "learning_rate": 0.00012560000000000002,
      "loss": 0.2759,
      "step": 314
    },
    {
      "epoch": 0.0014816975079259057,
      "grad_norm": 2.1334757804870605,
      "learning_rate": 0.000126,
      "loss": 0.2498,
      "step": 315
    },
    {
      "epoch": 0.001486401309538369,
      "grad_norm": 2.5756373405456543,
      "learning_rate": 0.0001264,
      "loss": 0.3827,
      "step": 316
    },
    {
      "epoch": 0.0014911051111508322,
      "grad_norm": 2.4243853092193604,
      "learning_rate": 0.00012680000000000002,
      "loss": 0.3527,
      "step": 317
    },
    {
      "epoch": 0.0014958089127632952,
      "grad_norm": 3.0257978439331055,
      "learning_rate": 0.0001272,
      "loss": 0.2736,
      "step": 318
    },
    {
      "epoch": 0.0015005127143757585,
      "grad_norm": 2.501581907272339,
      "learning_rate": 0.0001276,
      "loss": 0.5115,
      "step": 319
    },
    {
      "epoch": 0.0015052165159882217,
      "grad_norm": 2.4450430870056152,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.149,
      "step": 320
    },
    {
      "epoch": 0.0015099203176006848,
      "grad_norm": 4.12658166885376,
      "learning_rate": 0.0001284,
      "loss": 0.4058,
      "step": 321
    },
    {
      "epoch": 0.001514624119213148,
      "grad_norm": 5.315903186798096,
      "learning_rate": 0.00012880000000000001,
      "loss": 0.5214,
      "step": 322
    },
    {
      "epoch": 0.0015193279208256113,
      "grad_norm": 1.2464120388031006,
      "learning_rate": 0.00012920000000000002,
      "loss": 0.1062,
      "step": 323
    },
    {
      "epoch": 0.0015240317224380746,
      "grad_norm": 1.9358088970184326,
      "learning_rate": 0.0001296,
      "loss": 0.1757,
      "step": 324
    },
    {
      "epoch": 0.0015287355240505376,
      "grad_norm": 1.993677020072937,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.1378,
      "step": 325
    },
    {
      "epoch": 0.0015334393256630008,
      "grad_norm": 2.1281518936157227,
      "learning_rate": 0.0001304,
      "loss": 0.148,
      "step": 326
    },
    {
      "epoch": 0.001538143127275464,
      "grad_norm": 4.741973876953125,
      "learning_rate": 0.0001308,
      "loss": 0.3754,
      "step": 327
    },
    {
      "epoch": 0.0015428469288879271,
      "grad_norm": 1.0350943803787231,
      "learning_rate": 0.00013120000000000002,
      "loss": 0.0534,
      "step": 328
    },
    {
      "epoch": 0.0015475507305003904,
      "grad_norm": 7.16654109954834,
      "learning_rate": 0.0001316,
      "loss": 0.3577,
      "step": 329
    },
    {
      "epoch": 0.0015522545321128537,
      "grad_norm": 3.2450430393218994,
      "learning_rate": 0.000132,
      "loss": 0.1855,
      "step": 330
    },
    {
      "epoch": 0.001556958333725317,
      "grad_norm": 1.9723613262176514,
      "learning_rate": 0.00013240000000000002,
      "loss": 0.0674,
      "step": 331
    },
    {
      "epoch": 0.00156166213533778,
      "grad_norm": 2.4443912506103516,
      "learning_rate": 0.0001328,
      "loss": 0.0759,
      "step": 332
    },
    {
      "epoch": 0.0015663659369502432,
      "grad_norm": 2.283569097518921,
      "learning_rate": 0.0001332,
      "loss": 0.0385,
      "step": 333
    },
    {
      "epoch": 0.0015710697385627065,
      "grad_norm": 1.6785517930984497,
      "learning_rate": 0.00013360000000000002,
      "loss": 0.1018,
      "step": 334
    },
    {
      "epoch": 0.0015757735401751695,
      "grad_norm": 2.3101062774658203,
      "learning_rate": 0.000134,
      "loss": 0.0711,
      "step": 335
    },
    {
      "epoch": 0.0015804773417876328,
      "grad_norm": 8.354108810424805,
      "learning_rate": 0.00013440000000000001,
      "loss": 0.2996,
      "step": 336
    },
    {
      "epoch": 0.001585181143400096,
      "grad_norm": 0.7124830484390259,
      "learning_rate": 0.00013480000000000002,
      "loss": 0.0205,
      "step": 337
    },
    {
      "epoch": 0.001589884945012559,
      "grad_norm": 13.39250373840332,
      "learning_rate": 0.0001352,
      "loss": 1.2915,
      "step": 338
    },
    {
      "epoch": 0.0015945887466250223,
      "grad_norm": 4.381448268890381,
      "learning_rate": 0.00013560000000000002,
      "loss": 0.1355,
      "step": 339
    },
    {
      "epoch": 0.0015992925482374856,
      "grad_norm": 3.804953098297119,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.134,
      "step": 340
    },
    {
      "epoch": 0.0016039963498499488,
      "grad_norm": 17.82666015625,
      "learning_rate": 0.0001364,
      "loss": 0.8447,
      "step": 341
    },
    {
      "epoch": 0.0016087001514624119,
      "grad_norm": 9.356554985046387,
      "learning_rate": 0.00013680000000000002,
      "loss": 0.7243,
      "step": 342
    },
    {
      "epoch": 0.0016134039530748751,
      "grad_norm": 12.969133377075195,
      "learning_rate": 0.00013720000000000003,
      "loss": 0.9061,
      "step": 343
    },
    {
      "epoch": 0.0016181077546873384,
      "grad_norm": 6.46461296081543,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.3836,
      "step": 344
    },
    {
      "epoch": 0.0016228115562998014,
      "grad_norm": 14.767903327941895,
      "learning_rate": 0.000138,
      "loss": 0.9385,
      "step": 345
    },
    {
      "epoch": 0.0016275153579122647,
      "grad_norm": 4.248624324798584,
      "learning_rate": 0.0001384,
      "loss": 0.1006,
      "step": 346
    },
    {
      "epoch": 0.001632219159524728,
      "grad_norm": 5.127457141876221,
      "learning_rate": 0.00013879999999999999,
      "loss": 0.1415,
      "step": 347
    },
    {
      "epoch": 0.0016369229611371912,
      "grad_norm": 2.027552843093872,
      "learning_rate": 0.0001392,
      "loss": 0.0487,
      "step": 348
    },
    {
      "epoch": 0.0016416267627496542,
      "grad_norm": 0.6455308794975281,
      "learning_rate": 0.0001396,
      "loss": 0.0231,
      "step": 349
    },
    {
      "epoch": 0.0016463305643621175,
      "grad_norm": 4.503232955932617,
      "learning_rate": 0.00014,
      "loss": 0.3631,
      "step": 350
    },
    {
      "epoch": 0.0016510343659745807,
      "grad_norm": 5.003231525421143,
      "learning_rate": 0.0001404,
      "loss": 0.1904,
      "step": 351
    },
    {
      "epoch": 0.0016557381675870438,
      "grad_norm": 7.7387375831604,
      "learning_rate": 0.0001408,
      "loss": 0.3851,
      "step": 352
    },
    {
      "epoch": 0.001660441969199507,
      "grad_norm": 7.251846790313721,
      "learning_rate": 0.0001412,
      "loss": 0.3054,
      "step": 353
    },
    {
      "epoch": 0.0016651457708119703,
      "grad_norm": 0.8463186025619507,
      "learning_rate": 0.0001416,
      "loss": 0.0314,
      "step": 354
    },
    {
      "epoch": 0.0016698495724244335,
      "grad_norm": 3.3773341178894043,
      "learning_rate": 0.000142,
      "loss": 0.2139,
      "step": 355
    },
    {
      "epoch": 0.0016745533740368966,
      "grad_norm": 0.49571314454078674,
      "learning_rate": 0.0001424,
      "loss": 0.0212,
      "step": 356
    },
    {
      "epoch": 0.0016792571756493598,
      "grad_norm": 6.931609630584717,
      "learning_rate": 0.0001428,
      "loss": 0.5459,
      "step": 357
    },
    {
      "epoch": 0.001683960977261823,
      "grad_norm": 7.149381637573242,
      "learning_rate": 0.0001432,
      "loss": 0.5258,
      "step": 358
    },
    {
      "epoch": 0.0016886647788742861,
      "grad_norm": 10.123090744018555,
      "learning_rate": 0.0001436,
      "loss": 1.2159,
      "step": 359
    },
    {
      "epoch": 0.0016933685804867494,
      "grad_norm": 3.2398242950439453,
      "learning_rate": 0.000144,
      "loss": 0.1301,
      "step": 360
    },
    {
      "epoch": 0.0016980723820992126,
      "grad_norm": 1.6010240316390991,
      "learning_rate": 0.0001444,
      "loss": 0.0628,
      "step": 361
    },
    {
      "epoch": 0.0017027761837116757,
      "grad_norm": 0.8089118599891663,
      "learning_rate": 0.0001448,
      "loss": 0.0255,
      "step": 362
    },
    {
      "epoch": 0.001707479985324139,
      "grad_norm": 2.7672698497772217,
      "learning_rate": 0.0001452,
      "loss": 0.101,
      "step": 363
    },
    {
      "epoch": 0.0017121837869366022,
      "grad_norm": 4.092283248901367,
      "learning_rate": 0.00014560000000000002,
      "loss": 0.2196,
      "step": 364
    },
    {
      "epoch": 0.0017168875885490654,
      "grad_norm": 6.312417030334473,
      "learning_rate": 0.000146,
      "loss": 0.4104,
      "step": 365
    },
    {
      "epoch": 0.0017215913901615285,
      "grad_norm": 6.616907119750977,
      "learning_rate": 0.0001464,
      "loss": 0.3651,
      "step": 366
    },
    {
      "epoch": 0.0017262951917739917,
      "grad_norm": 4.744364261627197,
      "learning_rate": 0.00014680000000000002,
      "loss": 0.2126,
      "step": 367
    },
    {
      "epoch": 0.001730998993386455,
      "grad_norm": 1.456031084060669,
      "learning_rate": 0.0001472,
      "loss": 0.047,
      "step": 368
    },
    {
      "epoch": 0.001735702794998918,
      "grad_norm": 6.134387969970703,
      "learning_rate": 0.0001476,
      "loss": 0.5088,
      "step": 369
    },
    {
      "epoch": 0.0017404065966113813,
      "grad_norm": 8.216341018676758,
      "learning_rate": 0.000148,
      "loss": 0.6108,
      "step": 370
    },
    {
      "epoch": 0.0017451103982238445,
      "grad_norm": 3.7597427368164062,
      "learning_rate": 0.0001484,
      "loss": 0.149,
      "step": 371
    },
    {
      "epoch": 0.0017498141998363078,
      "grad_norm": 9.81719970703125,
      "learning_rate": 0.0001488,
      "loss": 0.8091,
      "step": 372
    },
    {
      "epoch": 0.0017545180014487708,
      "grad_norm": 9.361310005187988,
      "learning_rate": 0.0001492,
      "loss": 0.8549,
      "step": 373
    },
    {
      "epoch": 0.001759221803061234,
      "grad_norm": 9.573917388916016,
      "learning_rate": 0.0001496,
      "loss": 0.8137,
      "step": 374
    },
    {
      "epoch": 0.0017639256046736974,
      "grad_norm": 9.143319129943848,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1135,
      "step": 375
    },
    {
      "epoch": 0.0017686294062861604,
      "grad_norm": 3.718782663345337,
      "learning_rate": 0.0001504,
      "loss": 0.2149,
      "step": 376
    },
    {
      "epoch": 0.0017733332078986236,
      "grad_norm": 5.175637245178223,
      "learning_rate": 0.0001508,
      "loss": 0.3678,
      "step": 377
    },
    {
      "epoch": 0.001778037009511087,
      "grad_norm": 1.4497367143630981,
      "learning_rate": 0.00015120000000000002,
      "loss": 0.052,
      "step": 378
    },
    {
      "epoch": 0.0017827408111235502,
      "grad_norm": 6.6607794761657715,
      "learning_rate": 0.0001516,
      "loss": 0.5786,
      "step": 379
    },
    {
      "epoch": 0.0017874446127360132,
      "grad_norm": 1.5905323028564453,
      "learning_rate": 0.000152,
      "loss": 0.0694,
      "step": 380
    },
    {
      "epoch": 0.0017921484143484765,
      "grad_norm": 2.8540422916412354,
      "learning_rate": 0.00015240000000000002,
      "loss": 0.1495,
      "step": 381
    },
    {
      "epoch": 0.0017968522159609397,
      "grad_norm": 2.023268461227417,
      "learning_rate": 0.0001528,
      "loss": 0.0823,
      "step": 382
    },
    {
      "epoch": 0.0018015560175734027,
      "grad_norm": 0.8683632612228394,
      "learning_rate": 0.0001532,
      "loss": 0.0576,
      "step": 383
    },
    {
      "epoch": 0.001806259819185866,
      "grad_norm": 1.2248761653900146,
      "learning_rate": 0.00015360000000000002,
      "loss": 0.0439,
      "step": 384
    },
    {
      "epoch": 0.0018109636207983293,
      "grad_norm": 5.612423896789551,
      "learning_rate": 0.000154,
      "loss": 0.3471,
      "step": 385
    },
    {
      "epoch": 0.0018156674224107923,
      "grad_norm": 2.4408791065216064,
      "learning_rate": 0.0001544,
      "loss": 0.0914,
      "step": 386
    },
    {
      "epoch": 0.0018203712240232556,
      "grad_norm": 0.32082241773605347,
      "learning_rate": 0.00015480000000000002,
      "loss": 0.0091,
      "step": 387
    },
    {
      "epoch": 0.0018250750256357188,
      "grad_norm": 10.327925682067871,
      "learning_rate": 0.0001552,
      "loss": 0.571,
      "step": 388
    },
    {
      "epoch": 0.001829778827248182,
      "grad_norm": 0.12793880701065063,
      "learning_rate": 0.00015560000000000001,
      "loss": 0.0019,
      "step": 389
    },
    {
      "epoch": 0.001834482628860645,
      "grad_norm": 0.835273265838623,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0245,
      "step": 390
    },
    {
      "epoch": 0.0018391864304731084,
      "grad_norm": 0.30255475640296936,
      "learning_rate": 0.0001564,
      "loss": 0.0073,
      "step": 391
    },
    {
      "epoch": 0.0018438902320855716,
      "grad_norm": 8.770179748535156,
      "learning_rate": 0.00015680000000000002,
      "loss": 0.6341,
      "step": 392
    },
    {
      "epoch": 0.0018485940336980347,
      "grad_norm": 0.06474225968122482,
      "learning_rate": 0.00015720000000000003,
      "loss": 0.0012,
      "step": 393
    },
    {
      "epoch": 0.001853297835310498,
      "grad_norm": 12.610589027404785,
      "learning_rate": 0.0001576,
      "loss": 1.891,
      "step": 394
    },
    {
      "epoch": 0.0018580016369229612,
      "grad_norm": 7.723695278167725,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.8129,
      "step": 395
    },
    {
      "epoch": 0.0018627054385354244,
      "grad_norm": 7.2633280754089355,
      "learning_rate": 0.00015840000000000003,
      "loss": 0.6139,
      "step": 396
    },
    {
      "epoch": 0.0018674092401478875,
      "grad_norm": 0.46716001629829407,
      "learning_rate": 0.0001588,
      "loss": 0.0099,
      "step": 397
    },
    {
      "epoch": 0.0018721130417603507,
      "grad_norm": 7.197908401489258,
      "learning_rate": 0.00015920000000000002,
      "loss": 0.6898,
      "step": 398
    },
    {
      "epoch": 0.001876816843372814,
      "grad_norm": 0.5740136504173279,
      "learning_rate": 0.0001596,
      "loss": 0.0117,
      "step": 399
    },
    {
      "epoch": 0.001881520644985277,
      "grad_norm": 7.378072738647461,
      "learning_rate": 0.00016,
      "loss": 0.3782,
      "step": 400
    },
    {
      "epoch": 0.0018862244465977403,
      "grad_norm": 1.9648194313049316,
      "learning_rate": 0.00016040000000000002,
      "loss": 0.0921,
      "step": 401
    },
    {
      "epoch": 0.0018909282482102035,
      "grad_norm": 2.49945068359375,
      "learning_rate": 0.0001608,
      "loss": 0.1016,
      "step": 402
    },
    {
      "epoch": 0.0018956320498226668,
      "grad_norm": 8.174307823181152,
      "learning_rate": 0.00016120000000000002,
      "loss": 0.9645,
      "step": 403
    },
    {
      "epoch": 0.0019003358514351298,
      "grad_norm": 0.46510857343673706,
      "learning_rate": 0.00016160000000000002,
      "loss": 0.0127,
      "step": 404
    },
    {
      "epoch": 0.001905039653047593,
      "grad_norm": 5.25247049331665,
      "learning_rate": 0.000162,
      "loss": 0.6457,
      "step": 405
    },
    {
      "epoch": 0.0019097434546600563,
      "grad_norm": 5.902981758117676,
      "learning_rate": 0.00016240000000000002,
      "loss": 0.268,
      "step": 406
    },
    {
      "epoch": 0.0019144472562725194,
      "grad_norm": 8.578977584838867,
      "learning_rate": 0.0001628,
      "loss": 1.5509,
      "step": 407
    },
    {
      "epoch": 0.0019191510578849826,
      "grad_norm": 1.3066927194595337,
      "learning_rate": 0.0001632,
      "loss": 0.1135,
      "step": 408
    },
    {
      "epoch": 0.0019238548594974459,
      "grad_norm": 1.2204082012176514,
      "learning_rate": 0.0001636,
      "loss": 0.1076,
      "step": 409
    },
    {
      "epoch": 0.001928558661109909,
      "grad_norm": 3.1726009845733643,
      "learning_rate": 0.000164,
      "loss": 0.1829,
      "step": 410
    },
    {
      "epoch": 0.0019332624627223722,
      "grad_norm": 10.586771011352539,
      "learning_rate": 0.0001644,
      "loss": 0.4915,
      "step": 411
    },
    {
      "epoch": 0.0019379662643348354,
      "grad_norm": 5.281284809112549,
      "learning_rate": 0.0001648,
      "loss": 0.5445,
      "step": 412
    },
    {
      "epoch": 0.0019426700659472987,
      "grad_norm": 7.744019985198975,
      "learning_rate": 0.0001652,
      "loss": 0.4866,
      "step": 413
    },
    {
      "epoch": 0.0019473738675597617,
      "grad_norm": 2.30348801612854,
      "learning_rate": 0.0001656,
      "loss": 0.1509,
      "step": 414
    },
    {
      "epoch": 0.001952077669172225,
      "grad_norm": 5.621163368225098,
      "learning_rate": 0.000166,
      "loss": 0.4085,
      "step": 415
    },
    {
      "epoch": 0.001956781470784688,
      "grad_norm": 2.718182325363159,
      "learning_rate": 0.0001664,
      "loss": 0.1989,
      "step": 416
    },
    {
      "epoch": 0.0019614852723971515,
      "grad_norm": 5.092797756195068,
      "learning_rate": 0.0001668,
      "loss": 0.6662,
      "step": 417
    },
    {
      "epoch": 0.0019661890740096145,
      "grad_norm": 6.925167083740234,
      "learning_rate": 0.0001672,
      "loss": 0.5721,
      "step": 418
    },
    {
      "epoch": 0.0019708928756220776,
      "grad_norm": 1.394405722618103,
      "learning_rate": 0.0001676,
      "loss": 0.0616,
      "step": 419
    },
    {
      "epoch": 0.001975596677234541,
      "grad_norm": 3.7499024868011475,
      "learning_rate": 0.000168,
      "loss": 0.2968,
      "step": 420
    },
    {
      "epoch": 0.001980300478847004,
      "grad_norm": 2.0088789463043213,
      "learning_rate": 0.0001684,
      "loss": 0.1763,
      "step": 421
    },
    {
      "epoch": 0.001985004280459467,
      "grad_norm": 3.8349266052246094,
      "learning_rate": 0.0001688,
      "loss": 0.3431,
      "step": 422
    },
    {
      "epoch": 0.0019897080820719306,
      "grad_norm": 1.7392799854278564,
      "learning_rate": 0.0001692,
      "loss": 0.164,
      "step": 423
    },
    {
      "epoch": 0.0019944118836843936,
      "grad_norm": 2.1135196685791016,
      "learning_rate": 0.0001696,
      "loss": 0.188,
      "step": 424
    },
    {
      "epoch": 0.001999115685296857,
      "grad_norm": 3.892944812774658,
      "learning_rate": 0.00017,
      "loss": 0.2384,
      "step": 425
    },
    {
      "epoch": 0.00200381948690932,
      "grad_norm": 0.41945555806159973,
      "learning_rate": 0.0001704,
      "loss": 0.0252,
      "step": 426
    },
    {
      "epoch": 0.002008523288521783,
      "grad_norm": 2.866692066192627,
      "learning_rate": 0.0001708,
      "loss": 0.3043,
      "step": 427
    },
    {
      "epoch": 0.0020132270901342467,
      "grad_norm": 3.9674036502838135,
      "learning_rate": 0.00017120000000000001,
      "loss": 0.45,
      "step": 428
    },
    {
      "epoch": 0.0020179308917467097,
      "grad_norm": 2.7942330837249756,
      "learning_rate": 0.0001716,
      "loss": 0.2072,
      "step": 429
    },
    {
      "epoch": 0.0020226346933591727,
      "grad_norm": 1.9509483575820923,
      "learning_rate": 0.000172,
      "loss": 0.1296,
      "step": 430
    },
    {
      "epoch": 0.0020273384949716362,
      "grad_norm": 6.762782573699951,
      "learning_rate": 0.00017240000000000002,
      "loss": 0.5934,
      "step": 431
    },
    {
      "epoch": 0.0020320422965840993,
      "grad_norm": 6.404508113861084,
      "learning_rate": 0.0001728,
      "loss": 0.6801,
      "step": 432
    },
    {
      "epoch": 0.0020367460981965623,
      "grad_norm": 1.2534981966018677,
      "learning_rate": 0.0001732,
      "loss": 0.1217,
      "step": 433
    },
    {
      "epoch": 0.0020414498998090258,
      "grad_norm": 1.5184433460235596,
      "learning_rate": 0.00017360000000000002,
      "loss": 0.0942,
      "step": 434
    },
    {
      "epoch": 0.002046153701421489,
      "grad_norm": 5.89815092086792,
      "learning_rate": 0.000174,
      "loss": 0.654,
      "step": 435
    },
    {
      "epoch": 0.002050857503033952,
      "grad_norm": 3.3252062797546387,
      "learning_rate": 0.0001744,
      "loss": 0.2026,
      "step": 436
    },
    {
      "epoch": 0.0020555613046464153,
      "grad_norm": 0.7325847744941711,
      "learning_rate": 0.00017480000000000002,
      "loss": 0.0299,
      "step": 437
    },
    {
      "epoch": 0.0020602651062588784,
      "grad_norm": 2.149791955947876,
      "learning_rate": 0.0001752,
      "loss": 0.1405,
      "step": 438
    },
    {
      "epoch": 0.002064968907871342,
      "grad_norm": 4.564706802368164,
      "learning_rate": 0.0001756,
      "loss": 0.593,
      "step": 439
    },
    {
      "epoch": 0.002069672709483805,
      "grad_norm": 0.5548848509788513,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0314,
      "step": 440
    },
    {
      "epoch": 0.002074376511096268,
      "grad_norm": 0.5786711573600769,
      "learning_rate": 0.0001764,
      "loss": 0.0227,
      "step": 441
    },
    {
      "epoch": 0.0020790803127087314,
      "grad_norm": 7.1305131912231445,
      "learning_rate": 0.00017680000000000001,
      "loss": 1.4823,
      "step": 442
    },
    {
      "epoch": 0.0020837841143211944,
      "grad_norm": 0.2096025049686432,
      "learning_rate": 0.0001772,
      "loss": 0.0069,
      "step": 443
    },
    {
      "epoch": 0.0020884879159336575,
      "grad_norm": 4.333567142486572,
      "learning_rate": 0.0001776,
      "loss": 0.2295,
      "step": 444
    },
    {
      "epoch": 0.002093191717546121,
      "grad_norm": 3.485567331314087,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.3651,
      "step": 445
    },
    {
      "epoch": 0.002097895519158584,
      "grad_norm": 5.096225261688232,
      "learning_rate": 0.0001784,
      "loss": 0.6821,
      "step": 446
    },
    {
      "epoch": 0.002102599320771047,
      "grad_norm": 5.824506759643555,
      "learning_rate": 0.0001788,
      "loss": 0.7786,
      "step": 447
    },
    {
      "epoch": 0.0021073031223835105,
      "grad_norm": 0.23080496490001678,
      "learning_rate": 0.00017920000000000002,
      "loss": 0.0075,
      "step": 448
    },
    {
      "epoch": 0.0021120069239959735,
      "grad_norm": 1.3946672677993774,
      "learning_rate": 0.0001796,
      "loss": 0.0798,
      "step": 449
    },
    {
      "epoch": 0.0021167107256084366,
      "grad_norm": 4.821137428283691,
      "learning_rate": 0.00018,
      "loss": 0.5363,
      "step": 450
    },
    {
      "epoch": 0.0021214145272209,
      "grad_norm": 1.5111775398254395,
      "learning_rate": 0.00018040000000000002,
      "loss": 0.1087,
      "step": 451
    },
    {
      "epoch": 0.002126118328833363,
      "grad_norm": 6.121540069580078,
      "learning_rate": 0.0001808,
      "loss": 0.2188,
      "step": 452
    },
    {
      "epoch": 0.002130822130445826,
      "grad_norm": 6.667111873626709,
      "learning_rate": 0.0001812,
      "loss": 0.8351,
      "step": 453
    },
    {
      "epoch": 0.0021355259320582896,
      "grad_norm": 1.9470659494400024,
      "learning_rate": 0.00018160000000000002,
      "loss": 0.0979,
      "step": 454
    },
    {
      "epoch": 0.0021402297336707526,
      "grad_norm": 1.9844422340393066,
      "learning_rate": 0.000182,
      "loss": 0.115,
      "step": 455
    },
    {
      "epoch": 0.002144933535283216,
      "grad_norm": 3.2512948513031006,
      "learning_rate": 0.00018240000000000002,
      "loss": 0.1841,
      "step": 456
    },
    {
      "epoch": 0.002149637336895679,
      "grad_norm": 1.5105171203613281,
      "learning_rate": 0.00018280000000000003,
      "loss": 0.1943,
      "step": 457
    },
    {
      "epoch": 0.002154341138508142,
      "grad_norm": 1.6252412796020508,
      "learning_rate": 0.0001832,
      "loss": 0.0982,
      "step": 458
    },
    {
      "epoch": 0.0021590449401206056,
      "grad_norm": 2.7867486476898193,
      "learning_rate": 0.00018360000000000002,
      "loss": 0.2819,
      "step": 459
    },
    {
      "epoch": 0.0021637487417330687,
      "grad_norm": 3.677786350250244,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3459,
      "step": 460
    },
    {
      "epoch": 0.0021684525433455317,
      "grad_norm": 1.1252609491348267,
      "learning_rate": 0.0001844,
      "loss": 0.1023,
      "step": 461
    },
    {
      "epoch": 0.002173156344957995,
      "grad_norm": 0.34616151452064514,
      "learning_rate": 0.00018480000000000002,
      "loss": 0.019,
      "step": 462
    },
    {
      "epoch": 0.0021778601465704582,
      "grad_norm": 1.7373374700546265,
      "learning_rate": 0.00018520000000000003,
      "loss": 0.2261,
      "step": 463
    },
    {
      "epoch": 0.0021825639481829213,
      "grad_norm": 0.8922981023788452,
      "learning_rate": 0.0001856,
      "loss": 0.0461,
      "step": 464
    },
    {
      "epoch": 0.0021872677497953848,
      "grad_norm": 4.738465785980225,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.2765,
      "step": 465
    },
    {
      "epoch": 0.002191971551407848,
      "grad_norm": 4.691183567047119,
      "learning_rate": 0.00018640000000000003,
      "loss": 0.992,
      "step": 466
    },
    {
      "epoch": 0.002196675353020311,
      "grad_norm": 3.280848503112793,
      "learning_rate": 0.00018680000000000001,
      "loss": 0.1805,
      "step": 467
    },
    {
      "epoch": 0.0022013791546327743,
      "grad_norm": 2.4245572090148926,
      "learning_rate": 0.00018720000000000002,
      "loss": 0.2653,
      "step": 468
    },
    {
      "epoch": 0.0022060829562452373,
      "grad_norm": 4.130688667297363,
      "learning_rate": 0.0001876,
      "loss": 0.2599,
      "step": 469
    },
    {
      "epoch": 0.0022107867578577004,
      "grad_norm": 0.6237450242042542,
      "learning_rate": 0.000188,
      "loss": 0.021,
      "step": 470
    },
    {
      "epoch": 0.002215490559470164,
      "grad_norm": 0.15407976508140564,
      "learning_rate": 0.0001884,
      "loss": 0.0045,
      "step": 471
    },
    {
      "epoch": 0.002220194361082627,
      "grad_norm": 5.603780746459961,
      "learning_rate": 0.0001888,
      "loss": 0.5043,
      "step": 472
    },
    {
      "epoch": 0.0022248981626950904,
      "grad_norm": 9.7479829788208,
      "learning_rate": 0.0001892,
      "loss": 0.9334,
      "step": 473
    },
    {
      "epoch": 0.0022296019643075534,
      "grad_norm": 6.137670993804932,
      "learning_rate": 0.0001896,
      "loss": 0.3281,
      "step": 474
    },
    {
      "epoch": 0.0022343057659200164,
      "grad_norm": 5.173526287078857,
      "learning_rate": 0.00019,
      "loss": 0.4695,
      "step": 475
    },
    {
      "epoch": 0.00223900956753248,
      "grad_norm": 3.5330491065979004,
      "learning_rate": 0.0001904,
      "loss": 0.3578,
      "step": 476
    },
    {
      "epoch": 0.002243713369144943,
      "grad_norm": 3.5067591667175293,
      "learning_rate": 0.0001908,
      "loss": 0.2513,
      "step": 477
    },
    {
      "epoch": 0.002248417170757406,
      "grad_norm": 2.5952534675598145,
      "learning_rate": 0.0001912,
      "loss": 0.1811,
      "step": 478
    },
    {
      "epoch": 0.0022531209723698695,
      "grad_norm": 7.664407730102539,
      "learning_rate": 0.0001916,
      "loss": 0.55,
      "step": 479
    },
    {
      "epoch": 0.0022578247739823325,
      "grad_norm": 12.325654029846191,
      "learning_rate": 0.000192,
      "loss": 0.419,
      "step": 480
    },
    {
      "epoch": 0.0022625285755947955,
      "grad_norm": 5.907459259033203,
      "learning_rate": 0.00019240000000000001,
      "loss": 0.6165,
      "step": 481
    },
    {
      "epoch": 0.002267232377207259,
      "grad_norm": 2.6723532676696777,
      "learning_rate": 0.0001928,
      "loss": 0.2301,
      "step": 482
    },
    {
      "epoch": 0.002271936178819722,
      "grad_norm": 2.586239814758301,
      "learning_rate": 0.0001932,
      "loss": 0.2677,
      "step": 483
    },
    {
      "epoch": 0.002276639980432185,
      "grad_norm": 4.0440144538879395,
      "learning_rate": 0.00019360000000000002,
      "loss": 0.5411,
      "step": 484
    },
    {
      "epoch": 0.0022813437820446486,
      "grad_norm": 3.957775354385376,
      "learning_rate": 0.000194,
      "loss": 0.3404,
      "step": 485
    },
    {
      "epoch": 0.0022860475836571116,
      "grad_norm": 1.59304678440094,
      "learning_rate": 0.0001944,
      "loss": 0.085,
      "step": 486
    },
    {
      "epoch": 0.002290751385269575,
      "grad_norm": 2.6465861797332764,
      "learning_rate": 0.0001948,
      "loss": 0.1939,
      "step": 487
    },
    {
      "epoch": 0.002295455186882038,
      "grad_norm": 1.9595757722854614,
      "learning_rate": 0.0001952,
      "loss": 0.1307,
      "step": 488
    },
    {
      "epoch": 0.002300158988494501,
      "grad_norm": 4.419256210327148,
      "learning_rate": 0.0001956,
      "loss": 0.495,
      "step": 489
    },
    {
      "epoch": 0.0023048627901069646,
      "grad_norm": 3.186998128890991,
      "learning_rate": 0.000196,
      "loss": 0.2656,
      "step": 490
    },
    {
      "epoch": 0.0023095665917194277,
      "grad_norm": 3.9447336196899414,
      "learning_rate": 0.0001964,
      "loss": 0.5269,
      "step": 491
    },
    {
      "epoch": 0.0023142703933318907,
      "grad_norm": 1.285272479057312,
      "learning_rate": 0.0001968,
      "loss": 0.0928,
      "step": 492
    },
    {
      "epoch": 0.002318974194944354,
      "grad_norm": 5.694546222686768,
      "learning_rate": 0.0001972,
      "loss": 0.9207,
      "step": 493
    },
    {
      "epoch": 0.0023236779965568172,
      "grad_norm": 7.832240581512451,
      "learning_rate": 0.0001976,
      "loss": 0.8962,
      "step": 494
    },
    {
      "epoch": 0.0023283817981692803,
      "grad_norm": 6.968967914581299,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.1535,
      "step": 495
    },
    {
      "epoch": 0.0023330855997817437,
      "grad_norm": 3.456744432449341,
      "learning_rate": 0.0001984,
      "loss": 0.4575,
      "step": 496
    },
    {
      "epoch": 0.0023377894013942068,
      "grad_norm": 3.8342556953430176,
      "learning_rate": 0.0001988,
      "loss": 0.6583,
      "step": 497
    },
    {
      "epoch": 0.00234249320300667,
      "grad_norm": 3.798800230026245,
      "learning_rate": 0.00019920000000000002,
      "loss": 0.6581,
      "step": 498
    },
    {
      "epoch": 0.0023471970046191333,
      "grad_norm": 1.041553020477295,
      "learning_rate": 0.0001996,
      "loss": 0.1214,
      "step": 499
    },
    {
      "epoch": 0.0023519008062315963,
      "grad_norm": 1.7373206615447998,
      "learning_rate": 0.0002,
      "loss": 0.1671,
      "step": 500
    },
    {
      "epoch": 0.0023566046078440594,
      "grad_norm": 2.733170509338379,
      "learning_rate": 0.00019999905702188653,
      "loss": 0.2806,
      "step": 501
    },
    {
      "epoch": 0.002361308409456523,
      "grad_norm": 1.8406935930252075,
      "learning_rate": 0.00019999811404377305,
      "loss": 0.1807,
      "step": 502
    },
    {
      "epoch": 0.002366012211068986,
      "grad_norm": 0.8960445523262024,
      "learning_rate": 0.0001999971710656596,
      "loss": 0.0481,
      "step": 503
    },
    {
      "epoch": 0.0023707160126814493,
      "grad_norm": 2.177539110183716,
      "learning_rate": 0.0001999962280875461,
      "loss": 0.2986,
      "step": 504
    },
    {
      "epoch": 0.0023754198142939124,
      "grad_norm": 3.447396755218506,
      "learning_rate": 0.00019999528510943263,
      "loss": 0.3535,
      "step": 505
    },
    {
      "epoch": 0.0023801236159063754,
      "grad_norm": 1.0203230381011963,
      "learning_rate": 0.00019999434213131912,
      "loss": 0.0744,
      "step": 506
    },
    {
      "epoch": 0.002384827417518839,
      "grad_norm": 2.2024595737457275,
      "learning_rate": 0.00019999339915320567,
      "loss": 0.2709,
      "step": 507
    },
    {
      "epoch": 0.002389531219131302,
      "grad_norm": 2.625650405883789,
      "learning_rate": 0.00019999245617509219,
      "loss": 0.1525,
      "step": 508
    },
    {
      "epoch": 0.002394235020743765,
      "grad_norm": 1.6832960844039917,
      "learning_rate": 0.0001999915131969787,
      "loss": 0.1382,
      "step": 509
    },
    {
      "epoch": 0.0023989388223562285,
      "grad_norm": 4.251504898071289,
      "learning_rate": 0.00019999057021886522,
      "loss": 0.2626,
      "step": 510
    },
    {
      "epoch": 0.0024036426239686915,
      "grad_norm": 3.4471030235290527,
      "learning_rate": 0.00019998962724075174,
      "loss": 0.2386,
      "step": 511
    },
    {
      "epoch": 0.0024083464255811545,
      "grad_norm": 4.806637287139893,
      "learning_rate": 0.0001999886842626383,
      "loss": 0.3893,
      "step": 512
    },
    {
      "epoch": 0.002413050227193618,
      "grad_norm": 6.059075832366943,
      "learning_rate": 0.0001999877412845248,
      "loss": 0.3727,
      "step": 513
    },
    {
      "epoch": 0.002417754028806081,
      "grad_norm": 2.395587921142578,
      "learning_rate": 0.00019998679830641132,
      "loss": 0.208,
      "step": 514
    },
    {
      "epoch": 0.002422457830418544,
      "grad_norm": 3.651764392852783,
      "learning_rate": 0.00019998585532829784,
      "loss": 0.2421,
      "step": 515
    },
    {
      "epoch": 0.0024271616320310076,
      "grad_norm": 6.9652910232543945,
      "learning_rate": 0.00019998491235018436,
      "loss": 0.7684,
      "step": 516
    },
    {
      "epoch": 0.0024318654336434706,
      "grad_norm": 6.124037265777588,
      "learning_rate": 0.00019998396937207088,
      "loss": 0.5321,
      "step": 517
    },
    {
      "epoch": 0.0024365692352559336,
      "grad_norm": 4.278610706329346,
      "learning_rate": 0.0001999830263939574,
      "loss": 0.292,
      "step": 518
    },
    {
      "epoch": 0.002441273036868397,
      "grad_norm": 1.1434571743011475,
      "learning_rate": 0.00019998208341584392,
      "loss": 0.0476,
      "step": 519
    },
    {
      "epoch": 0.00244597683848086,
      "grad_norm": 0.5602289438247681,
      "learning_rate": 0.00019998114043773044,
      "loss": 0.0371,
      "step": 520
    },
    {
      "epoch": 0.0024506806400933236,
      "grad_norm": 5.589516639709473,
      "learning_rate": 0.00019998019745961698,
      "loss": 0.9811,
      "step": 521
    },
    {
      "epoch": 0.0024553844417057867,
      "grad_norm": 7.275086402893066,
      "learning_rate": 0.0001999792544815035,
      "loss": 0.8024,
      "step": 522
    },
    {
      "epoch": 0.0024600882433182497,
      "grad_norm": 3.6631767749786377,
      "learning_rate": 0.00019997831150339002,
      "loss": 0.2967,
      "step": 523
    },
    {
      "epoch": 0.002464792044930713,
      "grad_norm": 1.0775015354156494,
      "learning_rate": 0.00019997736852527654,
      "loss": 0.0552,
      "step": 524
    },
    {
      "epoch": 0.002469495846543176,
      "grad_norm": 1.2534099817276,
      "learning_rate": 0.00019997642554716308,
      "loss": 0.0823,
      "step": 525
    },
    {
      "epoch": 0.0024741996481556392,
      "grad_norm": 1.1758276224136353,
      "learning_rate": 0.00019997548256904958,
      "loss": 0.0907,
      "step": 526
    },
    {
      "epoch": 0.0024789034497681027,
      "grad_norm": 1.0112347602844238,
      "learning_rate": 0.0001999745395909361,
      "loss": 0.0559,
      "step": 527
    },
    {
      "epoch": 0.0024836072513805658,
      "grad_norm": 4.944222450256348,
      "learning_rate": 0.0001999735966128226,
      "loss": 0.5837,
      "step": 528
    },
    {
      "epoch": 0.002488311052993029,
      "grad_norm": 1.8065460920333862,
      "learning_rate": 0.00019997265363470913,
      "loss": 0.1347,
      "step": 529
    },
    {
      "epoch": 0.0024930148546054923,
      "grad_norm": 0.922940731048584,
      "learning_rate": 0.00019997171065659568,
      "loss": 0.0388,
      "step": 530
    },
    {
      "epoch": 0.0024977186562179553,
      "grad_norm": 3.3904435634613037,
      "learning_rate": 0.0001999707676784822,
      "loss": 0.3006,
      "step": 531
    },
    {
      "epoch": 0.0025024224578304183,
      "grad_norm": 4.31658411026001,
      "learning_rate": 0.00019996982470036871,
      "loss": 0.4903,
      "step": 532
    },
    {
      "epoch": 0.002507126259442882,
      "grad_norm": 5.422018527984619,
      "learning_rate": 0.00019996888172225523,
      "loss": 0.5437,
      "step": 533
    },
    {
      "epoch": 0.002511830061055345,
      "grad_norm": 2.776301383972168,
      "learning_rate": 0.00019996793874414175,
      "loss": 0.168,
      "step": 534
    },
    {
      "epoch": 0.0025165338626678083,
      "grad_norm": 2.6906278133392334,
      "learning_rate": 0.0001999669957660283,
      "loss": 0.1889,
      "step": 535
    },
    {
      "epoch": 0.0025212376642802714,
      "grad_norm": 10.224071502685547,
      "learning_rate": 0.00019996605278791482,
      "loss": 1.355,
      "step": 536
    },
    {
      "epoch": 0.0025259414658927344,
      "grad_norm": 10.28413200378418,
      "learning_rate": 0.0001999651098098013,
      "loss": 1.1856,
      "step": 537
    },
    {
      "epoch": 0.002530645267505198,
      "grad_norm": 1.4848287105560303,
      "learning_rate": 0.00019996416683168783,
      "loss": 0.0659,
      "step": 538
    },
    {
      "epoch": 0.002535349069117661,
      "grad_norm": 2.585472583770752,
      "learning_rate": 0.00019996322385357437,
      "loss": 0.2186,
      "step": 539
    },
    {
      "epoch": 0.002540052870730124,
      "grad_norm": 1.6322215795516968,
      "learning_rate": 0.0001999622808754609,
      "loss": 0.0678,
      "step": 540
    },
    {
      "epoch": 0.0025447566723425874,
      "grad_norm": 1.3138689994812012,
      "learning_rate": 0.0001999613378973474,
      "loss": 0.0577,
      "step": 541
    },
    {
      "epoch": 0.0025494604739550505,
      "grad_norm": 5.464565753936768,
      "learning_rate": 0.00019996039491923393,
      "loss": 0.7226,
      "step": 542
    },
    {
      "epoch": 0.0025541642755675135,
      "grad_norm": 2.3187978267669678,
      "learning_rate": 0.00019995945194112045,
      "loss": 0.1611,
      "step": 543
    },
    {
      "epoch": 0.002558868077179977,
      "grad_norm": 5.241946220397949,
      "learning_rate": 0.000199958508963007,
      "loss": 0.7109,
      "step": 544
    },
    {
      "epoch": 0.00256357187879244,
      "grad_norm": 7.148946285247803,
      "learning_rate": 0.0001999575659848935,
      "loss": 0.9148,
      "step": 545
    },
    {
      "epoch": 0.002568275680404903,
      "grad_norm": 4.584968566894531,
      "learning_rate": 0.00019995662300678003,
      "loss": 0.8222,
      "step": 546
    },
    {
      "epoch": 0.0025729794820173665,
      "grad_norm": 1.0163313150405884,
      "learning_rate": 0.00019995568002866655,
      "loss": 0.1344,
      "step": 547
    },
    {
      "epoch": 0.0025776832836298296,
      "grad_norm": 2.384119987487793,
      "learning_rate": 0.00019995473705055307,
      "loss": 0.4091,
      "step": 548
    },
    {
      "epoch": 0.0025823870852422926,
      "grad_norm": 1.4941052198410034,
      "learning_rate": 0.00019995379407243959,
      "loss": 0.2154,
      "step": 549
    },
    {
      "epoch": 0.002587090886854756,
      "grad_norm": 1.1270636320114136,
      "learning_rate": 0.0001999528510943261,
      "loss": 0.0647,
      "step": 550
    },
    {
      "epoch": 0.002591794688467219,
      "grad_norm": 1.4387873411178589,
      "learning_rate": 0.00019995190811621262,
      "loss": 0.1066,
      "step": 551
    },
    {
      "epoch": 0.0025964984900796826,
      "grad_norm": 7.61419677734375,
      "learning_rate": 0.00019995096513809914,
      "loss": 0.9263,
      "step": 552
    },
    {
      "epoch": 0.0026012022916921456,
      "grad_norm": 2.3461380004882812,
      "learning_rate": 0.0001999500221599857,
      "loss": 0.1271,
      "step": 553
    },
    {
      "epoch": 0.0026059060933046087,
      "grad_norm": 3.3745126724243164,
      "learning_rate": 0.0001999490791818722,
      "loss": 0.3913,
      "step": 554
    },
    {
      "epoch": 0.002610609894917072,
      "grad_norm": 1.1886646747589111,
      "learning_rate": 0.00019994813620375872,
      "loss": 0.1279,
      "step": 555
    },
    {
      "epoch": 0.002615313696529535,
      "grad_norm": 5.635507106781006,
      "learning_rate": 0.00019994719322564524,
      "loss": 0.6348,
      "step": 556
    },
    {
      "epoch": 0.0026200174981419982,
      "grad_norm": 3.9375510215759277,
      "learning_rate": 0.00019994625024753176,
      "loss": 0.2696,
      "step": 557
    },
    {
      "epoch": 0.0026247212997544617,
      "grad_norm": 6.265040874481201,
      "learning_rate": 0.00019994530726941828,
      "loss": 0.8545,
      "step": 558
    },
    {
      "epoch": 0.0026294251013669247,
      "grad_norm": 5.659762859344482,
      "learning_rate": 0.0001999443642913048,
      "loss": 0.5455,
      "step": 559
    },
    {
      "epoch": 0.0026341289029793878,
      "grad_norm": 1.0451126098632812,
      "learning_rate": 0.00019994342131319132,
      "loss": 0.0981,
      "step": 560
    },
    {
      "epoch": 0.0026388327045918513,
      "grad_norm": 2.7365615367889404,
      "learning_rate": 0.00019994247833507784,
      "loss": 0.1385,
      "step": 561
    },
    {
      "epoch": 0.0026435365062043143,
      "grad_norm": 3.3486101627349854,
      "learning_rate": 0.00019994153535696438,
      "loss": 0.3833,
      "step": 562
    },
    {
      "epoch": 0.0026482403078167773,
      "grad_norm": 1.8383309841156006,
      "learning_rate": 0.0001999405923788509,
      "loss": 0.0886,
      "step": 563
    },
    {
      "epoch": 0.002652944109429241,
      "grad_norm": 5.769720077514648,
      "learning_rate": 0.00019993964940073742,
      "loss": 0.4087,
      "step": 564
    },
    {
      "epoch": 0.002657647911041704,
      "grad_norm": 3.2684357166290283,
      "learning_rate": 0.00019993870642262394,
      "loss": 0.3887,
      "step": 565
    },
    {
      "epoch": 0.002662351712654167,
      "grad_norm": 5.807337760925293,
      "learning_rate": 0.00019993776344451048,
      "loss": 0.6165,
      "step": 566
    },
    {
      "epoch": 0.0026670555142666304,
      "grad_norm": 5.6973958015441895,
      "learning_rate": 0.000199936820466397,
      "loss": 0.2833,
      "step": 567
    },
    {
      "epoch": 0.0026717593158790934,
      "grad_norm": 10.67391300201416,
      "learning_rate": 0.0001999358774882835,
      "loss": 0.8161,
      "step": 568
    },
    {
      "epoch": 0.002676463117491557,
      "grad_norm": 3.006187677383423,
      "learning_rate": 0.00019993493451017,
      "loss": 0.1708,
      "step": 569
    },
    {
      "epoch": 0.00268116691910402,
      "grad_norm": 3.5090417861938477,
      "learning_rate": 0.00019993399153205653,
      "loss": 0.2324,
      "step": 570
    },
    {
      "epoch": 0.002685870720716483,
      "grad_norm": 1.9104583263397217,
      "learning_rate": 0.00019993304855394308,
      "loss": 0.1244,
      "step": 571
    },
    {
      "epoch": 0.0026905745223289464,
      "grad_norm": 4.58314323425293,
      "learning_rate": 0.0001999321055758296,
      "loss": 0.6577,
      "step": 572
    },
    {
      "epoch": 0.0026952783239414095,
      "grad_norm": 3.729210615158081,
      "learning_rate": 0.00019993116259771611,
      "loss": 0.2797,
      "step": 573
    },
    {
      "epoch": 0.0026999821255538725,
      "grad_norm": 0.3151964545249939,
      "learning_rate": 0.00019993021961960263,
      "loss": 0.0111,
      "step": 574
    },
    {
      "epoch": 0.002704685927166336,
      "grad_norm": 3.0578484535217285,
      "learning_rate": 0.00019992927664148918,
      "loss": 0.2106,
      "step": 575
    },
    {
      "epoch": 0.002709389728778799,
      "grad_norm": 0.36270835995674133,
      "learning_rate": 0.0001999283336633757,
      "loss": 0.0131,
      "step": 576
    },
    {
      "epoch": 0.002714093530391262,
      "grad_norm": 4.578805446624756,
      "learning_rate": 0.00019992739068526222,
      "loss": 0.6182,
      "step": 577
    },
    {
      "epoch": 0.0027187973320037255,
      "grad_norm": 1.4137585163116455,
      "learning_rate": 0.00019992644770714873,
      "loss": 0.1828,
      "step": 578
    },
    {
      "epoch": 0.0027235011336161886,
      "grad_norm": 3.771151304244995,
      "learning_rate": 0.00019992550472903525,
      "loss": 0.5986,
      "step": 579
    },
    {
      "epoch": 0.0027282049352286516,
      "grad_norm": 5.3465495109558105,
      "learning_rate": 0.00019992456175092177,
      "loss": 0.598,
      "step": 580
    },
    {
      "epoch": 0.002732908736841115,
      "grad_norm": 2.330317497253418,
      "learning_rate": 0.0001999236187728083,
      "loss": 0.2394,
      "step": 581
    },
    {
      "epoch": 0.002737612538453578,
      "grad_norm": 2.4833929538726807,
      "learning_rate": 0.0001999226757946948,
      "loss": 0.1781,
      "step": 582
    },
    {
      "epoch": 0.0027423163400660416,
      "grad_norm": 1.0978729724884033,
      "learning_rate": 0.00019992173281658133,
      "loss": 0.0827,
      "step": 583
    },
    {
      "epoch": 0.0027470201416785046,
      "grad_norm": 2.2897653579711914,
      "learning_rate": 0.00019992078983846785,
      "loss": 0.1497,
      "step": 584
    },
    {
      "epoch": 0.0027517239432909677,
      "grad_norm": 3.143242359161377,
      "learning_rate": 0.0001999198468603544,
      "loss": 0.4842,
      "step": 585
    },
    {
      "epoch": 0.002756427744903431,
      "grad_norm": 2.6832809448242188,
      "learning_rate": 0.0001999189038822409,
      "loss": 0.2921,
      "step": 586
    },
    {
      "epoch": 0.002761131546515894,
      "grad_norm": 5.166665554046631,
      "learning_rate": 0.00019991796090412743,
      "loss": 0.5207,
      "step": 587
    },
    {
      "epoch": 0.002765835348128357,
      "grad_norm": 0.8477105498313904,
      "learning_rate": 0.00019991701792601395,
      "loss": 0.0472,
      "step": 588
    },
    {
      "epoch": 0.0027705391497408207,
      "grad_norm": 1.8871163129806519,
      "learning_rate": 0.00019991607494790047,
      "loss": 0.1533,
      "step": 589
    },
    {
      "epoch": 0.0027752429513532837,
      "grad_norm": 1.741644024848938,
      "learning_rate": 0.00019991513196978699,
      "loss": 0.1795,
      "step": 590
    },
    {
      "epoch": 0.0027799467529657468,
      "grad_norm": 1.1360101699829102,
      "learning_rate": 0.0001999141889916735,
      "loss": 0.1051,
      "step": 591
    },
    {
      "epoch": 0.0027846505545782102,
      "grad_norm": 2.9317915439605713,
      "learning_rate": 0.00019991324601356002,
      "loss": 0.2745,
      "step": 592
    },
    {
      "epoch": 0.0027893543561906733,
      "grad_norm": 8.545475959777832,
      "learning_rate": 0.00019991230303544654,
      "loss": 0.2086,
      "step": 593
    },
    {
      "epoch": 0.0027940581578031363,
      "grad_norm": 1.1745413541793823,
      "learning_rate": 0.0001999113600573331,
      "loss": 0.0503,
      "step": 594
    },
    {
      "epoch": 0.0027987619594156,
      "grad_norm": 1.6579742431640625,
      "learning_rate": 0.0001999104170792196,
      "loss": 0.2281,
      "step": 595
    },
    {
      "epoch": 0.002803465761028063,
      "grad_norm": 4.4159626960754395,
      "learning_rate": 0.00019990947410110612,
      "loss": 0.257,
      "step": 596
    },
    {
      "epoch": 0.002808169562640526,
      "grad_norm": 1.1932029724121094,
      "learning_rate": 0.00019990853112299264,
      "loss": 0.0608,
      "step": 597
    },
    {
      "epoch": 0.0028128733642529893,
      "grad_norm": 1.2683508396148682,
      "learning_rate": 0.0001999075881448792,
      "loss": 0.0994,
      "step": 598
    },
    {
      "epoch": 0.0028175771658654524,
      "grad_norm": 6.023769378662109,
      "learning_rate": 0.00019990664516676568,
      "loss": 0.9381,
      "step": 599
    },
    {
      "epoch": 0.002822280967477916,
      "grad_norm": 6.486326217651367,
      "learning_rate": 0.0001999057021886522,
      "loss": 0.7768,
      "step": 600
    },
    {
      "epoch": 0.002826984769090379,
      "grad_norm": 5.9107489585876465,
      "learning_rate": 0.00019990475921053872,
      "loss": 0.3047,
      "step": 601
    },
    {
      "epoch": 0.002831688570702842,
      "grad_norm": 1.7816431522369385,
      "learning_rate": 0.00019990381623242524,
      "loss": 0.1483,
      "step": 602
    },
    {
      "epoch": 0.0028363923723153054,
      "grad_norm": 5.979990482330322,
      "learning_rate": 0.00019990287325431178,
      "loss": 0.308,
      "step": 603
    },
    {
      "epoch": 0.0028410961739277684,
      "grad_norm": 2.1106746196746826,
      "learning_rate": 0.0001999019302761983,
      "loss": 0.0951,
      "step": 604
    },
    {
      "epoch": 0.0028457999755402315,
      "grad_norm": 5.376126766204834,
      "learning_rate": 0.00019990098729808482,
      "loss": 0.2781,
      "step": 605
    },
    {
      "epoch": 0.002850503777152695,
      "grad_norm": 3.920464277267456,
      "learning_rate": 0.00019990004431997134,
      "loss": 0.5376,
      "step": 606
    },
    {
      "epoch": 0.002855207578765158,
      "grad_norm": 1.4465391635894775,
      "learning_rate": 0.00019989910134185788,
      "loss": 0.1126,
      "step": 607
    },
    {
      "epoch": 0.002859911380377621,
      "grad_norm": 3.4608986377716064,
      "learning_rate": 0.0001998981583637444,
      "loss": 0.3299,
      "step": 608
    },
    {
      "epoch": 0.0028646151819900845,
      "grad_norm": 6.820397853851318,
      "learning_rate": 0.00019989721538563092,
      "loss": 0.5168,
      "step": 609
    },
    {
      "epoch": 0.0028693189836025475,
      "grad_norm": 7.750339031219482,
      "learning_rate": 0.00019989627240751744,
      "loss": 1.1041,
      "step": 610
    },
    {
      "epoch": 0.0028740227852150106,
      "grad_norm": 3.512814521789551,
      "learning_rate": 0.00019989532942940393,
      "loss": 0.4075,
      "step": 611
    },
    {
      "epoch": 0.002878726586827474,
      "grad_norm": 3.448805809020996,
      "learning_rate": 0.00019989438645129048,
      "loss": 0.3298,
      "step": 612
    },
    {
      "epoch": 0.002883430388439937,
      "grad_norm": 0.8876003623008728,
      "learning_rate": 0.000199893443473177,
      "loss": 0.0687,
      "step": 613
    },
    {
      "epoch": 0.0028881341900524,
      "grad_norm": 2.7814791202545166,
      "learning_rate": 0.00019989250049506351,
      "loss": 0.2559,
      "step": 614
    },
    {
      "epoch": 0.0028928379916648636,
      "grad_norm": 4.265305042266846,
      "learning_rate": 0.00019989155751695003,
      "loss": 0.4086,
      "step": 615
    },
    {
      "epoch": 0.0028975417932773266,
      "grad_norm": 2.155289888381958,
      "learning_rate": 0.00019989061453883658,
      "loss": 0.1587,
      "step": 616
    },
    {
      "epoch": 0.00290224559488979,
      "grad_norm": 2.114879846572876,
      "learning_rate": 0.0001998896715607231,
      "loss": 0.1158,
      "step": 617
    },
    {
      "epoch": 0.002906949396502253,
      "grad_norm": 4.858074188232422,
      "learning_rate": 0.00019988872858260962,
      "loss": 0.6083,
      "step": 618
    },
    {
      "epoch": 0.002911653198114716,
      "grad_norm": 1.7556989192962646,
      "learning_rate": 0.00019988778560449613,
      "loss": 0.1904,
      "step": 619
    },
    {
      "epoch": 0.0029163569997271797,
      "grad_norm": 5.425992012023926,
      "learning_rate": 0.00019988684262638265,
      "loss": 0.2776,
      "step": 620
    },
    {
      "epoch": 0.0029210608013396427,
      "grad_norm": 2.372183322906494,
      "learning_rate": 0.00019988589964826917,
      "loss": 0.1843,
      "step": 621
    },
    {
      "epoch": 0.0029257646029521057,
      "grad_norm": 4.964325428009033,
      "learning_rate": 0.0001998849566701557,
      "loss": 0.3778,
      "step": 622
    },
    {
      "epoch": 0.0029304684045645692,
      "grad_norm": 2.822535991668701,
      "learning_rate": 0.0001998840136920422,
      "loss": 0.2228,
      "step": 623
    },
    {
      "epoch": 0.0029351722061770323,
      "grad_norm": 5.761855602264404,
      "learning_rate": 0.00019988307071392873,
      "loss": 1.0201,
      "step": 624
    },
    {
      "epoch": 0.0029398760077894953,
      "grad_norm": 4.019589424133301,
      "learning_rate": 0.00019988212773581527,
      "loss": 0.3764,
      "step": 625
    },
    {
      "epoch": 0.0029445798094019588,
      "grad_norm": 1.0415745973587036,
      "learning_rate": 0.0001998811847577018,
      "loss": 0.0586,
      "step": 626
    },
    {
      "epoch": 0.002949283611014422,
      "grad_norm": 4.722923278808594,
      "learning_rate": 0.0001998802417795883,
      "loss": 0.3379,
      "step": 627
    },
    {
      "epoch": 0.002953987412626885,
      "grad_norm": 5.970535755157471,
      "learning_rate": 0.00019987929880147483,
      "loss": 0.7326,
      "step": 628
    },
    {
      "epoch": 0.0029586912142393483,
      "grad_norm": 2.435281276702881,
      "learning_rate": 0.00019987835582336135,
      "loss": 0.3266,
      "step": 629
    },
    {
      "epoch": 0.0029633950158518114,
      "grad_norm": 1.466345191001892,
      "learning_rate": 0.00019987741284524787,
      "loss": 0.0475,
      "step": 630
    },
    {
      "epoch": 0.002968098817464275,
      "grad_norm": 0.9785760641098022,
      "learning_rate": 0.00019987646986713439,
      "loss": 0.0651,
      "step": 631
    },
    {
      "epoch": 0.002972802619076738,
      "grad_norm": 2.968933343887329,
      "learning_rate": 0.0001998755268890209,
      "loss": 0.3145,
      "step": 632
    },
    {
      "epoch": 0.002977506420689201,
      "grad_norm": 0.6597616672515869,
      "learning_rate": 0.00019987458391090742,
      "loss": 0.0372,
      "step": 633
    },
    {
      "epoch": 0.0029822102223016644,
      "grad_norm": 1.2770494222640991,
      "learning_rate": 0.00019987364093279394,
      "loss": 0.0838,
      "step": 634
    },
    {
      "epoch": 0.0029869140239141274,
      "grad_norm": 3.5290069580078125,
      "learning_rate": 0.0001998726979546805,
      "loss": 0.3791,
      "step": 635
    },
    {
      "epoch": 0.0029916178255265905,
      "grad_norm": 5.321798324584961,
      "learning_rate": 0.000199871754976567,
      "loss": 0.6737,
      "step": 636
    },
    {
      "epoch": 0.002996321627139054,
      "grad_norm": 7.353355407714844,
      "learning_rate": 0.00019987081199845352,
      "loss": 0.9599,
      "step": 637
    },
    {
      "epoch": 0.003001025428751517,
      "grad_norm": 4.455419063568115,
      "learning_rate": 0.00019986986902034004,
      "loss": 0.4745,
      "step": 638
    },
    {
      "epoch": 0.00300572923036398,
      "grad_norm": 5.390873908996582,
      "learning_rate": 0.0001998689260422266,
      "loss": 0.7398,
      "step": 639
    },
    {
      "epoch": 0.0030104330319764435,
      "grad_norm": 5.923178195953369,
      "learning_rate": 0.0001998679830641131,
      "loss": 0.697,
      "step": 640
    },
    {
      "epoch": 0.0030151368335889065,
      "grad_norm": 0.9895789623260498,
      "learning_rate": 0.0001998670400859996,
      "loss": 0.0672,
      "step": 641
    },
    {
      "epoch": 0.0030198406352013696,
      "grad_norm": 4.6699981689453125,
      "learning_rate": 0.00019986609710788612,
      "loss": 0.3894,
      "step": 642
    },
    {
      "epoch": 0.003024544436813833,
      "grad_norm": 2.9827492237091064,
      "learning_rate": 0.00019986515412977264,
      "loss": 0.2429,
      "step": 643
    },
    {
      "epoch": 0.003029248238426296,
      "grad_norm": 4.89498233795166,
      "learning_rate": 0.00019986421115165918,
      "loss": 0.4751,
      "step": 644
    },
    {
      "epoch": 0.003033952040038759,
      "grad_norm": 0.9322036504745483,
      "learning_rate": 0.0001998632681735457,
      "loss": 0.0484,
      "step": 645
    },
    {
      "epoch": 0.0030386558416512226,
      "grad_norm": 1.255699872970581,
      "learning_rate": 0.00019986232519543222,
      "loss": 0.1231,
      "step": 646
    },
    {
      "epoch": 0.0030433596432636856,
      "grad_norm": 4.447022438049316,
      "learning_rate": 0.00019986138221731874,
      "loss": 0.755,
      "step": 647
    },
    {
      "epoch": 0.003048063444876149,
      "grad_norm": 2.8242523670196533,
      "learning_rate": 0.00019986043923920528,
      "loss": 0.1157,
      "step": 648
    },
    {
      "epoch": 0.003052767246488612,
      "grad_norm": 2.3320181369781494,
      "learning_rate": 0.0001998594962610918,
      "loss": 0.2693,
      "step": 649
    },
    {
      "epoch": 0.003057471048101075,
      "grad_norm": 1.925260066986084,
      "learning_rate": 0.00019985855328297832,
      "loss": 0.1919,
      "step": 650
    },
    {
      "epoch": 0.0030621748497135387,
      "grad_norm": 1.8796541690826416,
      "learning_rate": 0.00019985761030486484,
      "loss": 0.117,
      "step": 651
    },
    {
      "epoch": 0.0030668786513260017,
      "grad_norm": 1.1869040727615356,
      "learning_rate": 0.00019985666732675136,
      "loss": 0.0477,
      "step": 652
    },
    {
      "epoch": 0.0030715824529384647,
      "grad_norm": 5.200232982635498,
      "learning_rate": 0.00019985572434863788,
      "loss": 0.1342,
      "step": 653
    },
    {
      "epoch": 0.003076286254550928,
      "grad_norm": 4.148637294769287,
      "learning_rate": 0.0001998547813705244,
      "loss": 0.2333,
      "step": 654
    },
    {
      "epoch": 0.0030809900561633912,
      "grad_norm": 2.3496146202087402,
      "learning_rate": 0.00019985383839241091,
      "loss": 0.1796,
      "step": 655
    },
    {
      "epoch": 0.0030856938577758543,
      "grad_norm": 4.071491241455078,
      "learning_rate": 0.00019985289541429743,
      "loss": 0.5679,
      "step": 656
    },
    {
      "epoch": 0.0030903976593883178,
      "grad_norm": 4.623934745788574,
      "learning_rate": 0.00019985195243618398,
      "loss": 0.5293,
      "step": 657
    },
    {
      "epoch": 0.003095101461000781,
      "grad_norm": 3.2329652309417725,
      "learning_rate": 0.0001998510094580705,
      "loss": 0.3043,
      "step": 658
    },
    {
      "epoch": 0.003099805262613244,
      "grad_norm": 5.119171142578125,
      "learning_rate": 0.00019985006647995702,
      "loss": 0.3622,
      "step": 659
    },
    {
      "epoch": 0.0031045090642257073,
      "grad_norm": 2.5936732292175293,
      "learning_rate": 0.00019984912350184353,
      "loss": 0.1239,
      "step": 660
    },
    {
      "epoch": 0.0031092128658381703,
      "grad_norm": 1.5400404930114746,
      "learning_rate": 0.00019984818052373005,
      "loss": 0.2147,
      "step": 661
    },
    {
      "epoch": 0.003113916667450634,
      "grad_norm": 4.7632222175598145,
      "learning_rate": 0.00019984723754561657,
      "loss": 0.7331,
      "step": 662
    },
    {
      "epoch": 0.003118620469063097,
      "grad_norm": 3.956862688064575,
      "learning_rate": 0.0001998462945675031,
      "loss": 0.2979,
      "step": 663
    },
    {
      "epoch": 0.00312332427067556,
      "grad_norm": 3.5224850177764893,
      "learning_rate": 0.0001998453515893896,
      "loss": 0.3799,
      "step": 664
    },
    {
      "epoch": 0.0031280280722880234,
      "grad_norm": 2.1835744380950928,
      "learning_rate": 0.00019984440861127613,
      "loss": 0.2335,
      "step": 665
    },
    {
      "epoch": 0.0031327318739004864,
      "grad_norm": 3.460556745529175,
      "learning_rate": 0.00019984346563316267,
      "loss": 0.3413,
      "step": 666
    },
    {
      "epoch": 0.0031374356755129494,
      "grad_norm": 2.0403270721435547,
      "learning_rate": 0.0001998425226550492,
      "loss": 0.1253,
      "step": 667
    },
    {
      "epoch": 0.003142139477125413,
      "grad_norm": 1.8948084115982056,
      "learning_rate": 0.0001998415796769357,
      "loss": 0.0892,
      "step": 668
    },
    {
      "epoch": 0.003146843278737876,
      "grad_norm": 4.522554874420166,
      "learning_rate": 0.00019984063669882223,
      "loss": 0.3471,
      "step": 669
    },
    {
      "epoch": 0.003151547080350339,
      "grad_norm": 3.44358229637146,
      "learning_rate": 0.00019983969372070875,
      "loss": 0.296,
      "step": 670
    },
    {
      "epoch": 0.0031562508819628025,
      "grad_norm": 2.3977081775665283,
      "learning_rate": 0.0001998387507425953,
      "loss": 0.2188,
      "step": 671
    },
    {
      "epoch": 0.0031609546835752655,
      "grad_norm": 2.8824875354766846,
      "learning_rate": 0.00019983780776448179,
      "loss": 0.275,
      "step": 672
    },
    {
      "epoch": 0.0031656584851877285,
      "grad_norm": 1.496519684791565,
      "learning_rate": 0.0001998368647863683,
      "loss": 0.0925,
      "step": 673
    },
    {
      "epoch": 0.003170362286800192,
      "grad_norm": 2.536027193069458,
      "learning_rate": 0.00019983592180825482,
      "loss": 0.4141,
      "step": 674
    },
    {
      "epoch": 0.003175066088412655,
      "grad_norm": 4.470987796783447,
      "learning_rate": 0.00019983497883014137,
      "loss": 0.7461,
      "step": 675
    },
    {
      "epoch": 0.003179769890025118,
      "grad_norm": 7.664361476898193,
      "learning_rate": 0.0001998340358520279,
      "loss": 0.6813,
      "step": 676
    },
    {
      "epoch": 0.0031844736916375816,
      "grad_norm": 0.46496066451072693,
      "learning_rate": 0.0001998330928739144,
      "loss": 0.0201,
      "step": 677
    },
    {
      "epoch": 0.0031891774932500446,
      "grad_norm": 2.6213464736938477,
      "learning_rate": 0.00019983214989580092,
      "loss": 0.3449,
      "step": 678
    },
    {
      "epoch": 0.003193881294862508,
      "grad_norm": 2.133852481842041,
      "learning_rate": 0.00019983120691768744,
      "loss": 0.2775,
      "step": 679
    },
    {
      "epoch": 0.003198585096474971,
      "grad_norm": 2.3365325927734375,
      "learning_rate": 0.000199830263939574,
      "loss": 0.3445,
      "step": 680
    },
    {
      "epoch": 0.003203288898087434,
      "grad_norm": 3.1866402626037598,
      "learning_rate": 0.0001998293209614605,
      "loss": 0.4403,
      "step": 681
    },
    {
      "epoch": 0.0032079926996998976,
      "grad_norm": 0.8377223014831543,
      "learning_rate": 0.00019982837798334703,
      "loss": 0.0395,
      "step": 682
    },
    {
      "epoch": 0.0032126965013123607,
      "grad_norm": 3.9217641353607178,
      "learning_rate": 0.00019982743500523355,
      "loss": 0.2284,
      "step": 683
    },
    {
      "epoch": 0.0032174003029248237,
      "grad_norm": 2.0801703929901123,
      "learning_rate": 0.00019982649202712004,
      "loss": 0.2378,
      "step": 684
    },
    {
      "epoch": 0.003222104104537287,
      "grad_norm": 1.5071691274642944,
      "learning_rate": 0.00019982554904900658,
      "loss": 0.1527,
      "step": 685
    },
    {
      "epoch": 0.0032268079061497502,
      "grad_norm": 1.7807644605636597,
      "learning_rate": 0.0001998246060708931,
      "loss": 0.1528,
      "step": 686
    },
    {
      "epoch": 0.0032315117077622133,
      "grad_norm": 0.8965314030647278,
      "learning_rate": 0.00019982366309277962,
      "loss": 0.042,
      "step": 687
    },
    {
      "epoch": 0.0032362155093746767,
      "grad_norm": 1.6664773225784302,
      "learning_rate": 0.00019982272011466614,
      "loss": 0.1817,
      "step": 688
    },
    {
      "epoch": 0.0032409193109871398,
      "grad_norm": 2.259044885635376,
      "learning_rate": 0.00019982177713655268,
      "loss": 0.1059,
      "step": 689
    },
    {
      "epoch": 0.003245623112599603,
      "grad_norm": 3.732649326324463,
      "learning_rate": 0.0001998208341584392,
      "loss": 0.3466,
      "step": 690
    },
    {
      "epoch": 0.0032503269142120663,
      "grad_norm": 0.6865069270133972,
      "learning_rate": 0.00019981989118032572,
      "loss": 0.0369,
      "step": 691
    },
    {
      "epoch": 0.0032550307158245293,
      "grad_norm": 2.923830032348633,
      "learning_rate": 0.00019981894820221224,
      "loss": 0.3983,
      "step": 692
    },
    {
      "epoch": 0.0032597345174369924,
      "grad_norm": 1.0155783891677856,
      "learning_rate": 0.00019981800522409876,
      "loss": 0.1191,
      "step": 693
    },
    {
      "epoch": 0.003264438319049456,
      "grad_norm": 4.116394996643066,
      "learning_rate": 0.00019981706224598528,
      "loss": 0.7991,
      "step": 694
    },
    {
      "epoch": 0.003269142120661919,
      "grad_norm": 0.26660841703414917,
      "learning_rate": 0.0001998161192678718,
      "loss": 0.0093,
      "step": 695
    },
    {
      "epoch": 0.0032738459222743824,
      "grad_norm": 0.049822740256786346,
      "learning_rate": 0.00019981517628975831,
      "loss": 0.0009,
      "step": 696
    },
    {
      "epoch": 0.0032785497238868454,
      "grad_norm": 0.35425907373428345,
      "learning_rate": 0.00019981423331164483,
      "loss": 0.0152,
      "step": 697
    },
    {
      "epoch": 0.0032832535254993084,
      "grad_norm": 4.4763031005859375,
      "learning_rate": 0.00019981329033353138,
      "loss": 0.5249,
      "step": 698
    },
    {
      "epoch": 0.003287957327111772,
      "grad_norm": 1.7317616939544678,
      "learning_rate": 0.0001998123473554179,
      "loss": 0.1375,
      "step": 699
    },
    {
      "epoch": 0.003292661128724235,
      "grad_norm": 2.0036051273345947,
      "learning_rate": 0.00019981140437730442,
      "loss": 0.1594,
      "step": 700
    },
    {
      "epoch": 0.003297364930336698,
      "grad_norm": 6.914712429046631,
      "learning_rate": 0.00019981046139919093,
      "loss": 0.4164,
      "step": 701
    },
    {
      "epoch": 0.0033020687319491615,
      "grad_norm": 7.351229667663574,
      "learning_rate": 0.00019980951842107745,
      "loss": 0.3932,
      "step": 702
    },
    {
      "epoch": 0.0033067725335616245,
      "grad_norm": 1.9157869815826416,
      "learning_rate": 0.00019980857544296397,
      "loss": 0.0914,
      "step": 703
    },
    {
      "epoch": 0.0033114763351740875,
      "grad_norm": 4.803820610046387,
      "learning_rate": 0.0001998076324648505,
      "loss": 0.3791,
      "step": 704
    },
    {
      "epoch": 0.003316180136786551,
      "grad_norm": 4.092446804046631,
      "learning_rate": 0.000199806689486737,
      "loss": 0.3111,
      "step": 705
    },
    {
      "epoch": 0.003320883938399014,
      "grad_norm": 8.289037704467773,
      "learning_rate": 0.00019980574650862353,
      "loss": 0.414,
      "step": 706
    },
    {
      "epoch": 0.003325587740011477,
      "grad_norm": 1.6593165397644043,
      "learning_rate": 0.00019980480353051007,
      "loss": 0.1817,
      "step": 707
    },
    {
      "epoch": 0.0033302915416239406,
      "grad_norm": 1.3535640239715576,
      "learning_rate": 0.0001998038605523966,
      "loss": 0.0798,
      "step": 708
    },
    {
      "epoch": 0.0033349953432364036,
      "grad_norm": 3.2224557399749756,
      "learning_rate": 0.0001998029175742831,
      "loss": 0.2774,
      "step": 709
    },
    {
      "epoch": 0.003339699144848867,
      "grad_norm": 1.0758998394012451,
      "learning_rate": 0.00019980197459616963,
      "loss": 0.0917,
      "step": 710
    },
    {
      "epoch": 0.00334440294646133,
      "grad_norm": 7.0475568771362305,
      "learning_rate": 0.00019980103161805615,
      "loss": 0.5593,
      "step": 711
    },
    {
      "epoch": 0.003349106748073793,
      "grad_norm": 0.8616170287132263,
      "learning_rate": 0.0001998000886399427,
      "loss": 0.0488,
      "step": 712
    },
    {
      "epoch": 0.0033538105496862566,
      "grad_norm": 0.9882147908210754,
      "learning_rate": 0.0001997991456618292,
      "loss": 0.0289,
      "step": 713
    },
    {
      "epoch": 0.0033585143512987197,
      "grad_norm": 0.3512348532676697,
      "learning_rate": 0.00019979820268371573,
      "loss": 0.0171,
      "step": 714
    },
    {
      "epoch": 0.0033632181529111827,
      "grad_norm": 7.051764011383057,
      "learning_rate": 0.00019979725970560222,
      "loss": 1.9375,
      "step": 715
    },
    {
      "epoch": 0.003367921954523646,
      "grad_norm": 1.6180366277694702,
      "learning_rate": 0.00019979631672748877,
      "loss": 0.0407,
      "step": 716
    },
    {
      "epoch": 0.003372625756136109,
      "grad_norm": 2.079805612564087,
      "learning_rate": 0.0001997953737493753,
      "loss": 0.0991,
      "step": 717
    },
    {
      "epoch": 0.0033773295577485722,
      "grad_norm": 2.244856119155884,
      "learning_rate": 0.0001997944307712618,
      "loss": 0.1052,
      "step": 718
    },
    {
      "epoch": 0.0033820333593610357,
      "grad_norm": 5.23406982421875,
      "learning_rate": 0.00019979348779314832,
      "loss": 0.5572,
      "step": 719
    },
    {
      "epoch": 0.0033867371609734988,
      "grad_norm": 4.127285480499268,
      "learning_rate": 0.00019979254481503484,
      "loss": 0.5623,
      "step": 720
    },
    {
      "epoch": 0.003391440962585962,
      "grad_norm": 2.6614346504211426,
      "learning_rate": 0.0001997916018369214,
      "loss": 0.1108,
      "step": 721
    },
    {
      "epoch": 0.0033961447641984253,
      "grad_norm": 6.999116897583008,
      "learning_rate": 0.0001997906588588079,
      "loss": 0.3679,
      "step": 722
    },
    {
      "epoch": 0.0034008485658108883,
      "grad_norm": 5.0894389152526855,
      "learning_rate": 0.00019978971588069443,
      "loss": 0.7167,
      "step": 723
    },
    {
      "epoch": 0.0034055523674233514,
      "grad_norm": 0.3045954704284668,
      "learning_rate": 0.00019978877290258094,
      "loss": 0.0132,
      "step": 724
    },
    {
      "epoch": 0.003410256169035815,
      "grad_norm": 3.3110711574554443,
      "learning_rate": 0.00019978782992446746,
      "loss": 0.2442,
      "step": 725
    },
    {
      "epoch": 0.003414959970648278,
      "grad_norm": 7.648102760314941,
      "learning_rate": 0.00019978688694635398,
      "loss": 0.6009,
      "step": 726
    },
    {
      "epoch": 0.0034196637722607413,
      "grad_norm": 7.214248180389404,
      "learning_rate": 0.0001997859439682405,
      "loss": 0.524,
      "step": 727
    },
    {
      "epoch": 0.0034243675738732044,
      "grad_norm": 1.3795311450958252,
      "learning_rate": 0.00019978500099012702,
      "loss": 0.1033,
      "step": 728
    },
    {
      "epoch": 0.0034290713754856674,
      "grad_norm": 4.430289268493652,
      "learning_rate": 0.00019978405801201354,
      "loss": 0.2421,
      "step": 729
    },
    {
      "epoch": 0.003433775177098131,
      "grad_norm": 0.9835808277130127,
      "learning_rate": 0.00019978311503390008,
      "loss": 0.058,
      "step": 730
    },
    {
      "epoch": 0.003438478978710594,
      "grad_norm": 4.867995262145996,
      "learning_rate": 0.0001997821720557866,
      "loss": 0.4837,
      "step": 731
    },
    {
      "epoch": 0.003443182780323057,
      "grad_norm": 1.2088003158569336,
      "learning_rate": 0.00019978122907767312,
      "loss": 0.0817,
      "step": 732
    },
    {
      "epoch": 0.0034478865819355204,
      "grad_norm": 3.145982027053833,
      "learning_rate": 0.00019978028609955964,
      "loss": 0.2513,
      "step": 733
    },
    {
      "epoch": 0.0034525903835479835,
      "grad_norm": 1.0700922012329102,
      "learning_rate": 0.00019977934312144616,
      "loss": 0.0782,
      "step": 734
    },
    {
      "epoch": 0.0034572941851604465,
      "grad_norm": 4.819093704223633,
      "learning_rate": 0.00019977840014333268,
      "loss": 0.4716,
      "step": 735
    },
    {
      "epoch": 0.00346199798677291,
      "grad_norm": 1.0987012386322021,
      "learning_rate": 0.0001997774571652192,
      "loss": 0.0838,
      "step": 736
    },
    {
      "epoch": 0.003466701788385373,
      "grad_norm": 4.498098373413086,
      "learning_rate": 0.00019977651418710571,
      "loss": 0.813,
      "step": 737
    },
    {
      "epoch": 0.003471405589997836,
      "grad_norm": 2.007070541381836,
      "learning_rate": 0.00019977557120899223,
      "loss": 0.1707,
      "step": 738
    },
    {
      "epoch": 0.0034761093916102995,
      "grad_norm": 1.4923171997070312,
      "learning_rate": 0.00019977462823087878,
      "loss": 0.0989,
      "step": 739
    },
    {
      "epoch": 0.0034808131932227626,
      "grad_norm": 2.7298333644866943,
      "learning_rate": 0.0001997736852527653,
      "loss": 0.4036,
      "step": 740
    },
    {
      "epoch": 0.0034855169948352256,
      "grad_norm": 3.2051470279693604,
      "learning_rate": 0.00019977274227465182,
      "loss": 0.2748,
      "step": 741
    },
    {
      "epoch": 0.003490220796447689,
      "grad_norm": 2.9899046421051025,
      "learning_rate": 0.00019977179929653833,
      "loss": 0.2817,
      "step": 742
    },
    {
      "epoch": 0.003494924598060152,
      "grad_norm": 2.592867851257324,
      "learning_rate": 0.00019977085631842485,
      "loss": 0.4408,
      "step": 743
    },
    {
      "epoch": 0.0034996283996726156,
      "grad_norm": 2.420393466949463,
      "learning_rate": 0.0001997699133403114,
      "loss": 0.3379,
      "step": 744
    },
    {
      "epoch": 0.0035043322012850786,
      "grad_norm": 2.5421671867370605,
      "learning_rate": 0.00019976897036219792,
      "loss": 0.3067,
      "step": 745
    },
    {
      "epoch": 0.0035090360028975417,
      "grad_norm": 1.3961797952651978,
      "learning_rate": 0.0001997680273840844,
      "loss": 0.0816,
      "step": 746
    },
    {
      "epoch": 0.003513739804510005,
      "grad_norm": 1.6264028549194336,
      "learning_rate": 0.00019976708440597093,
      "loss": 0.2849,
      "step": 747
    },
    {
      "epoch": 0.003518443606122468,
      "grad_norm": 4.887629985809326,
      "learning_rate": 0.00019976614142785747,
      "loss": 0.5643,
      "step": 748
    },
    {
      "epoch": 0.0035231474077349312,
      "grad_norm": 1.7852598428726196,
      "learning_rate": 0.000199765198449744,
      "loss": 0.1302,
      "step": 749
    },
    {
      "epoch": 0.0035278512093473947,
      "grad_norm": 3.496859550476074,
      "learning_rate": 0.0001997642554716305,
      "loss": 0.4507,
      "step": 750
    },
    {
      "epoch": 0.0035325550109598577,
      "grad_norm": 1.9377378225326538,
      "learning_rate": 0.00019976331249351703,
      "loss": 0.1132,
      "step": 751
    },
    {
      "epoch": 0.003537258812572321,
      "grad_norm": 3.2283592224121094,
      "learning_rate": 0.00019976236951540355,
      "loss": 0.1464,
      "step": 752
    },
    {
      "epoch": 0.0035419626141847843,
      "grad_norm": 1.0126895904541016,
      "learning_rate": 0.0001997614265372901,
      "loss": 0.0642,
      "step": 753
    },
    {
      "epoch": 0.0035466664157972473,
      "grad_norm": 2.15046763420105,
      "learning_rate": 0.0001997604835591766,
      "loss": 0.1565,
      "step": 754
    },
    {
      "epoch": 0.0035513702174097103,
      "grad_norm": 4.371125221252441,
      "learning_rate": 0.00019975954058106313,
      "loss": 0.3923,
      "step": 755
    },
    {
      "epoch": 0.003556074019022174,
      "grad_norm": 0.9291052222251892,
      "learning_rate": 0.00019975859760294965,
      "loss": 0.057,
      "step": 756
    },
    {
      "epoch": 0.003560777820634637,
      "grad_norm": 4.891977787017822,
      "learning_rate": 0.00019975765462483617,
      "loss": 0.4021,
      "step": 757
    },
    {
      "epoch": 0.0035654816222471003,
      "grad_norm": 6.337736129760742,
      "learning_rate": 0.0001997567116467227,
      "loss": 0.5628,
      "step": 758
    },
    {
      "epoch": 0.0035701854238595634,
      "grad_norm": 5.258932590484619,
      "learning_rate": 0.0001997557686686092,
      "loss": 0.4926,
      "step": 759
    },
    {
      "epoch": 0.0035748892254720264,
      "grad_norm": 8.956976890563965,
      "learning_rate": 0.00019975482569049572,
      "loss": 0.4917,
      "step": 760
    },
    {
      "epoch": 0.00357959302708449,
      "grad_norm": 0.9098829030990601,
      "learning_rate": 0.00019975388271238224,
      "loss": 0.0507,
      "step": 761
    },
    {
      "epoch": 0.003584296828696953,
      "grad_norm": 9.902016639709473,
      "learning_rate": 0.0001997529397342688,
      "loss": 1.2121,
      "step": 762
    },
    {
      "epoch": 0.003589000630309416,
      "grad_norm": 13.188253402709961,
      "learning_rate": 0.0001997519967561553,
      "loss": 0.5873,
      "step": 763
    },
    {
      "epoch": 0.0035937044319218794,
      "grad_norm": 5.708322525024414,
      "learning_rate": 0.00019975105377804183,
      "loss": 0.847,
      "step": 764
    },
    {
      "epoch": 0.0035984082335343425,
      "grad_norm": 1.9623807668685913,
      "learning_rate": 0.00019975011079992834,
      "loss": 0.123,
      "step": 765
    },
    {
      "epoch": 0.0036031120351468055,
      "grad_norm": 3.1812825202941895,
      "learning_rate": 0.00019974916782181486,
      "loss": 0.1545,
      "step": 766
    },
    {
      "epoch": 0.003607815836759269,
      "grad_norm": 2.3966028690338135,
      "learning_rate": 0.00019974822484370138,
      "loss": 0.1213,
      "step": 767
    },
    {
      "epoch": 0.003612519638371732,
      "grad_norm": 6.531218528747559,
      "learning_rate": 0.0001997472818655879,
      "loss": 0.5429,
      "step": 768
    },
    {
      "epoch": 0.003617223439984195,
      "grad_norm": 2.932382106781006,
      "learning_rate": 0.00019974633888747442,
      "loss": 0.468,
      "step": 769
    },
    {
      "epoch": 0.0036219272415966585,
      "grad_norm": 7.266598224639893,
      "learning_rate": 0.00019974539590936094,
      "loss": 0.6348,
      "step": 770
    },
    {
      "epoch": 0.0036266310432091216,
      "grad_norm": 10.07787799835205,
      "learning_rate": 0.00019974445293124748,
      "loss": 0.245,
      "step": 771
    },
    {
      "epoch": 0.0036313348448215846,
      "grad_norm": 5.017037391662598,
      "learning_rate": 0.000199743509953134,
      "loss": 0.6583,
      "step": 772
    },
    {
      "epoch": 0.003636038646434048,
      "grad_norm": 4.218452453613281,
      "learning_rate": 0.00019974256697502052,
      "loss": 0.6163,
      "step": 773
    },
    {
      "epoch": 0.003640742448046511,
      "grad_norm": 0.5723045468330383,
      "learning_rate": 0.00019974162399690704,
      "loss": 0.0312,
      "step": 774
    },
    {
      "epoch": 0.0036454462496589746,
      "grad_norm": 1.5858299732208252,
      "learning_rate": 0.00019974068101879359,
      "loss": 0.1595,
      "step": 775
    },
    {
      "epoch": 0.0036501500512714376,
      "grad_norm": 1.7094080448150635,
      "learning_rate": 0.0001997397380406801,
      "loss": 0.1687,
      "step": 776
    },
    {
      "epoch": 0.0036548538528839007,
      "grad_norm": 4.929214954376221,
      "learning_rate": 0.0001997387950625666,
      "loss": 0.7163,
      "step": 777
    },
    {
      "epoch": 0.003659557654496364,
      "grad_norm": 0.8485893607139587,
      "learning_rate": 0.00019973785208445311,
      "loss": 0.0713,
      "step": 778
    },
    {
      "epoch": 0.003664261456108827,
      "grad_norm": 2.429117202758789,
      "learning_rate": 0.00019973690910633963,
      "loss": 0.1789,
      "step": 779
    },
    {
      "epoch": 0.00366896525772129,
      "grad_norm": 6.601913928985596,
      "learning_rate": 0.00019973596612822618,
      "loss": 0.6066,
      "step": 780
    },
    {
      "epoch": 0.0036736690593337537,
      "grad_norm": 0.6980249285697937,
      "learning_rate": 0.0001997350231501127,
      "loss": 0.0355,
      "step": 781
    },
    {
      "epoch": 0.0036783728609462167,
      "grad_norm": 3.3183889389038086,
      "learning_rate": 0.00019973408017199922,
      "loss": 0.2565,
      "step": 782
    },
    {
      "epoch": 0.0036830766625586798,
      "grad_norm": 1.7383477687835693,
      "learning_rate": 0.00019973313719388573,
      "loss": 0.1905,
      "step": 783
    },
    {
      "epoch": 0.0036877804641711432,
      "grad_norm": 1.0041919946670532,
      "learning_rate": 0.00019973219421577228,
      "loss": 0.0642,
      "step": 784
    },
    {
      "epoch": 0.0036924842657836063,
      "grad_norm": 6.271389484405518,
      "learning_rate": 0.0001997312512376588,
      "loss": 0.7233,
      "step": 785
    },
    {
      "epoch": 0.0036971880673960693,
      "grad_norm": 1.2432334423065186,
      "learning_rate": 0.00019973030825954532,
      "loss": 0.0708,
      "step": 786
    },
    {
      "epoch": 0.003701891869008533,
      "grad_norm": 0.7254047393798828,
      "learning_rate": 0.00019972936528143184,
      "loss": 0.035,
      "step": 787
    },
    {
      "epoch": 0.003706595670620996,
      "grad_norm": 0.41578084230422974,
      "learning_rate": 0.00019972842230331833,
      "loss": 0.0147,
      "step": 788
    },
    {
      "epoch": 0.003711299472233459,
      "grad_norm": 7.647470474243164,
      "learning_rate": 0.00019972747932520487,
      "loss": 0.9813,
      "step": 789
    },
    {
      "epoch": 0.0037160032738459223,
      "grad_norm": 9.043394088745117,
      "learning_rate": 0.0001997265363470914,
      "loss": 0.5683,
      "step": 790
    },
    {
      "epoch": 0.0037207070754583854,
      "grad_norm": 4.077976703643799,
      "learning_rate": 0.0001997255933689779,
      "loss": 0.2939,
      "step": 791
    },
    {
      "epoch": 0.003725410877070849,
      "grad_norm": 4.999049663543701,
      "learning_rate": 0.00019972465039086443,
      "loss": 0.3417,
      "step": 792
    },
    {
      "epoch": 0.003730114678683312,
      "grad_norm": 2.734470844268799,
      "learning_rate": 0.00019972370741275095,
      "loss": 0.2965,
      "step": 793
    },
    {
      "epoch": 0.003734818480295775,
      "grad_norm": 4.27565336227417,
      "learning_rate": 0.0001997227644346375,
      "loss": 0.4357,
      "step": 794
    },
    {
      "epoch": 0.0037395222819082384,
      "grad_norm": 1.8730779886245728,
      "learning_rate": 0.000199721821456524,
      "loss": 0.2044,
      "step": 795
    },
    {
      "epoch": 0.0037442260835207014,
      "grad_norm": 6.089479923248291,
      "learning_rate": 0.00019972087847841053,
      "loss": 0.6735,
      "step": 796
    },
    {
      "epoch": 0.0037489298851331645,
      "grad_norm": 4.855020999908447,
      "learning_rate": 0.00019971993550029705,
      "loss": 0.319,
      "step": 797
    },
    {
      "epoch": 0.003753633686745628,
      "grad_norm": 5.966873645782471,
      "learning_rate": 0.00019971899252218357,
      "loss": 0.6209,
      "step": 798
    },
    {
      "epoch": 0.003758337488358091,
      "grad_norm": 2.4595608711242676,
      "learning_rate": 0.0001997180495440701,
      "loss": 0.2008,
      "step": 799
    },
    {
      "epoch": 0.003763041289970554,
      "grad_norm": 3.0964159965515137,
      "learning_rate": 0.0001997171065659566,
      "loss": 0.3928,
      "step": 800
    },
    {
      "epoch": 0.0037677450915830175,
      "grad_norm": 3.017940044403076,
      "learning_rate": 0.00019971616358784312,
      "loss": 0.1625,
      "step": 801
    },
    {
      "epoch": 0.0037724488931954805,
      "grad_norm": 3.077155828475952,
      "learning_rate": 0.00019971522060972964,
      "loss": 0.1405,
      "step": 802
    },
    {
      "epoch": 0.0037771526948079436,
      "grad_norm": 3.839731454849243,
      "learning_rate": 0.0001997142776316162,
      "loss": 0.1925,
      "step": 803
    },
    {
      "epoch": 0.003781856496420407,
      "grad_norm": 3.9447035789489746,
      "learning_rate": 0.0001997133346535027,
      "loss": 0.3221,
      "step": 804
    },
    {
      "epoch": 0.00378656029803287,
      "grad_norm": 3.661208152770996,
      "learning_rate": 0.00019971239167538923,
      "loss": 0.3085,
      "step": 805
    },
    {
      "epoch": 0.0037912640996453336,
      "grad_norm": 3.171854019165039,
      "learning_rate": 0.00019971144869727574,
      "loss": 0.2067,
      "step": 806
    },
    {
      "epoch": 0.0037959679012577966,
      "grad_norm": 4.156179428100586,
      "learning_rate": 0.0001997105057191623,
      "loss": 0.3249,
      "step": 807
    },
    {
      "epoch": 0.0038006717028702596,
      "grad_norm": 0.7393237352371216,
      "learning_rate": 0.00019970956274104878,
      "loss": 0.0492,
      "step": 808
    },
    {
      "epoch": 0.003805375504482723,
      "grad_norm": 1.9175344705581665,
      "learning_rate": 0.0001997086197629353,
      "loss": 0.1697,
      "step": 809
    },
    {
      "epoch": 0.003810079306095186,
      "grad_norm": 0.8247316479682922,
      "learning_rate": 0.00019970767678482182,
      "loss": 0.0854,
      "step": 810
    },
    {
      "epoch": 0.003814783107707649,
      "grad_norm": 4.238018035888672,
      "learning_rate": 0.00019970673380670834,
      "loss": 0.5455,
      "step": 811
    },
    {
      "epoch": 0.0038194869093201127,
      "grad_norm": 2.976881504058838,
      "learning_rate": 0.00019970579082859488,
      "loss": 0.3302,
      "step": 812
    },
    {
      "epoch": 0.0038241907109325757,
      "grad_norm": 1.8273224830627441,
      "learning_rate": 0.0001997048478504814,
      "loss": 0.1593,
      "step": 813
    },
    {
      "epoch": 0.0038288945125450388,
      "grad_norm": 0.7396630048751831,
      "learning_rate": 0.00019970390487236792,
      "loss": 0.0598,
      "step": 814
    },
    {
      "epoch": 0.0038335983141575022,
      "grad_norm": 1.375240683555603,
      "learning_rate": 0.00019970296189425444,
      "loss": 0.0856,
      "step": 815
    },
    {
      "epoch": 0.0038383021157699653,
      "grad_norm": 2.4276986122131348,
      "learning_rate": 0.00019970201891614099,
      "loss": 0.1118,
      "step": 816
    },
    {
      "epoch": 0.0038430059173824283,
      "grad_norm": 3.337308406829834,
      "learning_rate": 0.0001997010759380275,
      "loss": 0.1846,
      "step": 817
    },
    {
      "epoch": 0.0038477097189948918,
      "grad_norm": 2.7963943481445312,
      "learning_rate": 0.00019970013295991402,
      "loss": 0.2228,
      "step": 818
    },
    {
      "epoch": 0.003852413520607355,
      "grad_norm": 3.234858274459839,
      "learning_rate": 0.00019969918998180051,
      "loss": 0.2644,
      "step": 819
    },
    {
      "epoch": 0.003857117322219818,
      "grad_norm": 3.1895601749420166,
      "learning_rate": 0.00019969824700368703,
      "loss": 0.5279,
      "step": 820
    },
    {
      "epoch": 0.0038618211238322813,
      "grad_norm": 2.9223039150238037,
      "learning_rate": 0.00019969730402557358,
      "loss": 0.4018,
      "step": 821
    },
    {
      "epoch": 0.0038665249254447444,
      "grad_norm": 4.152519226074219,
      "learning_rate": 0.0001996963610474601,
      "loss": 0.417,
      "step": 822
    },
    {
      "epoch": 0.003871228727057208,
      "grad_norm": 3.0384833812713623,
      "learning_rate": 0.00019969541806934662,
      "loss": 0.2001,
      "step": 823
    },
    {
      "epoch": 0.003875932528669671,
      "grad_norm": 2.0597362518310547,
      "learning_rate": 0.00019969447509123313,
      "loss": 0.1356,
      "step": 824
    },
    {
      "epoch": 0.003880636330282134,
      "grad_norm": 1.1083074808120728,
      "learning_rate": 0.00019969353211311968,
      "loss": 0.112,
      "step": 825
    },
    {
      "epoch": 0.0038853401318945974,
      "grad_norm": 3.516921043395996,
      "learning_rate": 0.0001996925891350062,
      "loss": 0.3874,
      "step": 826
    },
    {
      "epoch": 0.0038900439335070604,
      "grad_norm": 0.8500960469245911,
      "learning_rate": 0.00019969164615689272,
      "loss": 0.0675,
      "step": 827
    },
    {
      "epoch": 0.0038947477351195235,
      "grad_norm": 0.9188404679298401,
      "learning_rate": 0.00019969070317877924,
      "loss": 0.0569,
      "step": 828
    },
    {
      "epoch": 0.003899451536731987,
      "grad_norm": 1.9219945669174194,
      "learning_rate": 0.00019968976020066576,
      "loss": 0.1342,
      "step": 829
    },
    {
      "epoch": 0.00390415533834445,
      "grad_norm": 3.1163229942321777,
      "learning_rate": 0.00019968881722255227,
      "loss": 0.2113,
      "step": 830
    },
    {
      "epoch": 0.003908859139956913,
      "grad_norm": 0.3294054865837097,
      "learning_rate": 0.0001996878742444388,
      "loss": 0.0127,
      "step": 831
    },
    {
      "epoch": 0.003913562941569376,
      "grad_norm": 3.5172455310821533,
      "learning_rate": 0.0001996869312663253,
      "loss": 0.2742,
      "step": 832
    },
    {
      "epoch": 0.00391826674318184,
      "grad_norm": 1.927416205406189,
      "learning_rate": 0.00019968598828821183,
      "loss": 0.1041,
      "step": 833
    },
    {
      "epoch": 0.003922970544794303,
      "grad_norm": 1.5283513069152832,
      "learning_rate": 0.00019968504531009838,
      "loss": 0.0608,
      "step": 834
    },
    {
      "epoch": 0.003927674346406766,
      "grad_norm": 4.355809211730957,
      "learning_rate": 0.0001996841023319849,
      "loss": 0.6392,
      "step": 835
    },
    {
      "epoch": 0.003932378148019229,
      "grad_norm": 7.324599266052246,
      "learning_rate": 0.0001996831593538714,
      "loss": 0.9752,
      "step": 836
    },
    {
      "epoch": 0.003937081949631692,
      "grad_norm": 1.6721248626708984,
      "learning_rate": 0.00019968221637575793,
      "loss": 0.0868,
      "step": 837
    },
    {
      "epoch": 0.003941785751244155,
      "grad_norm": 3.6601593494415283,
      "learning_rate": 0.00019968127339764445,
      "loss": 0.1553,
      "step": 838
    },
    {
      "epoch": 0.003946489552856619,
      "grad_norm": 3.891674757003784,
      "learning_rate": 0.00019968033041953097,
      "loss": 0.4995,
      "step": 839
    },
    {
      "epoch": 0.003951193354469082,
      "grad_norm": 5.520595073699951,
      "learning_rate": 0.0001996793874414175,
      "loss": 0.342,
      "step": 840
    },
    {
      "epoch": 0.003955897156081545,
      "grad_norm": 5.791471004486084,
      "learning_rate": 0.000199678444463304,
      "loss": 0.6729,
      "step": 841
    },
    {
      "epoch": 0.003960600957694008,
      "grad_norm": 0.6360845565795898,
      "learning_rate": 0.00019967750148519052,
      "loss": 0.0391,
      "step": 842
    },
    {
      "epoch": 0.003965304759306471,
      "grad_norm": 4.245542526245117,
      "learning_rate": 0.00019967655850707704,
      "loss": 0.5969,
      "step": 843
    },
    {
      "epoch": 0.003970008560918934,
      "grad_norm": 0.9177573323249817,
      "learning_rate": 0.0001996756155289636,
      "loss": 0.0385,
      "step": 844
    },
    {
      "epoch": 0.003974712362531398,
      "grad_norm": 0.5353983044624329,
      "learning_rate": 0.0001996746725508501,
      "loss": 0.0204,
      "step": 845
    },
    {
      "epoch": 0.003979416164143861,
      "grad_norm": 3.2036917209625244,
      "learning_rate": 0.00019967372957273663,
      "loss": 0.2564,
      "step": 846
    },
    {
      "epoch": 0.003984119965756324,
      "grad_norm": 5.347578048706055,
      "learning_rate": 0.00019967278659462314,
      "loss": 0.7149,
      "step": 847
    },
    {
      "epoch": 0.003988823767368787,
      "grad_norm": 3.8765556812286377,
      "learning_rate": 0.0001996718436165097,
      "loss": 0.4441,
      "step": 848
    },
    {
      "epoch": 0.00399352756898125,
      "grad_norm": 3.894300699234009,
      "learning_rate": 0.0001996709006383962,
      "loss": 0.4679,
      "step": 849
    },
    {
      "epoch": 0.003998231370593714,
      "grad_norm": 2.2992677688598633,
      "learning_rate": 0.0001996699576602827,
      "loss": 0.3826,
      "step": 850
    },
    {
      "epoch": 0.004002935172206177,
      "grad_norm": 5.999057292938232,
      "learning_rate": 0.00019966901468216922,
      "loss": 0.3428,
      "step": 851
    },
    {
      "epoch": 0.00400763897381864,
      "grad_norm": 2.2287259101867676,
      "learning_rate": 0.00019966807170405574,
      "loss": 0.1011,
      "step": 852
    },
    {
      "epoch": 0.004012342775431103,
      "grad_norm": 2.062133312225342,
      "learning_rate": 0.00019966712872594228,
      "loss": 0.1273,
      "step": 853
    },
    {
      "epoch": 0.004017046577043566,
      "grad_norm": 5.200307369232178,
      "learning_rate": 0.0001996661857478288,
      "loss": 1.0722,
      "step": 854
    },
    {
      "epoch": 0.004021750378656029,
      "grad_norm": 1.7610176801681519,
      "learning_rate": 0.00019966524276971532,
      "loss": 0.1183,
      "step": 855
    },
    {
      "epoch": 0.004026454180268493,
      "grad_norm": 3.9438118934631348,
      "learning_rate": 0.00019966429979160184,
      "loss": 0.576,
      "step": 856
    },
    {
      "epoch": 0.004031157981880956,
      "grad_norm": 2.1197304725646973,
      "learning_rate": 0.00019966335681348839,
      "loss": 0.2238,
      "step": 857
    },
    {
      "epoch": 0.004035861783493419,
      "grad_norm": 6.491802215576172,
      "learning_rate": 0.0001996624138353749,
      "loss": 0.7893,
      "step": 858
    },
    {
      "epoch": 0.0040405655851058824,
      "grad_norm": 6.291945934295654,
      "learning_rate": 0.00019966147085726142,
      "loss": 1.132,
      "step": 859
    },
    {
      "epoch": 0.0040452693867183455,
      "grad_norm": 5.788924217224121,
      "learning_rate": 0.00019966052787914794,
      "loss": 0.8024,
      "step": 860
    },
    {
      "epoch": 0.0040499731883308085,
      "grad_norm": 4.322174549102783,
      "learning_rate": 0.00019965958490103446,
      "loss": 0.6629,
      "step": 861
    },
    {
      "epoch": 0.0040546769899432724,
      "grad_norm": 2.0006439685821533,
      "learning_rate": 0.00019965864192292098,
      "loss": 0.151,
      "step": 862
    },
    {
      "epoch": 0.0040593807915557355,
      "grad_norm": 5.606760501861572,
      "learning_rate": 0.0001996576989448075,
      "loss": 0.8002,
      "step": 863
    },
    {
      "epoch": 0.0040640845931681985,
      "grad_norm": 1.7605232000350952,
      "learning_rate": 0.00019965675596669402,
      "loss": 0.1832,
      "step": 864
    },
    {
      "epoch": 0.0040687883947806616,
      "grad_norm": 1.3694320917129517,
      "learning_rate": 0.00019965581298858053,
      "loss": 0.3688,
      "step": 865
    },
    {
      "epoch": 0.004073492196393125,
      "grad_norm": 1.5993189811706543,
      "learning_rate": 0.00019965487001046708,
      "loss": 0.2427,
      "step": 866
    },
    {
      "epoch": 0.0040781959980055885,
      "grad_norm": 1.4998372793197632,
      "learning_rate": 0.0001996539270323536,
      "loss": 0.2507,
      "step": 867
    },
    {
      "epoch": 0.0040828997996180515,
      "grad_norm": 1.5021344423294067,
      "learning_rate": 0.00019965298405424012,
      "loss": 0.1601,
      "step": 868
    },
    {
      "epoch": 0.004087603601230515,
      "grad_norm": 0.8515642285346985,
      "learning_rate": 0.00019965204107612664,
      "loss": 0.1471,
      "step": 869
    },
    {
      "epoch": 0.004092307402842978,
      "grad_norm": 0.9317629337310791,
      "learning_rate": 0.00019965109809801316,
      "loss": 0.1031,
      "step": 870
    },
    {
      "epoch": 0.004097011204455441,
      "grad_norm": 2.0518393516540527,
      "learning_rate": 0.00019965015511989967,
      "loss": 0.2911,
      "step": 871
    },
    {
      "epoch": 0.004101715006067904,
      "grad_norm": 2.4519801139831543,
      "learning_rate": 0.0001996492121417862,
      "loss": 0.2398,
      "step": 872
    },
    {
      "epoch": 0.004106418807680368,
      "grad_norm": 1.7986570596694946,
      "learning_rate": 0.0001996482691636727,
      "loss": 0.3043,
      "step": 873
    },
    {
      "epoch": 0.004111122609292831,
      "grad_norm": 2.651280403137207,
      "learning_rate": 0.00019964732618555923,
      "loss": 0.2834,
      "step": 874
    },
    {
      "epoch": 0.004115826410905294,
      "grad_norm": 0.057060301303863525,
      "learning_rate": 0.00019964638320744578,
      "loss": 0.0026,
      "step": 875
    },
    {
      "epoch": 0.004120530212517757,
      "grad_norm": 1.8324172496795654,
      "learning_rate": 0.0001996454402293323,
      "loss": 0.1715,
      "step": 876
    },
    {
      "epoch": 0.00412523401413022,
      "grad_norm": 2.8559815883636475,
      "learning_rate": 0.0001996444972512188,
      "loss": 0.1747,
      "step": 877
    },
    {
      "epoch": 0.004129937815742684,
      "grad_norm": 4.312530994415283,
      "learning_rate": 0.00019964355427310533,
      "loss": 0.4679,
      "step": 878
    },
    {
      "epoch": 0.004134641617355147,
      "grad_norm": 3.6600635051727295,
      "learning_rate": 0.00019964261129499185,
      "loss": 0.3443,
      "step": 879
    },
    {
      "epoch": 0.00413934541896761,
      "grad_norm": 7.24200963973999,
      "learning_rate": 0.0001996416683168784,
      "loss": 1.1127,
      "step": 880
    },
    {
      "epoch": 0.004144049220580073,
      "grad_norm": 5.192946910858154,
      "learning_rate": 0.0001996407253387649,
      "loss": 0.2734,
      "step": 881
    },
    {
      "epoch": 0.004148753022192536,
      "grad_norm": 2.2150843143463135,
      "learning_rate": 0.0001996397823606514,
      "loss": 0.1784,
      "step": 882
    },
    {
      "epoch": 0.004153456823804999,
      "grad_norm": 3.5504543781280518,
      "learning_rate": 0.00019963883938253792,
      "loss": 0.4418,
      "step": 883
    },
    {
      "epoch": 0.004158160625417463,
      "grad_norm": 0.7357102036476135,
      "learning_rate": 0.00019963789640442447,
      "loss": 0.0306,
      "step": 884
    },
    {
      "epoch": 0.004162864427029926,
      "grad_norm": 4.456344127655029,
      "learning_rate": 0.000199636953426311,
      "loss": 0.2673,
      "step": 885
    },
    {
      "epoch": 0.004167568228642389,
      "grad_norm": 4.1521501541137695,
      "learning_rate": 0.0001996360104481975,
      "loss": 0.4662,
      "step": 886
    },
    {
      "epoch": 0.004172272030254852,
      "grad_norm": 1.6022080183029175,
      "learning_rate": 0.00019963506747008403,
      "loss": 0.0802,
      "step": 887
    },
    {
      "epoch": 0.004176975831867315,
      "grad_norm": 2.3389244079589844,
      "learning_rate": 0.00019963412449197054,
      "loss": 0.1645,
      "step": 888
    },
    {
      "epoch": 0.004181679633479778,
      "grad_norm": 2.216244697570801,
      "learning_rate": 0.0001996331815138571,
      "loss": 0.32,
      "step": 889
    },
    {
      "epoch": 0.004186383435092242,
      "grad_norm": 1.5978516340255737,
      "learning_rate": 0.0001996322385357436,
      "loss": 0.267,
      "step": 890
    },
    {
      "epoch": 0.004191087236704705,
      "grad_norm": 1.8133282661437988,
      "learning_rate": 0.00019963129555763013,
      "loss": 0.2402,
      "step": 891
    },
    {
      "epoch": 0.004195791038317168,
      "grad_norm": 2.2511255741119385,
      "learning_rate": 0.00019963035257951662,
      "loss": 0.1581,
      "step": 892
    },
    {
      "epoch": 0.004200494839929631,
      "grad_norm": 2.2536075115203857,
      "learning_rate": 0.00019962940960140314,
      "loss": 0.2668,
      "step": 893
    },
    {
      "epoch": 0.004205198641542094,
      "grad_norm": 3.0754106044769287,
      "learning_rate": 0.00019962846662328968,
      "loss": 0.3943,
      "step": 894
    },
    {
      "epoch": 0.004209902443154558,
      "grad_norm": 4.3067121505737305,
      "learning_rate": 0.0001996275236451762,
      "loss": 0.7683,
      "step": 895
    },
    {
      "epoch": 0.004214606244767021,
      "grad_norm": 1.6551918983459473,
      "learning_rate": 0.00019962658066706272,
      "loss": 0.1083,
      "step": 896
    },
    {
      "epoch": 0.004219310046379484,
      "grad_norm": 1.1116377115249634,
      "learning_rate": 0.00019962563768894924,
      "loss": 0.0797,
      "step": 897
    },
    {
      "epoch": 0.004224013847991947,
      "grad_norm": 5.60377311706543,
      "learning_rate": 0.00019962469471083579,
      "loss": 0.6621,
      "step": 898
    },
    {
      "epoch": 0.00422871764960441,
      "grad_norm": 2.269144296646118,
      "learning_rate": 0.0001996237517327223,
      "loss": 0.1391,
      "step": 899
    },
    {
      "epoch": 0.004233421451216873,
      "grad_norm": 4.111459732055664,
      "learning_rate": 0.00019962280875460882,
      "loss": 0.5594,
      "step": 900
    },
    {
      "epoch": 0.004238125252829337,
      "grad_norm": 12.483025550842285,
      "learning_rate": 0.00019962186577649534,
      "loss": 0.6212,
      "step": 901
    },
    {
      "epoch": 0.0042428290544418,
      "grad_norm": 4.360960483551025,
      "learning_rate": 0.00019962092279838186,
      "loss": 0.4917,
      "step": 902
    },
    {
      "epoch": 0.004247532856054263,
      "grad_norm": 5.507785797119141,
      "learning_rate": 0.00019961997982026838,
      "loss": 0.4741,
      "step": 903
    },
    {
      "epoch": 0.004252236657666726,
      "grad_norm": 2.8661887645721436,
      "learning_rate": 0.0001996190368421549,
      "loss": 0.3985,
      "step": 904
    },
    {
      "epoch": 0.004256940459279189,
      "grad_norm": 5.462536811828613,
      "learning_rate": 0.00019961809386404142,
      "loss": 1.1777,
      "step": 905
    },
    {
      "epoch": 0.004261644260891652,
      "grad_norm": 2.271923780441284,
      "learning_rate": 0.00019961715088592793,
      "loss": 0.3083,
      "step": 906
    },
    {
      "epoch": 0.004266348062504116,
      "grad_norm": 3.3140461444854736,
      "learning_rate": 0.00019961620790781448,
      "loss": 0.4028,
      "step": 907
    },
    {
      "epoch": 0.004271051864116579,
      "grad_norm": 1.091633915901184,
      "learning_rate": 0.000199615264929701,
      "loss": 0.0983,
      "step": 908
    },
    {
      "epoch": 0.004275755665729042,
      "grad_norm": 1.4230220317840576,
      "learning_rate": 0.00019961432195158752,
      "loss": 0.1459,
      "step": 909
    },
    {
      "epoch": 0.004280459467341505,
      "grad_norm": 2.245314121246338,
      "learning_rate": 0.00019961337897347404,
      "loss": 0.3139,
      "step": 910
    },
    {
      "epoch": 0.004285163268953968,
      "grad_norm": 2.5795958042144775,
      "learning_rate": 0.00019961243599536056,
      "loss": 0.3275,
      "step": 911
    },
    {
      "epoch": 0.004289867070566432,
      "grad_norm": 3.3103761672973633,
      "learning_rate": 0.00019961149301724707,
      "loss": 0.384,
      "step": 912
    },
    {
      "epoch": 0.004294570872178895,
      "grad_norm": 1.2406344413757324,
      "learning_rate": 0.0001996105500391336,
      "loss": 0.1954,
      "step": 913
    },
    {
      "epoch": 0.004299274673791358,
      "grad_norm": 1.357040524482727,
      "learning_rate": 0.0001996096070610201,
      "loss": 0.1226,
      "step": 914
    },
    {
      "epoch": 0.004303978475403821,
      "grad_norm": 3.5683462619781494,
      "learning_rate": 0.00019960866408290663,
      "loss": 0.5051,
      "step": 915
    },
    {
      "epoch": 0.004308682277016284,
      "grad_norm": 3.2784461975097656,
      "learning_rate": 0.00019960772110479318,
      "loss": 0.4417,
      "step": 916
    },
    {
      "epoch": 0.004313386078628747,
      "grad_norm": 2.0401432514190674,
      "learning_rate": 0.0001996067781266797,
      "loss": 0.1924,
      "step": 917
    },
    {
      "epoch": 0.004318089880241211,
      "grad_norm": 2.8115358352661133,
      "learning_rate": 0.0001996058351485662,
      "loss": 0.517,
      "step": 918
    },
    {
      "epoch": 0.004322793681853674,
      "grad_norm": 1.46018385887146,
      "learning_rate": 0.00019960489217045273,
      "loss": 0.1038,
      "step": 919
    },
    {
      "epoch": 0.004327497483466137,
      "grad_norm": 0.4845624268054962,
      "learning_rate": 0.00019960394919233925,
      "loss": 0.0317,
      "step": 920
    },
    {
      "epoch": 0.0043322012850786,
      "grad_norm": 1.2271971702575684,
      "learning_rate": 0.0001996030062142258,
      "loss": 0.1474,
      "step": 921
    },
    {
      "epoch": 0.0043369050866910635,
      "grad_norm": 2.8424367904663086,
      "learning_rate": 0.00019960206323611231,
      "loss": 0.4868,
      "step": 922
    },
    {
      "epoch": 0.0043416088883035265,
      "grad_norm": 1.5377953052520752,
      "learning_rate": 0.0001996011202579988,
      "loss": 0.1404,
      "step": 923
    },
    {
      "epoch": 0.00434631268991599,
      "grad_norm": 1.2843941450119019,
      "learning_rate": 0.00019960017727988532,
      "loss": 0.1417,
      "step": 924
    },
    {
      "epoch": 0.0043510164915284534,
      "grad_norm": 1.3842206001281738,
      "learning_rate": 0.00019959923430177187,
      "loss": 0.083,
      "step": 925
    },
    {
      "epoch": 0.0043557202931409165,
      "grad_norm": 1.7149360179901123,
      "learning_rate": 0.0001995982913236584,
      "loss": 0.1455,
      "step": 926
    },
    {
      "epoch": 0.0043604240947533795,
      "grad_norm": 4.338540077209473,
      "learning_rate": 0.0001995973483455449,
      "loss": 0.7917,
      "step": 927
    },
    {
      "epoch": 0.0043651278963658426,
      "grad_norm": 1.5320348739624023,
      "learning_rate": 0.00019959640536743143,
      "loss": 0.0967,
      "step": 928
    },
    {
      "epoch": 0.0043698316979783065,
      "grad_norm": 2.3303372859954834,
      "learning_rate": 0.00019959546238931794,
      "loss": 0.1819,
      "step": 929
    },
    {
      "epoch": 0.0043745354995907695,
      "grad_norm": 1.3356075286865234,
      "learning_rate": 0.0001995945194112045,
      "loss": 0.1309,
      "step": 930
    },
    {
      "epoch": 0.0043792393012032325,
      "grad_norm": 3.839742660522461,
      "learning_rate": 0.000199593576433091,
      "loss": 0.5089,
      "step": 931
    },
    {
      "epoch": 0.004383943102815696,
      "grad_norm": 8.025230407714844,
      "learning_rate": 0.00019959263345497753,
      "loss": 1.1945,
      "step": 932
    },
    {
      "epoch": 0.004388646904428159,
      "grad_norm": 1.2798118591308594,
      "learning_rate": 0.00019959169047686405,
      "loss": 0.058,
      "step": 933
    },
    {
      "epoch": 0.004393350706040622,
      "grad_norm": 4.224756717681885,
      "learning_rate": 0.00019959074749875057,
      "loss": 0.3425,
      "step": 934
    },
    {
      "epoch": 0.004398054507653086,
      "grad_norm": 3.9899611473083496,
      "learning_rate": 0.00019958980452063708,
      "loss": 0.2297,
      "step": 935
    },
    {
      "epoch": 0.004402758309265549,
      "grad_norm": 4.7733869552612305,
      "learning_rate": 0.0001995888615425236,
      "loss": 0.5158,
      "step": 936
    },
    {
      "epoch": 0.004407462110878012,
      "grad_norm": 1.0257493257522583,
      "learning_rate": 0.00019958791856441012,
      "loss": 0.0502,
      "step": 937
    },
    {
      "epoch": 0.004412165912490475,
      "grad_norm": 2.2008612155914307,
      "learning_rate": 0.00019958697558629664,
      "loss": 0.2069,
      "step": 938
    },
    {
      "epoch": 0.004416869714102938,
      "grad_norm": 0.045743364840745926,
      "learning_rate": 0.00019958603260818319,
      "loss": 0.0019,
      "step": 939
    },
    {
      "epoch": 0.004421573515715401,
      "grad_norm": 2.251311779022217,
      "learning_rate": 0.0001995850896300697,
      "loss": 0.1402,
      "step": 940
    },
    {
      "epoch": 0.004426277317327865,
      "grad_norm": 6.030086517333984,
      "learning_rate": 0.00019958414665195622,
      "loss": 0.7199,
      "step": 941
    },
    {
      "epoch": 0.004430981118940328,
      "grad_norm": 4.009695529937744,
      "learning_rate": 0.00019958320367384274,
      "loss": 0.4455,
      "step": 942
    },
    {
      "epoch": 0.004435684920552791,
      "grad_norm": 5.446501731872559,
      "learning_rate": 0.00019958226069572926,
      "loss": 1.0093,
      "step": 943
    },
    {
      "epoch": 0.004440388722165254,
      "grad_norm": 3.7866311073303223,
      "learning_rate": 0.00019958131771761578,
      "loss": 0.9183,
      "step": 944
    },
    {
      "epoch": 0.004445092523777717,
      "grad_norm": 1.9830983877182007,
      "learning_rate": 0.0001995803747395023,
      "loss": 0.103,
      "step": 945
    },
    {
      "epoch": 0.004449796325390181,
      "grad_norm": 0.4683980345726013,
      "learning_rate": 0.00019957943176138882,
      "loss": 0.0283,
      "step": 946
    },
    {
      "epoch": 0.004454500127002644,
      "grad_norm": 0.39663249254226685,
      "learning_rate": 0.00019957848878327533,
      "loss": 0.0194,
      "step": 947
    },
    {
      "epoch": 0.004459203928615107,
      "grad_norm": 1.2187803983688354,
      "learning_rate": 0.00019957754580516188,
      "loss": 0.1114,
      "step": 948
    },
    {
      "epoch": 0.00446390773022757,
      "grad_norm": 1.4147366285324097,
      "learning_rate": 0.0001995766028270484,
      "loss": 0.1365,
      "step": 949
    },
    {
      "epoch": 0.004468611531840033,
      "grad_norm": 1.4079573154449463,
      "learning_rate": 0.00019957565984893492,
      "loss": 0.0555,
      "step": 950
    },
    {
      "epoch": 0.004473315333452496,
      "grad_norm": 5.449100494384766,
      "learning_rate": 0.00019957471687082144,
      "loss": 0.8513,
      "step": 951
    },
    {
      "epoch": 0.00447801913506496,
      "grad_norm": 3.629546642303467,
      "learning_rate": 0.00019957377389270795,
      "loss": 0.3315,
      "step": 952
    },
    {
      "epoch": 0.004482722936677423,
      "grad_norm": 2.6535229682922363,
      "learning_rate": 0.0001995728309145945,
      "loss": 0.195,
      "step": 953
    },
    {
      "epoch": 0.004487426738289886,
      "grad_norm": 3.9355952739715576,
      "learning_rate": 0.000199571887936481,
      "loss": 0.3604,
      "step": 954
    },
    {
      "epoch": 0.004492130539902349,
      "grad_norm": 8.626532554626465,
      "learning_rate": 0.0001995709449583675,
      "loss": 0.8112,
      "step": 955
    },
    {
      "epoch": 0.004496834341514812,
      "grad_norm": 0.9100933074951172,
      "learning_rate": 0.00019957000198025403,
      "loss": 0.1116,
      "step": 956
    },
    {
      "epoch": 0.004501538143127276,
      "grad_norm": 0.6352298855781555,
      "learning_rate": 0.00019956905900214058,
      "loss": 0.0467,
      "step": 957
    },
    {
      "epoch": 0.004506241944739739,
      "grad_norm": 1.5351358652114868,
      "learning_rate": 0.0001995681160240271,
      "loss": 0.1405,
      "step": 958
    },
    {
      "epoch": 0.004510945746352202,
      "grad_norm": 3.540846824645996,
      "learning_rate": 0.0001995671730459136,
      "loss": 0.4124,
      "step": 959
    },
    {
      "epoch": 0.004515649547964665,
      "grad_norm": 4.930948734283447,
      "learning_rate": 0.00019956623006780013,
      "loss": 0.693,
      "step": 960
    },
    {
      "epoch": 0.004520353349577128,
      "grad_norm": 5.22108793258667,
      "learning_rate": 0.00019956528708968665,
      "loss": 0.9958,
      "step": 961
    },
    {
      "epoch": 0.004525057151189591,
      "grad_norm": 1.3187599182128906,
      "learning_rate": 0.0001995643441115732,
      "loss": 0.088,
      "step": 962
    },
    {
      "epoch": 0.004529760952802055,
      "grad_norm": 2.4778757095336914,
      "learning_rate": 0.00019956340113345971,
      "loss": 0.2594,
      "step": 963
    },
    {
      "epoch": 0.004534464754414518,
      "grad_norm": 3.469446897506714,
      "learning_rate": 0.00019956245815534623,
      "loss": 0.1832,
      "step": 964
    },
    {
      "epoch": 0.004539168556026981,
      "grad_norm": 3.6709208488464355,
      "learning_rate": 0.00019956151517723275,
      "loss": 0.459,
      "step": 965
    },
    {
      "epoch": 0.004543872357639444,
      "grad_norm": 4.824512958526611,
      "learning_rate": 0.00019956057219911927,
      "loss": 0.3857,
      "step": 966
    },
    {
      "epoch": 0.004548576159251907,
      "grad_norm": 2.7064156532287598,
      "learning_rate": 0.0001995596292210058,
      "loss": 0.2129,
      "step": 967
    },
    {
      "epoch": 0.00455327996086437,
      "grad_norm": 3.5702197551727295,
      "learning_rate": 0.0001995586862428923,
      "loss": 0.3311,
      "step": 968
    },
    {
      "epoch": 0.004557983762476834,
      "grad_norm": 3.2822742462158203,
      "learning_rate": 0.00019955774326477883,
      "loss": 0.3924,
      "step": 969
    },
    {
      "epoch": 0.004562687564089297,
      "grad_norm": 0.755229115486145,
      "learning_rate": 0.00019955680028666534,
      "loss": 0.0562,
      "step": 970
    },
    {
      "epoch": 0.00456739136570176,
      "grad_norm": 2.853001594543457,
      "learning_rate": 0.0001995558573085519,
      "loss": 0.2717,
      "step": 971
    },
    {
      "epoch": 0.004572095167314223,
      "grad_norm": 1.9549243450164795,
      "learning_rate": 0.0001995549143304384,
      "loss": 0.2454,
      "step": 972
    },
    {
      "epoch": 0.004576798968926686,
      "grad_norm": 1.1374268531799316,
      "learning_rate": 0.00019955397135232493,
      "loss": 0.1016,
      "step": 973
    },
    {
      "epoch": 0.00458150277053915,
      "grad_norm": 2.5307137966156006,
      "learning_rate": 0.00019955302837421145,
      "loss": 0.2085,
      "step": 974
    },
    {
      "epoch": 0.004586206572151613,
      "grad_norm": 0.48829177021980286,
      "learning_rate": 0.00019955208539609797,
      "loss": 0.0417,
      "step": 975
    },
    {
      "epoch": 0.004590910373764076,
      "grad_norm": 3.1844186782836914,
      "learning_rate": 0.00019955114241798448,
      "loss": 0.3265,
      "step": 976
    },
    {
      "epoch": 0.004595614175376539,
      "grad_norm": 3.6221959590911865,
      "learning_rate": 0.000199550199439871,
      "loss": 0.3608,
      "step": 977
    },
    {
      "epoch": 0.004600317976989002,
      "grad_norm": 2.9251344203948975,
      "learning_rate": 0.00019954925646175752,
      "loss": 0.2564,
      "step": 978
    },
    {
      "epoch": 0.004605021778601465,
      "grad_norm": 1.3449184894561768,
      "learning_rate": 0.00019954831348364404,
      "loss": 0.108,
      "step": 979
    },
    {
      "epoch": 0.004609725580213929,
      "grad_norm": 1.962941288948059,
      "learning_rate": 0.00019954737050553059,
      "loss": 0.1655,
      "step": 980
    },
    {
      "epoch": 0.004614429381826392,
      "grad_norm": 2.5324764251708984,
      "learning_rate": 0.0001995464275274171,
      "loss": 0.3142,
      "step": 981
    },
    {
      "epoch": 0.004619133183438855,
      "grad_norm": 11.012393951416016,
      "learning_rate": 0.00019954548454930362,
      "loss": 1.0578,
      "step": 982
    },
    {
      "epoch": 0.004623836985051318,
      "grad_norm": 0.5263782143592834,
      "learning_rate": 0.00019954454157119014,
      "loss": 0.0407,
      "step": 983
    },
    {
      "epoch": 0.004628540786663781,
      "grad_norm": 3.5418176651000977,
      "learning_rate": 0.0001995435985930767,
      "loss": 0.4474,
      "step": 984
    },
    {
      "epoch": 0.0046332445882762445,
      "grad_norm": 5.036700248718262,
      "learning_rate": 0.00019954265561496318,
      "loss": 0.6779,
      "step": 985
    },
    {
      "epoch": 0.004637948389888708,
      "grad_norm": 2.3286685943603516,
      "learning_rate": 0.0001995417126368497,
      "loss": 0.1594,
      "step": 986
    },
    {
      "epoch": 0.004642652191501171,
      "grad_norm": 5.382603168487549,
      "learning_rate": 0.00019954076965873622,
      "loss": 0.7271,
      "step": 987
    },
    {
      "epoch": 0.0046473559931136344,
      "grad_norm": 4.186121463775635,
      "learning_rate": 0.00019953982668062273,
      "loss": 0.5224,
      "step": 988
    },
    {
      "epoch": 0.0046520597947260975,
      "grad_norm": 1.1043283939361572,
      "learning_rate": 0.00019953888370250928,
      "loss": 0.1904,
      "step": 989
    },
    {
      "epoch": 0.0046567635963385605,
      "grad_norm": 3.55419921875,
      "learning_rate": 0.0001995379407243958,
      "loss": 0.5333,
      "step": 990
    },
    {
      "epoch": 0.004661467397951024,
      "grad_norm": 0.3639831840991974,
      "learning_rate": 0.00019953699774628232,
      "loss": 0.0229,
      "step": 991
    },
    {
      "epoch": 0.0046661711995634875,
      "grad_norm": 4.613901138305664,
      "learning_rate": 0.00019953605476816884,
      "loss": 0.4689,
      "step": 992
    },
    {
      "epoch": 0.0046708750011759505,
      "grad_norm": 2.176645517349243,
      "learning_rate": 0.00019953511179005538,
      "loss": 0.4317,
      "step": 993
    },
    {
      "epoch": 0.0046755788027884135,
      "grad_norm": 0.9780870079994202,
      "learning_rate": 0.0001995341688119419,
      "loss": 0.0627,
      "step": 994
    },
    {
      "epoch": 0.004680282604400877,
      "grad_norm": 3.4286422729492188,
      "learning_rate": 0.00019953322583382842,
      "loss": 0.5877,
      "step": 995
    },
    {
      "epoch": 0.00468498640601334,
      "grad_norm": 0.7196195721626282,
      "learning_rate": 0.00019953228285571494,
      "loss": 0.0755,
      "step": 996
    },
    {
      "epoch": 0.0046896902076258035,
      "grad_norm": 3.8133199214935303,
      "learning_rate": 0.00019953133987760143,
      "loss": 0.7722,
      "step": 997
    },
    {
      "epoch": 0.004694394009238267,
      "grad_norm": 0.9880680441856384,
      "learning_rate": 0.00019953039689948798,
      "loss": 0.1169,
      "step": 998
    },
    {
      "epoch": 0.00469909781085073,
      "grad_norm": 2.7337920665740967,
      "learning_rate": 0.0001995294539213745,
      "loss": 0.6155,
      "step": 999
    },
    {
      "epoch": 0.004703801612463193,
      "grad_norm": 1.417164921760559,
      "learning_rate": 0.000199528510943261,
      "loss": 0.3805,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 212594,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.991820981420032e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
