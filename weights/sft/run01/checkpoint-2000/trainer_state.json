{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.009407603224926385,
  "eval_steps": 42519,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 4.7038016124631924e-06,
      "grad_norm": 3.8041722774505615,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.7375,
      "step": 1
    },
    {
      "epoch": 9.407603224926385e-06,
      "grad_norm": 4.354788303375244,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.5438,
      "step": 2
    },
    {
      "epoch": 1.4111404837389578e-05,
      "grad_norm": 6.505493640899658,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.6008,
      "step": 3
    },
    {
      "epoch": 1.881520644985277e-05,
      "grad_norm": 4.024658203125,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.5406,
      "step": 4
    },
    {
      "epoch": 2.3519008062315965e-05,
      "grad_norm": 0.6205708980560303,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0346,
      "step": 5
    },
    {
      "epoch": 2.8222809674779156e-05,
      "grad_norm": 2.1557469367980957,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.2234,
      "step": 6
    },
    {
      "epoch": 3.292661128724235e-05,
      "grad_norm": 2.4203062057495117,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.1888,
      "step": 7
    },
    {
      "epoch": 3.763041289970554e-05,
      "grad_norm": 4.038275718688965,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.5306,
      "step": 8
    },
    {
      "epoch": 4.2334214512168735e-05,
      "grad_norm": 5.567150592803955,
      "learning_rate": 3.6e-06,
      "loss": 0.8904,
      "step": 9
    },
    {
      "epoch": 4.703801612463193e-05,
      "grad_norm": 2.0147883892059326,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.2581,
      "step": 10
    },
    {
      "epoch": 5.174181773709512e-05,
      "grad_norm": 2.8323683738708496,
      "learning_rate": 4.4e-06,
      "loss": 0.1862,
      "step": 11
    },
    {
      "epoch": 5.644561934955831e-05,
      "grad_norm": 1.6947802305221558,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.1079,
      "step": 12
    },
    {
      "epoch": 6.11494209620215e-05,
      "grad_norm": 4.809198379516602,
      "learning_rate": 5.2e-06,
      "loss": 0.6683,
      "step": 13
    },
    {
      "epoch": 6.58532225744847e-05,
      "grad_norm": 5.638553142547607,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.8748,
      "step": 14
    },
    {
      "epoch": 7.055702418694789e-05,
      "grad_norm": 1.4174034595489502,
      "learning_rate": 6e-06,
      "loss": 0.1869,
      "step": 15
    },
    {
      "epoch": 7.526082579941108e-05,
      "grad_norm": 7.593025207519531,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.7924,
      "step": 16
    },
    {
      "epoch": 7.996462741187428e-05,
      "grad_norm": 7.029094696044922,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.8385,
      "step": 17
    },
    {
      "epoch": 8.466842902433747e-05,
      "grad_norm": 4.548736095428467,
      "learning_rate": 7.2e-06,
      "loss": 0.7518,
      "step": 18
    },
    {
      "epoch": 8.937223063680066e-05,
      "grad_norm": 2.9964427947998047,
      "learning_rate": 7.6e-06,
      "loss": 0.6038,
      "step": 19
    },
    {
      "epoch": 9.407603224926386e-05,
      "grad_norm": 7.868130207061768,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.625,
      "step": 20
    },
    {
      "epoch": 9.877983386172705e-05,
      "grad_norm": 2.2138407230377197,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.1034,
      "step": 21
    },
    {
      "epoch": 0.00010348363547419024,
      "grad_norm": 2.5939178466796875,
      "learning_rate": 8.8e-06,
      "loss": 0.3425,
      "step": 22
    },
    {
      "epoch": 0.00010818743708665344,
      "grad_norm": 1.3374449014663696,
      "learning_rate": 9.2e-06,
      "loss": 0.076,
      "step": 23
    },
    {
      "epoch": 0.00011289123869911663,
      "grad_norm": 2.0057127475738525,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.3457,
      "step": 24
    },
    {
      "epoch": 0.00011759504031157981,
      "grad_norm": 2.428433418273926,
      "learning_rate": 1e-05,
      "loss": 0.2852,
      "step": 25
    },
    {
      "epoch": 0.000122298841924043,
      "grad_norm": 4.798130512237549,
      "learning_rate": 1.04e-05,
      "loss": 0.55,
      "step": 26
    },
    {
      "epoch": 0.0001270026435365062,
      "grad_norm": 1.618207335472107,
      "learning_rate": 1.08e-05,
      "loss": 0.219,
      "step": 27
    },
    {
      "epoch": 0.0001317064451489694,
      "grad_norm": 1.1659034490585327,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.053,
      "step": 28
    },
    {
      "epoch": 0.00013641024676143258,
      "grad_norm": 2.4309582710266113,
      "learning_rate": 1.16e-05,
      "loss": 0.346,
      "step": 29
    },
    {
      "epoch": 0.00014111404837389578,
      "grad_norm": 0.9599677324295044,
      "learning_rate": 1.2e-05,
      "loss": 0.0486,
      "step": 30
    },
    {
      "epoch": 0.00014581784998635898,
      "grad_norm": 4.665159225463867,
      "learning_rate": 1.24e-05,
      "loss": 0.4113,
      "step": 31
    },
    {
      "epoch": 0.00015052165159882216,
      "grad_norm": 5.331118106842041,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.9317,
      "step": 32
    },
    {
      "epoch": 0.00015522545321128536,
      "grad_norm": 1.710689663887024,
      "learning_rate": 1.32e-05,
      "loss": 0.2711,
      "step": 33
    },
    {
      "epoch": 0.00015992925482374856,
      "grad_norm": 2.14216947555542,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.1654,
      "step": 34
    },
    {
      "epoch": 0.00016463305643621174,
      "grad_norm": 4.485231399536133,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5813,
      "step": 35
    },
    {
      "epoch": 0.00016933685804867494,
      "grad_norm": 1.9404183626174927,
      "learning_rate": 1.44e-05,
      "loss": 0.3459,
      "step": 36
    },
    {
      "epoch": 0.00017404065966113814,
      "grad_norm": 1.6494685411453247,
      "learning_rate": 1.48e-05,
      "loss": 0.2261,
      "step": 37
    },
    {
      "epoch": 0.00017874446127360131,
      "grad_norm": 2.5063393115997314,
      "learning_rate": 1.52e-05,
      "loss": 0.2159,
      "step": 38
    },
    {
      "epoch": 0.00018344826288606452,
      "grad_norm": 1.0466309785842896,
      "learning_rate": 1.56e-05,
      "loss": 0.0309,
      "step": 39
    },
    {
      "epoch": 0.00018815206449852772,
      "grad_norm": 1.0847481489181519,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1703,
      "step": 40
    },
    {
      "epoch": 0.0001928558661109909,
      "grad_norm": 3.547720193862915,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.119,
      "step": 41
    },
    {
      "epoch": 0.0001975596677234541,
      "grad_norm": 1.8616689443588257,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2958,
      "step": 42
    },
    {
      "epoch": 0.0002022634693359173,
      "grad_norm": 5.395574569702148,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.3685,
      "step": 43
    },
    {
      "epoch": 0.00020696727094838047,
      "grad_norm": 2.6937150955200195,
      "learning_rate": 1.76e-05,
      "loss": 0.1679,
      "step": 44
    },
    {
      "epoch": 0.00021167107256084367,
      "grad_norm": 2.65310001373291,
      "learning_rate": 1.8e-05,
      "loss": 0.1311,
      "step": 45
    },
    {
      "epoch": 0.00021637487417330687,
      "grad_norm": 3.083963632583618,
      "learning_rate": 1.84e-05,
      "loss": 0.3188,
      "step": 46
    },
    {
      "epoch": 0.00022107867578577005,
      "grad_norm": 2.4717369079589844,
      "learning_rate": 1.88e-05,
      "loss": 0.075,
      "step": 47
    },
    {
      "epoch": 0.00022578247739823325,
      "grad_norm": 2.2474513053894043,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.2237,
      "step": 48
    },
    {
      "epoch": 0.00023048627901069645,
      "grad_norm": 2.5590081214904785,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.1322,
      "step": 49
    },
    {
      "epoch": 0.00023519008062315963,
      "grad_norm": 3.6861019134521484,
      "learning_rate": 2e-05,
      "loss": 0.253,
      "step": 50
    },
    {
      "epoch": 0.00023989388223562283,
      "grad_norm": 5.394125461578369,
      "learning_rate": 2.04e-05,
      "loss": 0.1632,
      "step": 51
    },
    {
      "epoch": 0.000244597683848086,
      "grad_norm": 2.0115935802459717,
      "learning_rate": 2.08e-05,
      "loss": 0.1024,
      "step": 52
    },
    {
      "epoch": 0.0002493014854605492,
      "grad_norm": 3.8001034259796143,
      "learning_rate": 2.12e-05,
      "loss": 0.3323,
      "step": 53
    },
    {
      "epoch": 0.0002540052870730124,
      "grad_norm": 5.794704914093018,
      "learning_rate": 2.16e-05,
      "loss": 0.1992,
      "step": 54
    },
    {
      "epoch": 0.0002587090886854756,
      "grad_norm": 10.996417999267578,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.933,
      "step": 55
    },
    {
      "epoch": 0.0002634128902979388,
      "grad_norm": 10.299786567687988,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.9505,
      "step": 56
    },
    {
      "epoch": 0.000268116691910402,
      "grad_norm": 12.242238998413086,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.0156,
      "step": 57
    },
    {
      "epoch": 0.00027282049352286516,
      "grad_norm": 2.7500998973846436,
      "learning_rate": 2.32e-05,
      "loss": 0.271,
      "step": 58
    },
    {
      "epoch": 0.00027752429513532836,
      "grad_norm": 2.10526442527771,
      "learning_rate": 2.36e-05,
      "loss": 0.089,
      "step": 59
    },
    {
      "epoch": 0.00028222809674779156,
      "grad_norm": 12.716087341308594,
      "learning_rate": 2.4e-05,
      "loss": 0.8743,
      "step": 60
    },
    {
      "epoch": 0.00028693189836025477,
      "grad_norm": 9.265603065490723,
      "learning_rate": 2.44e-05,
      "loss": 0.5664,
      "step": 61
    },
    {
      "epoch": 0.00029163569997271797,
      "grad_norm": 13.476297378540039,
      "learning_rate": 2.48e-05,
      "loss": 0.6459,
      "step": 62
    },
    {
      "epoch": 0.00029633950158518117,
      "grad_norm": 2.3783257007598877,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0892,
      "step": 63
    },
    {
      "epoch": 0.0003010433031976443,
      "grad_norm": 12.59670639038086,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.4919,
      "step": 64
    },
    {
      "epoch": 0.0003057471048101075,
      "grad_norm": 9.37648868560791,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4956,
      "step": 65
    },
    {
      "epoch": 0.0003104509064225707,
      "grad_norm": 3.158296585083008,
      "learning_rate": 2.64e-05,
      "loss": 0.1308,
      "step": 66
    },
    {
      "epoch": 0.0003151547080350339,
      "grad_norm": 8.29719352722168,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.2386,
      "step": 67
    },
    {
      "epoch": 0.0003198585096474971,
      "grad_norm": 4.998426914215088,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.2414,
      "step": 68
    },
    {
      "epoch": 0.0003245623112599603,
      "grad_norm": 3.8499088287353516,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.1199,
      "step": 69
    },
    {
      "epoch": 0.00032926611287242347,
      "grad_norm": 7.454540729522705,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.451,
      "step": 70
    },
    {
      "epoch": 0.0003339699144848867,
      "grad_norm": 4.821732997894287,
      "learning_rate": 2.84e-05,
      "loss": 0.2166,
      "step": 71
    },
    {
      "epoch": 0.0003386737160973499,
      "grad_norm": 2.726874589920044,
      "learning_rate": 2.88e-05,
      "loss": 0.0991,
      "step": 72
    },
    {
      "epoch": 0.0003433775177098131,
      "grad_norm": 13.291203498840332,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.9011,
      "step": 73
    },
    {
      "epoch": 0.0003480813193222763,
      "grad_norm": 8.086989402770996,
      "learning_rate": 2.96e-05,
      "loss": 0.3664,
      "step": 74
    },
    {
      "epoch": 0.0003527851209347395,
      "grad_norm": 14.738718032836914,
      "learning_rate": 3e-05,
      "loss": 0.6584,
      "step": 75
    },
    {
      "epoch": 0.00035748892254720263,
      "grad_norm": 10.834789276123047,
      "learning_rate": 3.04e-05,
      "loss": 0.5116,
      "step": 76
    },
    {
      "epoch": 0.00036219272415966583,
      "grad_norm": 2.7025794982910156,
      "learning_rate": 3.08e-05,
      "loss": 0.1983,
      "step": 77
    },
    {
      "epoch": 0.00036689652577212903,
      "grad_norm": 2.4473915100097656,
      "learning_rate": 3.12e-05,
      "loss": 0.1744,
      "step": 78
    },
    {
      "epoch": 0.00037160032738459223,
      "grad_norm": 6.345713138580322,
      "learning_rate": 3.16e-05,
      "loss": 0.229,
      "step": 79
    },
    {
      "epoch": 0.00037630412899705544,
      "grad_norm": 10.160054206848145,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.4571,
      "step": 80
    },
    {
      "epoch": 0.00038100793060951864,
      "grad_norm": 2.898986339569092,
      "learning_rate": 3.24e-05,
      "loss": 0.0716,
      "step": 81
    },
    {
      "epoch": 0.0003857117322219818,
      "grad_norm": 5.519388198852539,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.1854,
      "step": 82
    },
    {
      "epoch": 0.000390415533834445,
      "grad_norm": 4.479554653167725,
      "learning_rate": 3.32e-05,
      "loss": 0.2935,
      "step": 83
    },
    {
      "epoch": 0.0003951193354469082,
      "grad_norm": 10.21839714050293,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.4533,
      "step": 84
    },
    {
      "epoch": 0.0003998231370593714,
      "grad_norm": 1.8370931148529053,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0839,
      "step": 85
    },
    {
      "epoch": 0.0004045269386718346,
      "grad_norm": 6.385246276855469,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.306,
      "step": 86
    },
    {
      "epoch": 0.0004092307402842978,
      "grad_norm": 9.136309623718262,
      "learning_rate": 3.48e-05,
      "loss": 0.3864,
      "step": 87
    },
    {
      "epoch": 0.00041393454189676094,
      "grad_norm": 6.78175163269043,
      "learning_rate": 3.52e-05,
      "loss": 0.2843,
      "step": 88
    },
    {
      "epoch": 0.00041863834350922414,
      "grad_norm": 1.9543720483779907,
      "learning_rate": 3.56e-05,
      "loss": 0.0363,
      "step": 89
    },
    {
      "epoch": 0.00042334214512168735,
      "grad_norm": 4.420919895172119,
      "learning_rate": 3.6e-05,
      "loss": 0.2107,
      "step": 90
    },
    {
      "epoch": 0.00042804594673415055,
      "grad_norm": 5.062385559082031,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.1204,
      "step": 91
    },
    {
      "epoch": 0.00043274974834661375,
      "grad_norm": 4.584104537963867,
      "learning_rate": 3.68e-05,
      "loss": 0.5167,
      "step": 92
    },
    {
      "epoch": 0.00043745354995907695,
      "grad_norm": 11.308633804321289,
      "learning_rate": 3.72e-05,
      "loss": 0.5346,
      "step": 93
    },
    {
      "epoch": 0.0004421573515715401,
      "grad_norm": 9.862894058227539,
      "learning_rate": 3.76e-05,
      "loss": 0.7457,
      "step": 94
    },
    {
      "epoch": 0.0004468611531840033,
      "grad_norm": 33.0572509765625,
      "learning_rate": 3.8e-05,
      "loss": 0.7491,
      "step": 95
    },
    {
      "epoch": 0.0004515649547964665,
      "grad_norm": 2.078057289123535,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0662,
      "step": 96
    },
    {
      "epoch": 0.0004562687564089297,
      "grad_norm": 9.548422813415527,
      "learning_rate": 3.88e-05,
      "loss": 1.217,
      "step": 97
    },
    {
      "epoch": 0.0004609725580213929,
      "grad_norm": 3.2684428691864014,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.1829,
      "step": 98
    },
    {
      "epoch": 0.0004656763596338561,
      "grad_norm": 13.482787132263184,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.3653,
      "step": 99
    },
    {
      "epoch": 0.00047038016124631925,
      "grad_norm": 2.9813427925109863,
      "learning_rate": 4e-05,
      "loss": 0.1295,
      "step": 100
    },
    {
      "epoch": 0.00047508396285878246,
      "grad_norm": 12.643174171447754,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.6676,
      "step": 101
    },
    {
      "epoch": 0.00047978776447124566,
      "grad_norm": 18.31464195251465,
      "learning_rate": 4.08e-05,
      "loss": 1.6749,
      "step": 102
    },
    {
      "epoch": 0.00048449156608370886,
      "grad_norm": 5.860870361328125,
      "learning_rate": 4.12e-05,
      "loss": 0.1404,
      "step": 103
    },
    {
      "epoch": 0.000489195367696172,
      "grad_norm": 7.620203495025635,
      "learning_rate": 4.16e-05,
      "loss": 0.3323,
      "step": 104
    },
    {
      "epoch": 0.0004938991693086353,
      "grad_norm": 4.882566928863525,
      "learning_rate": 4.2e-05,
      "loss": 0.1472,
      "step": 105
    },
    {
      "epoch": 0.0004986029709210984,
      "grad_norm": 7.276484489440918,
      "learning_rate": 4.24e-05,
      "loss": 0.1996,
      "step": 106
    },
    {
      "epoch": 0.0005033067725335617,
      "grad_norm": 9.876720428466797,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.7294,
      "step": 107
    },
    {
      "epoch": 0.0005080105741460248,
      "grad_norm": 8.190587043762207,
      "learning_rate": 4.32e-05,
      "loss": 0.4564,
      "step": 108
    },
    {
      "epoch": 0.000512714375758488,
      "grad_norm": 12.742801666259766,
      "learning_rate": 4.36e-05,
      "loss": 1.1462,
      "step": 109
    },
    {
      "epoch": 0.0005174181773709512,
      "grad_norm": 4.173835277557373,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.1501,
      "step": 110
    },
    {
      "epoch": 0.0005221219789834144,
      "grad_norm": 2.1551454067230225,
      "learning_rate": 4.44e-05,
      "loss": 0.0968,
      "step": 111
    },
    {
      "epoch": 0.0005268257805958776,
      "grad_norm": 9.060416221618652,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.9367,
      "step": 112
    },
    {
      "epoch": 0.0005315295822083408,
      "grad_norm": 1.3614718914031982,
      "learning_rate": 4.52e-05,
      "loss": 0.0317,
      "step": 113
    },
    {
      "epoch": 0.000536233383820804,
      "grad_norm": 7.594012260437012,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.8011,
      "step": 114
    },
    {
      "epoch": 0.0005409371854332672,
      "grad_norm": 8.204193115234375,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.4185,
      "step": 115
    },
    {
      "epoch": 0.0005456409870457303,
      "grad_norm": 1.4568963050842285,
      "learning_rate": 4.64e-05,
      "loss": 0.0542,
      "step": 116
    },
    {
      "epoch": 0.0005503447886581936,
      "grad_norm": 9.194090843200684,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.469,
      "step": 117
    },
    {
      "epoch": 0.0005550485902706567,
      "grad_norm": 2.9352447986602783,
      "learning_rate": 4.72e-05,
      "loss": 0.2225,
      "step": 118
    },
    {
      "epoch": 0.00055975239188312,
      "grad_norm": 18.89095115661621,
      "learning_rate": 4.76e-05,
      "loss": 0.8045,
      "step": 119
    },
    {
      "epoch": 0.0005644561934955831,
      "grad_norm": 1.3745579719543457,
      "learning_rate": 4.8e-05,
      "loss": 0.0519,
      "step": 120
    },
    {
      "epoch": 0.0005691599951080463,
      "grad_norm": 6.6944475173950195,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.4802,
      "step": 121
    },
    {
      "epoch": 0.0005738637967205095,
      "grad_norm": 3.0428519248962402,
      "learning_rate": 4.88e-05,
      "loss": 0.1497,
      "step": 122
    },
    {
      "epoch": 0.0005785675983329727,
      "grad_norm": 5.592650413513184,
      "learning_rate": 4.92e-05,
      "loss": 0.5897,
      "step": 123
    },
    {
      "epoch": 0.0005832713999454359,
      "grad_norm": 2.0244908332824707,
      "learning_rate": 4.96e-05,
      "loss": 0.0951,
      "step": 124
    },
    {
      "epoch": 0.0005879752015578991,
      "grad_norm": 4.579395771026611,
      "learning_rate": 5e-05,
      "loss": 0.2466,
      "step": 125
    },
    {
      "epoch": 0.0005926790031703623,
      "grad_norm": 7.484107971191406,
      "learning_rate": 5.0400000000000005e-05,
      "loss": 0.4343,
      "step": 126
    },
    {
      "epoch": 0.0005973828047828255,
      "grad_norm": 3.6764602661132812,
      "learning_rate": 5.08e-05,
      "loss": 0.2355,
      "step": 127
    },
    {
      "epoch": 0.0006020866063952886,
      "grad_norm": 2.7435081005096436,
      "learning_rate": 5.1200000000000004e-05,
      "loss": 0.11,
      "step": 128
    },
    {
      "epoch": 0.0006067904080077519,
      "grad_norm": 2.7252235412597656,
      "learning_rate": 5.16e-05,
      "loss": 0.1106,
      "step": 129
    },
    {
      "epoch": 0.000611494209620215,
      "grad_norm": 5.371652126312256,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.262,
      "step": 130
    },
    {
      "epoch": 0.0006161980112326783,
      "grad_norm": 5.665862560272217,
      "learning_rate": 5.2400000000000007e-05,
      "loss": 0.4071,
      "step": 131
    },
    {
      "epoch": 0.0006209018128451414,
      "grad_norm": 6.609068870544434,
      "learning_rate": 5.28e-05,
      "loss": 0.5057,
      "step": 132
    },
    {
      "epoch": 0.0006256056144576046,
      "grad_norm": 1.3313148021697998,
      "learning_rate": 5.3200000000000006e-05,
      "loss": 0.0579,
      "step": 133
    },
    {
      "epoch": 0.0006303094160700678,
      "grad_norm": 3.316894769668579,
      "learning_rate": 5.360000000000001e-05,
      "loss": 0.1622,
      "step": 134
    },
    {
      "epoch": 0.000635013217682531,
      "grad_norm": 4.961429595947266,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.3672,
      "step": 135
    },
    {
      "epoch": 0.0006397170192949942,
      "grad_norm": 5.017652988433838,
      "learning_rate": 5.440000000000001e-05,
      "loss": 0.3129,
      "step": 136
    },
    {
      "epoch": 0.0006444208209074574,
      "grad_norm": 8.691753387451172,
      "learning_rate": 5.4800000000000004e-05,
      "loss": 0.5734,
      "step": 137
    },
    {
      "epoch": 0.0006491246225199207,
      "grad_norm": 0.6221335530281067,
      "learning_rate": 5.520000000000001e-05,
      "loss": 0.0149,
      "step": 138
    },
    {
      "epoch": 0.0006538284241323838,
      "grad_norm": 1.8843748569488525,
      "learning_rate": 5.560000000000001e-05,
      "loss": 0.1999,
      "step": 139
    },
    {
      "epoch": 0.0006585322257448469,
      "grad_norm": 9.793206214904785,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.6915,
      "step": 140
    },
    {
      "epoch": 0.0006632360273573102,
      "grad_norm": 9.097029685974121,
      "learning_rate": 5.6399999999999995e-05,
      "loss": 0.8844,
      "step": 141
    },
    {
      "epoch": 0.0006679398289697733,
      "grad_norm": 5.145073413848877,
      "learning_rate": 5.68e-05,
      "loss": 0.3095,
      "step": 142
    },
    {
      "epoch": 0.0006726436305822366,
      "grad_norm": 5.8812785148620605,
      "learning_rate": 5.72e-05,
      "loss": 0.4756,
      "step": 143
    },
    {
      "epoch": 0.0006773474321946998,
      "grad_norm": 7.443398952484131,
      "learning_rate": 5.76e-05,
      "loss": 0.4334,
      "step": 144
    },
    {
      "epoch": 0.0006820512338071629,
      "grad_norm": 11.967938423156738,
      "learning_rate": 5.8e-05,
      "loss": 0.7469,
      "step": 145
    },
    {
      "epoch": 0.0006867550354196262,
      "grad_norm": 1.9351905584335327,
      "learning_rate": 5.8399999999999997e-05,
      "loss": 0.0628,
      "step": 146
    },
    {
      "epoch": 0.0006914588370320893,
      "grad_norm": 3.3493361473083496,
      "learning_rate": 5.88e-05,
      "loss": 0.1065,
      "step": 147
    },
    {
      "epoch": 0.0006961626386445526,
      "grad_norm": 6.269193172454834,
      "learning_rate": 5.92e-05,
      "loss": 0.4698,
      "step": 148
    },
    {
      "epoch": 0.0007008664402570157,
      "grad_norm": 0.9900681972503662,
      "learning_rate": 5.96e-05,
      "loss": 0.0214,
      "step": 149
    },
    {
      "epoch": 0.000705570241869479,
      "grad_norm": 3.5989089012145996,
      "learning_rate": 6e-05,
      "loss": 0.3623,
      "step": 150
    },
    {
      "epoch": 0.0007102740434819421,
      "grad_norm": 1.4215503931045532,
      "learning_rate": 6.04e-05,
      "loss": 0.1001,
      "step": 151
    },
    {
      "epoch": 0.0007149778450944053,
      "grad_norm": 2.1982524394989014,
      "learning_rate": 6.08e-05,
      "loss": 0.0498,
      "step": 152
    },
    {
      "epoch": 0.0007196816467068685,
      "grad_norm": 4.4305267333984375,
      "learning_rate": 6.12e-05,
      "loss": 0.3794,
      "step": 153
    },
    {
      "epoch": 0.0007243854483193317,
      "grad_norm": 1.8252030611038208,
      "learning_rate": 6.16e-05,
      "loss": 0.1198,
      "step": 154
    },
    {
      "epoch": 0.0007290892499317949,
      "grad_norm": 6.528279781341553,
      "learning_rate": 6.2e-05,
      "loss": 0.2826,
      "step": 155
    },
    {
      "epoch": 0.0007337930515442581,
      "grad_norm": 3.325836658477783,
      "learning_rate": 6.24e-05,
      "loss": 0.0713,
      "step": 156
    },
    {
      "epoch": 0.0007384968531567212,
      "grad_norm": 2.212897539138794,
      "learning_rate": 6.280000000000001e-05,
      "loss": 0.1107,
      "step": 157
    },
    {
      "epoch": 0.0007432006547691845,
      "grad_norm": 12.83935260772705,
      "learning_rate": 6.32e-05,
      "loss": 0.7381,
      "step": 158
    },
    {
      "epoch": 0.0007479044563816476,
      "grad_norm": 6.565043926239014,
      "learning_rate": 6.36e-05,
      "loss": 0.28,
      "step": 159
    },
    {
      "epoch": 0.0007526082579941109,
      "grad_norm": 7.796690940856934,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.957,
      "step": 160
    },
    {
      "epoch": 0.000757312059606574,
      "grad_norm": 1.417212963104248,
      "learning_rate": 6.440000000000001e-05,
      "loss": 0.0339,
      "step": 161
    },
    {
      "epoch": 0.0007620158612190373,
      "grad_norm": 8.943127632141113,
      "learning_rate": 6.48e-05,
      "loss": 0.8647,
      "step": 162
    },
    {
      "epoch": 0.0007667196628315004,
      "grad_norm": 4.862338542938232,
      "learning_rate": 6.52e-05,
      "loss": 0.2068,
      "step": 163
    },
    {
      "epoch": 0.0007714234644439636,
      "grad_norm": 11.924006462097168,
      "learning_rate": 6.560000000000001e-05,
      "loss": 0.9752,
      "step": 164
    },
    {
      "epoch": 0.0007761272660564268,
      "grad_norm": 9.445218086242676,
      "learning_rate": 6.6e-05,
      "loss": 0.7182,
      "step": 165
    },
    {
      "epoch": 0.00078083106766889,
      "grad_norm": 13.994558334350586,
      "learning_rate": 6.64e-05,
      "loss": 0.857,
      "step": 166
    },
    {
      "epoch": 0.0007855348692813532,
      "grad_norm": 0.8699049949645996,
      "learning_rate": 6.680000000000001e-05,
      "loss": 0.0215,
      "step": 167
    },
    {
      "epoch": 0.0007902386708938164,
      "grad_norm": 1.8705347776412964,
      "learning_rate": 6.720000000000001e-05,
      "loss": 0.0448,
      "step": 168
    },
    {
      "epoch": 0.0007949424725062795,
      "grad_norm": 0.3770587742328644,
      "learning_rate": 6.76e-05,
      "loss": 0.009,
      "step": 169
    },
    {
      "epoch": 0.0007996462741187428,
      "grad_norm": 0.5122890472412109,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0142,
      "step": 170
    },
    {
      "epoch": 0.0008043500757312059,
      "grad_norm": 0.4145972430706024,
      "learning_rate": 6.840000000000001e-05,
      "loss": 0.0102,
      "step": 171
    },
    {
      "epoch": 0.0008090538773436692,
      "grad_norm": 3.248856782913208,
      "learning_rate": 6.879999999999999e-05,
      "loss": 0.1887,
      "step": 172
    },
    {
      "epoch": 0.0008137576789561323,
      "grad_norm": 4.414053440093994,
      "learning_rate": 6.92e-05,
      "loss": 0.4014,
      "step": 173
    },
    {
      "epoch": 0.0008184614805685956,
      "grad_norm": 5.474277019500732,
      "learning_rate": 6.96e-05,
      "loss": 0.418,
      "step": 174
    },
    {
      "epoch": 0.0008231652821810587,
      "grad_norm": 4.56879997253418,
      "learning_rate": 7e-05,
      "loss": 0.3402,
      "step": 175
    },
    {
      "epoch": 0.0008278690837935219,
      "grad_norm": 9.654291152954102,
      "learning_rate": 7.04e-05,
      "loss": 1.128,
      "step": 176
    },
    {
      "epoch": 0.0008325728854059851,
      "grad_norm": 5.972589015960693,
      "learning_rate": 7.08e-05,
      "loss": 0.4881,
      "step": 177
    },
    {
      "epoch": 0.0008372766870184483,
      "grad_norm": 10.788414001464844,
      "learning_rate": 7.12e-05,
      "loss": 0.3418,
      "step": 178
    },
    {
      "epoch": 0.0008419804886309115,
      "grad_norm": 6.574192523956299,
      "learning_rate": 7.16e-05,
      "loss": 0.7221,
      "step": 179
    },
    {
      "epoch": 0.0008466842902433747,
      "grad_norm": 9.723762512207031,
      "learning_rate": 7.2e-05,
      "loss": 0.9245,
      "step": 180
    },
    {
      "epoch": 0.0008513880918558378,
      "grad_norm": 1.9738357067108154,
      "learning_rate": 7.24e-05,
      "loss": 0.1339,
      "step": 181
    },
    {
      "epoch": 0.0008560918934683011,
      "grad_norm": 6.301769733428955,
      "learning_rate": 7.280000000000001e-05,
      "loss": 0.377,
      "step": 182
    },
    {
      "epoch": 0.0008607956950807642,
      "grad_norm": 2.496238946914673,
      "learning_rate": 7.32e-05,
      "loss": 0.2578,
      "step": 183
    },
    {
      "epoch": 0.0008654994966932275,
      "grad_norm": 0.6077069044113159,
      "learning_rate": 7.36e-05,
      "loss": 0.0142,
      "step": 184
    },
    {
      "epoch": 0.0008702032983056906,
      "grad_norm": 5.066396236419678,
      "learning_rate": 7.4e-05,
      "loss": 0.5662,
      "step": 185
    },
    {
      "epoch": 0.0008749070999181539,
      "grad_norm": 5.250504016876221,
      "learning_rate": 7.44e-05,
      "loss": 0.3519,
      "step": 186
    },
    {
      "epoch": 0.000879610901530617,
      "grad_norm": 1.6067606210708618,
      "learning_rate": 7.48e-05,
      "loss": 0.2353,
      "step": 187
    },
    {
      "epoch": 0.0008843147031430802,
      "grad_norm": 4.391499042510986,
      "learning_rate": 7.52e-05,
      "loss": 0.2789,
      "step": 188
    },
    {
      "epoch": 0.0008890185047555435,
      "grad_norm": 4.714088439941406,
      "learning_rate": 7.560000000000001e-05,
      "loss": 0.3386,
      "step": 189
    },
    {
      "epoch": 0.0008937223063680066,
      "grad_norm": 1.2048958539962769,
      "learning_rate": 7.6e-05,
      "loss": 0.0683,
      "step": 190
    },
    {
      "epoch": 0.0008984261079804699,
      "grad_norm": 2.281769275665283,
      "learning_rate": 7.64e-05,
      "loss": 0.2103,
      "step": 191
    },
    {
      "epoch": 0.000903129909592933,
      "grad_norm": 2.5437893867492676,
      "learning_rate": 7.680000000000001e-05,
      "loss": 0.3243,
      "step": 192
    },
    {
      "epoch": 0.0009078337112053962,
      "grad_norm": 4.540826320648193,
      "learning_rate": 7.72e-05,
      "loss": 0.3501,
      "step": 193
    },
    {
      "epoch": 0.0009125375128178594,
      "grad_norm": 5.794620037078857,
      "learning_rate": 7.76e-05,
      "loss": 0.469,
      "step": 194
    },
    {
      "epoch": 0.0009172413144303226,
      "grad_norm": 4.361606121063232,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.4278,
      "step": 195
    },
    {
      "epoch": 0.0009219451160427858,
      "grad_norm": 4.752676963806152,
      "learning_rate": 7.840000000000001e-05,
      "loss": 0.2079,
      "step": 196
    },
    {
      "epoch": 0.000926648917655249,
      "grad_norm": 6.17271614074707,
      "learning_rate": 7.88e-05,
      "loss": 0.2917,
      "step": 197
    },
    {
      "epoch": 0.0009313527192677122,
      "grad_norm": 9.199727058410645,
      "learning_rate": 7.920000000000001e-05,
      "loss": 0.5121,
      "step": 198
    },
    {
      "epoch": 0.0009360565208801754,
      "grad_norm": 7.618838787078857,
      "learning_rate": 7.960000000000001e-05,
      "loss": 0.4281,
      "step": 199
    },
    {
      "epoch": 0.0009407603224926385,
      "grad_norm": 3.7657148838043213,
      "learning_rate": 8e-05,
      "loss": 0.1815,
      "step": 200
    },
    {
      "epoch": 0.0009454641241051018,
      "grad_norm": 6.839115142822266,
      "learning_rate": 8.04e-05,
      "loss": 0.3815,
      "step": 201
    },
    {
      "epoch": 0.0009501679257175649,
      "grad_norm": 14.295804023742676,
      "learning_rate": 8.080000000000001e-05,
      "loss": 0.2907,
      "step": 202
    },
    {
      "epoch": 0.0009548717273300282,
      "grad_norm": 3.368785858154297,
      "learning_rate": 8.120000000000001e-05,
      "loss": 0.2574,
      "step": 203
    },
    {
      "epoch": 0.0009595755289424913,
      "grad_norm": 2.79160475730896,
      "learning_rate": 8.16e-05,
      "loss": 0.1702,
      "step": 204
    },
    {
      "epoch": 0.0009642793305549545,
      "grad_norm": 6.467050552368164,
      "learning_rate": 8.2e-05,
      "loss": 0.3644,
      "step": 205
    },
    {
      "epoch": 0.0009689831321674177,
      "grad_norm": 2.370896339416504,
      "learning_rate": 8.24e-05,
      "loss": 0.2172,
      "step": 206
    },
    {
      "epoch": 0.0009736869337798809,
      "grad_norm": 3.2137718200683594,
      "learning_rate": 8.28e-05,
      "loss": 0.1011,
      "step": 207
    },
    {
      "epoch": 0.000978390735392344,
      "grad_norm": 1.7555370330810547,
      "learning_rate": 8.32e-05,
      "loss": 0.1656,
      "step": 208
    },
    {
      "epoch": 0.0009830945370048073,
      "grad_norm": 2.7426974773406982,
      "learning_rate": 8.36e-05,
      "loss": 0.273,
      "step": 209
    },
    {
      "epoch": 0.0009877983386172705,
      "grad_norm": 2.8553502559661865,
      "learning_rate": 8.4e-05,
      "loss": 0.1907,
      "step": 210
    },
    {
      "epoch": 0.0009925021402297336,
      "grad_norm": 1.90424382686615,
      "learning_rate": 8.44e-05,
      "loss": 0.107,
      "step": 211
    },
    {
      "epoch": 0.0009972059418421968,
      "grad_norm": 5.73706579208374,
      "learning_rate": 8.48e-05,
      "loss": 0.3738,
      "step": 212
    },
    {
      "epoch": 0.00100190974345466,
      "grad_norm": 2.8048489093780518,
      "learning_rate": 8.52e-05,
      "loss": 0.4472,
      "step": 213
    },
    {
      "epoch": 0.0010066135450671233,
      "grad_norm": 1.6752967834472656,
      "learning_rate": 8.560000000000001e-05,
      "loss": 0.0435,
      "step": 214
    },
    {
      "epoch": 0.0010113173466795864,
      "grad_norm": 2.529334306716919,
      "learning_rate": 8.6e-05,
      "loss": 0.0836,
      "step": 215
    },
    {
      "epoch": 0.0010160211482920496,
      "grad_norm": 4.091884613037109,
      "learning_rate": 8.64e-05,
      "loss": 0.4066,
      "step": 216
    },
    {
      "epoch": 0.0010207249499045129,
      "grad_norm": 2.7089121341705322,
      "learning_rate": 8.680000000000001e-05,
      "loss": 0.3219,
      "step": 217
    },
    {
      "epoch": 0.001025428751516976,
      "grad_norm": 1.5982224941253662,
      "learning_rate": 8.72e-05,
      "loss": 0.0786,
      "step": 218
    },
    {
      "epoch": 0.0010301325531294392,
      "grad_norm": 4.552698135375977,
      "learning_rate": 8.76e-05,
      "loss": 0.3143,
      "step": 219
    },
    {
      "epoch": 0.0010348363547419024,
      "grad_norm": 2.8155717849731445,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.1612,
      "step": 220
    },
    {
      "epoch": 0.0010395401563543657,
      "grad_norm": 1.075063943862915,
      "learning_rate": 8.840000000000001e-05,
      "loss": 0.0651,
      "step": 221
    },
    {
      "epoch": 0.0010442439579668287,
      "grad_norm": 5.97920036315918,
      "learning_rate": 8.88e-05,
      "loss": 0.3641,
      "step": 222
    },
    {
      "epoch": 0.001048947759579292,
      "grad_norm": 3.6123642921447754,
      "learning_rate": 8.92e-05,
      "loss": 0.1178,
      "step": 223
    },
    {
      "epoch": 0.0010536515611917552,
      "grad_norm": 5.538098335266113,
      "learning_rate": 8.960000000000001e-05,
      "loss": 0.2042,
      "step": 224
    },
    {
      "epoch": 0.0010583553628042183,
      "grad_norm": 6.724918842315674,
      "learning_rate": 9e-05,
      "loss": 0.5351,
      "step": 225
    },
    {
      "epoch": 0.0010630591644166815,
      "grad_norm": 2.9325978755950928,
      "learning_rate": 9.04e-05,
      "loss": 0.105,
      "step": 226
    },
    {
      "epoch": 0.0010677629660291448,
      "grad_norm": 9.571220397949219,
      "learning_rate": 9.080000000000001e-05,
      "loss": 0.8588,
      "step": 227
    },
    {
      "epoch": 0.001072466767641608,
      "grad_norm": 6.101480484008789,
      "learning_rate": 9.120000000000001e-05,
      "loss": 0.6106,
      "step": 228
    },
    {
      "epoch": 0.001077170569254071,
      "grad_norm": 10.276091575622559,
      "learning_rate": 9.16e-05,
      "loss": 0.7576,
      "step": 229
    },
    {
      "epoch": 0.0010818743708665343,
      "grad_norm": 1.127669095993042,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.0297,
      "step": 230
    },
    {
      "epoch": 0.0010865781724789976,
      "grad_norm": 2.7628986835479736,
      "learning_rate": 9.240000000000001e-05,
      "loss": 0.1571,
      "step": 231
    },
    {
      "epoch": 0.0010912819740914606,
      "grad_norm": 14.119653701782227,
      "learning_rate": 9.28e-05,
      "loss": 1.9333,
      "step": 232
    },
    {
      "epoch": 0.001095985775703924,
      "grad_norm": 13.460287094116211,
      "learning_rate": 9.320000000000002e-05,
      "loss": 1.212,
      "step": 233
    },
    {
      "epoch": 0.0011006895773163872,
      "grad_norm": 4.873778343200684,
      "learning_rate": 9.360000000000001e-05,
      "loss": 0.3513,
      "step": 234
    },
    {
      "epoch": 0.0011053933789288502,
      "grad_norm": 3.878368616104126,
      "learning_rate": 9.4e-05,
      "loss": 0.2724,
      "step": 235
    },
    {
      "epoch": 0.0011100971805413134,
      "grad_norm": 3.217156410217285,
      "learning_rate": 9.44e-05,
      "loss": 0.2127,
      "step": 236
    },
    {
      "epoch": 0.0011148009821537767,
      "grad_norm": 6.07342004776001,
      "learning_rate": 9.48e-05,
      "loss": 0.2077,
      "step": 237
    },
    {
      "epoch": 0.00111950478376624,
      "grad_norm": 1.9611207246780396,
      "learning_rate": 9.52e-05,
      "loss": 0.1384,
      "step": 238
    },
    {
      "epoch": 0.001124208585378703,
      "grad_norm": 3.120346784591675,
      "learning_rate": 9.56e-05,
      "loss": 0.3784,
      "step": 239
    },
    {
      "epoch": 0.0011289123869911663,
      "grad_norm": 5.3007493019104,
      "learning_rate": 9.6e-05,
      "loss": 0.8169,
      "step": 240
    },
    {
      "epoch": 0.0011336161886036295,
      "grad_norm": 0.5454070568084717,
      "learning_rate": 9.64e-05,
      "loss": 0.0179,
      "step": 241
    },
    {
      "epoch": 0.0011383199902160925,
      "grad_norm": 13.921612739562988,
      "learning_rate": 9.680000000000001e-05,
      "loss": 0.5207,
      "step": 242
    },
    {
      "epoch": 0.0011430237918285558,
      "grad_norm": 3.9617912769317627,
      "learning_rate": 9.72e-05,
      "loss": 0.1699,
      "step": 243
    },
    {
      "epoch": 0.001147727593441019,
      "grad_norm": 2.149620532989502,
      "learning_rate": 9.76e-05,
      "loss": 0.1292,
      "step": 244
    },
    {
      "epoch": 0.0011524313950534823,
      "grad_norm": 5.69052791595459,
      "learning_rate": 9.8e-05,
      "loss": 0.5643,
      "step": 245
    },
    {
      "epoch": 0.0011571351966659454,
      "grad_norm": 2.9579975605010986,
      "learning_rate": 9.84e-05,
      "loss": 0.2219,
      "step": 246
    },
    {
      "epoch": 0.0011618389982784086,
      "grad_norm": 2.4756321907043457,
      "learning_rate": 9.88e-05,
      "loss": 0.2636,
      "step": 247
    },
    {
      "epoch": 0.0011665427998908719,
      "grad_norm": 3.988203287124634,
      "learning_rate": 9.92e-05,
      "loss": 0.491,
      "step": 248
    },
    {
      "epoch": 0.001171246601503335,
      "grad_norm": 1.099124789237976,
      "learning_rate": 9.960000000000001e-05,
      "loss": 0.0622,
      "step": 249
    },
    {
      "epoch": 0.0011759504031157982,
      "grad_norm": 4.605346202850342,
      "learning_rate": 0.0001,
      "loss": 0.2227,
      "step": 250
    },
    {
      "epoch": 0.0011806542047282614,
      "grad_norm": 0.6628114581108093,
      "learning_rate": 0.0001004,
      "loss": 0.0529,
      "step": 251
    },
    {
      "epoch": 0.0011853580063407247,
      "grad_norm": 4.205711364746094,
      "learning_rate": 0.00010080000000000001,
      "loss": 0.2924,
      "step": 252
    },
    {
      "epoch": 0.0011900618079531877,
      "grad_norm": 3.8498294353485107,
      "learning_rate": 0.00010120000000000001,
      "loss": 0.2712,
      "step": 253
    },
    {
      "epoch": 0.001194765609565651,
      "grad_norm": 9.447155952453613,
      "learning_rate": 0.0001016,
      "loss": 0.9995,
      "step": 254
    },
    {
      "epoch": 0.0011994694111781142,
      "grad_norm": 3.0022261142730713,
      "learning_rate": 0.00010200000000000001,
      "loss": 0.3952,
      "step": 255
    },
    {
      "epoch": 0.0012041732127905773,
      "grad_norm": 6.012879371643066,
      "learning_rate": 0.00010240000000000001,
      "loss": 1.2967,
      "step": 256
    },
    {
      "epoch": 0.0012088770144030405,
      "grad_norm": 2.265723466873169,
      "learning_rate": 0.0001028,
      "loss": 0.1677,
      "step": 257
    },
    {
      "epoch": 0.0012135808160155038,
      "grad_norm": 1.6405222415924072,
      "learning_rate": 0.0001032,
      "loss": 0.2574,
      "step": 258
    },
    {
      "epoch": 0.0012182846176279668,
      "grad_norm": 2.443418025970459,
      "learning_rate": 0.00010360000000000001,
      "loss": 0.1905,
      "step": 259
    },
    {
      "epoch": 0.00122298841924043,
      "grad_norm": 4.350813865661621,
      "learning_rate": 0.00010400000000000001,
      "loss": 0.267,
      "step": 260
    },
    {
      "epoch": 0.0012276922208528933,
      "grad_norm": 1.9887791872024536,
      "learning_rate": 0.0001044,
      "loss": 0.1648,
      "step": 261
    },
    {
      "epoch": 0.0012323960224653566,
      "grad_norm": 1.3817839622497559,
      "learning_rate": 0.00010480000000000001,
      "loss": 0.0726,
      "step": 262
    },
    {
      "epoch": 0.0012370998240778196,
      "grad_norm": 3.970262050628662,
      "learning_rate": 0.00010520000000000001,
      "loss": 0.2769,
      "step": 263
    },
    {
      "epoch": 0.0012418036256902829,
      "grad_norm": 13.086316108703613,
      "learning_rate": 0.0001056,
      "loss": 1.1528,
      "step": 264
    },
    {
      "epoch": 0.0012465074273027461,
      "grad_norm": 8.955090522766113,
      "learning_rate": 0.00010600000000000002,
      "loss": 0.5816,
      "step": 265
    },
    {
      "epoch": 0.0012512112289152092,
      "grad_norm": 7.970674991607666,
      "learning_rate": 0.00010640000000000001,
      "loss": 0.371,
      "step": 266
    },
    {
      "epoch": 0.0012559150305276724,
      "grad_norm": 5.113158226013184,
      "learning_rate": 0.00010680000000000001,
      "loss": 0.6101,
      "step": 267
    },
    {
      "epoch": 0.0012606188321401357,
      "grad_norm": 6.568826675415039,
      "learning_rate": 0.00010720000000000002,
      "loss": 0.3781,
      "step": 268
    },
    {
      "epoch": 0.001265322633752599,
      "grad_norm": 6.245065689086914,
      "learning_rate": 0.00010760000000000001,
      "loss": 0.3579,
      "step": 269
    },
    {
      "epoch": 0.001270026435365062,
      "grad_norm": 2.730668306350708,
      "learning_rate": 0.00010800000000000001,
      "loss": 0.1458,
      "step": 270
    },
    {
      "epoch": 0.0012747302369775252,
      "grad_norm": 2.522618532180786,
      "learning_rate": 0.00010840000000000002,
      "loss": 0.2655,
      "step": 271
    },
    {
      "epoch": 0.0012794340385899885,
      "grad_norm": 3.7141852378845215,
      "learning_rate": 0.00010880000000000002,
      "loss": 0.4087,
      "step": 272
    },
    {
      "epoch": 0.0012841378402024515,
      "grad_norm": 5.851397514343262,
      "learning_rate": 0.00010920000000000001,
      "loss": 0.363,
      "step": 273
    },
    {
      "epoch": 0.0012888416418149148,
      "grad_norm": 4.247869491577148,
      "learning_rate": 0.00010960000000000001,
      "loss": 0.2285,
      "step": 274
    },
    {
      "epoch": 0.001293545443427378,
      "grad_norm": 2.001089572906494,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.1478,
      "step": 275
    },
    {
      "epoch": 0.0012982492450398413,
      "grad_norm": 0.5621914267539978,
      "learning_rate": 0.00011040000000000001,
      "loss": 0.0185,
      "step": 276
    },
    {
      "epoch": 0.0013029530466523043,
      "grad_norm": 2.1327598094940186,
      "learning_rate": 0.00011080000000000001,
      "loss": 0.1601,
      "step": 277
    },
    {
      "epoch": 0.0013076568482647676,
      "grad_norm": 3.787323236465454,
      "learning_rate": 0.00011120000000000002,
      "loss": 0.2316,
      "step": 278
    },
    {
      "epoch": 0.0013123606498772309,
      "grad_norm": 7.50827169418335,
      "learning_rate": 0.00011160000000000002,
      "loss": 0.7375,
      "step": 279
    },
    {
      "epoch": 0.0013170644514896939,
      "grad_norm": 2.4351136684417725,
      "learning_rate": 0.00011200000000000001,
      "loss": 0.1164,
      "step": 280
    },
    {
      "epoch": 0.0013217682531021571,
      "grad_norm": 6.235156536102295,
      "learning_rate": 0.00011240000000000002,
      "loss": 0.6748,
      "step": 281
    },
    {
      "epoch": 0.0013264720547146204,
      "grad_norm": 6.498373985290527,
      "learning_rate": 0.00011279999999999999,
      "loss": 0.3911,
      "step": 282
    },
    {
      "epoch": 0.0013311758563270834,
      "grad_norm": 1.9898738861083984,
      "learning_rate": 0.0001132,
      "loss": 0.1857,
      "step": 283
    },
    {
      "epoch": 0.0013358796579395467,
      "grad_norm": 5.861527442932129,
      "learning_rate": 0.0001136,
      "loss": 0.7763,
      "step": 284
    },
    {
      "epoch": 0.00134058345955201,
      "grad_norm": 4.748760223388672,
      "learning_rate": 0.00011399999999999999,
      "loss": 0.483,
      "step": 285
    },
    {
      "epoch": 0.0013452872611644732,
      "grad_norm": 0.20194639265537262,
      "learning_rate": 0.0001144,
      "loss": 0.0063,
      "step": 286
    },
    {
      "epoch": 0.0013499910627769362,
      "grad_norm": 5.861070156097412,
      "learning_rate": 0.0001148,
      "loss": 0.8052,
      "step": 287
    },
    {
      "epoch": 0.0013546948643893995,
      "grad_norm": 4.721028804779053,
      "learning_rate": 0.0001152,
      "loss": 0.7506,
      "step": 288
    },
    {
      "epoch": 0.0013593986660018628,
      "grad_norm": 0.6236014366149902,
      "learning_rate": 0.00011559999999999999,
      "loss": 0.0229,
      "step": 289
    },
    {
      "epoch": 0.0013641024676143258,
      "grad_norm": 4.265346050262451,
      "learning_rate": 0.000116,
      "loss": 0.4207,
      "step": 290
    },
    {
      "epoch": 0.001368806269226789,
      "grad_norm": 1.9097594022750854,
      "learning_rate": 0.0001164,
      "loss": 0.1345,
      "step": 291
    },
    {
      "epoch": 0.0013735100708392523,
      "grad_norm": 7.239140510559082,
      "learning_rate": 0.00011679999999999999,
      "loss": 0.6882,
      "step": 292
    },
    {
      "epoch": 0.0013782138724517156,
      "grad_norm": 3.469158887863159,
      "learning_rate": 0.0001172,
      "loss": 0.537,
      "step": 293
    },
    {
      "epoch": 0.0013829176740641786,
      "grad_norm": 6.196928024291992,
      "learning_rate": 0.0001176,
      "loss": 0.652,
      "step": 294
    },
    {
      "epoch": 0.0013876214756766419,
      "grad_norm": 5.817218780517578,
      "learning_rate": 0.000118,
      "loss": 0.5883,
      "step": 295
    },
    {
      "epoch": 0.0013923252772891051,
      "grad_norm": 0.9719970226287842,
      "learning_rate": 0.0001184,
      "loss": 0.0995,
      "step": 296
    },
    {
      "epoch": 0.0013970290789015682,
      "grad_norm": 1.451691746711731,
      "learning_rate": 0.0001188,
      "loss": 0.0822,
      "step": 297
    },
    {
      "epoch": 0.0014017328805140314,
      "grad_norm": 4.7092790603637695,
      "learning_rate": 0.0001192,
      "loss": 0.6913,
      "step": 298
    },
    {
      "epoch": 0.0014064366821264947,
      "grad_norm": 2.3695647716522217,
      "learning_rate": 0.00011960000000000001,
      "loss": 0.1064,
      "step": 299
    },
    {
      "epoch": 0.001411140483738958,
      "grad_norm": 1.1952271461486816,
      "learning_rate": 0.00012,
      "loss": 0.2535,
      "step": 300
    },
    {
      "epoch": 0.001415844285351421,
      "grad_norm": 5.761165142059326,
      "learning_rate": 0.0001204,
      "loss": 0.4578,
      "step": 301
    },
    {
      "epoch": 0.0014205480869638842,
      "grad_norm": 9.61013126373291,
      "learning_rate": 0.0001208,
      "loss": 0.5669,
      "step": 302
    },
    {
      "epoch": 0.0014252518885763475,
      "grad_norm": 4.267979621887207,
      "learning_rate": 0.0001212,
      "loss": 0.4776,
      "step": 303
    },
    {
      "epoch": 0.0014299556901888105,
      "grad_norm": 3.4100570678710938,
      "learning_rate": 0.0001216,
      "loss": 0.3825,
      "step": 304
    },
    {
      "epoch": 0.0014346594918012738,
      "grad_norm": 2.896878719329834,
      "learning_rate": 0.000122,
      "loss": 0.479,
      "step": 305
    },
    {
      "epoch": 0.001439363293413737,
      "grad_norm": 7.535650730133057,
      "learning_rate": 0.0001224,
      "loss": 0.988,
      "step": 306
    },
    {
      "epoch": 0.0014440670950262,
      "grad_norm": 7.952488899230957,
      "learning_rate": 0.0001228,
      "loss": 1.0579,
      "step": 307
    },
    {
      "epoch": 0.0014487708966386633,
      "grad_norm": 1.957253336906433,
      "learning_rate": 0.0001232,
      "loss": 0.2634,
      "step": 308
    },
    {
      "epoch": 0.0014534746982511266,
      "grad_norm": 4.205195426940918,
      "learning_rate": 0.0001236,
      "loss": 0.5535,
      "step": 309
    },
    {
      "epoch": 0.0014581784998635898,
      "grad_norm": 2.924506425857544,
      "learning_rate": 0.000124,
      "loss": 0.4655,
      "step": 310
    },
    {
      "epoch": 0.0014628823014760529,
      "grad_norm": 3.6749260425567627,
      "learning_rate": 0.00012440000000000002,
      "loss": 0.5132,
      "step": 311
    },
    {
      "epoch": 0.0014675861030885161,
      "grad_norm": 2.4244179725646973,
      "learning_rate": 0.0001248,
      "loss": 0.4429,
      "step": 312
    },
    {
      "epoch": 0.0014722899047009794,
      "grad_norm": 2.774231195449829,
      "learning_rate": 0.0001252,
      "loss": 0.3405,
      "step": 313
    },
    {
      "epoch": 0.0014769937063134424,
      "grad_norm": 2.4018137454986572,
      "learning_rate": 0.00012560000000000002,
      "loss": 0.2759,
      "step": 314
    },
    {
      "epoch": 0.0014816975079259057,
      "grad_norm": 2.1334757804870605,
      "learning_rate": 0.000126,
      "loss": 0.2498,
      "step": 315
    },
    {
      "epoch": 0.001486401309538369,
      "grad_norm": 2.5756373405456543,
      "learning_rate": 0.0001264,
      "loss": 0.3827,
      "step": 316
    },
    {
      "epoch": 0.0014911051111508322,
      "grad_norm": 2.4243853092193604,
      "learning_rate": 0.00012680000000000002,
      "loss": 0.3527,
      "step": 317
    },
    {
      "epoch": 0.0014958089127632952,
      "grad_norm": 3.0257978439331055,
      "learning_rate": 0.0001272,
      "loss": 0.2736,
      "step": 318
    },
    {
      "epoch": 0.0015005127143757585,
      "grad_norm": 2.501581907272339,
      "learning_rate": 0.0001276,
      "loss": 0.5115,
      "step": 319
    },
    {
      "epoch": 0.0015052165159882217,
      "grad_norm": 2.4450430870056152,
      "learning_rate": 0.00012800000000000002,
      "loss": 0.149,
      "step": 320
    },
    {
      "epoch": 0.0015099203176006848,
      "grad_norm": 4.12658166885376,
      "learning_rate": 0.0001284,
      "loss": 0.4058,
      "step": 321
    },
    {
      "epoch": 0.001514624119213148,
      "grad_norm": 5.315903186798096,
      "learning_rate": 0.00012880000000000001,
      "loss": 0.5214,
      "step": 322
    },
    {
      "epoch": 0.0015193279208256113,
      "grad_norm": 1.2464120388031006,
      "learning_rate": 0.00012920000000000002,
      "loss": 0.1062,
      "step": 323
    },
    {
      "epoch": 0.0015240317224380746,
      "grad_norm": 1.9358088970184326,
      "learning_rate": 0.0001296,
      "loss": 0.1757,
      "step": 324
    },
    {
      "epoch": 0.0015287355240505376,
      "grad_norm": 1.993677020072937,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.1378,
      "step": 325
    },
    {
      "epoch": 0.0015334393256630008,
      "grad_norm": 2.1281518936157227,
      "learning_rate": 0.0001304,
      "loss": 0.148,
      "step": 326
    },
    {
      "epoch": 0.001538143127275464,
      "grad_norm": 4.741973876953125,
      "learning_rate": 0.0001308,
      "loss": 0.3754,
      "step": 327
    },
    {
      "epoch": 0.0015428469288879271,
      "grad_norm": 1.0350943803787231,
      "learning_rate": 0.00013120000000000002,
      "loss": 0.0534,
      "step": 328
    },
    {
      "epoch": 0.0015475507305003904,
      "grad_norm": 7.16654109954834,
      "learning_rate": 0.0001316,
      "loss": 0.3577,
      "step": 329
    },
    {
      "epoch": 0.0015522545321128537,
      "grad_norm": 3.2450430393218994,
      "learning_rate": 0.000132,
      "loss": 0.1855,
      "step": 330
    },
    {
      "epoch": 0.001556958333725317,
      "grad_norm": 1.9723613262176514,
      "learning_rate": 0.00013240000000000002,
      "loss": 0.0674,
      "step": 331
    },
    {
      "epoch": 0.00156166213533778,
      "grad_norm": 2.4443912506103516,
      "learning_rate": 0.0001328,
      "loss": 0.0759,
      "step": 332
    },
    {
      "epoch": 0.0015663659369502432,
      "grad_norm": 2.283569097518921,
      "learning_rate": 0.0001332,
      "loss": 0.0385,
      "step": 333
    },
    {
      "epoch": 0.0015710697385627065,
      "grad_norm": 1.6785517930984497,
      "learning_rate": 0.00013360000000000002,
      "loss": 0.1018,
      "step": 334
    },
    {
      "epoch": 0.0015757735401751695,
      "grad_norm": 2.3101062774658203,
      "learning_rate": 0.000134,
      "loss": 0.0711,
      "step": 335
    },
    {
      "epoch": 0.0015804773417876328,
      "grad_norm": 8.354108810424805,
      "learning_rate": 0.00013440000000000001,
      "loss": 0.2996,
      "step": 336
    },
    {
      "epoch": 0.001585181143400096,
      "grad_norm": 0.7124830484390259,
      "learning_rate": 0.00013480000000000002,
      "loss": 0.0205,
      "step": 337
    },
    {
      "epoch": 0.001589884945012559,
      "grad_norm": 13.39250373840332,
      "learning_rate": 0.0001352,
      "loss": 1.2915,
      "step": 338
    },
    {
      "epoch": 0.0015945887466250223,
      "grad_norm": 4.381448268890381,
      "learning_rate": 0.00013560000000000002,
      "loss": 0.1355,
      "step": 339
    },
    {
      "epoch": 0.0015992925482374856,
      "grad_norm": 3.804953098297119,
      "learning_rate": 0.00013600000000000003,
      "loss": 0.134,
      "step": 340
    },
    {
      "epoch": 0.0016039963498499488,
      "grad_norm": 17.82666015625,
      "learning_rate": 0.0001364,
      "loss": 0.8447,
      "step": 341
    },
    {
      "epoch": 0.0016087001514624119,
      "grad_norm": 9.356554985046387,
      "learning_rate": 0.00013680000000000002,
      "loss": 0.7243,
      "step": 342
    },
    {
      "epoch": 0.0016134039530748751,
      "grad_norm": 12.969133377075195,
      "learning_rate": 0.00013720000000000003,
      "loss": 0.9061,
      "step": 343
    },
    {
      "epoch": 0.0016181077546873384,
      "grad_norm": 6.46461296081543,
      "learning_rate": 0.00013759999999999998,
      "loss": 0.3836,
      "step": 344
    },
    {
      "epoch": 0.0016228115562998014,
      "grad_norm": 14.767903327941895,
      "learning_rate": 0.000138,
      "loss": 0.9385,
      "step": 345
    },
    {
      "epoch": 0.0016275153579122647,
      "grad_norm": 4.248624324798584,
      "learning_rate": 0.0001384,
      "loss": 0.1006,
      "step": 346
    },
    {
      "epoch": 0.001632219159524728,
      "grad_norm": 5.127457141876221,
      "learning_rate": 0.00013879999999999999,
      "loss": 0.1415,
      "step": 347
    },
    {
      "epoch": 0.0016369229611371912,
      "grad_norm": 2.027552843093872,
      "learning_rate": 0.0001392,
      "loss": 0.0487,
      "step": 348
    },
    {
      "epoch": 0.0016416267627496542,
      "grad_norm": 0.6455308794975281,
      "learning_rate": 0.0001396,
      "loss": 0.0231,
      "step": 349
    },
    {
      "epoch": 0.0016463305643621175,
      "grad_norm": 4.503232955932617,
      "learning_rate": 0.00014,
      "loss": 0.3631,
      "step": 350
    },
    {
      "epoch": 0.0016510343659745807,
      "grad_norm": 5.003231525421143,
      "learning_rate": 0.0001404,
      "loss": 0.1904,
      "step": 351
    },
    {
      "epoch": 0.0016557381675870438,
      "grad_norm": 7.7387375831604,
      "learning_rate": 0.0001408,
      "loss": 0.3851,
      "step": 352
    },
    {
      "epoch": 0.001660441969199507,
      "grad_norm": 7.251846790313721,
      "learning_rate": 0.0001412,
      "loss": 0.3054,
      "step": 353
    },
    {
      "epoch": 0.0016651457708119703,
      "grad_norm": 0.8463186025619507,
      "learning_rate": 0.0001416,
      "loss": 0.0314,
      "step": 354
    },
    {
      "epoch": 0.0016698495724244335,
      "grad_norm": 3.3773341178894043,
      "learning_rate": 0.000142,
      "loss": 0.2139,
      "step": 355
    },
    {
      "epoch": 0.0016745533740368966,
      "grad_norm": 0.49571314454078674,
      "learning_rate": 0.0001424,
      "loss": 0.0212,
      "step": 356
    },
    {
      "epoch": 0.0016792571756493598,
      "grad_norm": 6.931609630584717,
      "learning_rate": 0.0001428,
      "loss": 0.5459,
      "step": 357
    },
    {
      "epoch": 0.001683960977261823,
      "grad_norm": 7.149381637573242,
      "learning_rate": 0.0001432,
      "loss": 0.5258,
      "step": 358
    },
    {
      "epoch": 0.0016886647788742861,
      "grad_norm": 10.123090744018555,
      "learning_rate": 0.0001436,
      "loss": 1.2159,
      "step": 359
    },
    {
      "epoch": 0.0016933685804867494,
      "grad_norm": 3.2398242950439453,
      "learning_rate": 0.000144,
      "loss": 0.1301,
      "step": 360
    },
    {
      "epoch": 0.0016980723820992126,
      "grad_norm": 1.6010240316390991,
      "learning_rate": 0.0001444,
      "loss": 0.0628,
      "step": 361
    },
    {
      "epoch": 0.0017027761837116757,
      "grad_norm": 0.8089118599891663,
      "learning_rate": 0.0001448,
      "loss": 0.0255,
      "step": 362
    },
    {
      "epoch": 0.001707479985324139,
      "grad_norm": 2.7672698497772217,
      "learning_rate": 0.0001452,
      "loss": 0.101,
      "step": 363
    },
    {
      "epoch": 0.0017121837869366022,
      "grad_norm": 4.092283248901367,
      "learning_rate": 0.00014560000000000002,
      "loss": 0.2196,
      "step": 364
    },
    {
      "epoch": 0.0017168875885490654,
      "grad_norm": 6.312417030334473,
      "learning_rate": 0.000146,
      "loss": 0.4104,
      "step": 365
    },
    {
      "epoch": 0.0017215913901615285,
      "grad_norm": 6.616907119750977,
      "learning_rate": 0.0001464,
      "loss": 0.3651,
      "step": 366
    },
    {
      "epoch": 0.0017262951917739917,
      "grad_norm": 4.744364261627197,
      "learning_rate": 0.00014680000000000002,
      "loss": 0.2126,
      "step": 367
    },
    {
      "epoch": 0.001730998993386455,
      "grad_norm": 1.456031084060669,
      "learning_rate": 0.0001472,
      "loss": 0.047,
      "step": 368
    },
    {
      "epoch": 0.001735702794998918,
      "grad_norm": 6.134387969970703,
      "learning_rate": 0.0001476,
      "loss": 0.5088,
      "step": 369
    },
    {
      "epoch": 0.0017404065966113813,
      "grad_norm": 8.216341018676758,
      "learning_rate": 0.000148,
      "loss": 0.6108,
      "step": 370
    },
    {
      "epoch": 0.0017451103982238445,
      "grad_norm": 3.7597427368164062,
      "learning_rate": 0.0001484,
      "loss": 0.149,
      "step": 371
    },
    {
      "epoch": 0.0017498141998363078,
      "grad_norm": 9.81719970703125,
      "learning_rate": 0.0001488,
      "loss": 0.8091,
      "step": 372
    },
    {
      "epoch": 0.0017545180014487708,
      "grad_norm": 9.361310005187988,
      "learning_rate": 0.0001492,
      "loss": 0.8549,
      "step": 373
    },
    {
      "epoch": 0.001759221803061234,
      "grad_norm": 9.573917388916016,
      "learning_rate": 0.0001496,
      "loss": 0.8137,
      "step": 374
    },
    {
      "epoch": 0.0017639256046736974,
      "grad_norm": 9.143319129943848,
      "learning_rate": 0.00015000000000000001,
      "loss": 1.1135,
      "step": 375
    },
    {
      "epoch": 0.0017686294062861604,
      "grad_norm": 3.718782663345337,
      "learning_rate": 0.0001504,
      "loss": 0.2149,
      "step": 376
    },
    {
      "epoch": 0.0017733332078986236,
      "grad_norm": 5.175637245178223,
      "learning_rate": 0.0001508,
      "loss": 0.3678,
      "step": 377
    },
    {
      "epoch": 0.001778037009511087,
      "grad_norm": 1.4497367143630981,
      "learning_rate": 0.00015120000000000002,
      "loss": 0.052,
      "step": 378
    },
    {
      "epoch": 0.0017827408111235502,
      "grad_norm": 6.6607794761657715,
      "learning_rate": 0.0001516,
      "loss": 0.5786,
      "step": 379
    },
    {
      "epoch": 0.0017874446127360132,
      "grad_norm": 1.5905323028564453,
      "learning_rate": 0.000152,
      "loss": 0.0694,
      "step": 380
    },
    {
      "epoch": 0.0017921484143484765,
      "grad_norm": 2.8540422916412354,
      "learning_rate": 0.00015240000000000002,
      "loss": 0.1495,
      "step": 381
    },
    {
      "epoch": 0.0017968522159609397,
      "grad_norm": 2.023268461227417,
      "learning_rate": 0.0001528,
      "loss": 0.0823,
      "step": 382
    },
    {
      "epoch": 0.0018015560175734027,
      "grad_norm": 0.8683632612228394,
      "learning_rate": 0.0001532,
      "loss": 0.0576,
      "step": 383
    },
    {
      "epoch": 0.001806259819185866,
      "grad_norm": 1.2248761653900146,
      "learning_rate": 0.00015360000000000002,
      "loss": 0.0439,
      "step": 384
    },
    {
      "epoch": 0.0018109636207983293,
      "grad_norm": 5.612423896789551,
      "learning_rate": 0.000154,
      "loss": 0.3471,
      "step": 385
    },
    {
      "epoch": 0.0018156674224107923,
      "grad_norm": 2.4408791065216064,
      "learning_rate": 0.0001544,
      "loss": 0.0914,
      "step": 386
    },
    {
      "epoch": 0.0018203712240232556,
      "grad_norm": 0.32082241773605347,
      "learning_rate": 0.00015480000000000002,
      "loss": 0.0091,
      "step": 387
    },
    {
      "epoch": 0.0018250750256357188,
      "grad_norm": 10.327925682067871,
      "learning_rate": 0.0001552,
      "loss": 0.571,
      "step": 388
    },
    {
      "epoch": 0.001829778827248182,
      "grad_norm": 0.12793880701065063,
      "learning_rate": 0.00015560000000000001,
      "loss": 0.0019,
      "step": 389
    },
    {
      "epoch": 0.001834482628860645,
      "grad_norm": 0.835273265838623,
      "learning_rate": 0.00015600000000000002,
      "loss": 0.0245,
      "step": 390
    },
    {
      "epoch": 0.0018391864304731084,
      "grad_norm": 0.30255475640296936,
      "learning_rate": 0.0001564,
      "loss": 0.0073,
      "step": 391
    },
    {
      "epoch": 0.0018438902320855716,
      "grad_norm": 8.770179748535156,
      "learning_rate": 0.00015680000000000002,
      "loss": 0.6341,
      "step": 392
    },
    {
      "epoch": 0.0018485940336980347,
      "grad_norm": 0.06474225968122482,
      "learning_rate": 0.00015720000000000003,
      "loss": 0.0012,
      "step": 393
    },
    {
      "epoch": 0.001853297835310498,
      "grad_norm": 12.610589027404785,
      "learning_rate": 0.0001576,
      "loss": 1.891,
      "step": 394
    },
    {
      "epoch": 0.0018580016369229612,
      "grad_norm": 7.723695278167725,
      "learning_rate": 0.00015800000000000002,
      "loss": 0.8129,
      "step": 395
    },
    {
      "epoch": 0.0018627054385354244,
      "grad_norm": 7.2633280754089355,
      "learning_rate": 0.00015840000000000003,
      "loss": 0.6139,
      "step": 396
    },
    {
      "epoch": 0.0018674092401478875,
      "grad_norm": 0.46716001629829407,
      "learning_rate": 0.0001588,
      "loss": 0.0099,
      "step": 397
    },
    {
      "epoch": 0.0018721130417603507,
      "grad_norm": 7.197908401489258,
      "learning_rate": 0.00015920000000000002,
      "loss": 0.6898,
      "step": 398
    },
    {
      "epoch": 0.001876816843372814,
      "grad_norm": 0.5740136504173279,
      "learning_rate": 0.0001596,
      "loss": 0.0117,
      "step": 399
    },
    {
      "epoch": 0.001881520644985277,
      "grad_norm": 7.378072738647461,
      "learning_rate": 0.00016,
      "loss": 0.3782,
      "step": 400
    },
    {
      "epoch": 0.0018862244465977403,
      "grad_norm": 1.9648194313049316,
      "learning_rate": 0.00016040000000000002,
      "loss": 0.0921,
      "step": 401
    },
    {
      "epoch": 0.0018909282482102035,
      "grad_norm": 2.49945068359375,
      "learning_rate": 0.0001608,
      "loss": 0.1016,
      "step": 402
    },
    {
      "epoch": 0.0018956320498226668,
      "grad_norm": 8.174307823181152,
      "learning_rate": 0.00016120000000000002,
      "loss": 0.9645,
      "step": 403
    },
    {
      "epoch": 0.0019003358514351298,
      "grad_norm": 0.46510857343673706,
      "learning_rate": 0.00016160000000000002,
      "loss": 0.0127,
      "step": 404
    },
    {
      "epoch": 0.001905039653047593,
      "grad_norm": 5.25247049331665,
      "learning_rate": 0.000162,
      "loss": 0.6457,
      "step": 405
    },
    {
      "epoch": 0.0019097434546600563,
      "grad_norm": 5.902981758117676,
      "learning_rate": 0.00016240000000000002,
      "loss": 0.268,
      "step": 406
    },
    {
      "epoch": 0.0019144472562725194,
      "grad_norm": 8.578977584838867,
      "learning_rate": 0.0001628,
      "loss": 1.5509,
      "step": 407
    },
    {
      "epoch": 0.0019191510578849826,
      "grad_norm": 1.3066927194595337,
      "learning_rate": 0.0001632,
      "loss": 0.1135,
      "step": 408
    },
    {
      "epoch": 0.0019238548594974459,
      "grad_norm": 1.2204082012176514,
      "learning_rate": 0.0001636,
      "loss": 0.1076,
      "step": 409
    },
    {
      "epoch": 0.001928558661109909,
      "grad_norm": 3.1726009845733643,
      "learning_rate": 0.000164,
      "loss": 0.1829,
      "step": 410
    },
    {
      "epoch": 0.0019332624627223722,
      "grad_norm": 10.586771011352539,
      "learning_rate": 0.0001644,
      "loss": 0.4915,
      "step": 411
    },
    {
      "epoch": 0.0019379662643348354,
      "grad_norm": 5.281284809112549,
      "learning_rate": 0.0001648,
      "loss": 0.5445,
      "step": 412
    },
    {
      "epoch": 0.0019426700659472987,
      "grad_norm": 7.744019985198975,
      "learning_rate": 0.0001652,
      "loss": 0.4866,
      "step": 413
    },
    {
      "epoch": 0.0019473738675597617,
      "grad_norm": 2.30348801612854,
      "learning_rate": 0.0001656,
      "loss": 0.1509,
      "step": 414
    },
    {
      "epoch": 0.001952077669172225,
      "grad_norm": 5.621163368225098,
      "learning_rate": 0.000166,
      "loss": 0.4085,
      "step": 415
    },
    {
      "epoch": 0.001956781470784688,
      "grad_norm": 2.718182325363159,
      "learning_rate": 0.0001664,
      "loss": 0.1989,
      "step": 416
    },
    {
      "epoch": 0.0019614852723971515,
      "grad_norm": 5.092797756195068,
      "learning_rate": 0.0001668,
      "loss": 0.6662,
      "step": 417
    },
    {
      "epoch": 0.0019661890740096145,
      "grad_norm": 6.925167083740234,
      "learning_rate": 0.0001672,
      "loss": 0.5721,
      "step": 418
    },
    {
      "epoch": 0.0019708928756220776,
      "grad_norm": 1.394405722618103,
      "learning_rate": 0.0001676,
      "loss": 0.0616,
      "step": 419
    },
    {
      "epoch": 0.001975596677234541,
      "grad_norm": 3.7499024868011475,
      "learning_rate": 0.000168,
      "loss": 0.2968,
      "step": 420
    },
    {
      "epoch": 0.001980300478847004,
      "grad_norm": 2.0088789463043213,
      "learning_rate": 0.0001684,
      "loss": 0.1763,
      "step": 421
    },
    {
      "epoch": 0.001985004280459467,
      "grad_norm": 3.8349266052246094,
      "learning_rate": 0.0001688,
      "loss": 0.3431,
      "step": 422
    },
    {
      "epoch": 0.0019897080820719306,
      "grad_norm": 1.7392799854278564,
      "learning_rate": 0.0001692,
      "loss": 0.164,
      "step": 423
    },
    {
      "epoch": 0.0019944118836843936,
      "grad_norm": 2.1135196685791016,
      "learning_rate": 0.0001696,
      "loss": 0.188,
      "step": 424
    },
    {
      "epoch": 0.001999115685296857,
      "grad_norm": 3.892944812774658,
      "learning_rate": 0.00017,
      "loss": 0.2384,
      "step": 425
    },
    {
      "epoch": 0.00200381948690932,
      "grad_norm": 0.41945555806159973,
      "learning_rate": 0.0001704,
      "loss": 0.0252,
      "step": 426
    },
    {
      "epoch": 0.002008523288521783,
      "grad_norm": 2.866692066192627,
      "learning_rate": 0.0001708,
      "loss": 0.3043,
      "step": 427
    },
    {
      "epoch": 0.0020132270901342467,
      "grad_norm": 3.9674036502838135,
      "learning_rate": 0.00017120000000000001,
      "loss": 0.45,
      "step": 428
    },
    {
      "epoch": 0.0020179308917467097,
      "grad_norm": 2.7942330837249756,
      "learning_rate": 0.0001716,
      "loss": 0.2072,
      "step": 429
    },
    {
      "epoch": 0.0020226346933591727,
      "grad_norm": 1.9509483575820923,
      "learning_rate": 0.000172,
      "loss": 0.1296,
      "step": 430
    },
    {
      "epoch": 0.0020273384949716362,
      "grad_norm": 6.762782573699951,
      "learning_rate": 0.00017240000000000002,
      "loss": 0.5934,
      "step": 431
    },
    {
      "epoch": 0.0020320422965840993,
      "grad_norm": 6.404508113861084,
      "learning_rate": 0.0001728,
      "loss": 0.6801,
      "step": 432
    },
    {
      "epoch": 0.0020367460981965623,
      "grad_norm": 1.2534981966018677,
      "learning_rate": 0.0001732,
      "loss": 0.1217,
      "step": 433
    },
    {
      "epoch": 0.0020414498998090258,
      "grad_norm": 1.5184433460235596,
      "learning_rate": 0.00017360000000000002,
      "loss": 0.0942,
      "step": 434
    },
    {
      "epoch": 0.002046153701421489,
      "grad_norm": 5.89815092086792,
      "learning_rate": 0.000174,
      "loss": 0.654,
      "step": 435
    },
    {
      "epoch": 0.002050857503033952,
      "grad_norm": 3.3252062797546387,
      "learning_rate": 0.0001744,
      "loss": 0.2026,
      "step": 436
    },
    {
      "epoch": 0.0020555613046464153,
      "grad_norm": 0.7325847744941711,
      "learning_rate": 0.00017480000000000002,
      "loss": 0.0299,
      "step": 437
    },
    {
      "epoch": 0.0020602651062588784,
      "grad_norm": 2.149791955947876,
      "learning_rate": 0.0001752,
      "loss": 0.1405,
      "step": 438
    },
    {
      "epoch": 0.002064968907871342,
      "grad_norm": 4.564706802368164,
      "learning_rate": 0.0001756,
      "loss": 0.593,
      "step": 439
    },
    {
      "epoch": 0.002069672709483805,
      "grad_norm": 0.5548848509788513,
      "learning_rate": 0.00017600000000000002,
      "loss": 0.0314,
      "step": 440
    },
    {
      "epoch": 0.002074376511096268,
      "grad_norm": 0.5786711573600769,
      "learning_rate": 0.0001764,
      "loss": 0.0227,
      "step": 441
    },
    {
      "epoch": 0.0020790803127087314,
      "grad_norm": 7.1305131912231445,
      "learning_rate": 0.00017680000000000001,
      "loss": 1.4823,
      "step": 442
    },
    {
      "epoch": 0.0020837841143211944,
      "grad_norm": 0.2096025049686432,
      "learning_rate": 0.0001772,
      "loss": 0.0069,
      "step": 443
    },
    {
      "epoch": 0.0020884879159336575,
      "grad_norm": 4.333567142486572,
      "learning_rate": 0.0001776,
      "loss": 0.2295,
      "step": 444
    },
    {
      "epoch": 0.002093191717546121,
      "grad_norm": 3.485567331314087,
      "learning_rate": 0.00017800000000000002,
      "loss": 0.3651,
      "step": 445
    },
    {
      "epoch": 0.002097895519158584,
      "grad_norm": 5.096225261688232,
      "learning_rate": 0.0001784,
      "loss": 0.6821,
      "step": 446
    },
    {
      "epoch": 0.002102599320771047,
      "grad_norm": 5.824506759643555,
      "learning_rate": 0.0001788,
      "loss": 0.7786,
      "step": 447
    },
    {
      "epoch": 0.0021073031223835105,
      "grad_norm": 0.23080496490001678,
      "learning_rate": 0.00017920000000000002,
      "loss": 0.0075,
      "step": 448
    },
    {
      "epoch": 0.0021120069239959735,
      "grad_norm": 1.3946672677993774,
      "learning_rate": 0.0001796,
      "loss": 0.0798,
      "step": 449
    },
    {
      "epoch": 0.0021167107256084366,
      "grad_norm": 4.821137428283691,
      "learning_rate": 0.00018,
      "loss": 0.5363,
      "step": 450
    },
    {
      "epoch": 0.0021214145272209,
      "grad_norm": 1.5111775398254395,
      "learning_rate": 0.00018040000000000002,
      "loss": 0.1087,
      "step": 451
    },
    {
      "epoch": 0.002126118328833363,
      "grad_norm": 6.121540069580078,
      "learning_rate": 0.0001808,
      "loss": 0.2188,
      "step": 452
    },
    {
      "epoch": 0.002130822130445826,
      "grad_norm": 6.667111873626709,
      "learning_rate": 0.0001812,
      "loss": 0.8351,
      "step": 453
    },
    {
      "epoch": 0.0021355259320582896,
      "grad_norm": 1.9470659494400024,
      "learning_rate": 0.00018160000000000002,
      "loss": 0.0979,
      "step": 454
    },
    {
      "epoch": 0.0021402297336707526,
      "grad_norm": 1.9844422340393066,
      "learning_rate": 0.000182,
      "loss": 0.115,
      "step": 455
    },
    {
      "epoch": 0.002144933535283216,
      "grad_norm": 3.2512948513031006,
      "learning_rate": 0.00018240000000000002,
      "loss": 0.1841,
      "step": 456
    },
    {
      "epoch": 0.002149637336895679,
      "grad_norm": 1.5105171203613281,
      "learning_rate": 0.00018280000000000003,
      "loss": 0.1943,
      "step": 457
    },
    {
      "epoch": 0.002154341138508142,
      "grad_norm": 1.6252412796020508,
      "learning_rate": 0.0001832,
      "loss": 0.0982,
      "step": 458
    },
    {
      "epoch": 0.0021590449401206056,
      "grad_norm": 2.7867486476898193,
      "learning_rate": 0.00018360000000000002,
      "loss": 0.2819,
      "step": 459
    },
    {
      "epoch": 0.0021637487417330687,
      "grad_norm": 3.677786350250244,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.3459,
      "step": 460
    },
    {
      "epoch": 0.0021684525433455317,
      "grad_norm": 1.1252609491348267,
      "learning_rate": 0.0001844,
      "loss": 0.1023,
      "step": 461
    },
    {
      "epoch": 0.002173156344957995,
      "grad_norm": 0.34616151452064514,
      "learning_rate": 0.00018480000000000002,
      "loss": 0.019,
      "step": 462
    },
    {
      "epoch": 0.0021778601465704582,
      "grad_norm": 1.7373374700546265,
      "learning_rate": 0.00018520000000000003,
      "loss": 0.2261,
      "step": 463
    },
    {
      "epoch": 0.0021825639481829213,
      "grad_norm": 0.8922981023788452,
      "learning_rate": 0.0001856,
      "loss": 0.0461,
      "step": 464
    },
    {
      "epoch": 0.0021872677497953848,
      "grad_norm": 4.738465785980225,
      "learning_rate": 0.00018600000000000002,
      "loss": 0.2765,
      "step": 465
    },
    {
      "epoch": 0.002191971551407848,
      "grad_norm": 4.691183567047119,
      "learning_rate": 0.00018640000000000003,
      "loss": 0.992,
      "step": 466
    },
    {
      "epoch": 0.002196675353020311,
      "grad_norm": 3.280848503112793,
      "learning_rate": 0.00018680000000000001,
      "loss": 0.1805,
      "step": 467
    },
    {
      "epoch": 0.0022013791546327743,
      "grad_norm": 2.4245572090148926,
      "learning_rate": 0.00018720000000000002,
      "loss": 0.2653,
      "step": 468
    },
    {
      "epoch": 0.0022060829562452373,
      "grad_norm": 4.130688667297363,
      "learning_rate": 0.0001876,
      "loss": 0.2599,
      "step": 469
    },
    {
      "epoch": 0.0022107867578577004,
      "grad_norm": 0.6237450242042542,
      "learning_rate": 0.000188,
      "loss": 0.021,
      "step": 470
    },
    {
      "epoch": 0.002215490559470164,
      "grad_norm": 0.15407976508140564,
      "learning_rate": 0.0001884,
      "loss": 0.0045,
      "step": 471
    },
    {
      "epoch": 0.002220194361082627,
      "grad_norm": 5.603780746459961,
      "learning_rate": 0.0001888,
      "loss": 0.5043,
      "step": 472
    },
    {
      "epoch": 0.0022248981626950904,
      "grad_norm": 9.7479829788208,
      "learning_rate": 0.0001892,
      "loss": 0.9334,
      "step": 473
    },
    {
      "epoch": 0.0022296019643075534,
      "grad_norm": 6.137670993804932,
      "learning_rate": 0.0001896,
      "loss": 0.3281,
      "step": 474
    },
    {
      "epoch": 0.0022343057659200164,
      "grad_norm": 5.173526287078857,
      "learning_rate": 0.00019,
      "loss": 0.4695,
      "step": 475
    },
    {
      "epoch": 0.00223900956753248,
      "grad_norm": 3.5330491065979004,
      "learning_rate": 0.0001904,
      "loss": 0.3578,
      "step": 476
    },
    {
      "epoch": 0.002243713369144943,
      "grad_norm": 3.5067591667175293,
      "learning_rate": 0.0001908,
      "loss": 0.2513,
      "step": 477
    },
    {
      "epoch": 0.002248417170757406,
      "grad_norm": 2.5952534675598145,
      "learning_rate": 0.0001912,
      "loss": 0.1811,
      "step": 478
    },
    {
      "epoch": 0.0022531209723698695,
      "grad_norm": 7.664407730102539,
      "learning_rate": 0.0001916,
      "loss": 0.55,
      "step": 479
    },
    {
      "epoch": 0.0022578247739823325,
      "grad_norm": 12.325654029846191,
      "learning_rate": 0.000192,
      "loss": 0.419,
      "step": 480
    },
    {
      "epoch": 0.0022625285755947955,
      "grad_norm": 5.907459259033203,
      "learning_rate": 0.00019240000000000001,
      "loss": 0.6165,
      "step": 481
    },
    {
      "epoch": 0.002267232377207259,
      "grad_norm": 2.6723532676696777,
      "learning_rate": 0.0001928,
      "loss": 0.2301,
      "step": 482
    },
    {
      "epoch": 0.002271936178819722,
      "grad_norm": 2.586239814758301,
      "learning_rate": 0.0001932,
      "loss": 0.2677,
      "step": 483
    },
    {
      "epoch": 0.002276639980432185,
      "grad_norm": 4.0440144538879395,
      "learning_rate": 0.00019360000000000002,
      "loss": 0.5411,
      "step": 484
    },
    {
      "epoch": 0.0022813437820446486,
      "grad_norm": 3.957775354385376,
      "learning_rate": 0.000194,
      "loss": 0.3404,
      "step": 485
    },
    {
      "epoch": 0.0022860475836571116,
      "grad_norm": 1.59304678440094,
      "learning_rate": 0.0001944,
      "loss": 0.085,
      "step": 486
    },
    {
      "epoch": 0.002290751385269575,
      "grad_norm": 2.6465861797332764,
      "learning_rate": 0.0001948,
      "loss": 0.1939,
      "step": 487
    },
    {
      "epoch": 0.002295455186882038,
      "grad_norm": 1.9595757722854614,
      "learning_rate": 0.0001952,
      "loss": 0.1307,
      "step": 488
    },
    {
      "epoch": 0.002300158988494501,
      "grad_norm": 4.419256210327148,
      "learning_rate": 0.0001956,
      "loss": 0.495,
      "step": 489
    },
    {
      "epoch": 0.0023048627901069646,
      "grad_norm": 3.186998128890991,
      "learning_rate": 0.000196,
      "loss": 0.2656,
      "step": 490
    },
    {
      "epoch": 0.0023095665917194277,
      "grad_norm": 3.9447336196899414,
      "learning_rate": 0.0001964,
      "loss": 0.5269,
      "step": 491
    },
    {
      "epoch": 0.0023142703933318907,
      "grad_norm": 1.285272479057312,
      "learning_rate": 0.0001968,
      "loss": 0.0928,
      "step": 492
    },
    {
      "epoch": 0.002318974194944354,
      "grad_norm": 5.694546222686768,
      "learning_rate": 0.0001972,
      "loss": 0.9207,
      "step": 493
    },
    {
      "epoch": 0.0023236779965568172,
      "grad_norm": 7.832240581512451,
      "learning_rate": 0.0001976,
      "loss": 0.8962,
      "step": 494
    },
    {
      "epoch": 0.0023283817981692803,
      "grad_norm": 6.968967914581299,
      "learning_rate": 0.00019800000000000002,
      "loss": 1.1535,
      "step": 495
    },
    {
      "epoch": 0.0023330855997817437,
      "grad_norm": 3.456744432449341,
      "learning_rate": 0.0001984,
      "loss": 0.4575,
      "step": 496
    },
    {
      "epoch": 0.0023377894013942068,
      "grad_norm": 3.8342556953430176,
      "learning_rate": 0.0001988,
      "loss": 0.6583,
      "step": 497
    },
    {
      "epoch": 0.00234249320300667,
      "grad_norm": 3.798800230026245,
      "learning_rate": 0.00019920000000000002,
      "loss": 0.6581,
      "step": 498
    },
    {
      "epoch": 0.0023471970046191333,
      "grad_norm": 1.041553020477295,
      "learning_rate": 0.0001996,
      "loss": 0.1214,
      "step": 499
    },
    {
      "epoch": 0.0023519008062315963,
      "grad_norm": 1.7373206615447998,
      "learning_rate": 0.0002,
      "loss": 0.1671,
      "step": 500
    },
    {
      "epoch": 0.0023566046078440594,
      "grad_norm": 2.733170509338379,
      "learning_rate": 0.00019999905702188653,
      "loss": 0.2806,
      "step": 501
    },
    {
      "epoch": 0.002361308409456523,
      "grad_norm": 1.8406935930252075,
      "learning_rate": 0.00019999811404377305,
      "loss": 0.1807,
      "step": 502
    },
    {
      "epoch": 0.002366012211068986,
      "grad_norm": 0.8960445523262024,
      "learning_rate": 0.0001999971710656596,
      "loss": 0.0481,
      "step": 503
    },
    {
      "epoch": 0.0023707160126814493,
      "grad_norm": 2.177539110183716,
      "learning_rate": 0.0001999962280875461,
      "loss": 0.2986,
      "step": 504
    },
    {
      "epoch": 0.0023754198142939124,
      "grad_norm": 3.447396755218506,
      "learning_rate": 0.00019999528510943263,
      "loss": 0.3535,
      "step": 505
    },
    {
      "epoch": 0.0023801236159063754,
      "grad_norm": 1.0203230381011963,
      "learning_rate": 0.00019999434213131912,
      "loss": 0.0744,
      "step": 506
    },
    {
      "epoch": 0.002384827417518839,
      "grad_norm": 2.2024595737457275,
      "learning_rate": 0.00019999339915320567,
      "loss": 0.2709,
      "step": 507
    },
    {
      "epoch": 0.002389531219131302,
      "grad_norm": 2.625650405883789,
      "learning_rate": 0.00019999245617509219,
      "loss": 0.1525,
      "step": 508
    },
    {
      "epoch": 0.002394235020743765,
      "grad_norm": 1.6832960844039917,
      "learning_rate": 0.0001999915131969787,
      "loss": 0.1382,
      "step": 509
    },
    {
      "epoch": 0.0023989388223562285,
      "grad_norm": 4.251504898071289,
      "learning_rate": 0.00019999057021886522,
      "loss": 0.2626,
      "step": 510
    },
    {
      "epoch": 0.0024036426239686915,
      "grad_norm": 3.4471030235290527,
      "learning_rate": 0.00019998962724075174,
      "loss": 0.2386,
      "step": 511
    },
    {
      "epoch": 0.0024083464255811545,
      "grad_norm": 4.806637287139893,
      "learning_rate": 0.0001999886842626383,
      "loss": 0.3893,
      "step": 512
    },
    {
      "epoch": 0.002413050227193618,
      "grad_norm": 6.059075832366943,
      "learning_rate": 0.0001999877412845248,
      "loss": 0.3727,
      "step": 513
    },
    {
      "epoch": 0.002417754028806081,
      "grad_norm": 2.395587921142578,
      "learning_rate": 0.00019998679830641132,
      "loss": 0.208,
      "step": 514
    },
    {
      "epoch": 0.002422457830418544,
      "grad_norm": 3.651764392852783,
      "learning_rate": 0.00019998585532829784,
      "loss": 0.2421,
      "step": 515
    },
    {
      "epoch": 0.0024271616320310076,
      "grad_norm": 6.9652910232543945,
      "learning_rate": 0.00019998491235018436,
      "loss": 0.7684,
      "step": 516
    },
    {
      "epoch": 0.0024318654336434706,
      "grad_norm": 6.124037265777588,
      "learning_rate": 0.00019998396937207088,
      "loss": 0.5321,
      "step": 517
    },
    {
      "epoch": 0.0024365692352559336,
      "grad_norm": 4.278610706329346,
      "learning_rate": 0.0001999830263939574,
      "loss": 0.292,
      "step": 518
    },
    {
      "epoch": 0.002441273036868397,
      "grad_norm": 1.1434571743011475,
      "learning_rate": 0.00019998208341584392,
      "loss": 0.0476,
      "step": 519
    },
    {
      "epoch": 0.00244597683848086,
      "grad_norm": 0.5602289438247681,
      "learning_rate": 0.00019998114043773044,
      "loss": 0.0371,
      "step": 520
    },
    {
      "epoch": 0.0024506806400933236,
      "grad_norm": 5.589516639709473,
      "learning_rate": 0.00019998019745961698,
      "loss": 0.9811,
      "step": 521
    },
    {
      "epoch": 0.0024553844417057867,
      "grad_norm": 7.275086402893066,
      "learning_rate": 0.0001999792544815035,
      "loss": 0.8024,
      "step": 522
    },
    {
      "epoch": 0.0024600882433182497,
      "grad_norm": 3.6631767749786377,
      "learning_rate": 0.00019997831150339002,
      "loss": 0.2967,
      "step": 523
    },
    {
      "epoch": 0.002464792044930713,
      "grad_norm": 1.0775015354156494,
      "learning_rate": 0.00019997736852527654,
      "loss": 0.0552,
      "step": 524
    },
    {
      "epoch": 0.002469495846543176,
      "grad_norm": 1.2534099817276,
      "learning_rate": 0.00019997642554716308,
      "loss": 0.0823,
      "step": 525
    },
    {
      "epoch": 0.0024741996481556392,
      "grad_norm": 1.1758276224136353,
      "learning_rate": 0.00019997548256904958,
      "loss": 0.0907,
      "step": 526
    },
    {
      "epoch": 0.0024789034497681027,
      "grad_norm": 1.0112347602844238,
      "learning_rate": 0.0001999745395909361,
      "loss": 0.0559,
      "step": 527
    },
    {
      "epoch": 0.0024836072513805658,
      "grad_norm": 4.944222450256348,
      "learning_rate": 0.0001999735966128226,
      "loss": 0.5837,
      "step": 528
    },
    {
      "epoch": 0.002488311052993029,
      "grad_norm": 1.8065460920333862,
      "learning_rate": 0.00019997265363470913,
      "loss": 0.1347,
      "step": 529
    },
    {
      "epoch": 0.0024930148546054923,
      "grad_norm": 0.922940731048584,
      "learning_rate": 0.00019997171065659568,
      "loss": 0.0388,
      "step": 530
    },
    {
      "epoch": 0.0024977186562179553,
      "grad_norm": 3.3904435634613037,
      "learning_rate": 0.0001999707676784822,
      "loss": 0.3006,
      "step": 531
    },
    {
      "epoch": 0.0025024224578304183,
      "grad_norm": 4.31658411026001,
      "learning_rate": 0.00019996982470036871,
      "loss": 0.4903,
      "step": 532
    },
    {
      "epoch": 0.002507126259442882,
      "grad_norm": 5.422018527984619,
      "learning_rate": 0.00019996888172225523,
      "loss": 0.5437,
      "step": 533
    },
    {
      "epoch": 0.002511830061055345,
      "grad_norm": 2.776301383972168,
      "learning_rate": 0.00019996793874414175,
      "loss": 0.168,
      "step": 534
    },
    {
      "epoch": 0.0025165338626678083,
      "grad_norm": 2.6906278133392334,
      "learning_rate": 0.0001999669957660283,
      "loss": 0.1889,
      "step": 535
    },
    {
      "epoch": 0.0025212376642802714,
      "grad_norm": 10.224071502685547,
      "learning_rate": 0.00019996605278791482,
      "loss": 1.355,
      "step": 536
    },
    {
      "epoch": 0.0025259414658927344,
      "grad_norm": 10.28413200378418,
      "learning_rate": 0.0001999651098098013,
      "loss": 1.1856,
      "step": 537
    },
    {
      "epoch": 0.002530645267505198,
      "grad_norm": 1.4848287105560303,
      "learning_rate": 0.00019996416683168783,
      "loss": 0.0659,
      "step": 538
    },
    {
      "epoch": 0.002535349069117661,
      "grad_norm": 2.585472583770752,
      "learning_rate": 0.00019996322385357437,
      "loss": 0.2186,
      "step": 539
    },
    {
      "epoch": 0.002540052870730124,
      "grad_norm": 1.6322215795516968,
      "learning_rate": 0.0001999622808754609,
      "loss": 0.0678,
      "step": 540
    },
    {
      "epoch": 0.0025447566723425874,
      "grad_norm": 1.3138689994812012,
      "learning_rate": 0.0001999613378973474,
      "loss": 0.0577,
      "step": 541
    },
    {
      "epoch": 0.0025494604739550505,
      "grad_norm": 5.464565753936768,
      "learning_rate": 0.00019996039491923393,
      "loss": 0.7226,
      "step": 542
    },
    {
      "epoch": 0.0025541642755675135,
      "grad_norm": 2.3187978267669678,
      "learning_rate": 0.00019995945194112045,
      "loss": 0.1611,
      "step": 543
    },
    {
      "epoch": 0.002558868077179977,
      "grad_norm": 5.241946220397949,
      "learning_rate": 0.000199958508963007,
      "loss": 0.7109,
      "step": 544
    },
    {
      "epoch": 0.00256357187879244,
      "grad_norm": 7.148946285247803,
      "learning_rate": 0.0001999575659848935,
      "loss": 0.9148,
      "step": 545
    },
    {
      "epoch": 0.002568275680404903,
      "grad_norm": 4.584968566894531,
      "learning_rate": 0.00019995662300678003,
      "loss": 0.8222,
      "step": 546
    },
    {
      "epoch": 0.0025729794820173665,
      "grad_norm": 1.0163313150405884,
      "learning_rate": 0.00019995568002866655,
      "loss": 0.1344,
      "step": 547
    },
    {
      "epoch": 0.0025776832836298296,
      "grad_norm": 2.384119987487793,
      "learning_rate": 0.00019995473705055307,
      "loss": 0.4091,
      "step": 548
    },
    {
      "epoch": 0.0025823870852422926,
      "grad_norm": 1.4941052198410034,
      "learning_rate": 0.00019995379407243959,
      "loss": 0.2154,
      "step": 549
    },
    {
      "epoch": 0.002587090886854756,
      "grad_norm": 1.1270636320114136,
      "learning_rate": 0.0001999528510943261,
      "loss": 0.0647,
      "step": 550
    },
    {
      "epoch": 0.002591794688467219,
      "grad_norm": 1.4387873411178589,
      "learning_rate": 0.00019995190811621262,
      "loss": 0.1066,
      "step": 551
    },
    {
      "epoch": 0.0025964984900796826,
      "grad_norm": 7.61419677734375,
      "learning_rate": 0.00019995096513809914,
      "loss": 0.9263,
      "step": 552
    },
    {
      "epoch": 0.0026012022916921456,
      "grad_norm": 2.3461380004882812,
      "learning_rate": 0.0001999500221599857,
      "loss": 0.1271,
      "step": 553
    },
    {
      "epoch": 0.0026059060933046087,
      "grad_norm": 3.3745126724243164,
      "learning_rate": 0.0001999490791818722,
      "loss": 0.3913,
      "step": 554
    },
    {
      "epoch": 0.002610609894917072,
      "grad_norm": 1.1886646747589111,
      "learning_rate": 0.00019994813620375872,
      "loss": 0.1279,
      "step": 555
    },
    {
      "epoch": 0.002615313696529535,
      "grad_norm": 5.635507106781006,
      "learning_rate": 0.00019994719322564524,
      "loss": 0.6348,
      "step": 556
    },
    {
      "epoch": 0.0026200174981419982,
      "grad_norm": 3.9375510215759277,
      "learning_rate": 0.00019994625024753176,
      "loss": 0.2696,
      "step": 557
    },
    {
      "epoch": 0.0026247212997544617,
      "grad_norm": 6.265040874481201,
      "learning_rate": 0.00019994530726941828,
      "loss": 0.8545,
      "step": 558
    },
    {
      "epoch": 0.0026294251013669247,
      "grad_norm": 5.659762859344482,
      "learning_rate": 0.0001999443642913048,
      "loss": 0.5455,
      "step": 559
    },
    {
      "epoch": 0.0026341289029793878,
      "grad_norm": 1.0451126098632812,
      "learning_rate": 0.00019994342131319132,
      "loss": 0.0981,
      "step": 560
    },
    {
      "epoch": 0.0026388327045918513,
      "grad_norm": 2.7365615367889404,
      "learning_rate": 0.00019994247833507784,
      "loss": 0.1385,
      "step": 561
    },
    {
      "epoch": 0.0026435365062043143,
      "grad_norm": 3.3486101627349854,
      "learning_rate": 0.00019994153535696438,
      "loss": 0.3833,
      "step": 562
    },
    {
      "epoch": 0.0026482403078167773,
      "grad_norm": 1.8383309841156006,
      "learning_rate": 0.0001999405923788509,
      "loss": 0.0886,
      "step": 563
    },
    {
      "epoch": 0.002652944109429241,
      "grad_norm": 5.769720077514648,
      "learning_rate": 0.00019993964940073742,
      "loss": 0.4087,
      "step": 564
    },
    {
      "epoch": 0.002657647911041704,
      "grad_norm": 3.2684357166290283,
      "learning_rate": 0.00019993870642262394,
      "loss": 0.3887,
      "step": 565
    },
    {
      "epoch": 0.002662351712654167,
      "grad_norm": 5.807337760925293,
      "learning_rate": 0.00019993776344451048,
      "loss": 0.6165,
      "step": 566
    },
    {
      "epoch": 0.0026670555142666304,
      "grad_norm": 5.6973958015441895,
      "learning_rate": 0.000199936820466397,
      "loss": 0.2833,
      "step": 567
    },
    {
      "epoch": 0.0026717593158790934,
      "grad_norm": 10.67391300201416,
      "learning_rate": 0.0001999358774882835,
      "loss": 0.8161,
      "step": 568
    },
    {
      "epoch": 0.002676463117491557,
      "grad_norm": 3.006187677383423,
      "learning_rate": 0.00019993493451017,
      "loss": 0.1708,
      "step": 569
    },
    {
      "epoch": 0.00268116691910402,
      "grad_norm": 3.5090417861938477,
      "learning_rate": 0.00019993399153205653,
      "loss": 0.2324,
      "step": 570
    },
    {
      "epoch": 0.002685870720716483,
      "grad_norm": 1.9104583263397217,
      "learning_rate": 0.00019993304855394308,
      "loss": 0.1244,
      "step": 571
    },
    {
      "epoch": 0.0026905745223289464,
      "grad_norm": 4.58314323425293,
      "learning_rate": 0.0001999321055758296,
      "loss": 0.6577,
      "step": 572
    },
    {
      "epoch": 0.0026952783239414095,
      "grad_norm": 3.729210615158081,
      "learning_rate": 0.00019993116259771611,
      "loss": 0.2797,
      "step": 573
    },
    {
      "epoch": 0.0026999821255538725,
      "grad_norm": 0.3151964545249939,
      "learning_rate": 0.00019993021961960263,
      "loss": 0.0111,
      "step": 574
    },
    {
      "epoch": 0.002704685927166336,
      "grad_norm": 3.0578484535217285,
      "learning_rate": 0.00019992927664148918,
      "loss": 0.2106,
      "step": 575
    },
    {
      "epoch": 0.002709389728778799,
      "grad_norm": 0.36270835995674133,
      "learning_rate": 0.0001999283336633757,
      "loss": 0.0131,
      "step": 576
    },
    {
      "epoch": 0.002714093530391262,
      "grad_norm": 4.578805446624756,
      "learning_rate": 0.00019992739068526222,
      "loss": 0.6182,
      "step": 577
    },
    {
      "epoch": 0.0027187973320037255,
      "grad_norm": 1.4137585163116455,
      "learning_rate": 0.00019992644770714873,
      "loss": 0.1828,
      "step": 578
    },
    {
      "epoch": 0.0027235011336161886,
      "grad_norm": 3.771151304244995,
      "learning_rate": 0.00019992550472903525,
      "loss": 0.5986,
      "step": 579
    },
    {
      "epoch": 0.0027282049352286516,
      "grad_norm": 5.3465495109558105,
      "learning_rate": 0.00019992456175092177,
      "loss": 0.598,
      "step": 580
    },
    {
      "epoch": 0.002732908736841115,
      "grad_norm": 2.330317497253418,
      "learning_rate": 0.0001999236187728083,
      "loss": 0.2394,
      "step": 581
    },
    {
      "epoch": 0.002737612538453578,
      "grad_norm": 2.4833929538726807,
      "learning_rate": 0.0001999226757946948,
      "loss": 0.1781,
      "step": 582
    },
    {
      "epoch": 0.0027423163400660416,
      "grad_norm": 1.0978729724884033,
      "learning_rate": 0.00019992173281658133,
      "loss": 0.0827,
      "step": 583
    },
    {
      "epoch": 0.0027470201416785046,
      "grad_norm": 2.2897653579711914,
      "learning_rate": 0.00019992078983846785,
      "loss": 0.1497,
      "step": 584
    },
    {
      "epoch": 0.0027517239432909677,
      "grad_norm": 3.143242359161377,
      "learning_rate": 0.0001999198468603544,
      "loss": 0.4842,
      "step": 585
    },
    {
      "epoch": 0.002756427744903431,
      "grad_norm": 2.6832809448242188,
      "learning_rate": 0.0001999189038822409,
      "loss": 0.2921,
      "step": 586
    },
    {
      "epoch": 0.002761131546515894,
      "grad_norm": 5.166665554046631,
      "learning_rate": 0.00019991796090412743,
      "loss": 0.5207,
      "step": 587
    },
    {
      "epoch": 0.002765835348128357,
      "grad_norm": 0.8477105498313904,
      "learning_rate": 0.00019991701792601395,
      "loss": 0.0472,
      "step": 588
    },
    {
      "epoch": 0.0027705391497408207,
      "grad_norm": 1.8871163129806519,
      "learning_rate": 0.00019991607494790047,
      "loss": 0.1533,
      "step": 589
    },
    {
      "epoch": 0.0027752429513532837,
      "grad_norm": 1.741644024848938,
      "learning_rate": 0.00019991513196978699,
      "loss": 0.1795,
      "step": 590
    },
    {
      "epoch": 0.0027799467529657468,
      "grad_norm": 1.1360101699829102,
      "learning_rate": 0.0001999141889916735,
      "loss": 0.1051,
      "step": 591
    },
    {
      "epoch": 0.0027846505545782102,
      "grad_norm": 2.9317915439605713,
      "learning_rate": 0.00019991324601356002,
      "loss": 0.2745,
      "step": 592
    },
    {
      "epoch": 0.0027893543561906733,
      "grad_norm": 8.545475959777832,
      "learning_rate": 0.00019991230303544654,
      "loss": 0.2086,
      "step": 593
    },
    {
      "epoch": 0.0027940581578031363,
      "grad_norm": 1.1745413541793823,
      "learning_rate": 0.0001999113600573331,
      "loss": 0.0503,
      "step": 594
    },
    {
      "epoch": 0.0027987619594156,
      "grad_norm": 1.6579742431640625,
      "learning_rate": 0.0001999104170792196,
      "loss": 0.2281,
      "step": 595
    },
    {
      "epoch": 0.002803465761028063,
      "grad_norm": 4.4159626960754395,
      "learning_rate": 0.00019990947410110612,
      "loss": 0.257,
      "step": 596
    },
    {
      "epoch": 0.002808169562640526,
      "grad_norm": 1.1932029724121094,
      "learning_rate": 0.00019990853112299264,
      "loss": 0.0608,
      "step": 597
    },
    {
      "epoch": 0.0028128733642529893,
      "grad_norm": 1.2683508396148682,
      "learning_rate": 0.0001999075881448792,
      "loss": 0.0994,
      "step": 598
    },
    {
      "epoch": 0.0028175771658654524,
      "grad_norm": 6.023769378662109,
      "learning_rate": 0.00019990664516676568,
      "loss": 0.9381,
      "step": 599
    },
    {
      "epoch": 0.002822280967477916,
      "grad_norm": 6.486326217651367,
      "learning_rate": 0.0001999057021886522,
      "loss": 0.7768,
      "step": 600
    },
    {
      "epoch": 0.002826984769090379,
      "grad_norm": 5.9107489585876465,
      "learning_rate": 0.00019990475921053872,
      "loss": 0.3047,
      "step": 601
    },
    {
      "epoch": 0.002831688570702842,
      "grad_norm": 1.7816431522369385,
      "learning_rate": 0.00019990381623242524,
      "loss": 0.1483,
      "step": 602
    },
    {
      "epoch": 0.0028363923723153054,
      "grad_norm": 5.979990482330322,
      "learning_rate": 0.00019990287325431178,
      "loss": 0.308,
      "step": 603
    },
    {
      "epoch": 0.0028410961739277684,
      "grad_norm": 2.1106746196746826,
      "learning_rate": 0.0001999019302761983,
      "loss": 0.0951,
      "step": 604
    },
    {
      "epoch": 0.0028457999755402315,
      "grad_norm": 5.376126766204834,
      "learning_rate": 0.00019990098729808482,
      "loss": 0.2781,
      "step": 605
    },
    {
      "epoch": 0.002850503777152695,
      "grad_norm": 3.920464277267456,
      "learning_rate": 0.00019990004431997134,
      "loss": 0.5376,
      "step": 606
    },
    {
      "epoch": 0.002855207578765158,
      "grad_norm": 1.4465391635894775,
      "learning_rate": 0.00019989910134185788,
      "loss": 0.1126,
      "step": 607
    },
    {
      "epoch": 0.002859911380377621,
      "grad_norm": 3.4608986377716064,
      "learning_rate": 0.0001998981583637444,
      "loss": 0.3299,
      "step": 608
    },
    {
      "epoch": 0.0028646151819900845,
      "grad_norm": 6.820397853851318,
      "learning_rate": 0.00019989721538563092,
      "loss": 0.5168,
      "step": 609
    },
    {
      "epoch": 0.0028693189836025475,
      "grad_norm": 7.750339031219482,
      "learning_rate": 0.00019989627240751744,
      "loss": 1.1041,
      "step": 610
    },
    {
      "epoch": 0.0028740227852150106,
      "grad_norm": 3.512814521789551,
      "learning_rate": 0.00019989532942940393,
      "loss": 0.4075,
      "step": 611
    },
    {
      "epoch": 0.002878726586827474,
      "grad_norm": 3.448805809020996,
      "learning_rate": 0.00019989438645129048,
      "loss": 0.3298,
      "step": 612
    },
    {
      "epoch": 0.002883430388439937,
      "grad_norm": 0.8876003623008728,
      "learning_rate": 0.000199893443473177,
      "loss": 0.0687,
      "step": 613
    },
    {
      "epoch": 0.0028881341900524,
      "grad_norm": 2.7814791202545166,
      "learning_rate": 0.00019989250049506351,
      "loss": 0.2559,
      "step": 614
    },
    {
      "epoch": 0.0028928379916648636,
      "grad_norm": 4.265305042266846,
      "learning_rate": 0.00019989155751695003,
      "loss": 0.4086,
      "step": 615
    },
    {
      "epoch": 0.0028975417932773266,
      "grad_norm": 2.155289888381958,
      "learning_rate": 0.00019989061453883658,
      "loss": 0.1587,
      "step": 616
    },
    {
      "epoch": 0.00290224559488979,
      "grad_norm": 2.114879846572876,
      "learning_rate": 0.0001998896715607231,
      "loss": 0.1158,
      "step": 617
    },
    {
      "epoch": 0.002906949396502253,
      "grad_norm": 4.858074188232422,
      "learning_rate": 0.00019988872858260962,
      "loss": 0.6083,
      "step": 618
    },
    {
      "epoch": 0.002911653198114716,
      "grad_norm": 1.7556989192962646,
      "learning_rate": 0.00019988778560449613,
      "loss": 0.1904,
      "step": 619
    },
    {
      "epoch": 0.0029163569997271797,
      "grad_norm": 5.425992012023926,
      "learning_rate": 0.00019988684262638265,
      "loss": 0.2776,
      "step": 620
    },
    {
      "epoch": 0.0029210608013396427,
      "grad_norm": 2.372183322906494,
      "learning_rate": 0.00019988589964826917,
      "loss": 0.1843,
      "step": 621
    },
    {
      "epoch": 0.0029257646029521057,
      "grad_norm": 4.964325428009033,
      "learning_rate": 0.0001998849566701557,
      "loss": 0.3778,
      "step": 622
    },
    {
      "epoch": 0.0029304684045645692,
      "grad_norm": 2.822535991668701,
      "learning_rate": 0.0001998840136920422,
      "loss": 0.2228,
      "step": 623
    },
    {
      "epoch": 0.0029351722061770323,
      "grad_norm": 5.761855602264404,
      "learning_rate": 0.00019988307071392873,
      "loss": 1.0201,
      "step": 624
    },
    {
      "epoch": 0.0029398760077894953,
      "grad_norm": 4.019589424133301,
      "learning_rate": 0.00019988212773581527,
      "loss": 0.3764,
      "step": 625
    },
    {
      "epoch": 0.0029445798094019588,
      "grad_norm": 1.0415745973587036,
      "learning_rate": 0.0001998811847577018,
      "loss": 0.0586,
      "step": 626
    },
    {
      "epoch": 0.002949283611014422,
      "grad_norm": 4.722923278808594,
      "learning_rate": 0.0001998802417795883,
      "loss": 0.3379,
      "step": 627
    },
    {
      "epoch": 0.002953987412626885,
      "grad_norm": 5.970535755157471,
      "learning_rate": 0.00019987929880147483,
      "loss": 0.7326,
      "step": 628
    },
    {
      "epoch": 0.0029586912142393483,
      "grad_norm": 2.435281276702881,
      "learning_rate": 0.00019987835582336135,
      "loss": 0.3266,
      "step": 629
    },
    {
      "epoch": 0.0029633950158518114,
      "grad_norm": 1.466345191001892,
      "learning_rate": 0.00019987741284524787,
      "loss": 0.0475,
      "step": 630
    },
    {
      "epoch": 0.002968098817464275,
      "grad_norm": 0.9785760641098022,
      "learning_rate": 0.00019987646986713439,
      "loss": 0.0651,
      "step": 631
    },
    {
      "epoch": 0.002972802619076738,
      "grad_norm": 2.968933343887329,
      "learning_rate": 0.0001998755268890209,
      "loss": 0.3145,
      "step": 632
    },
    {
      "epoch": 0.002977506420689201,
      "grad_norm": 0.6597616672515869,
      "learning_rate": 0.00019987458391090742,
      "loss": 0.0372,
      "step": 633
    },
    {
      "epoch": 0.0029822102223016644,
      "grad_norm": 1.2770494222640991,
      "learning_rate": 0.00019987364093279394,
      "loss": 0.0838,
      "step": 634
    },
    {
      "epoch": 0.0029869140239141274,
      "grad_norm": 3.5290069580078125,
      "learning_rate": 0.0001998726979546805,
      "loss": 0.3791,
      "step": 635
    },
    {
      "epoch": 0.0029916178255265905,
      "grad_norm": 5.321798324584961,
      "learning_rate": 0.000199871754976567,
      "loss": 0.6737,
      "step": 636
    },
    {
      "epoch": 0.002996321627139054,
      "grad_norm": 7.353355407714844,
      "learning_rate": 0.00019987081199845352,
      "loss": 0.9599,
      "step": 637
    },
    {
      "epoch": 0.003001025428751517,
      "grad_norm": 4.455419063568115,
      "learning_rate": 0.00019986986902034004,
      "loss": 0.4745,
      "step": 638
    },
    {
      "epoch": 0.00300572923036398,
      "grad_norm": 5.390873908996582,
      "learning_rate": 0.0001998689260422266,
      "loss": 0.7398,
      "step": 639
    },
    {
      "epoch": 0.0030104330319764435,
      "grad_norm": 5.923178195953369,
      "learning_rate": 0.0001998679830641131,
      "loss": 0.697,
      "step": 640
    },
    {
      "epoch": 0.0030151368335889065,
      "grad_norm": 0.9895789623260498,
      "learning_rate": 0.0001998670400859996,
      "loss": 0.0672,
      "step": 641
    },
    {
      "epoch": 0.0030198406352013696,
      "grad_norm": 4.6699981689453125,
      "learning_rate": 0.00019986609710788612,
      "loss": 0.3894,
      "step": 642
    },
    {
      "epoch": 0.003024544436813833,
      "grad_norm": 2.9827492237091064,
      "learning_rate": 0.00019986515412977264,
      "loss": 0.2429,
      "step": 643
    },
    {
      "epoch": 0.003029248238426296,
      "grad_norm": 4.89498233795166,
      "learning_rate": 0.00019986421115165918,
      "loss": 0.4751,
      "step": 644
    },
    {
      "epoch": 0.003033952040038759,
      "grad_norm": 0.9322036504745483,
      "learning_rate": 0.0001998632681735457,
      "loss": 0.0484,
      "step": 645
    },
    {
      "epoch": 0.0030386558416512226,
      "grad_norm": 1.255699872970581,
      "learning_rate": 0.00019986232519543222,
      "loss": 0.1231,
      "step": 646
    },
    {
      "epoch": 0.0030433596432636856,
      "grad_norm": 4.447022438049316,
      "learning_rate": 0.00019986138221731874,
      "loss": 0.755,
      "step": 647
    },
    {
      "epoch": 0.003048063444876149,
      "grad_norm": 2.8242523670196533,
      "learning_rate": 0.00019986043923920528,
      "loss": 0.1157,
      "step": 648
    },
    {
      "epoch": 0.003052767246488612,
      "grad_norm": 2.3320181369781494,
      "learning_rate": 0.0001998594962610918,
      "loss": 0.2693,
      "step": 649
    },
    {
      "epoch": 0.003057471048101075,
      "grad_norm": 1.925260066986084,
      "learning_rate": 0.00019985855328297832,
      "loss": 0.1919,
      "step": 650
    },
    {
      "epoch": 0.0030621748497135387,
      "grad_norm": 1.8796541690826416,
      "learning_rate": 0.00019985761030486484,
      "loss": 0.117,
      "step": 651
    },
    {
      "epoch": 0.0030668786513260017,
      "grad_norm": 1.1869040727615356,
      "learning_rate": 0.00019985666732675136,
      "loss": 0.0477,
      "step": 652
    },
    {
      "epoch": 0.0030715824529384647,
      "grad_norm": 5.200232982635498,
      "learning_rate": 0.00019985572434863788,
      "loss": 0.1342,
      "step": 653
    },
    {
      "epoch": 0.003076286254550928,
      "grad_norm": 4.148637294769287,
      "learning_rate": 0.0001998547813705244,
      "loss": 0.2333,
      "step": 654
    },
    {
      "epoch": 0.0030809900561633912,
      "grad_norm": 2.3496146202087402,
      "learning_rate": 0.00019985383839241091,
      "loss": 0.1796,
      "step": 655
    },
    {
      "epoch": 0.0030856938577758543,
      "grad_norm": 4.071491241455078,
      "learning_rate": 0.00019985289541429743,
      "loss": 0.5679,
      "step": 656
    },
    {
      "epoch": 0.0030903976593883178,
      "grad_norm": 4.623934745788574,
      "learning_rate": 0.00019985195243618398,
      "loss": 0.5293,
      "step": 657
    },
    {
      "epoch": 0.003095101461000781,
      "grad_norm": 3.2329652309417725,
      "learning_rate": 0.0001998510094580705,
      "loss": 0.3043,
      "step": 658
    },
    {
      "epoch": 0.003099805262613244,
      "grad_norm": 5.119171142578125,
      "learning_rate": 0.00019985006647995702,
      "loss": 0.3622,
      "step": 659
    },
    {
      "epoch": 0.0031045090642257073,
      "grad_norm": 2.5936732292175293,
      "learning_rate": 0.00019984912350184353,
      "loss": 0.1239,
      "step": 660
    },
    {
      "epoch": 0.0031092128658381703,
      "grad_norm": 1.5400404930114746,
      "learning_rate": 0.00019984818052373005,
      "loss": 0.2147,
      "step": 661
    },
    {
      "epoch": 0.003113916667450634,
      "grad_norm": 4.7632222175598145,
      "learning_rate": 0.00019984723754561657,
      "loss": 0.7331,
      "step": 662
    },
    {
      "epoch": 0.003118620469063097,
      "grad_norm": 3.956862688064575,
      "learning_rate": 0.0001998462945675031,
      "loss": 0.2979,
      "step": 663
    },
    {
      "epoch": 0.00312332427067556,
      "grad_norm": 3.5224850177764893,
      "learning_rate": 0.0001998453515893896,
      "loss": 0.3799,
      "step": 664
    },
    {
      "epoch": 0.0031280280722880234,
      "grad_norm": 2.1835744380950928,
      "learning_rate": 0.00019984440861127613,
      "loss": 0.2335,
      "step": 665
    },
    {
      "epoch": 0.0031327318739004864,
      "grad_norm": 3.460556745529175,
      "learning_rate": 0.00019984346563316267,
      "loss": 0.3413,
      "step": 666
    },
    {
      "epoch": 0.0031374356755129494,
      "grad_norm": 2.0403270721435547,
      "learning_rate": 0.0001998425226550492,
      "loss": 0.1253,
      "step": 667
    },
    {
      "epoch": 0.003142139477125413,
      "grad_norm": 1.8948084115982056,
      "learning_rate": 0.0001998415796769357,
      "loss": 0.0892,
      "step": 668
    },
    {
      "epoch": 0.003146843278737876,
      "grad_norm": 4.522554874420166,
      "learning_rate": 0.00019984063669882223,
      "loss": 0.3471,
      "step": 669
    },
    {
      "epoch": 0.003151547080350339,
      "grad_norm": 3.44358229637146,
      "learning_rate": 0.00019983969372070875,
      "loss": 0.296,
      "step": 670
    },
    {
      "epoch": 0.0031562508819628025,
      "grad_norm": 2.3977081775665283,
      "learning_rate": 0.0001998387507425953,
      "loss": 0.2188,
      "step": 671
    },
    {
      "epoch": 0.0031609546835752655,
      "grad_norm": 2.8824875354766846,
      "learning_rate": 0.00019983780776448179,
      "loss": 0.275,
      "step": 672
    },
    {
      "epoch": 0.0031656584851877285,
      "grad_norm": 1.496519684791565,
      "learning_rate": 0.0001998368647863683,
      "loss": 0.0925,
      "step": 673
    },
    {
      "epoch": 0.003170362286800192,
      "grad_norm": 2.536027193069458,
      "learning_rate": 0.00019983592180825482,
      "loss": 0.4141,
      "step": 674
    },
    {
      "epoch": 0.003175066088412655,
      "grad_norm": 4.470987796783447,
      "learning_rate": 0.00019983497883014137,
      "loss": 0.7461,
      "step": 675
    },
    {
      "epoch": 0.003179769890025118,
      "grad_norm": 7.664361476898193,
      "learning_rate": 0.0001998340358520279,
      "loss": 0.6813,
      "step": 676
    },
    {
      "epoch": 0.0031844736916375816,
      "grad_norm": 0.46496066451072693,
      "learning_rate": 0.0001998330928739144,
      "loss": 0.0201,
      "step": 677
    },
    {
      "epoch": 0.0031891774932500446,
      "grad_norm": 2.6213464736938477,
      "learning_rate": 0.00019983214989580092,
      "loss": 0.3449,
      "step": 678
    },
    {
      "epoch": 0.003193881294862508,
      "grad_norm": 2.133852481842041,
      "learning_rate": 0.00019983120691768744,
      "loss": 0.2775,
      "step": 679
    },
    {
      "epoch": 0.003198585096474971,
      "grad_norm": 2.3365325927734375,
      "learning_rate": 0.000199830263939574,
      "loss": 0.3445,
      "step": 680
    },
    {
      "epoch": 0.003203288898087434,
      "grad_norm": 3.1866402626037598,
      "learning_rate": 0.0001998293209614605,
      "loss": 0.4403,
      "step": 681
    },
    {
      "epoch": 0.0032079926996998976,
      "grad_norm": 0.8377223014831543,
      "learning_rate": 0.00019982837798334703,
      "loss": 0.0395,
      "step": 682
    },
    {
      "epoch": 0.0032126965013123607,
      "grad_norm": 3.9217641353607178,
      "learning_rate": 0.00019982743500523355,
      "loss": 0.2284,
      "step": 683
    },
    {
      "epoch": 0.0032174003029248237,
      "grad_norm": 2.0801703929901123,
      "learning_rate": 0.00019982649202712004,
      "loss": 0.2378,
      "step": 684
    },
    {
      "epoch": 0.003222104104537287,
      "grad_norm": 1.5071691274642944,
      "learning_rate": 0.00019982554904900658,
      "loss": 0.1527,
      "step": 685
    },
    {
      "epoch": 0.0032268079061497502,
      "grad_norm": 1.7807644605636597,
      "learning_rate": 0.0001998246060708931,
      "loss": 0.1528,
      "step": 686
    },
    {
      "epoch": 0.0032315117077622133,
      "grad_norm": 0.8965314030647278,
      "learning_rate": 0.00019982366309277962,
      "loss": 0.042,
      "step": 687
    },
    {
      "epoch": 0.0032362155093746767,
      "grad_norm": 1.6664773225784302,
      "learning_rate": 0.00019982272011466614,
      "loss": 0.1817,
      "step": 688
    },
    {
      "epoch": 0.0032409193109871398,
      "grad_norm": 2.259044885635376,
      "learning_rate": 0.00019982177713655268,
      "loss": 0.1059,
      "step": 689
    },
    {
      "epoch": 0.003245623112599603,
      "grad_norm": 3.732649326324463,
      "learning_rate": 0.0001998208341584392,
      "loss": 0.3466,
      "step": 690
    },
    {
      "epoch": 0.0032503269142120663,
      "grad_norm": 0.6865069270133972,
      "learning_rate": 0.00019981989118032572,
      "loss": 0.0369,
      "step": 691
    },
    {
      "epoch": 0.0032550307158245293,
      "grad_norm": 2.923830032348633,
      "learning_rate": 0.00019981894820221224,
      "loss": 0.3983,
      "step": 692
    },
    {
      "epoch": 0.0032597345174369924,
      "grad_norm": 1.0155783891677856,
      "learning_rate": 0.00019981800522409876,
      "loss": 0.1191,
      "step": 693
    },
    {
      "epoch": 0.003264438319049456,
      "grad_norm": 4.116394996643066,
      "learning_rate": 0.00019981706224598528,
      "loss": 0.7991,
      "step": 694
    },
    {
      "epoch": 0.003269142120661919,
      "grad_norm": 0.26660841703414917,
      "learning_rate": 0.0001998161192678718,
      "loss": 0.0093,
      "step": 695
    },
    {
      "epoch": 0.0032738459222743824,
      "grad_norm": 0.049822740256786346,
      "learning_rate": 0.00019981517628975831,
      "loss": 0.0009,
      "step": 696
    },
    {
      "epoch": 0.0032785497238868454,
      "grad_norm": 0.35425907373428345,
      "learning_rate": 0.00019981423331164483,
      "loss": 0.0152,
      "step": 697
    },
    {
      "epoch": 0.0032832535254993084,
      "grad_norm": 4.4763031005859375,
      "learning_rate": 0.00019981329033353138,
      "loss": 0.5249,
      "step": 698
    },
    {
      "epoch": 0.003287957327111772,
      "grad_norm": 1.7317616939544678,
      "learning_rate": 0.0001998123473554179,
      "loss": 0.1375,
      "step": 699
    },
    {
      "epoch": 0.003292661128724235,
      "grad_norm": 2.0036051273345947,
      "learning_rate": 0.00019981140437730442,
      "loss": 0.1594,
      "step": 700
    },
    {
      "epoch": 0.003297364930336698,
      "grad_norm": 6.914712429046631,
      "learning_rate": 0.00019981046139919093,
      "loss": 0.4164,
      "step": 701
    },
    {
      "epoch": 0.0033020687319491615,
      "grad_norm": 7.351229667663574,
      "learning_rate": 0.00019980951842107745,
      "loss": 0.3932,
      "step": 702
    },
    {
      "epoch": 0.0033067725335616245,
      "grad_norm": 1.9157869815826416,
      "learning_rate": 0.00019980857544296397,
      "loss": 0.0914,
      "step": 703
    },
    {
      "epoch": 0.0033114763351740875,
      "grad_norm": 4.803820610046387,
      "learning_rate": 0.0001998076324648505,
      "loss": 0.3791,
      "step": 704
    },
    {
      "epoch": 0.003316180136786551,
      "grad_norm": 4.092446804046631,
      "learning_rate": 0.000199806689486737,
      "loss": 0.3111,
      "step": 705
    },
    {
      "epoch": 0.003320883938399014,
      "grad_norm": 8.289037704467773,
      "learning_rate": 0.00019980574650862353,
      "loss": 0.414,
      "step": 706
    },
    {
      "epoch": 0.003325587740011477,
      "grad_norm": 1.6593165397644043,
      "learning_rate": 0.00019980480353051007,
      "loss": 0.1817,
      "step": 707
    },
    {
      "epoch": 0.0033302915416239406,
      "grad_norm": 1.3535640239715576,
      "learning_rate": 0.0001998038605523966,
      "loss": 0.0798,
      "step": 708
    },
    {
      "epoch": 0.0033349953432364036,
      "grad_norm": 3.2224557399749756,
      "learning_rate": 0.0001998029175742831,
      "loss": 0.2774,
      "step": 709
    },
    {
      "epoch": 0.003339699144848867,
      "grad_norm": 1.0758998394012451,
      "learning_rate": 0.00019980197459616963,
      "loss": 0.0917,
      "step": 710
    },
    {
      "epoch": 0.00334440294646133,
      "grad_norm": 7.0475568771362305,
      "learning_rate": 0.00019980103161805615,
      "loss": 0.5593,
      "step": 711
    },
    {
      "epoch": 0.003349106748073793,
      "grad_norm": 0.8616170287132263,
      "learning_rate": 0.0001998000886399427,
      "loss": 0.0488,
      "step": 712
    },
    {
      "epoch": 0.0033538105496862566,
      "grad_norm": 0.9882147908210754,
      "learning_rate": 0.0001997991456618292,
      "loss": 0.0289,
      "step": 713
    },
    {
      "epoch": 0.0033585143512987197,
      "grad_norm": 0.3512348532676697,
      "learning_rate": 0.00019979820268371573,
      "loss": 0.0171,
      "step": 714
    },
    {
      "epoch": 0.0033632181529111827,
      "grad_norm": 7.051764011383057,
      "learning_rate": 0.00019979725970560222,
      "loss": 1.9375,
      "step": 715
    },
    {
      "epoch": 0.003367921954523646,
      "grad_norm": 1.6180366277694702,
      "learning_rate": 0.00019979631672748877,
      "loss": 0.0407,
      "step": 716
    },
    {
      "epoch": 0.003372625756136109,
      "grad_norm": 2.079805612564087,
      "learning_rate": 0.0001997953737493753,
      "loss": 0.0991,
      "step": 717
    },
    {
      "epoch": 0.0033773295577485722,
      "grad_norm": 2.244856119155884,
      "learning_rate": 0.0001997944307712618,
      "loss": 0.1052,
      "step": 718
    },
    {
      "epoch": 0.0033820333593610357,
      "grad_norm": 5.23406982421875,
      "learning_rate": 0.00019979348779314832,
      "loss": 0.5572,
      "step": 719
    },
    {
      "epoch": 0.0033867371609734988,
      "grad_norm": 4.127285480499268,
      "learning_rate": 0.00019979254481503484,
      "loss": 0.5623,
      "step": 720
    },
    {
      "epoch": 0.003391440962585962,
      "grad_norm": 2.6614346504211426,
      "learning_rate": 0.0001997916018369214,
      "loss": 0.1108,
      "step": 721
    },
    {
      "epoch": 0.0033961447641984253,
      "grad_norm": 6.999116897583008,
      "learning_rate": 0.0001997906588588079,
      "loss": 0.3679,
      "step": 722
    },
    {
      "epoch": 0.0034008485658108883,
      "grad_norm": 5.0894389152526855,
      "learning_rate": 0.00019978971588069443,
      "loss": 0.7167,
      "step": 723
    },
    {
      "epoch": 0.0034055523674233514,
      "grad_norm": 0.3045954704284668,
      "learning_rate": 0.00019978877290258094,
      "loss": 0.0132,
      "step": 724
    },
    {
      "epoch": 0.003410256169035815,
      "grad_norm": 3.3110711574554443,
      "learning_rate": 0.00019978782992446746,
      "loss": 0.2442,
      "step": 725
    },
    {
      "epoch": 0.003414959970648278,
      "grad_norm": 7.648102760314941,
      "learning_rate": 0.00019978688694635398,
      "loss": 0.6009,
      "step": 726
    },
    {
      "epoch": 0.0034196637722607413,
      "grad_norm": 7.214248180389404,
      "learning_rate": 0.0001997859439682405,
      "loss": 0.524,
      "step": 727
    },
    {
      "epoch": 0.0034243675738732044,
      "grad_norm": 1.3795311450958252,
      "learning_rate": 0.00019978500099012702,
      "loss": 0.1033,
      "step": 728
    },
    {
      "epoch": 0.0034290713754856674,
      "grad_norm": 4.430289268493652,
      "learning_rate": 0.00019978405801201354,
      "loss": 0.2421,
      "step": 729
    },
    {
      "epoch": 0.003433775177098131,
      "grad_norm": 0.9835808277130127,
      "learning_rate": 0.00019978311503390008,
      "loss": 0.058,
      "step": 730
    },
    {
      "epoch": 0.003438478978710594,
      "grad_norm": 4.867995262145996,
      "learning_rate": 0.0001997821720557866,
      "loss": 0.4837,
      "step": 731
    },
    {
      "epoch": 0.003443182780323057,
      "grad_norm": 1.2088003158569336,
      "learning_rate": 0.00019978122907767312,
      "loss": 0.0817,
      "step": 732
    },
    {
      "epoch": 0.0034478865819355204,
      "grad_norm": 3.145982027053833,
      "learning_rate": 0.00019978028609955964,
      "loss": 0.2513,
      "step": 733
    },
    {
      "epoch": 0.0034525903835479835,
      "grad_norm": 1.0700922012329102,
      "learning_rate": 0.00019977934312144616,
      "loss": 0.0782,
      "step": 734
    },
    {
      "epoch": 0.0034572941851604465,
      "grad_norm": 4.819093704223633,
      "learning_rate": 0.00019977840014333268,
      "loss": 0.4716,
      "step": 735
    },
    {
      "epoch": 0.00346199798677291,
      "grad_norm": 1.0987012386322021,
      "learning_rate": 0.0001997774571652192,
      "loss": 0.0838,
      "step": 736
    },
    {
      "epoch": 0.003466701788385373,
      "grad_norm": 4.498098373413086,
      "learning_rate": 0.00019977651418710571,
      "loss": 0.813,
      "step": 737
    },
    {
      "epoch": 0.003471405589997836,
      "grad_norm": 2.007070541381836,
      "learning_rate": 0.00019977557120899223,
      "loss": 0.1707,
      "step": 738
    },
    {
      "epoch": 0.0034761093916102995,
      "grad_norm": 1.4923171997070312,
      "learning_rate": 0.00019977462823087878,
      "loss": 0.0989,
      "step": 739
    },
    {
      "epoch": 0.0034808131932227626,
      "grad_norm": 2.7298333644866943,
      "learning_rate": 0.0001997736852527653,
      "loss": 0.4036,
      "step": 740
    },
    {
      "epoch": 0.0034855169948352256,
      "grad_norm": 3.2051470279693604,
      "learning_rate": 0.00019977274227465182,
      "loss": 0.2748,
      "step": 741
    },
    {
      "epoch": 0.003490220796447689,
      "grad_norm": 2.9899046421051025,
      "learning_rate": 0.00019977179929653833,
      "loss": 0.2817,
      "step": 742
    },
    {
      "epoch": 0.003494924598060152,
      "grad_norm": 2.592867851257324,
      "learning_rate": 0.00019977085631842485,
      "loss": 0.4408,
      "step": 743
    },
    {
      "epoch": 0.0034996283996726156,
      "grad_norm": 2.420393466949463,
      "learning_rate": 0.0001997699133403114,
      "loss": 0.3379,
      "step": 744
    },
    {
      "epoch": 0.0035043322012850786,
      "grad_norm": 2.5421671867370605,
      "learning_rate": 0.00019976897036219792,
      "loss": 0.3067,
      "step": 745
    },
    {
      "epoch": 0.0035090360028975417,
      "grad_norm": 1.3961797952651978,
      "learning_rate": 0.0001997680273840844,
      "loss": 0.0816,
      "step": 746
    },
    {
      "epoch": 0.003513739804510005,
      "grad_norm": 1.6264028549194336,
      "learning_rate": 0.00019976708440597093,
      "loss": 0.2849,
      "step": 747
    },
    {
      "epoch": 0.003518443606122468,
      "grad_norm": 4.887629985809326,
      "learning_rate": 0.00019976614142785747,
      "loss": 0.5643,
      "step": 748
    },
    {
      "epoch": 0.0035231474077349312,
      "grad_norm": 1.7852598428726196,
      "learning_rate": 0.000199765198449744,
      "loss": 0.1302,
      "step": 749
    },
    {
      "epoch": 0.0035278512093473947,
      "grad_norm": 3.496859550476074,
      "learning_rate": 0.0001997642554716305,
      "loss": 0.4507,
      "step": 750
    },
    {
      "epoch": 0.0035325550109598577,
      "grad_norm": 1.9377378225326538,
      "learning_rate": 0.00019976331249351703,
      "loss": 0.1132,
      "step": 751
    },
    {
      "epoch": 0.003537258812572321,
      "grad_norm": 3.2283592224121094,
      "learning_rate": 0.00019976236951540355,
      "loss": 0.1464,
      "step": 752
    },
    {
      "epoch": 0.0035419626141847843,
      "grad_norm": 1.0126895904541016,
      "learning_rate": 0.0001997614265372901,
      "loss": 0.0642,
      "step": 753
    },
    {
      "epoch": 0.0035466664157972473,
      "grad_norm": 2.15046763420105,
      "learning_rate": 0.0001997604835591766,
      "loss": 0.1565,
      "step": 754
    },
    {
      "epoch": 0.0035513702174097103,
      "grad_norm": 4.371125221252441,
      "learning_rate": 0.00019975954058106313,
      "loss": 0.3923,
      "step": 755
    },
    {
      "epoch": 0.003556074019022174,
      "grad_norm": 0.9291052222251892,
      "learning_rate": 0.00019975859760294965,
      "loss": 0.057,
      "step": 756
    },
    {
      "epoch": 0.003560777820634637,
      "grad_norm": 4.891977787017822,
      "learning_rate": 0.00019975765462483617,
      "loss": 0.4021,
      "step": 757
    },
    {
      "epoch": 0.0035654816222471003,
      "grad_norm": 6.337736129760742,
      "learning_rate": 0.0001997567116467227,
      "loss": 0.5628,
      "step": 758
    },
    {
      "epoch": 0.0035701854238595634,
      "grad_norm": 5.258932590484619,
      "learning_rate": 0.0001997557686686092,
      "loss": 0.4926,
      "step": 759
    },
    {
      "epoch": 0.0035748892254720264,
      "grad_norm": 8.956976890563965,
      "learning_rate": 0.00019975482569049572,
      "loss": 0.4917,
      "step": 760
    },
    {
      "epoch": 0.00357959302708449,
      "grad_norm": 0.9098829030990601,
      "learning_rate": 0.00019975388271238224,
      "loss": 0.0507,
      "step": 761
    },
    {
      "epoch": 0.003584296828696953,
      "grad_norm": 9.902016639709473,
      "learning_rate": 0.0001997529397342688,
      "loss": 1.2121,
      "step": 762
    },
    {
      "epoch": 0.003589000630309416,
      "grad_norm": 13.188253402709961,
      "learning_rate": 0.0001997519967561553,
      "loss": 0.5873,
      "step": 763
    },
    {
      "epoch": 0.0035937044319218794,
      "grad_norm": 5.708322525024414,
      "learning_rate": 0.00019975105377804183,
      "loss": 0.847,
      "step": 764
    },
    {
      "epoch": 0.0035984082335343425,
      "grad_norm": 1.9623807668685913,
      "learning_rate": 0.00019975011079992834,
      "loss": 0.123,
      "step": 765
    },
    {
      "epoch": 0.0036031120351468055,
      "grad_norm": 3.1812825202941895,
      "learning_rate": 0.00019974916782181486,
      "loss": 0.1545,
      "step": 766
    },
    {
      "epoch": 0.003607815836759269,
      "grad_norm": 2.3966028690338135,
      "learning_rate": 0.00019974822484370138,
      "loss": 0.1213,
      "step": 767
    },
    {
      "epoch": 0.003612519638371732,
      "grad_norm": 6.531218528747559,
      "learning_rate": 0.0001997472818655879,
      "loss": 0.5429,
      "step": 768
    },
    {
      "epoch": 0.003617223439984195,
      "grad_norm": 2.932382106781006,
      "learning_rate": 0.00019974633888747442,
      "loss": 0.468,
      "step": 769
    },
    {
      "epoch": 0.0036219272415966585,
      "grad_norm": 7.266598224639893,
      "learning_rate": 0.00019974539590936094,
      "loss": 0.6348,
      "step": 770
    },
    {
      "epoch": 0.0036266310432091216,
      "grad_norm": 10.07787799835205,
      "learning_rate": 0.00019974445293124748,
      "loss": 0.245,
      "step": 771
    },
    {
      "epoch": 0.0036313348448215846,
      "grad_norm": 5.017037391662598,
      "learning_rate": 0.000199743509953134,
      "loss": 0.6583,
      "step": 772
    },
    {
      "epoch": 0.003636038646434048,
      "grad_norm": 4.218452453613281,
      "learning_rate": 0.00019974256697502052,
      "loss": 0.6163,
      "step": 773
    },
    {
      "epoch": 0.003640742448046511,
      "grad_norm": 0.5723045468330383,
      "learning_rate": 0.00019974162399690704,
      "loss": 0.0312,
      "step": 774
    },
    {
      "epoch": 0.0036454462496589746,
      "grad_norm": 1.5858299732208252,
      "learning_rate": 0.00019974068101879359,
      "loss": 0.1595,
      "step": 775
    },
    {
      "epoch": 0.0036501500512714376,
      "grad_norm": 1.7094080448150635,
      "learning_rate": 0.0001997397380406801,
      "loss": 0.1687,
      "step": 776
    },
    {
      "epoch": 0.0036548538528839007,
      "grad_norm": 4.929214954376221,
      "learning_rate": 0.0001997387950625666,
      "loss": 0.7163,
      "step": 777
    },
    {
      "epoch": 0.003659557654496364,
      "grad_norm": 0.8485893607139587,
      "learning_rate": 0.00019973785208445311,
      "loss": 0.0713,
      "step": 778
    },
    {
      "epoch": 0.003664261456108827,
      "grad_norm": 2.429117202758789,
      "learning_rate": 0.00019973690910633963,
      "loss": 0.1789,
      "step": 779
    },
    {
      "epoch": 0.00366896525772129,
      "grad_norm": 6.601913928985596,
      "learning_rate": 0.00019973596612822618,
      "loss": 0.6066,
      "step": 780
    },
    {
      "epoch": 0.0036736690593337537,
      "grad_norm": 0.6980249285697937,
      "learning_rate": 0.0001997350231501127,
      "loss": 0.0355,
      "step": 781
    },
    {
      "epoch": 0.0036783728609462167,
      "grad_norm": 3.3183889389038086,
      "learning_rate": 0.00019973408017199922,
      "loss": 0.2565,
      "step": 782
    },
    {
      "epoch": 0.0036830766625586798,
      "grad_norm": 1.7383477687835693,
      "learning_rate": 0.00019973313719388573,
      "loss": 0.1905,
      "step": 783
    },
    {
      "epoch": 0.0036877804641711432,
      "grad_norm": 1.0041919946670532,
      "learning_rate": 0.00019973219421577228,
      "loss": 0.0642,
      "step": 784
    },
    {
      "epoch": 0.0036924842657836063,
      "grad_norm": 6.271389484405518,
      "learning_rate": 0.0001997312512376588,
      "loss": 0.7233,
      "step": 785
    },
    {
      "epoch": 0.0036971880673960693,
      "grad_norm": 1.2432334423065186,
      "learning_rate": 0.00019973030825954532,
      "loss": 0.0708,
      "step": 786
    },
    {
      "epoch": 0.003701891869008533,
      "grad_norm": 0.7254047393798828,
      "learning_rate": 0.00019972936528143184,
      "loss": 0.035,
      "step": 787
    },
    {
      "epoch": 0.003706595670620996,
      "grad_norm": 0.41578084230422974,
      "learning_rate": 0.00019972842230331833,
      "loss": 0.0147,
      "step": 788
    },
    {
      "epoch": 0.003711299472233459,
      "grad_norm": 7.647470474243164,
      "learning_rate": 0.00019972747932520487,
      "loss": 0.9813,
      "step": 789
    },
    {
      "epoch": 0.0037160032738459223,
      "grad_norm": 9.043394088745117,
      "learning_rate": 0.0001997265363470914,
      "loss": 0.5683,
      "step": 790
    },
    {
      "epoch": 0.0037207070754583854,
      "grad_norm": 4.077976703643799,
      "learning_rate": 0.0001997255933689779,
      "loss": 0.2939,
      "step": 791
    },
    {
      "epoch": 0.003725410877070849,
      "grad_norm": 4.999049663543701,
      "learning_rate": 0.00019972465039086443,
      "loss": 0.3417,
      "step": 792
    },
    {
      "epoch": 0.003730114678683312,
      "grad_norm": 2.734470844268799,
      "learning_rate": 0.00019972370741275095,
      "loss": 0.2965,
      "step": 793
    },
    {
      "epoch": 0.003734818480295775,
      "grad_norm": 4.27565336227417,
      "learning_rate": 0.0001997227644346375,
      "loss": 0.4357,
      "step": 794
    },
    {
      "epoch": 0.0037395222819082384,
      "grad_norm": 1.8730779886245728,
      "learning_rate": 0.000199721821456524,
      "loss": 0.2044,
      "step": 795
    },
    {
      "epoch": 0.0037442260835207014,
      "grad_norm": 6.089479923248291,
      "learning_rate": 0.00019972087847841053,
      "loss": 0.6735,
      "step": 796
    },
    {
      "epoch": 0.0037489298851331645,
      "grad_norm": 4.855020999908447,
      "learning_rate": 0.00019971993550029705,
      "loss": 0.319,
      "step": 797
    },
    {
      "epoch": 0.003753633686745628,
      "grad_norm": 5.966873645782471,
      "learning_rate": 0.00019971899252218357,
      "loss": 0.6209,
      "step": 798
    },
    {
      "epoch": 0.003758337488358091,
      "grad_norm": 2.4595608711242676,
      "learning_rate": 0.0001997180495440701,
      "loss": 0.2008,
      "step": 799
    },
    {
      "epoch": 0.003763041289970554,
      "grad_norm": 3.0964159965515137,
      "learning_rate": 0.0001997171065659566,
      "loss": 0.3928,
      "step": 800
    },
    {
      "epoch": 0.0037677450915830175,
      "grad_norm": 3.017940044403076,
      "learning_rate": 0.00019971616358784312,
      "loss": 0.1625,
      "step": 801
    },
    {
      "epoch": 0.0037724488931954805,
      "grad_norm": 3.077155828475952,
      "learning_rate": 0.00019971522060972964,
      "loss": 0.1405,
      "step": 802
    },
    {
      "epoch": 0.0037771526948079436,
      "grad_norm": 3.839731454849243,
      "learning_rate": 0.0001997142776316162,
      "loss": 0.1925,
      "step": 803
    },
    {
      "epoch": 0.003781856496420407,
      "grad_norm": 3.9447035789489746,
      "learning_rate": 0.0001997133346535027,
      "loss": 0.3221,
      "step": 804
    },
    {
      "epoch": 0.00378656029803287,
      "grad_norm": 3.661208152770996,
      "learning_rate": 0.00019971239167538923,
      "loss": 0.3085,
      "step": 805
    },
    {
      "epoch": 0.0037912640996453336,
      "grad_norm": 3.171854019165039,
      "learning_rate": 0.00019971144869727574,
      "loss": 0.2067,
      "step": 806
    },
    {
      "epoch": 0.0037959679012577966,
      "grad_norm": 4.156179428100586,
      "learning_rate": 0.0001997105057191623,
      "loss": 0.3249,
      "step": 807
    },
    {
      "epoch": 0.0038006717028702596,
      "grad_norm": 0.7393237352371216,
      "learning_rate": 0.00019970956274104878,
      "loss": 0.0492,
      "step": 808
    },
    {
      "epoch": 0.003805375504482723,
      "grad_norm": 1.9175344705581665,
      "learning_rate": 0.0001997086197629353,
      "loss": 0.1697,
      "step": 809
    },
    {
      "epoch": 0.003810079306095186,
      "grad_norm": 0.8247316479682922,
      "learning_rate": 0.00019970767678482182,
      "loss": 0.0854,
      "step": 810
    },
    {
      "epoch": 0.003814783107707649,
      "grad_norm": 4.238018035888672,
      "learning_rate": 0.00019970673380670834,
      "loss": 0.5455,
      "step": 811
    },
    {
      "epoch": 0.0038194869093201127,
      "grad_norm": 2.976881504058838,
      "learning_rate": 0.00019970579082859488,
      "loss": 0.3302,
      "step": 812
    },
    {
      "epoch": 0.0038241907109325757,
      "grad_norm": 1.8273224830627441,
      "learning_rate": 0.0001997048478504814,
      "loss": 0.1593,
      "step": 813
    },
    {
      "epoch": 0.0038288945125450388,
      "grad_norm": 0.7396630048751831,
      "learning_rate": 0.00019970390487236792,
      "loss": 0.0598,
      "step": 814
    },
    {
      "epoch": 0.0038335983141575022,
      "grad_norm": 1.375240683555603,
      "learning_rate": 0.00019970296189425444,
      "loss": 0.0856,
      "step": 815
    },
    {
      "epoch": 0.0038383021157699653,
      "grad_norm": 2.4276986122131348,
      "learning_rate": 0.00019970201891614099,
      "loss": 0.1118,
      "step": 816
    },
    {
      "epoch": 0.0038430059173824283,
      "grad_norm": 3.337308406829834,
      "learning_rate": 0.0001997010759380275,
      "loss": 0.1846,
      "step": 817
    },
    {
      "epoch": 0.0038477097189948918,
      "grad_norm": 2.7963943481445312,
      "learning_rate": 0.00019970013295991402,
      "loss": 0.2228,
      "step": 818
    },
    {
      "epoch": 0.003852413520607355,
      "grad_norm": 3.234858274459839,
      "learning_rate": 0.00019969918998180051,
      "loss": 0.2644,
      "step": 819
    },
    {
      "epoch": 0.003857117322219818,
      "grad_norm": 3.1895601749420166,
      "learning_rate": 0.00019969824700368703,
      "loss": 0.5279,
      "step": 820
    },
    {
      "epoch": 0.0038618211238322813,
      "grad_norm": 2.9223039150238037,
      "learning_rate": 0.00019969730402557358,
      "loss": 0.4018,
      "step": 821
    },
    {
      "epoch": 0.0038665249254447444,
      "grad_norm": 4.152519226074219,
      "learning_rate": 0.0001996963610474601,
      "loss": 0.417,
      "step": 822
    },
    {
      "epoch": 0.003871228727057208,
      "grad_norm": 3.0384833812713623,
      "learning_rate": 0.00019969541806934662,
      "loss": 0.2001,
      "step": 823
    },
    {
      "epoch": 0.003875932528669671,
      "grad_norm": 2.0597362518310547,
      "learning_rate": 0.00019969447509123313,
      "loss": 0.1356,
      "step": 824
    },
    {
      "epoch": 0.003880636330282134,
      "grad_norm": 1.1083074808120728,
      "learning_rate": 0.00019969353211311968,
      "loss": 0.112,
      "step": 825
    },
    {
      "epoch": 0.0038853401318945974,
      "grad_norm": 3.516921043395996,
      "learning_rate": 0.0001996925891350062,
      "loss": 0.3874,
      "step": 826
    },
    {
      "epoch": 0.0038900439335070604,
      "grad_norm": 0.8500960469245911,
      "learning_rate": 0.00019969164615689272,
      "loss": 0.0675,
      "step": 827
    },
    {
      "epoch": 0.0038947477351195235,
      "grad_norm": 0.9188404679298401,
      "learning_rate": 0.00019969070317877924,
      "loss": 0.0569,
      "step": 828
    },
    {
      "epoch": 0.003899451536731987,
      "grad_norm": 1.9219945669174194,
      "learning_rate": 0.00019968976020066576,
      "loss": 0.1342,
      "step": 829
    },
    {
      "epoch": 0.00390415533834445,
      "grad_norm": 3.1163229942321777,
      "learning_rate": 0.00019968881722255227,
      "loss": 0.2113,
      "step": 830
    },
    {
      "epoch": 0.003908859139956913,
      "grad_norm": 0.3294054865837097,
      "learning_rate": 0.0001996878742444388,
      "loss": 0.0127,
      "step": 831
    },
    {
      "epoch": 0.003913562941569376,
      "grad_norm": 3.5172455310821533,
      "learning_rate": 0.0001996869312663253,
      "loss": 0.2742,
      "step": 832
    },
    {
      "epoch": 0.00391826674318184,
      "grad_norm": 1.927416205406189,
      "learning_rate": 0.00019968598828821183,
      "loss": 0.1041,
      "step": 833
    },
    {
      "epoch": 0.003922970544794303,
      "grad_norm": 1.5283513069152832,
      "learning_rate": 0.00019968504531009838,
      "loss": 0.0608,
      "step": 834
    },
    {
      "epoch": 0.003927674346406766,
      "grad_norm": 4.355809211730957,
      "learning_rate": 0.0001996841023319849,
      "loss": 0.6392,
      "step": 835
    },
    {
      "epoch": 0.003932378148019229,
      "grad_norm": 7.324599266052246,
      "learning_rate": 0.0001996831593538714,
      "loss": 0.9752,
      "step": 836
    },
    {
      "epoch": 0.003937081949631692,
      "grad_norm": 1.6721248626708984,
      "learning_rate": 0.00019968221637575793,
      "loss": 0.0868,
      "step": 837
    },
    {
      "epoch": 0.003941785751244155,
      "grad_norm": 3.6601593494415283,
      "learning_rate": 0.00019968127339764445,
      "loss": 0.1553,
      "step": 838
    },
    {
      "epoch": 0.003946489552856619,
      "grad_norm": 3.891674757003784,
      "learning_rate": 0.00019968033041953097,
      "loss": 0.4995,
      "step": 839
    },
    {
      "epoch": 0.003951193354469082,
      "grad_norm": 5.520595073699951,
      "learning_rate": 0.0001996793874414175,
      "loss": 0.342,
      "step": 840
    },
    {
      "epoch": 0.003955897156081545,
      "grad_norm": 5.791471004486084,
      "learning_rate": 0.000199678444463304,
      "loss": 0.6729,
      "step": 841
    },
    {
      "epoch": 0.003960600957694008,
      "grad_norm": 0.6360845565795898,
      "learning_rate": 0.00019967750148519052,
      "loss": 0.0391,
      "step": 842
    },
    {
      "epoch": 0.003965304759306471,
      "grad_norm": 4.245542526245117,
      "learning_rate": 0.00019967655850707704,
      "loss": 0.5969,
      "step": 843
    },
    {
      "epoch": 0.003970008560918934,
      "grad_norm": 0.9177573323249817,
      "learning_rate": 0.0001996756155289636,
      "loss": 0.0385,
      "step": 844
    },
    {
      "epoch": 0.003974712362531398,
      "grad_norm": 0.5353983044624329,
      "learning_rate": 0.0001996746725508501,
      "loss": 0.0204,
      "step": 845
    },
    {
      "epoch": 0.003979416164143861,
      "grad_norm": 3.2036917209625244,
      "learning_rate": 0.00019967372957273663,
      "loss": 0.2564,
      "step": 846
    },
    {
      "epoch": 0.003984119965756324,
      "grad_norm": 5.347578048706055,
      "learning_rate": 0.00019967278659462314,
      "loss": 0.7149,
      "step": 847
    },
    {
      "epoch": 0.003988823767368787,
      "grad_norm": 3.8765556812286377,
      "learning_rate": 0.0001996718436165097,
      "loss": 0.4441,
      "step": 848
    },
    {
      "epoch": 0.00399352756898125,
      "grad_norm": 3.894300699234009,
      "learning_rate": 0.0001996709006383962,
      "loss": 0.4679,
      "step": 849
    },
    {
      "epoch": 0.003998231370593714,
      "grad_norm": 2.2992677688598633,
      "learning_rate": 0.0001996699576602827,
      "loss": 0.3826,
      "step": 850
    },
    {
      "epoch": 0.004002935172206177,
      "grad_norm": 5.999057292938232,
      "learning_rate": 0.00019966901468216922,
      "loss": 0.3428,
      "step": 851
    },
    {
      "epoch": 0.00400763897381864,
      "grad_norm": 2.2287259101867676,
      "learning_rate": 0.00019966807170405574,
      "loss": 0.1011,
      "step": 852
    },
    {
      "epoch": 0.004012342775431103,
      "grad_norm": 2.062133312225342,
      "learning_rate": 0.00019966712872594228,
      "loss": 0.1273,
      "step": 853
    },
    {
      "epoch": 0.004017046577043566,
      "grad_norm": 5.200307369232178,
      "learning_rate": 0.0001996661857478288,
      "loss": 1.0722,
      "step": 854
    },
    {
      "epoch": 0.004021750378656029,
      "grad_norm": 1.7610176801681519,
      "learning_rate": 0.00019966524276971532,
      "loss": 0.1183,
      "step": 855
    },
    {
      "epoch": 0.004026454180268493,
      "grad_norm": 3.9438118934631348,
      "learning_rate": 0.00019966429979160184,
      "loss": 0.576,
      "step": 856
    },
    {
      "epoch": 0.004031157981880956,
      "grad_norm": 2.1197304725646973,
      "learning_rate": 0.00019966335681348839,
      "loss": 0.2238,
      "step": 857
    },
    {
      "epoch": 0.004035861783493419,
      "grad_norm": 6.491802215576172,
      "learning_rate": 0.0001996624138353749,
      "loss": 0.7893,
      "step": 858
    },
    {
      "epoch": 0.0040405655851058824,
      "grad_norm": 6.291945934295654,
      "learning_rate": 0.00019966147085726142,
      "loss": 1.132,
      "step": 859
    },
    {
      "epoch": 0.0040452693867183455,
      "grad_norm": 5.788924217224121,
      "learning_rate": 0.00019966052787914794,
      "loss": 0.8024,
      "step": 860
    },
    {
      "epoch": 0.0040499731883308085,
      "grad_norm": 4.322174549102783,
      "learning_rate": 0.00019965958490103446,
      "loss": 0.6629,
      "step": 861
    },
    {
      "epoch": 0.0040546769899432724,
      "grad_norm": 2.0006439685821533,
      "learning_rate": 0.00019965864192292098,
      "loss": 0.151,
      "step": 862
    },
    {
      "epoch": 0.0040593807915557355,
      "grad_norm": 5.606760501861572,
      "learning_rate": 0.0001996576989448075,
      "loss": 0.8002,
      "step": 863
    },
    {
      "epoch": 0.0040640845931681985,
      "grad_norm": 1.7605232000350952,
      "learning_rate": 0.00019965675596669402,
      "loss": 0.1832,
      "step": 864
    },
    {
      "epoch": 0.0040687883947806616,
      "grad_norm": 1.3694320917129517,
      "learning_rate": 0.00019965581298858053,
      "loss": 0.3688,
      "step": 865
    },
    {
      "epoch": 0.004073492196393125,
      "grad_norm": 1.5993189811706543,
      "learning_rate": 0.00019965487001046708,
      "loss": 0.2427,
      "step": 866
    },
    {
      "epoch": 0.0040781959980055885,
      "grad_norm": 1.4998372793197632,
      "learning_rate": 0.0001996539270323536,
      "loss": 0.2507,
      "step": 867
    },
    {
      "epoch": 0.0040828997996180515,
      "grad_norm": 1.5021344423294067,
      "learning_rate": 0.00019965298405424012,
      "loss": 0.1601,
      "step": 868
    },
    {
      "epoch": 0.004087603601230515,
      "grad_norm": 0.8515642285346985,
      "learning_rate": 0.00019965204107612664,
      "loss": 0.1471,
      "step": 869
    },
    {
      "epoch": 0.004092307402842978,
      "grad_norm": 0.9317629337310791,
      "learning_rate": 0.00019965109809801316,
      "loss": 0.1031,
      "step": 870
    },
    {
      "epoch": 0.004097011204455441,
      "grad_norm": 2.0518393516540527,
      "learning_rate": 0.00019965015511989967,
      "loss": 0.2911,
      "step": 871
    },
    {
      "epoch": 0.004101715006067904,
      "grad_norm": 2.4519801139831543,
      "learning_rate": 0.0001996492121417862,
      "loss": 0.2398,
      "step": 872
    },
    {
      "epoch": 0.004106418807680368,
      "grad_norm": 1.7986570596694946,
      "learning_rate": 0.0001996482691636727,
      "loss": 0.3043,
      "step": 873
    },
    {
      "epoch": 0.004111122609292831,
      "grad_norm": 2.651280403137207,
      "learning_rate": 0.00019964732618555923,
      "loss": 0.2834,
      "step": 874
    },
    {
      "epoch": 0.004115826410905294,
      "grad_norm": 0.057060301303863525,
      "learning_rate": 0.00019964638320744578,
      "loss": 0.0026,
      "step": 875
    },
    {
      "epoch": 0.004120530212517757,
      "grad_norm": 1.8324172496795654,
      "learning_rate": 0.0001996454402293323,
      "loss": 0.1715,
      "step": 876
    },
    {
      "epoch": 0.00412523401413022,
      "grad_norm": 2.8559815883636475,
      "learning_rate": 0.0001996444972512188,
      "loss": 0.1747,
      "step": 877
    },
    {
      "epoch": 0.004129937815742684,
      "grad_norm": 4.312530994415283,
      "learning_rate": 0.00019964355427310533,
      "loss": 0.4679,
      "step": 878
    },
    {
      "epoch": 0.004134641617355147,
      "grad_norm": 3.6600635051727295,
      "learning_rate": 0.00019964261129499185,
      "loss": 0.3443,
      "step": 879
    },
    {
      "epoch": 0.00413934541896761,
      "grad_norm": 7.24200963973999,
      "learning_rate": 0.0001996416683168784,
      "loss": 1.1127,
      "step": 880
    },
    {
      "epoch": 0.004144049220580073,
      "grad_norm": 5.192946910858154,
      "learning_rate": 0.0001996407253387649,
      "loss": 0.2734,
      "step": 881
    },
    {
      "epoch": 0.004148753022192536,
      "grad_norm": 2.2150843143463135,
      "learning_rate": 0.0001996397823606514,
      "loss": 0.1784,
      "step": 882
    },
    {
      "epoch": 0.004153456823804999,
      "grad_norm": 3.5504543781280518,
      "learning_rate": 0.00019963883938253792,
      "loss": 0.4418,
      "step": 883
    },
    {
      "epoch": 0.004158160625417463,
      "grad_norm": 0.7357102036476135,
      "learning_rate": 0.00019963789640442447,
      "loss": 0.0306,
      "step": 884
    },
    {
      "epoch": 0.004162864427029926,
      "grad_norm": 4.456344127655029,
      "learning_rate": 0.000199636953426311,
      "loss": 0.2673,
      "step": 885
    },
    {
      "epoch": 0.004167568228642389,
      "grad_norm": 4.1521501541137695,
      "learning_rate": 0.0001996360104481975,
      "loss": 0.4662,
      "step": 886
    },
    {
      "epoch": 0.004172272030254852,
      "grad_norm": 1.6022080183029175,
      "learning_rate": 0.00019963506747008403,
      "loss": 0.0802,
      "step": 887
    },
    {
      "epoch": 0.004176975831867315,
      "grad_norm": 2.3389244079589844,
      "learning_rate": 0.00019963412449197054,
      "loss": 0.1645,
      "step": 888
    },
    {
      "epoch": 0.004181679633479778,
      "grad_norm": 2.216244697570801,
      "learning_rate": 0.0001996331815138571,
      "loss": 0.32,
      "step": 889
    },
    {
      "epoch": 0.004186383435092242,
      "grad_norm": 1.5978516340255737,
      "learning_rate": 0.0001996322385357436,
      "loss": 0.267,
      "step": 890
    },
    {
      "epoch": 0.004191087236704705,
      "grad_norm": 1.8133282661437988,
      "learning_rate": 0.00019963129555763013,
      "loss": 0.2402,
      "step": 891
    },
    {
      "epoch": 0.004195791038317168,
      "grad_norm": 2.2511255741119385,
      "learning_rate": 0.00019963035257951662,
      "loss": 0.1581,
      "step": 892
    },
    {
      "epoch": 0.004200494839929631,
      "grad_norm": 2.2536075115203857,
      "learning_rate": 0.00019962940960140314,
      "loss": 0.2668,
      "step": 893
    },
    {
      "epoch": 0.004205198641542094,
      "grad_norm": 3.0754106044769287,
      "learning_rate": 0.00019962846662328968,
      "loss": 0.3943,
      "step": 894
    },
    {
      "epoch": 0.004209902443154558,
      "grad_norm": 4.3067121505737305,
      "learning_rate": 0.0001996275236451762,
      "loss": 0.7683,
      "step": 895
    },
    {
      "epoch": 0.004214606244767021,
      "grad_norm": 1.6551918983459473,
      "learning_rate": 0.00019962658066706272,
      "loss": 0.1083,
      "step": 896
    },
    {
      "epoch": 0.004219310046379484,
      "grad_norm": 1.1116377115249634,
      "learning_rate": 0.00019962563768894924,
      "loss": 0.0797,
      "step": 897
    },
    {
      "epoch": 0.004224013847991947,
      "grad_norm": 5.60377311706543,
      "learning_rate": 0.00019962469471083579,
      "loss": 0.6621,
      "step": 898
    },
    {
      "epoch": 0.00422871764960441,
      "grad_norm": 2.269144296646118,
      "learning_rate": 0.0001996237517327223,
      "loss": 0.1391,
      "step": 899
    },
    {
      "epoch": 0.004233421451216873,
      "grad_norm": 4.111459732055664,
      "learning_rate": 0.00019962280875460882,
      "loss": 0.5594,
      "step": 900
    },
    {
      "epoch": 0.004238125252829337,
      "grad_norm": 12.483025550842285,
      "learning_rate": 0.00019962186577649534,
      "loss": 0.6212,
      "step": 901
    },
    {
      "epoch": 0.0042428290544418,
      "grad_norm": 4.360960483551025,
      "learning_rate": 0.00019962092279838186,
      "loss": 0.4917,
      "step": 902
    },
    {
      "epoch": 0.004247532856054263,
      "grad_norm": 5.507785797119141,
      "learning_rate": 0.00019961997982026838,
      "loss": 0.4741,
      "step": 903
    },
    {
      "epoch": 0.004252236657666726,
      "grad_norm": 2.8661887645721436,
      "learning_rate": 0.0001996190368421549,
      "loss": 0.3985,
      "step": 904
    },
    {
      "epoch": 0.004256940459279189,
      "grad_norm": 5.462536811828613,
      "learning_rate": 0.00019961809386404142,
      "loss": 1.1777,
      "step": 905
    },
    {
      "epoch": 0.004261644260891652,
      "grad_norm": 2.271923780441284,
      "learning_rate": 0.00019961715088592793,
      "loss": 0.3083,
      "step": 906
    },
    {
      "epoch": 0.004266348062504116,
      "grad_norm": 3.3140461444854736,
      "learning_rate": 0.00019961620790781448,
      "loss": 0.4028,
      "step": 907
    },
    {
      "epoch": 0.004271051864116579,
      "grad_norm": 1.091633915901184,
      "learning_rate": 0.000199615264929701,
      "loss": 0.0983,
      "step": 908
    },
    {
      "epoch": 0.004275755665729042,
      "grad_norm": 1.4230220317840576,
      "learning_rate": 0.00019961432195158752,
      "loss": 0.1459,
      "step": 909
    },
    {
      "epoch": 0.004280459467341505,
      "grad_norm": 2.245314121246338,
      "learning_rate": 0.00019961337897347404,
      "loss": 0.3139,
      "step": 910
    },
    {
      "epoch": 0.004285163268953968,
      "grad_norm": 2.5795958042144775,
      "learning_rate": 0.00019961243599536056,
      "loss": 0.3275,
      "step": 911
    },
    {
      "epoch": 0.004289867070566432,
      "grad_norm": 3.3103761672973633,
      "learning_rate": 0.00019961149301724707,
      "loss": 0.384,
      "step": 912
    },
    {
      "epoch": 0.004294570872178895,
      "grad_norm": 1.2406344413757324,
      "learning_rate": 0.0001996105500391336,
      "loss": 0.1954,
      "step": 913
    },
    {
      "epoch": 0.004299274673791358,
      "grad_norm": 1.357040524482727,
      "learning_rate": 0.0001996096070610201,
      "loss": 0.1226,
      "step": 914
    },
    {
      "epoch": 0.004303978475403821,
      "grad_norm": 3.5683462619781494,
      "learning_rate": 0.00019960866408290663,
      "loss": 0.5051,
      "step": 915
    },
    {
      "epoch": 0.004308682277016284,
      "grad_norm": 3.2784461975097656,
      "learning_rate": 0.00019960772110479318,
      "loss": 0.4417,
      "step": 916
    },
    {
      "epoch": 0.004313386078628747,
      "grad_norm": 2.0401432514190674,
      "learning_rate": 0.0001996067781266797,
      "loss": 0.1924,
      "step": 917
    },
    {
      "epoch": 0.004318089880241211,
      "grad_norm": 2.8115358352661133,
      "learning_rate": 0.0001996058351485662,
      "loss": 0.517,
      "step": 918
    },
    {
      "epoch": 0.004322793681853674,
      "grad_norm": 1.46018385887146,
      "learning_rate": 0.00019960489217045273,
      "loss": 0.1038,
      "step": 919
    },
    {
      "epoch": 0.004327497483466137,
      "grad_norm": 0.4845624268054962,
      "learning_rate": 0.00019960394919233925,
      "loss": 0.0317,
      "step": 920
    },
    {
      "epoch": 0.0043322012850786,
      "grad_norm": 1.2271971702575684,
      "learning_rate": 0.0001996030062142258,
      "loss": 0.1474,
      "step": 921
    },
    {
      "epoch": 0.0043369050866910635,
      "grad_norm": 2.8424367904663086,
      "learning_rate": 0.00019960206323611231,
      "loss": 0.4868,
      "step": 922
    },
    {
      "epoch": 0.0043416088883035265,
      "grad_norm": 1.5377953052520752,
      "learning_rate": 0.0001996011202579988,
      "loss": 0.1404,
      "step": 923
    },
    {
      "epoch": 0.00434631268991599,
      "grad_norm": 1.2843941450119019,
      "learning_rate": 0.00019960017727988532,
      "loss": 0.1417,
      "step": 924
    },
    {
      "epoch": 0.0043510164915284534,
      "grad_norm": 1.3842206001281738,
      "learning_rate": 0.00019959923430177187,
      "loss": 0.083,
      "step": 925
    },
    {
      "epoch": 0.0043557202931409165,
      "grad_norm": 1.7149360179901123,
      "learning_rate": 0.0001995982913236584,
      "loss": 0.1455,
      "step": 926
    },
    {
      "epoch": 0.0043604240947533795,
      "grad_norm": 4.338540077209473,
      "learning_rate": 0.0001995973483455449,
      "loss": 0.7917,
      "step": 927
    },
    {
      "epoch": 0.0043651278963658426,
      "grad_norm": 1.5320348739624023,
      "learning_rate": 0.00019959640536743143,
      "loss": 0.0967,
      "step": 928
    },
    {
      "epoch": 0.0043698316979783065,
      "grad_norm": 2.3303372859954834,
      "learning_rate": 0.00019959546238931794,
      "loss": 0.1819,
      "step": 929
    },
    {
      "epoch": 0.0043745354995907695,
      "grad_norm": 1.3356075286865234,
      "learning_rate": 0.0001995945194112045,
      "loss": 0.1309,
      "step": 930
    },
    {
      "epoch": 0.0043792393012032325,
      "grad_norm": 3.839742660522461,
      "learning_rate": 0.000199593576433091,
      "loss": 0.5089,
      "step": 931
    },
    {
      "epoch": 0.004383943102815696,
      "grad_norm": 8.025230407714844,
      "learning_rate": 0.00019959263345497753,
      "loss": 1.1945,
      "step": 932
    },
    {
      "epoch": 0.004388646904428159,
      "grad_norm": 1.2798118591308594,
      "learning_rate": 0.00019959169047686405,
      "loss": 0.058,
      "step": 933
    },
    {
      "epoch": 0.004393350706040622,
      "grad_norm": 4.224756717681885,
      "learning_rate": 0.00019959074749875057,
      "loss": 0.3425,
      "step": 934
    },
    {
      "epoch": 0.004398054507653086,
      "grad_norm": 3.9899611473083496,
      "learning_rate": 0.00019958980452063708,
      "loss": 0.2297,
      "step": 935
    },
    {
      "epoch": 0.004402758309265549,
      "grad_norm": 4.7733869552612305,
      "learning_rate": 0.0001995888615425236,
      "loss": 0.5158,
      "step": 936
    },
    {
      "epoch": 0.004407462110878012,
      "grad_norm": 1.0257493257522583,
      "learning_rate": 0.00019958791856441012,
      "loss": 0.0502,
      "step": 937
    },
    {
      "epoch": 0.004412165912490475,
      "grad_norm": 2.2008612155914307,
      "learning_rate": 0.00019958697558629664,
      "loss": 0.2069,
      "step": 938
    },
    {
      "epoch": 0.004416869714102938,
      "grad_norm": 0.045743364840745926,
      "learning_rate": 0.00019958603260818319,
      "loss": 0.0019,
      "step": 939
    },
    {
      "epoch": 0.004421573515715401,
      "grad_norm": 2.251311779022217,
      "learning_rate": 0.0001995850896300697,
      "loss": 0.1402,
      "step": 940
    },
    {
      "epoch": 0.004426277317327865,
      "grad_norm": 6.030086517333984,
      "learning_rate": 0.00019958414665195622,
      "loss": 0.7199,
      "step": 941
    },
    {
      "epoch": 0.004430981118940328,
      "grad_norm": 4.009695529937744,
      "learning_rate": 0.00019958320367384274,
      "loss": 0.4455,
      "step": 942
    },
    {
      "epoch": 0.004435684920552791,
      "grad_norm": 5.446501731872559,
      "learning_rate": 0.00019958226069572926,
      "loss": 1.0093,
      "step": 943
    },
    {
      "epoch": 0.004440388722165254,
      "grad_norm": 3.7866311073303223,
      "learning_rate": 0.00019958131771761578,
      "loss": 0.9183,
      "step": 944
    },
    {
      "epoch": 0.004445092523777717,
      "grad_norm": 1.9830983877182007,
      "learning_rate": 0.0001995803747395023,
      "loss": 0.103,
      "step": 945
    },
    {
      "epoch": 0.004449796325390181,
      "grad_norm": 0.4683980345726013,
      "learning_rate": 0.00019957943176138882,
      "loss": 0.0283,
      "step": 946
    },
    {
      "epoch": 0.004454500127002644,
      "grad_norm": 0.39663249254226685,
      "learning_rate": 0.00019957848878327533,
      "loss": 0.0194,
      "step": 947
    },
    {
      "epoch": 0.004459203928615107,
      "grad_norm": 1.2187803983688354,
      "learning_rate": 0.00019957754580516188,
      "loss": 0.1114,
      "step": 948
    },
    {
      "epoch": 0.00446390773022757,
      "grad_norm": 1.4147366285324097,
      "learning_rate": 0.0001995766028270484,
      "loss": 0.1365,
      "step": 949
    },
    {
      "epoch": 0.004468611531840033,
      "grad_norm": 1.4079573154449463,
      "learning_rate": 0.00019957565984893492,
      "loss": 0.0555,
      "step": 950
    },
    {
      "epoch": 0.004473315333452496,
      "grad_norm": 5.449100494384766,
      "learning_rate": 0.00019957471687082144,
      "loss": 0.8513,
      "step": 951
    },
    {
      "epoch": 0.00447801913506496,
      "grad_norm": 3.629546642303467,
      "learning_rate": 0.00019957377389270795,
      "loss": 0.3315,
      "step": 952
    },
    {
      "epoch": 0.004482722936677423,
      "grad_norm": 2.6535229682922363,
      "learning_rate": 0.0001995728309145945,
      "loss": 0.195,
      "step": 953
    },
    {
      "epoch": 0.004487426738289886,
      "grad_norm": 3.9355952739715576,
      "learning_rate": 0.000199571887936481,
      "loss": 0.3604,
      "step": 954
    },
    {
      "epoch": 0.004492130539902349,
      "grad_norm": 8.626532554626465,
      "learning_rate": 0.0001995709449583675,
      "loss": 0.8112,
      "step": 955
    },
    {
      "epoch": 0.004496834341514812,
      "grad_norm": 0.9100933074951172,
      "learning_rate": 0.00019957000198025403,
      "loss": 0.1116,
      "step": 956
    },
    {
      "epoch": 0.004501538143127276,
      "grad_norm": 0.6352298855781555,
      "learning_rate": 0.00019956905900214058,
      "loss": 0.0467,
      "step": 957
    },
    {
      "epoch": 0.004506241944739739,
      "grad_norm": 1.5351358652114868,
      "learning_rate": 0.0001995681160240271,
      "loss": 0.1405,
      "step": 958
    },
    {
      "epoch": 0.004510945746352202,
      "grad_norm": 3.540846824645996,
      "learning_rate": 0.0001995671730459136,
      "loss": 0.4124,
      "step": 959
    },
    {
      "epoch": 0.004515649547964665,
      "grad_norm": 4.930948734283447,
      "learning_rate": 0.00019956623006780013,
      "loss": 0.693,
      "step": 960
    },
    {
      "epoch": 0.004520353349577128,
      "grad_norm": 5.22108793258667,
      "learning_rate": 0.00019956528708968665,
      "loss": 0.9958,
      "step": 961
    },
    {
      "epoch": 0.004525057151189591,
      "grad_norm": 1.3187599182128906,
      "learning_rate": 0.0001995643441115732,
      "loss": 0.088,
      "step": 962
    },
    {
      "epoch": 0.004529760952802055,
      "grad_norm": 2.4778757095336914,
      "learning_rate": 0.00019956340113345971,
      "loss": 0.2594,
      "step": 963
    },
    {
      "epoch": 0.004534464754414518,
      "grad_norm": 3.469446897506714,
      "learning_rate": 0.00019956245815534623,
      "loss": 0.1832,
      "step": 964
    },
    {
      "epoch": 0.004539168556026981,
      "grad_norm": 3.6709208488464355,
      "learning_rate": 0.00019956151517723275,
      "loss": 0.459,
      "step": 965
    },
    {
      "epoch": 0.004543872357639444,
      "grad_norm": 4.824512958526611,
      "learning_rate": 0.00019956057219911927,
      "loss": 0.3857,
      "step": 966
    },
    {
      "epoch": 0.004548576159251907,
      "grad_norm": 2.7064156532287598,
      "learning_rate": 0.0001995596292210058,
      "loss": 0.2129,
      "step": 967
    },
    {
      "epoch": 0.00455327996086437,
      "grad_norm": 3.5702197551727295,
      "learning_rate": 0.0001995586862428923,
      "loss": 0.3311,
      "step": 968
    },
    {
      "epoch": 0.004557983762476834,
      "grad_norm": 3.2822742462158203,
      "learning_rate": 0.00019955774326477883,
      "loss": 0.3924,
      "step": 969
    },
    {
      "epoch": 0.004562687564089297,
      "grad_norm": 0.755229115486145,
      "learning_rate": 0.00019955680028666534,
      "loss": 0.0562,
      "step": 970
    },
    {
      "epoch": 0.00456739136570176,
      "grad_norm": 2.853001594543457,
      "learning_rate": 0.0001995558573085519,
      "loss": 0.2717,
      "step": 971
    },
    {
      "epoch": 0.004572095167314223,
      "grad_norm": 1.9549243450164795,
      "learning_rate": 0.0001995549143304384,
      "loss": 0.2454,
      "step": 972
    },
    {
      "epoch": 0.004576798968926686,
      "grad_norm": 1.1374268531799316,
      "learning_rate": 0.00019955397135232493,
      "loss": 0.1016,
      "step": 973
    },
    {
      "epoch": 0.00458150277053915,
      "grad_norm": 2.5307137966156006,
      "learning_rate": 0.00019955302837421145,
      "loss": 0.2085,
      "step": 974
    },
    {
      "epoch": 0.004586206572151613,
      "grad_norm": 0.48829177021980286,
      "learning_rate": 0.00019955208539609797,
      "loss": 0.0417,
      "step": 975
    },
    {
      "epoch": 0.004590910373764076,
      "grad_norm": 3.1844186782836914,
      "learning_rate": 0.00019955114241798448,
      "loss": 0.3265,
      "step": 976
    },
    {
      "epoch": 0.004595614175376539,
      "grad_norm": 3.6221959590911865,
      "learning_rate": 0.000199550199439871,
      "loss": 0.3608,
      "step": 977
    },
    {
      "epoch": 0.004600317976989002,
      "grad_norm": 2.9251344203948975,
      "learning_rate": 0.00019954925646175752,
      "loss": 0.2564,
      "step": 978
    },
    {
      "epoch": 0.004605021778601465,
      "grad_norm": 1.3449184894561768,
      "learning_rate": 0.00019954831348364404,
      "loss": 0.108,
      "step": 979
    },
    {
      "epoch": 0.004609725580213929,
      "grad_norm": 1.962941288948059,
      "learning_rate": 0.00019954737050553059,
      "loss": 0.1655,
      "step": 980
    },
    {
      "epoch": 0.004614429381826392,
      "grad_norm": 2.5324764251708984,
      "learning_rate": 0.0001995464275274171,
      "loss": 0.3142,
      "step": 981
    },
    {
      "epoch": 0.004619133183438855,
      "grad_norm": 11.012393951416016,
      "learning_rate": 0.00019954548454930362,
      "loss": 1.0578,
      "step": 982
    },
    {
      "epoch": 0.004623836985051318,
      "grad_norm": 0.5263782143592834,
      "learning_rate": 0.00019954454157119014,
      "loss": 0.0407,
      "step": 983
    },
    {
      "epoch": 0.004628540786663781,
      "grad_norm": 3.5418176651000977,
      "learning_rate": 0.0001995435985930767,
      "loss": 0.4474,
      "step": 984
    },
    {
      "epoch": 0.0046332445882762445,
      "grad_norm": 5.036700248718262,
      "learning_rate": 0.00019954265561496318,
      "loss": 0.6779,
      "step": 985
    },
    {
      "epoch": 0.004637948389888708,
      "grad_norm": 2.3286685943603516,
      "learning_rate": 0.0001995417126368497,
      "loss": 0.1594,
      "step": 986
    },
    {
      "epoch": 0.004642652191501171,
      "grad_norm": 5.382603168487549,
      "learning_rate": 0.00019954076965873622,
      "loss": 0.7271,
      "step": 987
    },
    {
      "epoch": 0.0046473559931136344,
      "grad_norm": 4.186121463775635,
      "learning_rate": 0.00019953982668062273,
      "loss": 0.5224,
      "step": 988
    },
    {
      "epoch": 0.0046520597947260975,
      "grad_norm": 1.1043283939361572,
      "learning_rate": 0.00019953888370250928,
      "loss": 0.1904,
      "step": 989
    },
    {
      "epoch": 0.0046567635963385605,
      "grad_norm": 3.55419921875,
      "learning_rate": 0.0001995379407243958,
      "loss": 0.5333,
      "step": 990
    },
    {
      "epoch": 0.004661467397951024,
      "grad_norm": 0.3639831840991974,
      "learning_rate": 0.00019953699774628232,
      "loss": 0.0229,
      "step": 991
    },
    {
      "epoch": 0.0046661711995634875,
      "grad_norm": 4.613901138305664,
      "learning_rate": 0.00019953605476816884,
      "loss": 0.4689,
      "step": 992
    },
    {
      "epoch": 0.0046708750011759505,
      "grad_norm": 2.176645517349243,
      "learning_rate": 0.00019953511179005538,
      "loss": 0.4317,
      "step": 993
    },
    {
      "epoch": 0.0046755788027884135,
      "grad_norm": 0.9780870079994202,
      "learning_rate": 0.0001995341688119419,
      "loss": 0.0627,
      "step": 994
    },
    {
      "epoch": 0.004680282604400877,
      "grad_norm": 3.4286422729492188,
      "learning_rate": 0.00019953322583382842,
      "loss": 0.5877,
      "step": 995
    },
    {
      "epoch": 0.00468498640601334,
      "grad_norm": 0.7196195721626282,
      "learning_rate": 0.00019953228285571494,
      "loss": 0.0755,
      "step": 996
    },
    {
      "epoch": 0.0046896902076258035,
      "grad_norm": 3.8133199214935303,
      "learning_rate": 0.00019953133987760143,
      "loss": 0.7722,
      "step": 997
    },
    {
      "epoch": 0.004694394009238267,
      "grad_norm": 0.9880680441856384,
      "learning_rate": 0.00019953039689948798,
      "loss": 0.1169,
      "step": 998
    },
    {
      "epoch": 0.00469909781085073,
      "grad_norm": 2.7337920665740967,
      "learning_rate": 0.0001995294539213745,
      "loss": 0.6155,
      "step": 999
    },
    {
      "epoch": 0.004703801612463193,
      "grad_norm": 1.417164921760559,
      "learning_rate": 0.000199528510943261,
      "loss": 0.3805,
      "step": 1000
    },
    {
      "epoch": 0.004708505414075656,
      "grad_norm": 2.4144654273986816,
      "learning_rate": 0.00019952756796514753,
      "loss": 0.3085,
      "step": 1001
    },
    {
      "epoch": 0.004713209215688119,
      "grad_norm": 3.371823310852051,
      "learning_rate": 0.00019952662498703405,
      "loss": 0.2914,
      "step": 1002
    },
    {
      "epoch": 0.004717913017300583,
      "grad_norm": 3.0819480419158936,
      "learning_rate": 0.0001995256820089206,
      "loss": 0.4458,
      "step": 1003
    },
    {
      "epoch": 0.004722616818913046,
      "grad_norm": 0.5641683340072632,
      "learning_rate": 0.00019952473903080711,
      "loss": 0.0371,
      "step": 1004
    },
    {
      "epoch": 0.004727320620525509,
      "grad_norm": 1.409862756729126,
      "learning_rate": 0.00019952379605269363,
      "loss": 0.2671,
      "step": 1005
    },
    {
      "epoch": 0.004732024422137972,
      "grad_norm": 0.7071142196655273,
      "learning_rate": 0.00019952285307458015,
      "loss": 0.0589,
      "step": 1006
    },
    {
      "epoch": 0.004736728223750435,
      "grad_norm": 1.2442493438720703,
      "learning_rate": 0.00019952191009646667,
      "loss": 0.0891,
      "step": 1007
    },
    {
      "epoch": 0.004741432025362899,
      "grad_norm": 1.0347414016723633,
      "learning_rate": 0.0001995209671183532,
      "loss": 0.0956,
      "step": 1008
    },
    {
      "epoch": 0.004746135826975362,
      "grad_norm": 1.0161340236663818,
      "learning_rate": 0.0001995200241402397,
      "loss": 0.0977,
      "step": 1009
    },
    {
      "epoch": 0.004750839628587825,
      "grad_norm": 3.2515039443969727,
      "learning_rate": 0.00019951908116212623,
      "loss": 0.3356,
      "step": 1010
    },
    {
      "epoch": 0.004755543430200288,
      "grad_norm": 1.6685450077056885,
      "learning_rate": 0.00019951813818401274,
      "loss": 0.1069,
      "step": 1011
    },
    {
      "epoch": 0.004760247231812751,
      "grad_norm": 0.5254742503166199,
      "learning_rate": 0.0001995171952058993,
      "loss": 0.0391,
      "step": 1012
    },
    {
      "epoch": 0.004764951033425214,
      "grad_norm": 4.024691581726074,
      "learning_rate": 0.0001995162522277858,
      "loss": 0.3176,
      "step": 1013
    },
    {
      "epoch": 0.004769654835037678,
      "grad_norm": 2.1956052780151367,
      "learning_rate": 0.00019951530924967233,
      "loss": 0.2312,
      "step": 1014
    },
    {
      "epoch": 0.004774358636650141,
      "grad_norm": 0.5061267018318176,
      "learning_rate": 0.00019951436627155885,
      "loss": 0.0244,
      "step": 1015
    },
    {
      "epoch": 0.004779062438262604,
      "grad_norm": 2.4792463779449463,
      "learning_rate": 0.00019951342329344537,
      "loss": 0.1097,
      "step": 1016
    },
    {
      "epoch": 0.004783766239875067,
      "grad_norm": 1.4886369705200195,
      "learning_rate": 0.00019951248031533188,
      "loss": 0.083,
      "step": 1017
    },
    {
      "epoch": 0.00478847004148753,
      "grad_norm": 1.2577072381973267,
      "learning_rate": 0.0001995115373372184,
      "loss": 0.0507,
      "step": 1018
    },
    {
      "epoch": 0.004793173843099993,
      "grad_norm": 3.470728635787964,
      "learning_rate": 0.00019951059435910492,
      "loss": 0.4907,
      "step": 1019
    },
    {
      "epoch": 0.004797877644712457,
      "grad_norm": 3.469719648361206,
      "learning_rate": 0.00019950965138099144,
      "loss": 0.1951,
      "step": 1020
    },
    {
      "epoch": 0.00480258144632492,
      "grad_norm": 5.453260898590088,
      "learning_rate": 0.00019950870840287799,
      "loss": 0.3029,
      "step": 1021
    },
    {
      "epoch": 0.004807285247937383,
      "grad_norm": 5.863182544708252,
      "learning_rate": 0.0001995077654247645,
      "loss": 0.5312,
      "step": 1022
    },
    {
      "epoch": 0.004811989049549846,
      "grad_norm": 0.14239713549613953,
      "learning_rate": 0.00019950682244665102,
      "loss": 0.0063,
      "step": 1023
    },
    {
      "epoch": 0.004816692851162309,
      "grad_norm": 7.697413921356201,
      "learning_rate": 0.00019950587946853754,
      "loss": 0.8534,
      "step": 1024
    },
    {
      "epoch": 0.004821396652774773,
      "grad_norm": 0.8317485451698303,
      "learning_rate": 0.0001995049364904241,
      "loss": 0.0385,
      "step": 1025
    },
    {
      "epoch": 0.004826100454387236,
      "grad_norm": 1.4528212547302246,
      "learning_rate": 0.0001995039935123106,
      "loss": 0.0811,
      "step": 1026
    },
    {
      "epoch": 0.004830804255999699,
      "grad_norm": 3.97784423828125,
      "learning_rate": 0.00019950305053419712,
      "loss": 0.5784,
      "step": 1027
    },
    {
      "epoch": 0.004835508057612162,
      "grad_norm": 5.4048662185668945,
      "learning_rate": 0.00019950210755608362,
      "loss": 0.927,
      "step": 1028
    },
    {
      "epoch": 0.004840211859224625,
      "grad_norm": 0.3229825794696808,
      "learning_rate": 0.00019950116457797013,
      "loss": 0.0164,
      "step": 1029
    },
    {
      "epoch": 0.004844915660837088,
      "grad_norm": 2.857028007507324,
      "learning_rate": 0.00019950022159985668,
      "loss": 0.1135,
      "step": 1030
    },
    {
      "epoch": 0.004849619462449552,
      "grad_norm": 4.3562822341918945,
      "learning_rate": 0.0001994992786217432,
      "loss": 0.1896,
      "step": 1031
    },
    {
      "epoch": 0.004854323264062015,
      "grad_norm": 2.629474639892578,
      "learning_rate": 0.00019949833564362972,
      "loss": 0.1226,
      "step": 1032
    },
    {
      "epoch": 0.004859027065674478,
      "grad_norm": 1.9294987916946411,
      "learning_rate": 0.00019949739266551624,
      "loss": 0.1256,
      "step": 1033
    },
    {
      "epoch": 0.004863730867286941,
      "grad_norm": 3.276109457015991,
      "learning_rate": 0.00019949644968740278,
      "loss": 0.2731,
      "step": 1034
    },
    {
      "epoch": 0.004868434668899404,
      "grad_norm": 3.0557732582092285,
      "learning_rate": 0.0001994955067092893,
      "loss": 0.2279,
      "step": 1035
    },
    {
      "epoch": 0.004873138470511867,
      "grad_norm": 3.415196418762207,
      "learning_rate": 0.00019949456373117582,
      "loss": 0.2323,
      "step": 1036
    },
    {
      "epoch": 0.004877842272124331,
      "grad_norm": 4.686676502227783,
      "learning_rate": 0.00019949362075306234,
      "loss": 0.3226,
      "step": 1037
    },
    {
      "epoch": 0.004882546073736794,
      "grad_norm": 1.4375680685043335,
      "learning_rate": 0.00019949267777494886,
      "loss": 0.1089,
      "step": 1038
    },
    {
      "epoch": 0.004887249875349257,
      "grad_norm": 0.5946130752563477,
      "learning_rate": 0.00019949173479683538,
      "loss": 0.0292,
      "step": 1039
    },
    {
      "epoch": 0.00489195367696172,
      "grad_norm": 7.800351142883301,
      "learning_rate": 0.0001994907918187219,
      "loss": 0.381,
      "step": 1040
    },
    {
      "epoch": 0.004896657478574183,
      "grad_norm": 0.6906222105026245,
      "learning_rate": 0.0001994898488406084,
      "loss": 0.0345,
      "step": 1041
    },
    {
      "epoch": 0.004901361280186647,
      "grad_norm": 2.8716204166412354,
      "learning_rate": 0.00019948890586249493,
      "loss": 0.3979,
      "step": 1042
    },
    {
      "epoch": 0.00490606508179911,
      "grad_norm": 2.7221360206604004,
      "learning_rate": 0.00019948796288438148,
      "loss": 0.2312,
      "step": 1043
    },
    {
      "epoch": 0.004910768883411573,
      "grad_norm": 1.5596845149993896,
      "learning_rate": 0.000199487019906268,
      "loss": 0.1057,
      "step": 1044
    },
    {
      "epoch": 0.004915472685024036,
      "grad_norm": 5.640894889831543,
      "learning_rate": 0.00019948607692815451,
      "loss": 0.4494,
      "step": 1045
    },
    {
      "epoch": 0.004920176486636499,
      "grad_norm": 0.8089331984519958,
      "learning_rate": 0.00019948513395004103,
      "loss": 0.0661,
      "step": 1046
    },
    {
      "epoch": 0.004924880288248962,
      "grad_norm": 0.11882041394710541,
      "learning_rate": 0.00019948419097192755,
      "loss": 0.0048,
      "step": 1047
    },
    {
      "epoch": 0.004929584089861426,
      "grad_norm": 6.32000732421875,
      "learning_rate": 0.00019948324799381407,
      "loss": 0.8707,
      "step": 1048
    },
    {
      "epoch": 0.004934287891473889,
      "grad_norm": 4.88183069229126,
      "learning_rate": 0.0001994823050157006,
      "loss": 0.8122,
      "step": 1049
    },
    {
      "epoch": 0.004938991693086352,
      "grad_norm": 3.576549530029297,
      "learning_rate": 0.0001994813620375871,
      "loss": 0.2541,
      "step": 1050
    },
    {
      "epoch": 0.0049436954946988155,
      "grad_norm": 3.2755815982818604,
      "learning_rate": 0.00019948041905947363,
      "loss": 0.0851,
      "step": 1051
    },
    {
      "epoch": 0.0049483992963112785,
      "grad_norm": 6.1543660163879395,
      "learning_rate": 0.00019947947608136014,
      "loss": 0.6002,
      "step": 1052
    },
    {
      "epoch": 0.004953103097923742,
      "grad_norm": 4.3052659034729,
      "learning_rate": 0.0001994785331032467,
      "loss": 0.1786,
      "step": 1053
    },
    {
      "epoch": 0.0049578068995362054,
      "grad_norm": 7.985748291015625,
      "learning_rate": 0.0001994775901251332,
      "loss": 0.3858,
      "step": 1054
    },
    {
      "epoch": 0.0049625107011486685,
      "grad_norm": 4.39776086807251,
      "learning_rate": 0.00019947664714701973,
      "loss": 0.5021,
      "step": 1055
    },
    {
      "epoch": 0.0049672145027611315,
      "grad_norm": 6.377254486083984,
      "learning_rate": 0.00019947570416890625,
      "loss": 0.5788,
      "step": 1056
    },
    {
      "epoch": 0.0049719183043735946,
      "grad_norm": 4.167307376861572,
      "learning_rate": 0.0001994747611907928,
      "loss": 0.212,
      "step": 1057
    },
    {
      "epoch": 0.004976622105986058,
      "grad_norm": 1.1526685953140259,
      "learning_rate": 0.0001994738182126793,
      "loss": 0.0561,
      "step": 1058
    },
    {
      "epoch": 0.0049813259075985215,
      "grad_norm": 5.158437728881836,
      "learning_rate": 0.0001994728752345658,
      "loss": 0.2687,
      "step": 1059
    },
    {
      "epoch": 0.0049860297092109845,
      "grad_norm": 1.498111367225647,
      "learning_rate": 0.00019947193225645232,
      "loss": 0.1288,
      "step": 1060
    },
    {
      "epoch": 0.004990733510823448,
      "grad_norm": 1.9581708908081055,
      "learning_rate": 0.00019947098927833884,
      "loss": 0.1301,
      "step": 1061
    },
    {
      "epoch": 0.004995437312435911,
      "grad_norm": 3.626494884490967,
      "learning_rate": 0.00019947004630022539,
      "loss": 0.2672,
      "step": 1062
    },
    {
      "epoch": 0.005000141114048374,
      "grad_norm": 3.713090658187866,
      "learning_rate": 0.0001994691033221119,
      "loss": 0.262,
      "step": 1063
    },
    {
      "epoch": 0.005004844915660837,
      "grad_norm": 4.511417865753174,
      "learning_rate": 0.00019946816034399842,
      "loss": 0.3901,
      "step": 1064
    },
    {
      "epoch": 0.005009548717273301,
      "grad_norm": 3.0603978633880615,
      "learning_rate": 0.00019946721736588494,
      "loss": 0.4096,
      "step": 1065
    },
    {
      "epoch": 0.005014252518885764,
      "grad_norm": 8.997435569763184,
      "learning_rate": 0.0001994662743877715,
      "loss": 1.0569,
      "step": 1066
    },
    {
      "epoch": 0.005018956320498227,
      "grad_norm": 2.2681939601898193,
      "learning_rate": 0.000199465331409658,
      "loss": 0.2313,
      "step": 1067
    },
    {
      "epoch": 0.00502366012211069,
      "grad_norm": 4.060677528381348,
      "learning_rate": 0.00019946438843154452,
      "loss": 0.1254,
      "step": 1068
    },
    {
      "epoch": 0.005028363923723153,
      "grad_norm": 2.0920708179473877,
      "learning_rate": 0.00019946344545343104,
      "loss": 0.3237,
      "step": 1069
    },
    {
      "epoch": 0.005033067725335617,
      "grad_norm": 1.4260982275009155,
      "learning_rate": 0.00019946250247531753,
      "loss": 0.1982,
      "step": 1070
    },
    {
      "epoch": 0.00503777152694808,
      "grad_norm": 3.120993137359619,
      "learning_rate": 0.00019946155949720408,
      "loss": 0.4726,
      "step": 1071
    },
    {
      "epoch": 0.005042475328560543,
      "grad_norm": 0.5227658748626709,
      "learning_rate": 0.0001994606165190906,
      "loss": 0.0406,
      "step": 1072
    },
    {
      "epoch": 0.005047179130173006,
      "grad_norm": 1.3785842657089233,
      "learning_rate": 0.00019945967354097712,
      "loss": 0.0958,
      "step": 1073
    },
    {
      "epoch": 0.005051882931785469,
      "grad_norm": 0.9395413994789124,
      "learning_rate": 0.00019945873056286364,
      "loss": 0.0694,
      "step": 1074
    },
    {
      "epoch": 0.005056586733397932,
      "grad_norm": 2.5200278759002686,
      "learning_rate": 0.00019945778758475018,
      "loss": 0.2412,
      "step": 1075
    },
    {
      "epoch": 0.005061290535010396,
      "grad_norm": 1.4473766088485718,
      "learning_rate": 0.0001994568446066367,
      "loss": 0.2174,
      "step": 1076
    },
    {
      "epoch": 0.005065994336622859,
      "grad_norm": 0.6396288871765137,
      "learning_rate": 0.00019945590162852322,
      "loss": 0.0368,
      "step": 1077
    },
    {
      "epoch": 0.005070698138235322,
      "grad_norm": 6.36503267288208,
      "learning_rate": 0.00019945495865040974,
      "loss": 0.9595,
      "step": 1078
    },
    {
      "epoch": 0.005075401939847785,
      "grad_norm": 1.5787138938903809,
      "learning_rate": 0.00019945401567229626,
      "loss": 0.114,
      "step": 1079
    },
    {
      "epoch": 0.005080105741460248,
      "grad_norm": 1.5769119262695312,
      "learning_rate": 0.00019945307269418278,
      "loss": 0.1512,
      "step": 1080
    },
    {
      "epoch": 0.005084809543072711,
      "grad_norm": 0.9310958981513977,
      "learning_rate": 0.0001994521297160693,
      "loss": 0.0541,
      "step": 1081
    },
    {
      "epoch": 0.005089513344685175,
      "grad_norm": 0.862682044506073,
      "learning_rate": 0.0001994511867379558,
      "loss": 0.0366,
      "step": 1082
    },
    {
      "epoch": 0.005094217146297638,
      "grad_norm": 4.943583965301514,
      "learning_rate": 0.00019945024375984233,
      "loss": 0.7797,
      "step": 1083
    },
    {
      "epoch": 0.005098920947910101,
      "grad_norm": 5.206130027770996,
      "learning_rate": 0.00019944930078172888,
      "loss": 0.5981,
      "step": 1084
    },
    {
      "epoch": 0.005103624749522564,
      "grad_norm": 0.11009436100721359,
      "learning_rate": 0.0001994483578036154,
      "loss": 0.0057,
      "step": 1085
    },
    {
      "epoch": 0.005108328551135027,
      "grad_norm": 0.22992177307605743,
      "learning_rate": 0.00019944741482550191,
      "loss": 0.0108,
      "step": 1086
    },
    {
      "epoch": 0.005113032352747491,
      "grad_norm": 5.002813339233398,
      "learning_rate": 0.00019944647184738843,
      "loss": 0.6821,
      "step": 1087
    },
    {
      "epoch": 0.005117736154359954,
      "grad_norm": 2.4107606410980225,
      "learning_rate": 0.00019944552886927495,
      "loss": 0.2198,
      "step": 1088
    },
    {
      "epoch": 0.005122439955972417,
      "grad_norm": 3.9342703819274902,
      "learning_rate": 0.0001994445858911615,
      "loss": 0.3272,
      "step": 1089
    },
    {
      "epoch": 0.00512714375758488,
      "grad_norm": 2.632293224334717,
      "learning_rate": 0.000199443642913048,
      "loss": 0.1643,
      "step": 1090
    },
    {
      "epoch": 0.005131847559197343,
      "grad_norm": 0.6136506199836731,
      "learning_rate": 0.0001994426999349345,
      "loss": 0.0426,
      "step": 1091
    },
    {
      "epoch": 0.005136551360809806,
      "grad_norm": 5.331970691680908,
      "learning_rate": 0.00019944175695682103,
      "loss": 0.6165,
      "step": 1092
    },
    {
      "epoch": 0.00514125516242227,
      "grad_norm": 4.332890510559082,
      "learning_rate": 0.00019944081397870757,
      "loss": 0.5747,
      "step": 1093
    },
    {
      "epoch": 0.005145958964034733,
      "grad_norm": 0.876818835735321,
      "learning_rate": 0.0001994398710005941,
      "loss": 0.0445,
      "step": 1094
    },
    {
      "epoch": 0.005150662765647196,
      "grad_norm": 0.6209219098091125,
      "learning_rate": 0.0001994389280224806,
      "loss": 0.0415,
      "step": 1095
    },
    {
      "epoch": 0.005155366567259659,
      "grad_norm": 0.705991268157959,
      "learning_rate": 0.00019943798504436713,
      "loss": 0.0656,
      "step": 1096
    },
    {
      "epoch": 0.005160070368872122,
      "grad_norm": 2.3394858837127686,
      "learning_rate": 0.00019943704206625365,
      "loss": 0.228,
      "step": 1097
    },
    {
      "epoch": 0.005164774170484585,
      "grad_norm": 2.318864583969116,
      "learning_rate": 0.0001994360990881402,
      "loss": 0.2776,
      "step": 1098
    },
    {
      "epoch": 0.005169477972097049,
      "grad_norm": 2.3617846965789795,
      "learning_rate": 0.0001994351561100267,
      "loss": 0.1797,
      "step": 1099
    },
    {
      "epoch": 0.005174181773709512,
      "grad_norm": 2.4583628177642822,
      "learning_rate": 0.00019943421313191323,
      "loss": 0.3105,
      "step": 1100
    },
    {
      "epoch": 0.005178885575321975,
      "grad_norm": 4.5207133293151855,
      "learning_rate": 0.00019943327015379972,
      "loss": 0.3363,
      "step": 1101
    },
    {
      "epoch": 0.005183589376934438,
      "grad_norm": 2.1288506984710693,
      "learning_rate": 0.00019943232717568624,
      "loss": 0.0836,
      "step": 1102
    },
    {
      "epoch": 0.005188293178546901,
      "grad_norm": 0.9160679578781128,
      "learning_rate": 0.00019943138419757279,
      "loss": 0.0305,
      "step": 1103
    },
    {
      "epoch": 0.005192996980159365,
      "grad_norm": 6.8340582847595215,
      "learning_rate": 0.0001994304412194593,
      "loss": 0.2274,
      "step": 1104
    },
    {
      "epoch": 0.005197700781771828,
      "grad_norm": 5.055383682250977,
      "learning_rate": 0.00019942949824134582,
      "loss": 0.2578,
      "step": 1105
    },
    {
      "epoch": 0.005202404583384291,
      "grad_norm": 0.8925439119338989,
      "learning_rate": 0.00019942855526323234,
      "loss": 0.0442,
      "step": 1106
    },
    {
      "epoch": 0.005207108384996754,
      "grad_norm": 1.331179141998291,
      "learning_rate": 0.0001994276122851189,
      "loss": 0.0796,
      "step": 1107
    },
    {
      "epoch": 0.005211812186609217,
      "grad_norm": 2.715717315673828,
      "learning_rate": 0.0001994266693070054,
      "loss": 0.1744,
      "step": 1108
    },
    {
      "epoch": 0.00521651598822168,
      "grad_norm": 1.4115400314331055,
      "learning_rate": 0.00019942572632889192,
      "loss": 0.0591,
      "step": 1109
    },
    {
      "epoch": 0.005221219789834144,
      "grad_norm": 1.4880256652832031,
      "learning_rate": 0.00019942478335077844,
      "loss": 0.0843,
      "step": 1110
    },
    {
      "epoch": 0.005225923591446607,
      "grad_norm": 8.558088302612305,
      "learning_rate": 0.00019942384037266496,
      "loss": 1.2488,
      "step": 1111
    },
    {
      "epoch": 0.00523062739305907,
      "grad_norm": 7.5143046379089355,
      "learning_rate": 0.00019942289739455148,
      "loss": 0.9331,
      "step": 1112
    },
    {
      "epoch": 0.005235331194671533,
      "grad_norm": 6.310933589935303,
      "learning_rate": 0.000199421954416438,
      "loss": 0.589,
      "step": 1113
    },
    {
      "epoch": 0.0052400349962839965,
      "grad_norm": 4.477171421051025,
      "learning_rate": 0.00019942101143832452,
      "loss": 0.1327,
      "step": 1114
    },
    {
      "epoch": 0.0052447387978964595,
      "grad_norm": 5.244927883148193,
      "learning_rate": 0.00019942006846021104,
      "loss": 0.608,
      "step": 1115
    },
    {
      "epoch": 0.005249442599508923,
      "grad_norm": 4.488310813903809,
      "learning_rate": 0.00019941912548209758,
      "loss": 0.4816,
      "step": 1116
    },
    {
      "epoch": 0.0052541464011213864,
      "grad_norm": 5.4850335121154785,
      "learning_rate": 0.0001994181825039841,
      "loss": 0.3794,
      "step": 1117
    },
    {
      "epoch": 0.0052588502027338495,
      "grad_norm": 2.1099581718444824,
      "learning_rate": 0.00019941723952587062,
      "loss": 0.2771,
      "step": 1118
    },
    {
      "epoch": 0.0052635540043463125,
      "grad_norm": 4.633598327636719,
      "learning_rate": 0.00019941629654775714,
      "loss": 0.7191,
      "step": 1119
    },
    {
      "epoch": 0.0052682578059587756,
      "grad_norm": 3.6017260551452637,
      "learning_rate": 0.00019941535356964366,
      "loss": 0.2731,
      "step": 1120
    },
    {
      "epoch": 0.0052729616075712395,
      "grad_norm": 3.1258151531219482,
      "learning_rate": 0.00019941441059153018,
      "loss": 0.2204,
      "step": 1121
    },
    {
      "epoch": 0.0052776654091837025,
      "grad_norm": 6.469143867492676,
      "learning_rate": 0.0001994134676134167,
      "loss": 0.8975,
      "step": 1122
    },
    {
      "epoch": 0.0052823692107961655,
      "grad_norm": 0.25350576639175415,
      "learning_rate": 0.0001994125246353032,
      "loss": 0.0129,
      "step": 1123
    },
    {
      "epoch": 0.005287073012408629,
      "grad_norm": 4.364136695861816,
      "learning_rate": 0.00019941158165718973,
      "loss": 0.4651,
      "step": 1124
    },
    {
      "epoch": 0.005291776814021092,
      "grad_norm": 5.914736270904541,
      "learning_rate": 0.00019941063867907628,
      "loss": 0.7046,
      "step": 1125
    },
    {
      "epoch": 0.005296480615633555,
      "grad_norm": 3.7020633220672607,
      "learning_rate": 0.0001994096957009628,
      "loss": 0.1473,
      "step": 1126
    },
    {
      "epoch": 0.005301184417246019,
      "grad_norm": 0.35071516036987305,
      "learning_rate": 0.00019940875272284931,
      "loss": 0.0183,
      "step": 1127
    },
    {
      "epoch": 0.005305888218858482,
      "grad_norm": 4.434957981109619,
      "learning_rate": 0.00019940780974473583,
      "loss": 0.4541,
      "step": 1128
    },
    {
      "epoch": 0.005310592020470945,
      "grad_norm": 1.1684153079986572,
      "learning_rate": 0.00019940686676662235,
      "loss": 0.0652,
      "step": 1129
    },
    {
      "epoch": 0.005315295822083408,
      "grad_norm": 0.5167628526687622,
      "learning_rate": 0.0001994059237885089,
      "loss": 0.0307,
      "step": 1130
    },
    {
      "epoch": 0.005319999623695871,
      "grad_norm": 1.133687973022461,
      "learning_rate": 0.00019940498081039542,
      "loss": 0.0737,
      "step": 1131
    },
    {
      "epoch": 0.005324703425308334,
      "grad_norm": 3.4795587062835693,
      "learning_rate": 0.0001994040378322819,
      "loss": 0.193,
      "step": 1132
    },
    {
      "epoch": 0.005329407226920798,
      "grad_norm": 2.3188705444335938,
      "learning_rate": 0.00019940309485416843,
      "loss": 0.2018,
      "step": 1133
    },
    {
      "epoch": 0.005334111028533261,
      "grad_norm": 2.1599762439727783,
      "learning_rate": 0.00019940215187605497,
      "loss": 0.1705,
      "step": 1134
    },
    {
      "epoch": 0.005338814830145724,
      "grad_norm": 2.8607177734375,
      "learning_rate": 0.0001994012088979415,
      "loss": 0.1146,
      "step": 1135
    },
    {
      "epoch": 0.005343518631758187,
      "grad_norm": 4.604135513305664,
      "learning_rate": 0.000199400265919828,
      "loss": 0.4745,
      "step": 1136
    },
    {
      "epoch": 0.00534822243337065,
      "grad_norm": 0.7763581871986389,
      "learning_rate": 0.00019939932294171453,
      "loss": 0.0385,
      "step": 1137
    },
    {
      "epoch": 0.005352926234983114,
      "grad_norm": 6.071297645568848,
      "learning_rate": 0.00019939837996360105,
      "loss": 0.4392,
      "step": 1138
    },
    {
      "epoch": 0.005357630036595577,
      "grad_norm": 7.172021389007568,
      "learning_rate": 0.0001993974369854876,
      "loss": 0.5017,
      "step": 1139
    },
    {
      "epoch": 0.00536233383820804,
      "grad_norm": 2.903217077255249,
      "learning_rate": 0.0001993964940073741,
      "loss": 0.1705,
      "step": 1140
    },
    {
      "epoch": 0.005367037639820503,
      "grad_norm": 3.9746034145355225,
      "learning_rate": 0.00019939555102926063,
      "loss": 0.3467,
      "step": 1141
    },
    {
      "epoch": 0.005371741441432966,
      "grad_norm": 0.8437036275863647,
      "learning_rate": 0.00019939460805114715,
      "loss": 0.0665,
      "step": 1142
    },
    {
      "epoch": 0.005376445243045429,
      "grad_norm": 1.61798095703125,
      "learning_rate": 0.00019939366507303367,
      "loss": 0.0955,
      "step": 1143
    },
    {
      "epoch": 0.005381149044657893,
      "grad_norm": 4.404269218444824,
      "learning_rate": 0.00019939272209492019,
      "loss": 0.6919,
      "step": 1144
    },
    {
      "epoch": 0.005385852846270356,
      "grad_norm": 1.2087243795394897,
      "learning_rate": 0.0001993917791168067,
      "loss": 0.0666,
      "step": 1145
    },
    {
      "epoch": 0.005390556647882819,
      "grad_norm": 4.665010929107666,
      "learning_rate": 0.00019939083613869322,
      "loss": 0.3521,
      "step": 1146
    },
    {
      "epoch": 0.005395260449495282,
      "grad_norm": 1.894173264503479,
      "learning_rate": 0.00019938989316057974,
      "loss": 0.1211,
      "step": 1147
    },
    {
      "epoch": 0.005399964251107745,
      "grad_norm": 0.22987908124923706,
      "learning_rate": 0.0001993889501824663,
      "loss": 0.0077,
      "step": 1148
    },
    {
      "epoch": 0.005404668052720209,
      "grad_norm": 0.8262093663215637,
      "learning_rate": 0.0001993880072043528,
      "loss": 0.0554,
      "step": 1149
    },
    {
      "epoch": 0.005409371854332672,
      "grad_norm": 2.2996907234191895,
      "learning_rate": 0.00019938706422623932,
      "loss": 0.1415,
      "step": 1150
    },
    {
      "epoch": 0.005414075655945135,
      "grad_norm": 9.867459297180176,
      "learning_rate": 0.00019938612124812584,
      "loss": 0.5008,
      "step": 1151
    },
    {
      "epoch": 0.005418779457557598,
      "grad_norm": 2.409198045730591,
      "learning_rate": 0.00019938517827001236,
      "loss": 0.0591,
      "step": 1152
    },
    {
      "epoch": 0.005423483259170061,
      "grad_norm": 4.635859489440918,
      "learning_rate": 0.00019938423529189888,
      "loss": 0.5211,
      "step": 1153
    },
    {
      "epoch": 0.005428187060782524,
      "grad_norm": 5.254377841949463,
      "learning_rate": 0.0001993832923137854,
      "loss": 0.7986,
      "step": 1154
    },
    {
      "epoch": 0.005432890862394988,
      "grad_norm": 6.370785236358643,
      "learning_rate": 0.00019938234933567192,
      "loss": 0.5416,
      "step": 1155
    },
    {
      "epoch": 0.005437594664007451,
      "grad_norm": 5.404613494873047,
      "learning_rate": 0.00019938140635755844,
      "loss": 0.8931,
      "step": 1156
    },
    {
      "epoch": 0.005442298465619914,
      "grad_norm": 3.238684892654419,
      "learning_rate": 0.00019938046337944498,
      "loss": 0.1448,
      "step": 1157
    },
    {
      "epoch": 0.005447002267232377,
      "grad_norm": 10.10220718383789,
      "learning_rate": 0.0001993795204013315,
      "loss": 1.402,
      "step": 1158
    },
    {
      "epoch": 0.00545170606884484,
      "grad_norm": 8.26309871673584,
      "learning_rate": 0.00019937857742321802,
      "loss": 0.6499,
      "step": 1159
    },
    {
      "epoch": 0.005456409870457303,
      "grad_norm": 2.1420462131500244,
      "learning_rate": 0.00019937763444510454,
      "loss": 0.1453,
      "step": 1160
    },
    {
      "epoch": 0.005461113672069767,
      "grad_norm": 1.3605626821517944,
      "learning_rate": 0.00019937669146699106,
      "loss": 0.0672,
      "step": 1161
    },
    {
      "epoch": 0.00546581747368223,
      "grad_norm": 2.3025505542755127,
      "learning_rate": 0.0001993757484888776,
      "loss": 0.1737,
      "step": 1162
    },
    {
      "epoch": 0.005470521275294693,
      "grad_norm": 6.32482385635376,
      "learning_rate": 0.0001993748055107641,
      "loss": 0.9749,
      "step": 1163
    },
    {
      "epoch": 0.005475225076907156,
      "grad_norm": 3.37217378616333,
      "learning_rate": 0.0001993738625326506,
      "loss": 0.2535,
      "step": 1164
    },
    {
      "epoch": 0.005479928878519619,
      "grad_norm": 1.3887039422988892,
      "learning_rate": 0.00019937291955453713,
      "loss": 0.228,
      "step": 1165
    },
    {
      "epoch": 0.005484632680132083,
      "grad_norm": 0.38081851601600647,
      "learning_rate": 0.00019937197657642368,
      "loss": 0.0268,
      "step": 1166
    },
    {
      "epoch": 0.005489336481744546,
      "grad_norm": 0.858305811882019,
      "learning_rate": 0.0001993710335983102,
      "loss": 0.0476,
      "step": 1167
    },
    {
      "epoch": 0.005494040283357009,
      "grad_norm": 1.484155535697937,
      "learning_rate": 0.00019937009062019671,
      "loss": 0.0967,
      "step": 1168
    },
    {
      "epoch": 0.005498744084969472,
      "grad_norm": 3.1054580211639404,
      "learning_rate": 0.00019936914764208323,
      "loss": 0.2803,
      "step": 1169
    },
    {
      "epoch": 0.005503447886581935,
      "grad_norm": 2.6280763149261475,
      "learning_rate": 0.00019936820466396975,
      "loss": 0.2317,
      "step": 1170
    },
    {
      "epoch": 0.005508151688194398,
      "grad_norm": 2.470693826675415,
      "learning_rate": 0.0001993672616858563,
      "loss": 0.1874,
      "step": 1171
    },
    {
      "epoch": 0.005512855489806862,
      "grad_norm": 2.865025520324707,
      "learning_rate": 0.00019936631870774282,
      "loss": 0.2834,
      "step": 1172
    },
    {
      "epoch": 0.005517559291419325,
      "grad_norm": 0.11109992861747742,
      "learning_rate": 0.00019936537572962933,
      "loss": 0.0048,
      "step": 1173
    },
    {
      "epoch": 0.005522263093031788,
      "grad_norm": 6.851002216339111,
      "learning_rate": 0.00019936443275151583,
      "loss": 0.2993,
      "step": 1174
    },
    {
      "epoch": 0.005526966894644251,
      "grad_norm": 3.9289348125457764,
      "learning_rate": 0.00019936348977340237,
      "loss": 0.3913,
      "step": 1175
    },
    {
      "epoch": 0.005531670696256714,
      "grad_norm": 2.923616886138916,
      "learning_rate": 0.0001993625467952889,
      "loss": 0.3884,
      "step": 1176
    },
    {
      "epoch": 0.0055363744978691775,
      "grad_norm": 4.819845199584961,
      "learning_rate": 0.0001993616038171754,
      "loss": 0.9621,
      "step": 1177
    },
    {
      "epoch": 0.005541078299481641,
      "grad_norm": 2.939687967300415,
      "learning_rate": 0.00019936066083906193,
      "loss": 0.2993,
      "step": 1178
    },
    {
      "epoch": 0.005545782101094104,
      "grad_norm": 1.674376368522644,
      "learning_rate": 0.00019935971786094845,
      "loss": 0.1028,
      "step": 1179
    },
    {
      "epoch": 0.0055504859027065674,
      "grad_norm": 6.404157638549805,
      "learning_rate": 0.000199358774882835,
      "loss": 0.3606,
      "step": 1180
    },
    {
      "epoch": 0.0055551897043190305,
      "grad_norm": 2.5653977394104004,
      "learning_rate": 0.0001993578319047215,
      "loss": 0.2268,
      "step": 1181
    },
    {
      "epoch": 0.0055598935059314935,
      "grad_norm": 2.8671603202819824,
      "learning_rate": 0.00019935688892660803,
      "loss": 0.5163,
      "step": 1182
    },
    {
      "epoch": 0.0055645973075439574,
      "grad_norm": 6.628057479858398,
      "learning_rate": 0.00019935594594849455,
      "loss": 0.9266,
      "step": 1183
    },
    {
      "epoch": 0.0055693011091564205,
      "grad_norm": 4.409815311431885,
      "learning_rate": 0.00019935500297038107,
      "loss": 0.6785,
      "step": 1184
    },
    {
      "epoch": 0.0055740049107688835,
      "grad_norm": 4.213709354400635,
      "learning_rate": 0.00019935405999226759,
      "loss": 0.37,
      "step": 1185
    },
    {
      "epoch": 0.0055787087123813466,
      "grad_norm": 1.1696022748947144,
      "learning_rate": 0.0001993531170141541,
      "loss": 0.1483,
      "step": 1186
    },
    {
      "epoch": 0.00558341251399381,
      "grad_norm": 5.5416741371154785,
      "learning_rate": 0.00019935217403604062,
      "loss": 0.3633,
      "step": 1187
    },
    {
      "epoch": 0.005588116315606273,
      "grad_norm": 2.0847277641296387,
      "learning_rate": 0.00019935123105792714,
      "loss": 0.1944,
      "step": 1188
    },
    {
      "epoch": 0.0055928201172187365,
      "grad_norm": 0.5164781808853149,
      "learning_rate": 0.0001993502880798137,
      "loss": 0.0423,
      "step": 1189
    },
    {
      "epoch": 0.0055975239188312,
      "grad_norm": 5.6198811531066895,
      "learning_rate": 0.0001993493451017002,
      "loss": 0.7773,
      "step": 1190
    },
    {
      "epoch": 0.005602227720443663,
      "grad_norm": 3.4439780712127686,
      "learning_rate": 0.00019934840212358672,
      "loss": 0.7319,
      "step": 1191
    },
    {
      "epoch": 0.005606931522056126,
      "grad_norm": 0.892350435256958,
      "learning_rate": 0.00019934745914547324,
      "loss": 0.0901,
      "step": 1192
    },
    {
      "epoch": 0.005611635323668589,
      "grad_norm": 4.1819000244140625,
      "learning_rate": 0.0001993465161673598,
      "loss": 0.5658,
      "step": 1193
    },
    {
      "epoch": 0.005616339125281052,
      "grad_norm": 3.774714708328247,
      "learning_rate": 0.00019934557318924628,
      "loss": 0.3712,
      "step": 1194
    },
    {
      "epoch": 0.005621042926893516,
      "grad_norm": 2.5496082305908203,
      "learning_rate": 0.0001993446302111328,
      "loss": 0.2699,
      "step": 1195
    },
    {
      "epoch": 0.005625746728505979,
      "grad_norm": 3.96038556098938,
      "learning_rate": 0.00019934368723301932,
      "loss": 0.7296,
      "step": 1196
    },
    {
      "epoch": 0.005630450530118442,
      "grad_norm": 3.9397950172424316,
      "learning_rate": 0.00019934274425490584,
      "loss": 0.3104,
      "step": 1197
    },
    {
      "epoch": 0.005635154331730905,
      "grad_norm": 2.8164515495300293,
      "learning_rate": 0.00019934180127679238,
      "loss": 0.5098,
      "step": 1198
    },
    {
      "epoch": 0.005639858133343368,
      "grad_norm": 0.6950138211250305,
      "learning_rate": 0.0001993408582986789,
      "loss": 0.0363,
      "step": 1199
    },
    {
      "epoch": 0.005644561934955832,
      "grad_norm": 2.043074131011963,
      "learning_rate": 0.00019933991532056542,
      "loss": 0.3198,
      "step": 1200
    },
    {
      "epoch": 0.005649265736568295,
      "grad_norm": 4.8200297355651855,
      "learning_rate": 0.00019933897234245194,
      "loss": 0.6469,
      "step": 1201
    },
    {
      "epoch": 0.005653969538180758,
      "grad_norm": 7.070374488830566,
      "learning_rate": 0.00019933802936433846,
      "loss": 1.1239,
      "step": 1202
    },
    {
      "epoch": 0.005658673339793221,
      "grad_norm": 8.754776000976562,
      "learning_rate": 0.000199337086386225,
      "loss": 0.9654,
      "step": 1203
    },
    {
      "epoch": 0.005663377141405684,
      "grad_norm": 0.675666093826294,
      "learning_rate": 0.00019933614340811152,
      "loss": 0.0349,
      "step": 1204
    },
    {
      "epoch": 0.005668080943018147,
      "grad_norm": 4.707622528076172,
      "learning_rate": 0.000199335200429998,
      "loss": 0.7683,
      "step": 1205
    },
    {
      "epoch": 0.005672784744630611,
      "grad_norm": 2.05842661857605,
      "learning_rate": 0.00019933425745188453,
      "loss": 0.2405,
      "step": 1206
    },
    {
      "epoch": 0.005677488546243074,
      "grad_norm": 1.4938149452209473,
      "learning_rate": 0.00019933331447377108,
      "loss": 0.1945,
      "step": 1207
    },
    {
      "epoch": 0.005682192347855537,
      "grad_norm": 1.8888393640518188,
      "learning_rate": 0.0001993323714956576,
      "loss": 0.1516,
      "step": 1208
    },
    {
      "epoch": 0.005686896149468,
      "grad_norm": 3.495593547821045,
      "learning_rate": 0.00019933142851754411,
      "loss": 0.4761,
      "step": 1209
    },
    {
      "epoch": 0.005691599951080463,
      "grad_norm": 2.378688335418701,
      "learning_rate": 0.00019933048553943063,
      "loss": 0.5007,
      "step": 1210
    },
    {
      "epoch": 0.005696303752692926,
      "grad_norm": 2.0711324214935303,
      "learning_rate": 0.00019932954256131715,
      "loss": 0.262,
      "step": 1211
    },
    {
      "epoch": 0.00570100755430539,
      "grad_norm": 2.422760248184204,
      "learning_rate": 0.0001993285995832037,
      "loss": 0.4428,
      "step": 1212
    },
    {
      "epoch": 0.005705711355917853,
      "grad_norm": 1.0392595529556274,
      "learning_rate": 0.00019932765660509022,
      "loss": 0.1849,
      "step": 1213
    },
    {
      "epoch": 0.005710415157530316,
      "grad_norm": 1.4100192785263062,
      "learning_rate": 0.00019932671362697673,
      "loss": 0.2219,
      "step": 1214
    },
    {
      "epoch": 0.005715118959142779,
      "grad_norm": 1.1109975576400757,
      "learning_rate": 0.00019932577064886325,
      "loss": 0.3005,
      "step": 1215
    },
    {
      "epoch": 0.005719822760755242,
      "grad_norm": 1.5160828828811646,
      "learning_rate": 0.00019932482767074977,
      "loss": 0.1723,
      "step": 1216
    },
    {
      "epoch": 0.005724526562367706,
      "grad_norm": 2.116989850997925,
      "learning_rate": 0.0001993238846926363,
      "loss": 0.2312,
      "step": 1217
    },
    {
      "epoch": 0.005729230363980169,
      "grad_norm": 1.290832281112671,
      "learning_rate": 0.0001993229417145228,
      "loss": 0.1477,
      "step": 1218
    },
    {
      "epoch": 0.005733934165592632,
      "grad_norm": 1.3675892353057861,
      "learning_rate": 0.00019932199873640933,
      "loss": 0.1483,
      "step": 1219
    },
    {
      "epoch": 0.005738637967205095,
      "grad_norm": 1.387751579284668,
      "learning_rate": 0.00019932105575829585,
      "loss": 0.1479,
      "step": 1220
    },
    {
      "epoch": 0.005743341768817558,
      "grad_norm": 2.6929235458374023,
      "learning_rate": 0.0001993201127801824,
      "loss": 0.2617,
      "step": 1221
    },
    {
      "epoch": 0.005748045570430021,
      "grad_norm": 3.8289926052093506,
      "learning_rate": 0.0001993191698020689,
      "loss": 0.5473,
      "step": 1222
    },
    {
      "epoch": 0.005752749372042485,
      "grad_norm": 3.6065146923065186,
      "learning_rate": 0.00019931822682395543,
      "loss": 0.2511,
      "step": 1223
    },
    {
      "epoch": 0.005757453173654948,
      "grad_norm": 1.1976337432861328,
      "learning_rate": 0.00019931728384584195,
      "loss": 0.157,
      "step": 1224
    },
    {
      "epoch": 0.005762156975267411,
      "grad_norm": 1.3144774436950684,
      "learning_rate": 0.00019931634086772847,
      "loss": 0.1356,
      "step": 1225
    },
    {
      "epoch": 0.005766860776879874,
      "grad_norm": 1.150099515914917,
      "learning_rate": 0.00019931539788961499,
      "loss": 0.1195,
      "step": 1226
    },
    {
      "epoch": 0.005771564578492337,
      "grad_norm": 3.6199307441711426,
      "learning_rate": 0.0001993144549115015,
      "loss": 0.5751,
      "step": 1227
    },
    {
      "epoch": 0.0057762683801048,
      "grad_norm": 2.053744316101074,
      "learning_rate": 0.00019931351193338802,
      "loss": 0.1009,
      "step": 1228
    },
    {
      "epoch": 0.005780972181717264,
      "grad_norm": 1.6871464252471924,
      "learning_rate": 0.00019931256895527454,
      "loss": 0.2022,
      "step": 1229
    },
    {
      "epoch": 0.005785675983329727,
      "grad_norm": 0.793158233165741,
      "learning_rate": 0.0001993116259771611,
      "loss": 0.0363,
      "step": 1230
    },
    {
      "epoch": 0.00579037978494219,
      "grad_norm": 4.424693584442139,
      "learning_rate": 0.0001993106829990476,
      "loss": 0.5557,
      "step": 1231
    },
    {
      "epoch": 0.005795083586554653,
      "grad_norm": 1.5429329872131348,
      "learning_rate": 0.00019930974002093412,
      "loss": 0.1128,
      "step": 1232
    },
    {
      "epoch": 0.005799787388167116,
      "grad_norm": 6.781052112579346,
      "learning_rate": 0.00019930879704282064,
      "loss": 0.7655,
      "step": 1233
    },
    {
      "epoch": 0.00580449118977958,
      "grad_norm": 4.749333381652832,
      "learning_rate": 0.0001993078540647072,
      "loss": 0.6344,
      "step": 1234
    },
    {
      "epoch": 0.005809194991392043,
      "grad_norm": 1.4649970531463623,
      "learning_rate": 0.0001993069110865937,
      "loss": 0.1411,
      "step": 1235
    },
    {
      "epoch": 0.005813898793004506,
      "grad_norm": 5.697296619415283,
      "learning_rate": 0.0001993059681084802,
      "loss": 0.8614,
      "step": 1236
    },
    {
      "epoch": 0.005818602594616969,
      "grad_norm": 1.2889690399169922,
      "learning_rate": 0.00019930502513036672,
      "loss": 0.1595,
      "step": 1237
    },
    {
      "epoch": 0.005823306396229432,
      "grad_norm": 0.6478897333145142,
      "learning_rate": 0.00019930408215225324,
      "loss": 0.0436,
      "step": 1238
    },
    {
      "epoch": 0.005828010197841895,
      "grad_norm": 2.3113772869110107,
      "learning_rate": 0.00019930313917413978,
      "loss": 0.2199,
      "step": 1239
    },
    {
      "epoch": 0.005832713999454359,
      "grad_norm": 3.2041616439819336,
      "learning_rate": 0.0001993021961960263,
      "loss": 0.3884,
      "step": 1240
    },
    {
      "epoch": 0.005837417801066822,
      "grad_norm": 5.104762554168701,
      "learning_rate": 0.00019930125321791282,
      "loss": 0.61,
      "step": 1241
    },
    {
      "epoch": 0.005842121602679285,
      "grad_norm": 2.0124590396881104,
      "learning_rate": 0.00019930031023979934,
      "loss": 0.165,
      "step": 1242
    },
    {
      "epoch": 0.0058468254042917485,
      "grad_norm": 1.0103362798690796,
      "learning_rate": 0.00019929936726168588,
      "loss": 0.0749,
      "step": 1243
    },
    {
      "epoch": 0.0058515292059042115,
      "grad_norm": 5.156274795532227,
      "learning_rate": 0.0001992984242835724,
      "loss": 0.4305,
      "step": 1244
    },
    {
      "epoch": 0.005856233007516675,
      "grad_norm": 9.353336334228516,
      "learning_rate": 0.00019929748130545892,
      "loss": 0.6514,
      "step": 1245
    },
    {
      "epoch": 0.0058609368091291384,
      "grad_norm": 1.3498950004577637,
      "learning_rate": 0.00019929653832734544,
      "loss": 0.1577,
      "step": 1246
    },
    {
      "epoch": 0.0058656406107416015,
      "grad_norm": 1.5542339086532593,
      "learning_rate": 0.00019929559534923196,
      "loss": 0.1679,
      "step": 1247
    },
    {
      "epoch": 0.0058703444123540645,
      "grad_norm": 0.8223026394844055,
      "learning_rate": 0.00019929465237111848,
      "loss": 0.0989,
      "step": 1248
    },
    {
      "epoch": 0.0058750482139665276,
      "grad_norm": 1.3289713859558105,
      "learning_rate": 0.000199293709393005,
      "loss": 0.0787,
      "step": 1249
    },
    {
      "epoch": 0.005879752015578991,
      "grad_norm": 3.784435749053955,
      "learning_rate": 0.00019929276641489151,
      "loss": 0.2394,
      "step": 1250
    },
    {
      "epoch": 0.0058844558171914545,
      "grad_norm": 3.639416217803955,
      "learning_rate": 0.00019929182343677803,
      "loss": 0.2231,
      "step": 1251
    },
    {
      "epoch": 0.0058891596188039175,
      "grad_norm": 1.6251678466796875,
      "learning_rate": 0.00019929088045866458,
      "loss": 0.1206,
      "step": 1252
    },
    {
      "epoch": 0.005893863420416381,
      "grad_norm": 0.8225566148757935,
      "learning_rate": 0.0001992899374805511,
      "loss": 0.0313,
      "step": 1253
    },
    {
      "epoch": 0.005898567222028844,
      "grad_norm": 5.806878089904785,
      "learning_rate": 0.00019928899450243762,
      "loss": 0.607,
      "step": 1254
    },
    {
      "epoch": 0.005903271023641307,
      "grad_norm": 3.8365721702575684,
      "learning_rate": 0.00019928805152432413,
      "loss": 0.2764,
      "step": 1255
    },
    {
      "epoch": 0.00590797482525377,
      "grad_norm": 2.233706474304199,
      "learning_rate": 0.00019928710854621065,
      "loss": 0.1201,
      "step": 1256
    },
    {
      "epoch": 0.005912678626866234,
      "grad_norm": 6.764200210571289,
      "learning_rate": 0.00019928616556809717,
      "loss": 0.8666,
      "step": 1257
    },
    {
      "epoch": 0.005917382428478697,
      "grad_norm": 1.445852518081665,
      "learning_rate": 0.0001992852225899837,
      "loss": 0.0577,
      "step": 1258
    },
    {
      "epoch": 0.00592208623009116,
      "grad_norm": 7.526480197906494,
      "learning_rate": 0.0001992842796118702,
      "loss": 0.7086,
      "step": 1259
    },
    {
      "epoch": 0.005926790031703623,
      "grad_norm": 5.058367729187012,
      "learning_rate": 0.00019928333663375673,
      "loss": 0.3856,
      "step": 1260
    },
    {
      "epoch": 0.005931493833316086,
      "grad_norm": 3.0840954780578613,
      "learning_rate": 0.00019928239365564325,
      "loss": 0.1952,
      "step": 1261
    },
    {
      "epoch": 0.00593619763492855,
      "grad_norm": 4.15273380279541,
      "learning_rate": 0.0001992814506775298,
      "loss": 0.7649,
      "step": 1262
    },
    {
      "epoch": 0.005940901436541013,
      "grad_norm": 0.03401263430714607,
      "learning_rate": 0.0001992805076994163,
      "loss": 0.0014,
      "step": 1263
    },
    {
      "epoch": 0.005945605238153476,
      "grad_norm": 4.920039176940918,
      "learning_rate": 0.00019927956472130283,
      "loss": 0.2747,
      "step": 1264
    },
    {
      "epoch": 0.005950309039765939,
      "grad_norm": 5.3256659507751465,
      "learning_rate": 0.00019927862174318935,
      "loss": 1.202,
      "step": 1265
    },
    {
      "epoch": 0.005955012841378402,
      "grad_norm": 1.491474986076355,
      "learning_rate": 0.0001992776787650759,
      "loss": 0.0917,
      "step": 1266
    },
    {
      "epoch": 0.005959716642990865,
      "grad_norm": 1.0424631834030151,
      "learning_rate": 0.00019927673578696239,
      "loss": 0.039,
      "step": 1267
    },
    {
      "epoch": 0.005964420444603329,
      "grad_norm": 6.376051902770996,
      "learning_rate": 0.0001992757928088489,
      "loss": 0.4321,
      "step": 1268
    },
    {
      "epoch": 0.005969124246215792,
      "grad_norm": 2.305828332901001,
      "learning_rate": 0.00019927484983073542,
      "loss": 0.2922,
      "step": 1269
    },
    {
      "epoch": 0.005973828047828255,
      "grad_norm": 1.7370834350585938,
      "learning_rate": 0.00019927390685262194,
      "loss": 0.1118,
      "step": 1270
    },
    {
      "epoch": 0.005978531849440718,
      "grad_norm": 1.6610260009765625,
      "learning_rate": 0.0001992729638745085,
      "loss": 0.0997,
      "step": 1271
    },
    {
      "epoch": 0.005983235651053181,
      "grad_norm": 0.698682427406311,
      "learning_rate": 0.000199272020896395,
      "loss": 0.0609,
      "step": 1272
    },
    {
      "epoch": 0.005987939452665644,
      "grad_norm": 1.8116564750671387,
      "learning_rate": 0.00019927107791828152,
      "loss": 0.1286,
      "step": 1273
    },
    {
      "epoch": 0.005992643254278108,
      "grad_norm": 1.4065539836883545,
      "learning_rate": 0.00019927013494016804,
      "loss": 0.2025,
      "step": 1274
    },
    {
      "epoch": 0.005997347055890571,
      "grad_norm": 2.437939405441284,
      "learning_rate": 0.0001992691919620546,
      "loss": 0.1756,
      "step": 1275
    },
    {
      "epoch": 0.006002050857503034,
      "grad_norm": 2.970208168029785,
      "learning_rate": 0.0001992682489839411,
      "loss": 0.2693,
      "step": 1276
    },
    {
      "epoch": 0.006006754659115497,
      "grad_norm": 3.8471930027008057,
      "learning_rate": 0.00019926730600582763,
      "loss": 0.4362,
      "step": 1277
    },
    {
      "epoch": 0.00601145846072796,
      "grad_norm": 4.280054569244385,
      "learning_rate": 0.00019926636302771414,
      "loss": 0.3732,
      "step": 1278
    },
    {
      "epoch": 0.006016162262340424,
      "grad_norm": 1.6444324254989624,
      "learning_rate": 0.00019926542004960064,
      "loss": 0.1465,
      "step": 1279
    },
    {
      "epoch": 0.006020866063952887,
      "grad_norm": 2.0941598415374756,
      "learning_rate": 0.00019926447707148718,
      "loss": 0.2193,
      "step": 1280
    },
    {
      "epoch": 0.00602556986556535,
      "grad_norm": 2.719475030899048,
      "learning_rate": 0.0001992635340933737,
      "loss": 0.4009,
      "step": 1281
    },
    {
      "epoch": 0.006030273667177813,
      "grad_norm": 1.496988296508789,
      "learning_rate": 0.00019926259111526022,
      "loss": 0.0873,
      "step": 1282
    },
    {
      "epoch": 0.006034977468790276,
      "grad_norm": 0.8769496083259583,
      "learning_rate": 0.00019926164813714674,
      "loss": 0.0746,
      "step": 1283
    },
    {
      "epoch": 0.006039681270402739,
      "grad_norm": 0.7686470746994019,
      "learning_rate": 0.00019926070515903328,
      "loss": 0.0763,
      "step": 1284
    },
    {
      "epoch": 0.006044385072015203,
      "grad_norm": 1.3727295398712158,
      "learning_rate": 0.0001992597621809198,
      "loss": 0.1291,
      "step": 1285
    },
    {
      "epoch": 0.006049088873627666,
      "grad_norm": 3.052811622619629,
      "learning_rate": 0.00019925881920280632,
      "loss": 0.3593,
      "step": 1286
    },
    {
      "epoch": 0.006053792675240129,
      "grad_norm": 2.0017738342285156,
      "learning_rate": 0.00019925787622469284,
      "loss": 0.0959,
      "step": 1287
    },
    {
      "epoch": 0.006058496476852592,
      "grad_norm": 1.1170976161956787,
      "learning_rate": 0.00019925693324657936,
      "loss": 0.1149,
      "step": 1288
    },
    {
      "epoch": 0.006063200278465055,
      "grad_norm": 1.0299327373504639,
      "learning_rate": 0.00019925599026846588,
      "loss": 0.0733,
      "step": 1289
    },
    {
      "epoch": 0.006067904080077518,
      "grad_norm": 3.037893772125244,
      "learning_rate": 0.0001992550472903524,
      "loss": 0.2554,
      "step": 1290
    },
    {
      "epoch": 0.006072607881689982,
      "grad_norm": 4.019340515136719,
      "learning_rate": 0.00019925410431223891,
      "loss": 0.2672,
      "step": 1291
    },
    {
      "epoch": 0.006077311683302445,
      "grad_norm": 3.297081470489502,
      "learning_rate": 0.00019925316133412543,
      "loss": 0.1376,
      "step": 1292
    },
    {
      "epoch": 0.006082015484914908,
      "grad_norm": 5.918375015258789,
      "learning_rate": 0.00019925221835601198,
      "loss": 0.5497,
      "step": 1293
    },
    {
      "epoch": 0.006086719286527371,
      "grad_norm": 2.914677381515503,
      "learning_rate": 0.0001992512753778985,
      "loss": 0.1728,
      "step": 1294
    },
    {
      "epoch": 0.006091423088139834,
      "grad_norm": 3.6379780769348145,
      "learning_rate": 0.00019925033239978502,
      "loss": 0.9724,
      "step": 1295
    },
    {
      "epoch": 0.006096126889752298,
      "grad_norm": 2.665971279144287,
      "learning_rate": 0.00019924938942167153,
      "loss": 0.3171,
      "step": 1296
    },
    {
      "epoch": 0.006100830691364761,
      "grad_norm": 4.7336530685424805,
      "learning_rate": 0.00019924844644355805,
      "loss": 0.4826,
      "step": 1297
    },
    {
      "epoch": 0.006105534492977224,
      "grad_norm": 6.398085594177246,
      "learning_rate": 0.00019924750346544457,
      "loss": 0.5827,
      "step": 1298
    },
    {
      "epoch": 0.006110238294589687,
      "grad_norm": 1.6176749467849731,
      "learning_rate": 0.0001992465604873311,
      "loss": 0.1043,
      "step": 1299
    },
    {
      "epoch": 0.00611494209620215,
      "grad_norm": 3.5241875648498535,
      "learning_rate": 0.0001992456175092176,
      "loss": 0.4338,
      "step": 1300
    },
    {
      "epoch": 0.006119645897814613,
      "grad_norm": 11.880297660827637,
      "learning_rate": 0.00019924467453110413,
      "loss": 0.398,
      "step": 1301
    },
    {
      "epoch": 0.006124349699427077,
      "grad_norm": 1.2258449792861938,
      "learning_rate": 0.00019924373155299067,
      "loss": 0.0423,
      "step": 1302
    },
    {
      "epoch": 0.00612905350103954,
      "grad_norm": 4.117990493774414,
      "learning_rate": 0.0001992427885748772,
      "loss": 0.7353,
      "step": 1303
    },
    {
      "epoch": 0.006133757302652003,
      "grad_norm": 5.710389614105225,
      "learning_rate": 0.0001992418455967637,
      "loss": 0.3904,
      "step": 1304
    },
    {
      "epoch": 0.006138461104264466,
      "grad_norm": 1.8665326833724976,
      "learning_rate": 0.00019924090261865023,
      "loss": 0.1605,
      "step": 1305
    },
    {
      "epoch": 0.0061431649058769295,
      "grad_norm": 2.659217357635498,
      "learning_rate": 0.00019923995964053675,
      "loss": 0.1989,
      "step": 1306
    },
    {
      "epoch": 0.0061478687074893925,
      "grad_norm": 6.0066304206848145,
      "learning_rate": 0.0001992390166624233,
      "loss": 1.0185,
      "step": 1307
    },
    {
      "epoch": 0.006152572509101856,
      "grad_norm": 4.032823085784912,
      "learning_rate": 0.0001992380736843098,
      "loss": 0.3713,
      "step": 1308
    },
    {
      "epoch": 0.0061572763107143194,
      "grad_norm": 7.590004920959473,
      "learning_rate": 0.00019923713070619633,
      "loss": 0.3636,
      "step": 1309
    },
    {
      "epoch": 0.0061619801123267825,
      "grad_norm": 4.815395355224609,
      "learning_rate": 0.00019923618772808282,
      "loss": 1.1883,
      "step": 1310
    },
    {
      "epoch": 0.0061666839139392455,
      "grad_norm": 3.3513877391815186,
      "learning_rate": 0.00019923524474996934,
      "loss": 0.2407,
      "step": 1311
    },
    {
      "epoch": 0.0061713877155517086,
      "grad_norm": 3.7744569778442383,
      "learning_rate": 0.0001992343017718559,
      "loss": 0.35,
      "step": 1312
    },
    {
      "epoch": 0.0061760915171641725,
      "grad_norm": 2.939973831176758,
      "learning_rate": 0.0001992333587937424,
      "loss": 0.3727,
      "step": 1313
    },
    {
      "epoch": 0.0061807953187766355,
      "grad_norm": 2.994934320449829,
      "learning_rate": 0.00019923241581562892,
      "loss": 0.2833,
      "step": 1314
    },
    {
      "epoch": 0.0061854991203890985,
      "grad_norm": 3.2441821098327637,
      "learning_rate": 0.00019923147283751544,
      "loss": 0.4573,
      "step": 1315
    },
    {
      "epoch": 0.006190202922001562,
      "grad_norm": 1.7011607885360718,
      "learning_rate": 0.000199230529859402,
      "loss": 0.1733,
      "step": 1316
    },
    {
      "epoch": 0.006194906723614025,
      "grad_norm": 2.092859983444214,
      "learning_rate": 0.0001992295868812885,
      "loss": 0.2686,
      "step": 1317
    },
    {
      "epoch": 0.006199610525226488,
      "grad_norm": 2.8448615074157715,
      "learning_rate": 0.00019922864390317503,
      "loss": 0.3486,
      "step": 1318
    },
    {
      "epoch": 0.006204314326838952,
      "grad_norm": 2.7913870811462402,
      "learning_rate": 0.00019922770092506154,
      "loss": 0.1783,
      "step": 1319
    },
    {
      "epoch": 0.006209018128451415,
      "grad_norm": 1.182358741760254,
      "learning_rate": 0.00019922675794694806,
      "loss": 0.0861,
      "step": 1320
    },
    {
      "epoch": 0.006213721930063878,
      "grad_norm": 0.905224621295929,
      "learning_rate": 0.00019922581496883458,
      "loss": 0.0614,
      "step": 1321
    },
    {
      "epoch": 0.006218425731676341,
      "grad_norm": 1.8957051038742065,
      "learning_rate": 0.0001992248719907211,
      "loss": 0.2419,
      "step": 1322
    },
    {
      "epoch": 0.006223129533288804,
      "grad_norm": 0.7903734445571899,
      "learning_rate": 0.00019922392901260762,
      "loss": 0.046,
      "step": 1323
    },
    {
      "epoch": 0.006227833334901268,
      "grad_norm": 1.4794456958770752,
      "learning_rate": 0.00019922298603449414,
      "loss": 0.1599,
      "step": 1324
    },
    {
      "epoch": 0.006232537136513731,
      "grad_norm": 2.730485439300537,
      "learning_rate": 0.00019922204305638068,
      "loss": 0.2082,
      "step": 1325
    },
    {
      "epoch": 0.006237240938126194,
      "grad_norm": 1.6693755388259888,
      "learning_rate": 0.0001992211000782672,
      "loss": 0.1617,
      "step": 1326
    },
    {
      "epoch": 0.006241944739738657,
      "grad_norm": 1.9373129606246948,
      "learning_rate": 0.00019922015710015372,
      "loss": 0.1611,
      "step": 1327
    },
    {
      "epoch": 0.00624664854135112,
      "grad_norm": 0.09699622541666031,
      "learning_rate": 0.00019921921412204024,
      "loss": 0.0046,
      "step": 1328
    },
    {
      "epoch": 0.006251352342963583,
      "grad_norm": 3.1648166179656982,
      "learning_rate": 0.00019921827114392676,
      "loss": 0.1035,
      "step": 1329
    },
    {
      "epoch": 0.006256056144576047,
      "grad_norm": 3.2026867866516113,
      "learning_rate": 0.00019921732816581328,
      "loss": 0.2819,
      "step": 1330
    },
    {
      "epoch": 0.00626075994618851,
      "grad_norm": 0.1931099146604538,
      "learning_rate": 0.0001992163851876998,
      "loss": 0.0073,
      "step": 1331
    },
    {
      "epoch": 0.006265463747800973,
      "grad_norm": 4.8043437004089355,
      "learning_rate": 0.00019921544220958631,
      "loss": 0.7336,
      "step": 1332
    },
    {
      "epoch": 0.006270167549413436,
      "grad_norm": 2.653977155685425,
      "learning_rate": 0.00019921449923147283,
      "loss": 0.3132,
      "step": 1333
    },
    {
      "epoch": 0.006274871351025899,
      "grad_norm": 0.1970120072364807,
      "learning_rate": 0.00019921355625335938,
      "loss": 0.0163,
      "step": 1334
    },
    {
      "epoch": 0.006279575152638362,
      "grad_norm": 5.042530536651611,
      "learning_rate": 0.0001992126132752459,
      "loss": 0.4437,
      "step": 1335
    },
    {
      "epoch": 0.006284278954250826,
      "grad_norm": 1.7543840408325195,
      "learning_rate": 0.00019921167029713242,
      "loss": 0.1282,
      "step": 1336
    },
    {
      "epoch": 0.006288982755863289,
      "grad_norm": 3.4680984020233154,
      "learning_rate": 0.00019921072731901893,
      "loss": 0.5903,
      "step": 1337
    },
    {
      "epoch": 0.006293686557475752,
      "grad_norm": 3.2494592666625977,
      "learning_rate": 0.00019920978434090545,
      "loss": 0.4127,
      "step": 1338
    },
    {
      "epoch": 0.006298390359088215,
      "grad_norm": 0.4063134789466858,
      "learning_rate": 0.000199208841362792,
      "loss": 0.022,
      "step": 1339
    },
    {
      "epoch": 0.006303094160700678,
      "grad_norm": 3.947680950164795,
      "learning_rate": 0.00019920789838467852,
      "loss": 0.5213,
      "step": 1340
    },
    {
      "epoch": 0.006307797962313142,
      "grad_norm": 2.05122709274292,
      "learning_rate": 0.000199206955406565,
      "loss": 0.2825,
      "step": 1341
    },
    {
      "epoch": 0.006312501763925605,
      "grad_norm": 2.9824588298797607,
      "learning_rate": 0.00019920601242845153,
      "loss": 0.3805,
      "step": 1342
    },
    {
      "epoch": 0.006317205565538068,
      "grad_norm": 3.9097533226013184,
      "learning_rate": 0.00019920506945033807,
      "loss": 0.3822,
      "step": 1343
    },
    {
      "epoch": 0.006321909367150531,
      "grad_norm": 2.188673734664917,
      "learning_rate": 0.0001992041264722246,
      "loss": 0.418,
      "step": 1344
    },
    {
      "epoch": 0.006326613168762994,
      "grad_norm": 0.7709594964981079,
      "learning_rate": 0.0001992031834941111,
      "loss": 0.0867,
      "step": 1345
    },
    {
      "epoch": 0.006331316970375457,
      "grad_norm": 2.9926815032958984,
      "learning_rate": 0.00019920224051599763,
      "loss": 0.8724,
      "step": 1346
    },
    {
      "epoch": 0.006336020771987921,
      "grad_norm": 3.136735200881958,
      "learning_rate": 0.00019920129753788415,
      "loss": 0.3397,
      "step": 1347
    },
    {
      "epoch": 0.006340724573600384,
      "grad_norm": 2.52642822265625,
      "learning_rate": 0.0001992003545597707,
      "loss": 0.2683,
      "step": 1348
    },
    {
      "epoch": 0.006345428375212847,
      "grad_norm": 0.8175086379051208,
      "learning_rate": 0.0001991994115816572,
      "loss": 0.1242,
      "step": 1349
    },
    {
      "epoch": 0.00635013217682531,
      "grad_norm": 1.3646725416183472,
      "learning_rate": 0.00019919846860354373,
      "loss": 0.1223,
      "step": 1350
    },
    {
      "epoch": 0.006354835978437773,
      "grad_norm": 5.086792469024658,
      "learning_rate": 0.00019919752562543025,
      "loss": 0.4509,
      "step": 1351
    },
    {
      "epoch": 0.006359539780050236,
      "grad_norm": 4.30047082901001,
      "learning_rate": 0.00019919658264731677,
      "loss": 0.6292,
      "step": 1352
    },
    {
      "epoch": 0.0063642435816627,
      "grad_norm": 5.5526509284973145,
      "learning_rate": 0.0001991956396692033,
      "loss": 0.7155,
      "step": 1353
    },
    {
      "epoch": 0.006368947383275163,
      "grad_norm": 3.9304115772247314,
      "learning_rate": 0.0001991946966910898,
      "loss": 0.262,
      "step": 1354
    },
    {
      "epoch": 0.006373651184887626,
      "grad_norm": 2.0851359367370605,
      "learning_rate": 0.00019919375371297632,
      "loss": 0.1657,
      "step": 1355
    },
    {
      "epoch": 0.006378354986500089,
      "grad_norm": 1.7218279838562012,
      "learning_rate": 0.00019919281073486284,
      "loss": 0.1328,
      "step": 1356
    },
    {
      "epoch": 0.006383058788112552,
      "grad_norm": 4.667328834533691,
      "learning_rate": 0.0001991918677567494,
      "loss": 0.5193,
      "step": 1357
    },
    {
      "epoch": 0.006387762589725016,
      "grad_norm": 1.2759580612182617,
      "learning_rate": 0.0001991909247786359,
      "loss": 0.1134,
      "step": 1358
    },
    {
      "epoch": 0.006392466391337479,
      "grad_norm": 2.8780972957611084,
      "learning_rate": 0.00019918998180052243,
      "loss": 0.382,
      "step": 1359
    },
    {
      "epoch": 0.006397170192949942,
      "grad_norm": 0.7783670425415039,
      "learning_rate": 0.00019918903882240894,
      "loss": 0.0576,
      "step": 1360
    },
    {
      "epoch": 0.006401873994562405,
      "grad_norm": 3.7707276344299316,
      "learning_rate": 0.00019918809584429546,
      "loss": 0.228,
      "step": 1361
    },
    {
      "epoch": 0.006406577796174868,
      "grad_norm": 1.7454590797424316,
      "learning_rate": 0.00019918715286618198,
      "loss": 0.2348,
      "step": 1362
    },
    {
      "epoch": 0.006411281597787331,
      "grad_norm": 2.0401275157928467,
      "learning_rate": 0.0001991862098880685,
      "loss": 0.2113,
      "step": 1363
    },
    {
      "epoch": 0.006415985399399795,
      "grad_norm": 4.700932502746582,
      "learning_rate": 0.00019918526690995502,
      "loss": 0.5186,
      "step": 1364
    },
    {
      "epoch": 0.006420689201012258,
      "grad_norm": 2.6380412578582764,
      "learning_rate": 0.00019918432393184154,
      "loss": 0.2397,
      "step": 1365
    },
    {
      "epoch": 0.006425393002624721,
      "grad_norm": 1.5414738655090332,
      "learning_rate": 0.00019918338095372808,
      "loss": 0.1874,
      "step": 1366
    },
    {
      "epoch": 0.006430096804237184,
      "grad_norm": 2.1091878414154053,
      "learning_rate": 0.0001991824379756146,
      "loss": 0.1549,
      "step": 1367
    },
    {
      "epoch": 0.006434800605849647,
      "grad_norm": 0.7958643436431885,
      "learning_rate": 0.00019918149499750112,
      "loss": 0.0767,
      "step": 1368
    },
    {
      "epoch": 0.0064395044074621105,
      "grad_norm": 1.7204638719558716,
      "learning_rate": 0.00019918055201938764,
      "loss": 0.1667,
      "step": 1369
    },
    {
      "epoch": 0.006444208209074574,
      "grad_norm": 2.175060272216797,
      "learning_rate": 0.00019917960904127416,
      "loss": 0.271,
      "step": 1370
    },
    {
      "epoch": 0.006448912010687037,
      "grad_norm": 2.502535820007324,
      "learning_rate": 0.0001991786660631607,
      "loss": 0.3904,
      "step": 1371
    },
    {
      "epoch": 0.0064536158122995005,
      "grad_norm": 0.6607105731964111,
      "learning_rate": 0.0001991777230850472,
      "loss": 0.0731,
      "step": 1372
    },
    {
      "epoch": 0.0064583196139119635,
      "grad_norm": 1.9826887845993042,
      "learning_rate": 0.00019917678010693371,
      "loss": 0.1946,
      "step": 1373
    },
    {
      "epoch": 0.0064630234155244265,
      "grad_norm": 3.546515703201294,
      "learning_rate": 0.00019917583712882023,
      "loss": 0.817,
      "step": 1374
    },
    {
      "epoch": 0.0064677272171368904,
      "grad_norm": 2.5036728382110596,
      "learning_rate": 0.00019917489415070678,
      "loss": 0.4435,
      "step": 1375
    },
    {
      "epoch": 0.0064724310187493535,
      "grad_norm": 1.0384302139282227,
      "learning_rate": 0.0001991739511725933,
      "loss": 0.1082,
      "step": 1376
    },
    {
      "epoch": 0.0064771348203618165,
      "grad_norm": 2.7292237281799316,
      "learning_rate": 0.00019917300819447982,
      "loss": 0.2158,
      "step": 1377
    },
    {
      "epoch": 0.0064818386219742796,
      "grad_norm": 2.4711930751800537,
      "learning_rate": 0.00019917206521636633,
      "loss": 0.1978,
      "step": 1378
    },
    {
      "epoch": 0.006486542423586743,
      "grad_norm": 0.7609228491783142,
      "learning_rate": 0.00019917112223825285,
      "loss": 0.0473,
      "step": 1379
    },
    {
      "epoch": 0.006491246225199206,
      "grad_norm": 0.9524661302566528,
      "learning_rate": 0.0001991701792601394,
      "loss": 0.0895,
      "step": 1380
    },
    {
      "epoch": 0.0064959500268116695,
      "grad_norm": 2.503368854522705,
      "learning_rate": 0.00019916923628202592,
      "loss": 0.3195,
      "step": 1381
    },
    {
      "epoch": 0.006500653828424133,
      "grad_norm": 0.4589749574661255,
      "learning_rate": 0.00019916829330391244,
      "loss": 0.0277,
      "step": 1382
    },
    {
      "epoch": 0.006505357630036596,
      "grad_norm": 2.4975218772888184,
      "learning_rate": 0.00019916735032579893,
      "loss": 0.2668,
      "step": 1383
    },
    {
      "epoch": 0.006510061431649059,
      "grad_norm": 5.081094741821289,
      "learning_rate": 0.00019916640734768547,
      "loss": 0.9388,
      "step": 1384
    },
    {
      "epoch": 0.006514765233261522,
      "grad_norm": 2.3634145259857178,
      "learning_rate": 0.000199165464369572,
      "loss": 0.3255,
      "step": 1385
    },
    {
      "epoch": 0.006519469034873985,
      "grad_norm": 5.837757587432861,
      "learning_rate": 0.0001991645213914585,
      "loss": 1.0654,
      "step": 1386
    },
    {
      "epoch": 0.006524172836486449,
      "grad_norm": 2.6881825923919678,
      "learning_rate": 0.00019916357841334503,
      "loss": 0.395,
      "step": 1387
    },
    {
      "epoch": 0.006528876638098912,
      "grad_norm": 4.49833345413208,
      "learning_rate": 0.00019916263543523155,
      "loss": 0.7888,
      "step": 1388
    },
    {
      "epoch": 0.006533580439711375,
      "grad_norm": 0.671392023563385,
      "learning_rate": 0.0001991616924571181,
      "loss": 0.0617,
      "step": 1389
    },
    {
      "epoch": 0.006538284241323838,
      "grad_norm": 0.2141418755054474,
      "learning_rate": 0.0001991607494790046,
      "loss": 0.0087,
      "step": 1390
    },
    {
      "epoch": 0.006542988042936301,
      "grad_norm": 4.5547637939453125,
      "learning_rate": 0.00019915980650089113,
      "loss": 0.5384,
      "step": 1391
    },
    {
      "epoch": 0.006547691844548765,
      "grad_norm": 4.375422954559326,
      "learning_rate": 0.00019915886352277765,
      "loss": 0.4163,
      "step": 1392
    },
    {
      "epoch": 0.006552395646161228,
      "grad_norm": 2.95135498046875,
      "learning_rate": 0.00019915792054466417,
      "loss": 0.2635,
      "step": 1393
    },
    {
      "epoch": 0.006557099447773691,
      "grad_norm": 2.2018022537231445,
      "learning_rate": 0.0001991569775665507,
      "loss": 0.4005,
      "step": 1394
    },
    {
      "epoch": 0.006561803249386154,
      "grad_norm": 0.7557845711708069,
      "learning_rate": 0.0001991560345884372,
      "loss": 0.1261,
      "step": 1395
    },
    {
      "epoch": 0.006566507050998617,
      "grad_norm": 1.8487602472305298,
      "learning_rate": 0.00019915509161032372,
      "loss": 0.5149,
      "step": 1396
    },
    {
      "epoch": 0.00657121085261108,
      "grad_norm": 2.1005938053131104,
      "learning_rate": 0.00019915414863221024,
      "loss": 0.3672,
      "step": 1397
    },
    {
      "epoch": 0.006575914654223544,
      "grad_norm": 2.2446391582489014,
      "learning_rate": 0.0001991532056540968,
      "loss": 0.7652,
      "step": 1398
    },
    {
      "epoch": 0.006580618455836007,
      "grad_norm": 2.5361857414245605,
      "learning_rate": 0.0001991522626759833,
      "loss": 0.3595,
      "step": 1399
    },
    {
      "epoch": 0.00658532225744847,
      "grad_norm": 1.6888619661331177,
      "learning_rate": 0.00019915131969786983,
      "loss": 0.3998,
      "step": 1400
    },
    {
      "epoch": 0.006590026059060933,
      "grad_norm": 3.044670581817627,
      "learning_rate": 0.00019915037671975634,
      "loss": 0.3477,
      "step": 1401
    },
    {
      "epoch": 0.006594729860673396,
      "grad_norm": 4.274088382720947,
      "learning_rate": 0.0001991494337416429,
      "loss": 0.6977,
      "step": 1402
    },
    {
      "epoch": 0.006599433662285859,
      "grad_norm": 2.7150449752807617,
      "learning_rate": 0.00019914849076352938,
      "loss": 0.392,
      "step": 1403
    },
    {
      "epoch": 0.006604137463898323,
      "grad_norm": 1.520392656326294,
      "learning_rate": 0.0001991475477854159,
      "loss": 0.2205,
      "step": 1404
    },
    {
      "epoch": 0.006608841265510786,
      "grad_norm": 2.037027597427368,
      "learning_rate": 0.00019914660480730242,
      "loss": 0.6189,
      "step": 1405
    },
    {
      "epoch": 0.006613545067123249,
      "grad_norm": 2.222280979156494,
      "learning_rate": 0.00019914566182918894,
      "loss": 0.3542,
      "step": 1406
    },
    {
      "epoch": 0.006618248868735712,
      "grad_norm": 3.267590284347534,
      "learning_rate": 0.00019914471885107548,
      "loss": 0.4173,
      "step": 1407
    },
    {
      "epoch": 0.006622952670348175,
      "grad_norm": 2.785785675048828,
      "learning_rate": 0.000199143775872962,
      "loss": 0.2641,
      "step": 1408
    },
    {
      "epoch": 0.006627656471960639,
      "grad_norm": 12.447946548461914,
      "learning_rate": 0.00019914283289484852,
      "loss": 0.2517,
      "step": 1409
    },
    {
      "epoch": 0.006632360273573102,
      "grad_norm": 3.600961685180664,
      "learning_rate": 0.00019914188991673504,
      "loss": 0.3981,
      "step": 1410
    },
    {
      "epoch": 0.006637064075185565,
      "grad_norm": 2.8569252490997314,
      "learning_rate": 0.00019914094693862156,
      "loss": 0.5635,
      "step": 1411
    },
    {
      "epoch": 0.006641767876798028,
      "grad_norm": 3.2600643634796143,
      "learning_rate": 0.0001991400039605081,
      "loss": 0.7017,
      "step": 1412
    },
    {
      "epoch": 0.006646471678410491,
      "grad_norm": 1.5967565774917603,
      "learning_rate": 0.00019913906098239462,
      "loss": 0.211,
      "step": 1413
    },
    {
      "epoch": 0.006651175480022954,
      "grad_norm": 2.103419542312622,
      "learning_rate": 0.00019913811800428111,
      "loss": 0.2545,
      "step": 1414
    },
    {
      "epoch": 0.006655879281635418,
      "grad_norm": 1.984075665473938,
      "learning_rate": 0.00019913717502616763,
      "loss": 0.2367,
      "step": 1415
    },
    {
      "epoch": 0.006660583083247881,
      "grad_norm": 2.8564300537109375,
      "learning_rate": 0.00019913623204805418,
      "loss": 0.3994,
      "step": 1416
    },
    {
      "epoch": 0.006665286884860344,
      "grad_norm": 1.5458862781524658,
      "learning_rate": 0.0001991352890699407,
      "loss": 0.3583,
      "step": 1417
    },
    {
      "epoch": 0.006669990686472807,
      "grad_norm": 1.397250771522522,
      "learning_rate": 0.00019913434609182722,
      "loss": 0.2167,
      "step": 1418
    },
    {
      "epoch": 0.00667469448808527,
      "grad_norm": 2.3576314449310303,
      "learning_rate": 0.00019913340311371373,
      "loss": 0.3804,
      "step": 1419
    },
    {
      "epoch": 0.006679398289697734,
      "grad_norm": 1.610963225364685,
      "learning_rate": 0.00019913246013560025,
      "loss": 0.1914,
      "step": 1420
    },
    {
      "epoch": 0.006684102091310197,
      "grad_norm": 0.7774103283882141,
      "learning_rate": 0.0001991315171574868,
      "loss": 0.0732,
      "step": 1421
    },
    {
      "epoch": 0.00668880589292266,
      "grad_norm": 13.439309120178223,
      "learning_rate": 0.00019913057417937332,
      "loss": 0.5524,
      "step": 1422
    },
    {
      "epoch": 0.006693509694535123,
      "grad_norm": 0.961188554763794,
      "learning_rate": 0.00019912963120125984,
      "loss": 0.1207,
      "step": 1423
    },
    {
      "epoch": 0.006698213496147586,
      "grad_norm": 1.2676563262939453,
      "learning_rate": 0.00019912868822314635,
      "loss": 0.1904,
      "step": 1424
    },
    {
      "epoch": 0.006702917297760049,
      "grad_norm": 1.8618130683898926,
      "learning_rate": 0.00019912774524503287,
      "loss": 0.194,
      "step": 1425
    },
    {
      "epoch": 0.006707621099372513,
      "grad_norm": 1.6804386377334595,
      "learning_rate": 0.0001991268022669194,
      "loss": 0.1317,
      "step": 1426
    },
    {
      "epoch": 0.006712324900984976,
      "grad_norm": 2.542173385620117,
      "learning_rate": 0.0001991258592888059,
      "loss": 0.3242,
      "step": 1427
    },
    {
      "epoch": 0.006717028702597439,
      "grad_norm": 1.5305466651916504,
      "learning_rate": 0.00019912491631069243,
      "loss": 0.1365,
      "step": 1428
    },
    {
      "epoch": 0.006721732504209902,
      "grad_norm": 0.3391966223716736,
      "learning_rate": 0.00019912397333257895,
      "loss": 0.0216,
      "step": 1429
    },
    {
      "epoch": 0.006726436305822365,
      "grad_norm": 2.9247663021087646,
      "learning_rate": 0.0001991230303544655,
      "loss": 0.5385,
      "step": 1430
    },
    {
      "epoch": 0.0067311401074348284,
      "grad_norm": 3.7968170642852783,
      "learning_rate": 0.000199122087376352,
      "loss": 0.6024,
      "step": 1431
    },
    {
      "epoch": 0.006735843909047292,
      "grad_norm": 2.049414873123169,
      "learning_rate": 0.00019912114439823853,
      "loss": 0.1616,
      "step": 1432
    },
    {
      "epoch": 0.006740547710659755,
      "grad_norm": 3.8054070472717285,
      "learning_rate": 0.00019912020142012505,
      "loss": 0.4561,
      "step": 1433
    },
    {
      "epoch": 0.006745251512272218,
      "grad_norm": 4.383414268493652,
      "learning_rate": 0.00019911925844201157,
      "loss": 0.507,
      "step": 1434
    },
    {
      "epoch": 0.0067499553138846815,
      "grad_norm": 2.059438467025757,
      "learning_rate": 0.0001991183154638981,
      "loss": 0.4461,
      "step": 1435
    },
    {
      "epoch": 0.0067546591154971445,
      "grad_norm": 2.832028388977051,
      "learning_rate": 0.0001991173724857846,
      "loss": 0.344,
      "step": 1436
    },
    {
      "epoch": 0.006759362917109608,
      "grad_norm": 1.315283179283142,
      "learning_rate": 0.00019911642950767112,
      "loss": 0.1053,
      "step": 1437
    },
    {
      "epoch": 0.0067640667187220714,
      "grad_norm": 1.0429794788360596,
      "learning_rate": 0.00019911548652955764,
      "loss": 0.0673,
      "step": 1438
    },
    {
      "epoch": 0.0067687705203345345,
      "grad_norm": 10.740277290344238,
      "learning_rate": 0.0001991145435514442,
      "loss": 0.4821,
      "step": 1439
    },
    {
      "epoch": 0.0067734743219469975,
      "grad_norm": 2.0408830642700195,
      "learning_rate": 0.0001991136005733307,
      "loss": 0.1478,
      "step": 1440
    },
    {
      "epoch": 0.0067781781235594606,
      "grad_norm": 0.5398014187812805,
      "learning_rate": 0.00019911265759521723,
      "loss": 0.0329,
      "step": 1441
    },
    {
      "epoch": 0.006782881925171924,
      "grad_norm": 1.07991361618042,
      "learning_rate": 0.00019911171461710374,
      "loss": 0.1796,
      "step": 1442
    },
    {
      "epoch": 0.0067875857267843875,
      "grad_norm": 4.16876220703125,
      "learning_rate": 0.0001991107716389903,
      "loss": 0.3654,
      "step": 1443
    },
    {
      "epoch": 0.0067922895283968505,
      "grad_norm": 1.696366548538208,
      "learning_rate": 0.0001991098286608768,
      "loss": 0.08,
      "step": 1444
    },
    {
      "epoch": 0.006796993330009314,
      "grad_norm": 3.2078402042388916,
      "learning_rate": 0.0001991088856827633,
      "loss": 0.2976,
      "step": 1445
    },
    {
      "epoch": 0.006801697131621777,
      "grad_norm": 2.092254877090454,
      "learning_rate": 0.00019910794270464982,
      "loss": 0.2181,
      "step": 1446
    },
    {
      "epoch": 0.00680640093323424,
      "grad_norm": 0.9521049857139587,
      "learning_rate": 0.00019910699972653634,
      "loss": 0.0665,
      "step": 1447
    },
    {
      "epoch": 0.006811104734846703,
      "grad_norm": 2.443721294403076,
      "learning_rate": 0.00019910605674842288,
      "loss": 0.1891,
      "step": 1448
    },
    {
      "epoch": 0.006815808536459167,
      "grad_norm": 0.6312785744667053,
      "learning_rate": 0.0001991051137703094,
      "loss": 0.0265,
      "step": 1449
    },
    {
      "epoch": 0.00682051233807163,
      "grad_norm": 0.8142788410186768,
      "learning_rate": 0.00019910417079219592,
      "loss": 0.0296,
      "step": 1450
    },
    {
      "epoch": 0.006825216139684093,
      "grad_norm": 7.120420455932617,
      "learning_rate": 0.00019910322781408244,
      "loss": 1.0179,
      "step": 1451
    },
    {
      "epoch": 0.006829919941296556,
      "grad_norm": 8.266618728637695,
      "learning_rate": 0.00019910228483596899,
      "loss": 0.6852,
      "step": 1452
    },
    {
      "epoch": 0.006834623742909019,
      "grad_norm": 4.806761264801025,
      "learning_rate": 0.0001991013418578555,
      "loss": 0.2074,
      "step": 1453
    },
    {
      "epoch": 0.006839327544521483,
      "grad_norm": 5.498729228973389,
      "learning_rate": 0.00019910039887974202,
      "loss": 0.5235,
      "step": 1454
    },
    {
      "epoch": 0.006844031346133946,
      "grad_norm": 2.038437843322754,
      "learning_rate": 0.00019909945590162854,
      "loss": 0.1448,
      "step": 1455
    },
    {
      "epoch": 0.006848735147746409,
      "grad_norm": 10.403788566589355,
      "learning_rate": 0.00019909851292351503,
      "loss": 0.2314,
      "step": 1456
    },
    {
      "epoch": 0.006853438949358872,
      "grad_norm": 8.04789924621582,
      "learning_rate": 0.00019909756994540158,
      "loss": 1.1807,
      "step": 1457
    },
    {
      "epoch": 0.006858142750971335,
      "grad_norm": 2.0530431270599365,
      "learning_rate": 0.0001990966269672881,
      "loss": 0.0919,
      "step": 1458
    },
    {
      "epoch": 0.006862846552583798,
      "grad_norm": 2.5555202960968018,
      "learning_rate": 0.00019909568398917462,
      "loss": 0.1374,
      "step": 1459
    },
    {
      "epoch": 0.006867550354196262,
      "grad_norm": 5.663936614990234,
      "learning_rate": 0.00019909474101106113,
      "loss": 0.961,
      "step": 1460
    },
    {
      "epoch": 0.006872254155808725,
      "grad_norm": 0.42344313859939575,
      "learning_rate": 0.00019909379803294765,
      "loss": 0.0156,
      "step": 1461
    },
    {
      "epoch": 0.006876957957421188,
      "grad_norm": 5.350970268249512,
      "learning_rate": 0.0001990928550548342,
      "loss": 0.4753,
      "step": 1462
    },
    {
      "epoch": 0.006881661759033651,
      "grad_norm": 4.2557268142700195,
      "learning_rate": 0.00019909191207672072,
      "loss": 0.5099,
      "step": 1463
    },
    {
      "epoch": 0.006886365560646114,
      "grad_norm": 2.2290570735931396,
      "learning_rate": 0.00019909096909860724,
      "loss": 0.1745,
      "step": 1464
    },
    {
      "epoch": 0.006891069362258577,
      "grad_norm": 2.7260773181915283,
      "learning_rate": 0.00019909002612049375,
      "loss": 0.2966,
      "step": 1465
    },
    {
      "epoch": 0.006895773163871041,
      "grad_norm": 2.1883420944213867,
      "learning_rate": 0.00019908908314238027,
      "loss": 0.149,
      "step": 1466
    },
    {
      "epoch": 0.006900476965483504,
      "grad_norm": 0.6175577640533447,
      "learning_rate": 0.0001990881401642668,
      "loss": 0.032,
      "step": 1467
    },
    {
      "epoch": 0.006905180767095967,
      "grad_norm": 0.544040858745575,
      "learning_rate": 0.0001990871971861533,
      "loss": 0.0286,
      "step": 1468
    },
    {
      "epoch": 0.00690988456870843,
      "grad_norm": 3.0093536376953125,
      "learning_rate": 0.00019908625420803983,
      "loss": 0.4559,
      "step": 1469
    },
    {
      "epoch": 0.006914588370320893,
      "grad_norm": 1.665278673171997,
      "learning_rate": 0.00019908531122992635,
      "loss": 0.0873,
      "step": 1470
    },
    {
      "epoch": 0.006919292171933357,
      "grad_norm": 5.53358268737793,
      "learning_rate": 0.0001990843682518129,
      "loss": 0.5145,
      "step": 1471
    },
    {
      "epoch": 0.00692399597354582,
      "grad_norm": 2.2321808338165283,
      "learning_rate": 0.0001990834252736994,
      "loss": 0.4545,
      "step": 1472
    },
    {
      "epoch": 0.006928699775158283,
      "grad_norm": 4.483903884887695,
      "learning_rate": 0.00019908248229558593,
      "loss": 0.9831,
      "step": 1473
    },
    {
      "epoch": 0.006933403576770746,
      "grad_norm": 2.4293270111083984,
      "learning_rate": 0.00019908153931747245,
      "loss": 0.2008,
      "step": 1474
    },
    {
      "epoch": 0.006938107378383209,
      "grad_norm": 1.5354743003845215,
      "learning_rate": 0.000199080596339359,
      "loss": 0.1568,
      "step": 1475
    },
    {
      "epoch": 0.006942811179995672,
      "grad_norm": 2.4184625148773193,
      "learning_rate": 0.0001990796533612455,
      "loss": 0.0638,
      "step": 1476
    },
    {
      "epoch": 0.006947514981608136,
      "grad_norm": 4.437269687652588,
      "learning_rate": 0.000199078710383132,
      "loss": 0.4352,
      "step": 1477
    },
    {
      "epoch": 0.006952218783220599,
      "grad_norm": 2.7684755325317383,
      "learning_rate": 0.00019907776740501852,
      "loss": 0.2304,
      "step": 1478
    },
    {
      "epoch": 0.006956922584833062,
      "grad_norm": 2.8454320430755615,
      "learning_rate": 0.00019907682442690504,
      "loss": 0.3246,
      "step": 1479
    },
    {
      "epoch": 0.006961626386445525,
      "grad_norm": 2.215761184692383,
      "learning_rate": 0.0001990758814487916,
      "loss": 0.2598,
      "step": 1480
    },
    {
      "epoch": 0.006966330188057988,
      "grad_norm": 2.214871406555176,
      "learning_rate": 0.0001990749384706781,
      "loss": 0.1307,
      "step": 1481
    },
    {
      "epoch": 0.006971033989670451,
      "grad_norm": 1.1508687734603882,
      "learning_rate": 0.00019907399549256463,
      "loss": 0.1246,
      "step": 1482
    },
    {
      "epoch": 0.006975737791282915,
      "grad_norm": 2.698467254638672,
      "learning_rate": 0.00019907305251445114,
      "loss": 0.3449,
      "step": 1483
    },
    {
      "epoch": 0.006980441592895378,
      "grad_norm": 1.8462392091751099,
      "learning_rate": 0.0001990721095363377,
      "loss": 0.1848,
      "step": 1484
    },
    {
      "epoch": 0.006985145394507841,
      "grad_norm": 6.624638557434082,
      "learning_rate": 0.0001990711665582242,
      "loss": 0.3611,
      "step": 1485
    },
    {
      "epoch": 0.006989849196120304,
      "grad_norm": 1.5050383806228638,
      "learning_rate": 0.00019907022358011073,
      "loss": 0.1452,
      "step": 1486
    },
    {
      "epoch": 0.006994552997732767,
      "grad_norm": 2.9412803649902344,
      "learning_rate": 0.00019906928060199722,
      "loss": 0.2747,
      "step": 1487
    },
    {
      "epoch": 0.006999256799345231,
      "grad_norm": 1.803712248802185,
      "learning_rate": 0.00019906833762388374,
      "loss": 0.1882,
      "step": 1488
    },
    {
      "epoch": 0.007003960600957694,
      "grad_norm": 2.179441452026367,
      "learning_rate": 0.00019906739464577028,
      "loss": 0.1346,
      "step": 1489
    },
    {
      "epoch": 0.007008664402570157,
      "grad_norm": 4.041566848754883,
      "learning_rate": 0.0001990664516676568,
      "loss": 0.5996,
      "step": 1490
    },
    {
      "epoch": 0.00701336820418262,
      "grad_norm": 2.58475923538208,
      "learning_rate": 0.00019906550868954332,
      "loss": 0.2454,
      "step": 1491
    },
    {
      "epoch": 0.007018072005795083,
      "grad_norm": 1.6086567640304565,
      "learning_rate": 0.00019906456571142984,
      "loss": 0.061,
      "step": 1492
    },
    {
      "epoch": 0.007022775807407546,
      "grad_norm": 1.6899641752243042,
      "learning_rate": 0.00019906362273331639,
      "loss": 0.0899,
      "step": 1493
    },
    {
      "epoch": 0.00702747960902001,
      "grad_norm": 2.037184953689575,
      "learning_rate": 0.0001990626797552029,
      "loss": 0.1991,
      "step": 1494
    },
    {
      "epoch": 0.007032183410632473,
      "grad_norm": 1.201207160949707,
      "learning_rate": 0.00019906173677708942,
      "loss": 0.1137,
      "step": 1495
    },
    {
      "epoch": 0.007036887212244936,
      "grad_norm": 1.898914098739624,
      "learning_rate": 0.00019906079379897594,
      "loss": 0.1756,
      "step": 1496
    },
    {
      "epoch": 0.007041591013857399,
      "grad_norm": 3.409877300262451,
      "learning_rate": 0.00019905985082086246,
      "loss": 0.6678,
      "step": 1497
    },
    {
      "epoch": 0.0070462948154698625,
      "grad_norm": 3.231858491897583,
      "learning_rate": 0.00019905890784274898,
      "loss": 0.5474,
      "step": 1498
    },
    {
      "epoch": 0.0070509986170823255,
      "grad_norm": 5.482954502105713,
      "learning_rate": 0.0001990579648646355,
      "loss": 0.2262,
      "step": 1499
    },
    {
      "epoch": 0.007055702418694789,
      "grad_norm": 1.6555808782577515,
      "learning_rate": 0.00019905702188652202,
      "loss": 0.1211,
      "step": 1500
    },
    {
      "epoch": 0.0070604062203072524,
      "grad_norm": 2.4366302490234375,
      "learning_rate": 0.00019905607890840853,
      "loss": 0.2164,
      "step": 1501
    },
    {
      "epoch": 0.0070651100219197155,
      "grad_norm": 3.3971059322357178,
      "learning_rate": 0.00019905513593029508,
      "loss": 0.3128,
      "step": 1502
    },
    {
      "epoch": 0.0070698138235321785,
      "grad_norm": 3.0080647468566895,
      "learning_rate": 0.0001990541929521816,
      "loss": 0.2526,
      "step": 1503
    },
    {
      "epoch": 0.007074517625144642,
      "grad_norm": 2.450955867767334,
      "learning_rate": 0.00019905324997406812,
      "loss": 0.1475,
      "step": 1504
    },
    {
      "epoch": 0.0070792214267571055,
      "grad_norm": 5.756219387054443,
      "learning_rate": 0.00019905230699595464,
      "loss": 0.4208,
      "step": 1505
    },
    {
      "epoch": 0.0070839252283695685,
      "grad_norm": 4.120522975921631,
      "learning_rate": 0.00019905136401784115,
      "loss": 0.3369,
      "step": 1506
    },
    {
      "epoch": 0.0070886290299820316,
      "grad_norm": 2.4948503971099854,
      "learning_rate": 0.00019905042103972767,
      "loss": 0.1831,
      "step": 1507
    },
    {
      "epoch": 0.007093332831594495,
      "grad_norm": 2.53379487991333,
      "learning_rate": 0.0001990494780616142,
      "loss": 0.1221,
      "step": 1508
    },
    {
      "epoch": 0.007098036633206958,
      "grad_norm": 1.535423755645752,
      "learning_rate": 0.0001990485350835007,
      "loss": 0.1534,
      "step": 1509
    },
    {
      "epoch": 0.007102740434819421,
      "grad_norm": 0.9368466138839722,
      "learning_rate": 0.00019904759210538723,
      "loss": 0.0593,
      "step": 1510
    },
    {
      "epoch": 0.007107444236431885,
      "grad_norm": 5.832233428955078,
      "learning_rate": 0.00019904664912727377,
      "loss": 0.2394,
      "step": 1511
    },
    {
      "epoch": 0.007112148038044348,
      "grad_norm": 1.7118804454803467,
      "learning_rate": 0.0001990457061491603,
      "loss": 0.1549,
      "step": 1512
    },
    {
      "epoch": 0.007116851839656811,
      "grad_norm": 0.8583953976631165,
      "learning_rate": 0.0001990447631710468,
      "loss": 0.0562,
      "step": 1513
    },
    {
      "epoch": 0.007121555641269274,
      "grad_norm": 7.437216758728027,
      "learning_rate": 0.00019904382019293333,
      "loss": 0.5879,
      "step": 1514
    },
    {
      "epoch": 0.007126259442881737,
      "grad_norm": 3.0423264503479004,
      "learning_rate": 0.00019904287721481985,
      "loss": 0.3856,
      "step": 1515
    },
    {
      "epoch": 0.007130963244494201,
      "grad_norm": 3.520587921142578,
      "learning_rate": 0.0001990419342367064,
      "loss": 0.2589,
      "step": 1516
    },
    {
      "epoch": 0.007135667046106664,
      "grad_norm": 5.40837287902832,
      "learning_rate": 0.00019904099125859291,
      "loss": 0.6494,
      "step": 1517
    },
    {
      "epoch": 0.007140370847719127,
      "grad_norm": 3.3999195098876953,
      "learning_rate": 0.0001990400482804794,
      "loss": 0.3274,
      "step": 1518
    },
    {
      "epoch": 0.00714507464933159,
      "grad_norm": 9.082784652709961,
      "learning_rate": 0.00019903910530236592,
      "loss": 0.9494,
      "step": 1519
    },
    {
      "epoch": 0.007149778450944053,
      "grad_norm": 4.172074794769287,
      "learning_rate": 0.00019903816232425244,
      "loss": 0.4622,
      "step": 1520
    },
    {
      "epoch": 0.007154482252556516,
      "grad_norm": 4.414341449737549,
      "learning_rate": 0.000199037219346139,
      "loss": 0.277,
      "step": 1521
    },
    {
      "epoch": 0.00715918605416898,
      "grad_norm": 1.6569184064865112,
      "learning_rate": 0.0001990362763680255,
      "loss": 0.1599,
      "step": 1522
    },
    {
      "epoch": 0.007163889855781443,
      "grad_norm": 3.0851776599884033,
      "learning_rate": 0.00019903533338991203,
      "loss": 0.4116,
      "step": 1523
    },
    {
      "epoch": 0.007168593657393906,
      "grad_norm": 3.0310823917388916,
      "learning_rate": 0.00019903439041179854,
      "loss": 0.2694,
      "step": 1524
    },
    {
      "epoch": 0.007173297459006369,
      "grad_norm": 2.8986172676086426,
      "learning_rate": 0.0001990334474336851,
      "loss": 0.1286,
      "step": 1525
    },
    {
      "epoch": 0.007178001260618832,
      "grad_norm": 0.8381897807121277,
      "learning_rate": 0.0001990325044555716,
      "loss": 0.0751,
      "step": 1526
    },
    {
      "epoch": 0.007182705062231295,
      "grad_norm": 1.0870673656463623,
      "learning_rate": 0.00019903156147745813,
      "loss": 0.1325,
      "step": 1527
    },
    {
      "epoch": 0.007187408863843759,
      "grad_norm": 1.6816426515579224,
      "learning_rate": 0.00019903061849934465,
      "loss": 0.2098,
      "step": 1528
    },
    {
      "epoch": 0.007192112665456222,
      "grad_norm": 6.085613250732422,
      "learning_rate": 0.00019902967552123116,
      "loss": 0.9937,
      "step": 1529
    },
    {
      "epoch": 0.007196816467068685,
      "grad_norm": 0.9858291149139404,
      "learning_rate": 0.00019902873254311768,
      "loss": 0.1192,
      "step": 1530
    },
    {
      "epoch": 0.007201520268681148,
      "grad_norm": 4.775461673736572,
      "learning_rate": 0.0001990277895650042,
      "loss": 0.5346,
      "step": 1531
    },
    {
      "epoch": 0.007206224070293611,
      "grad_norm": 1.5665034055709839,
      "learning_rate": 0.00019902684658689072,
      "loss": 0.3857,
      "step": 1532
    },
    {
      "epoch": 0.007210927871906075,
      "grad_norm": 2.7294039726257324,
      "learning_rate": 0.00019902590360877724,
      "loss": 0.4847,
      "step": 1533
    },
    {
      "epoch": 0.007215631673518538,
      "grad_norm": 3.447383165359497,
      "learning_rate": 0.00019902496063066378,
      "loss": 0.5959,
      "step": 1534
    },
    {
      "epoch": 0.007220335475131001,
      "grad_norm": 3.1211118698120117,
      "learning_rate": 0.0001990240176525503,
      "loss": 0.3974,
      "step": 1535
    },
    {
      "epoch": 0.007225039276743464,
      "grad_norm": 1.7299609184265137,
      "learning_rate": 0.00019902307467443682,
      "loss": 0.1973,
      "step": 1536
    },
    {
      "epoch": 0.007229743078355927,
      "grad_norm": 5.461467266082764,
      "learning_rate": 0.00019902213169632334,
      "loss": 0.5661,
      "step": 1537
    },
    {
      "epoch": 0.00723444687996839,
      "grad_norm": 1.9634644985198975,
      "learning_rate": 0.00019902118871820986,
      "loss": 0.2772,
      "step": 1538
    },
    {
      "epoch": 0.007239150681580854,
      "grad_norm": 0.828944742679596,
      "learning_rate": 0.00019902024574009638,
      "loss": 0.0678,
      "step": 1539
    },
    {
      "epoch": 0.007243854483193317,
      "grad_norm": 0.9554455280303955,
      "learning_rate": 0.0001990193027619829,
      "loss": 0.122,
      "step": 1540
    },
    {
      "epoch": 0.00724855828480578,
      "grad_norm": 0.7532808780670166,
      "learning_rate": 0.00019901835978386942,
      "loss": 0.0742,
      "step": 1541
    },
    {
      "epoch": 0.007253262086418243,
      "grad_norm": 1.1933282613754272,
      "learning_rate": 0.00019901741680575593,
      "loss": 0.1734,
      "step": 1542
    },
    {
      "epoch": 0.007257965888030706,
      "grad_norm": 1.5679826736450195,
      "learning_rate": 0.00019901647382764248,
      "loss": 0.2128,
      "step": 1543
    },
    {
      "epoch": 0.007262669689643169,
      "grad_norm": 0.9316748380661011,
      "learning_rate": 0.000199015530849529,
      "loss": 0.0925,
      "step": 1544
    },
    {
      "epoch": 0.007267373491255633,
      "grad_norm": 1.129256248474121,
      "learning_rate": 0.00019901458787141552,
      "loss": 0.1711,
      "step": 1545
    },
    {
      "epoch": 0.007272077292868096,
      "grad_norm": 4.774512767791748,
      "learning_rate": 0.00019901364489330204,
      "loss": 1.1239,
      "step": 1546
    },
    {
      "epoch": 0.007276781094480559,
      "grad_norm": 1.0059623718261719,
      "learning_rate": 0.00019901270191518855,
      "loss": 0.1474,
      "step": 1547
    },
    {
      "epoch": 0.007281484896093022,
      "grad_norm": 0.8260228037834167,
      "learning_rate": 0.0001990117589370751,
      "loss": 0.0874,
      "step": 1548
    },
    {
      "epoch": 0.007286188697705485,
      "grad_norm": 2.1862740516662598,
      "learning_rate": 0.0001990108159589616,
      "loss": 0.5407,
      "step": 1549
    },
    {
      "epoch": 0.007290892499317949,
      "grad_norm": 3.150632381439209,
      "learning_rate": 0.0001990098729808481,
      "loss": 0.287,
      "step": 1550
    },
    {
      "epoch": 0.007295596300930412,
      "grad_norm": 0.4887999892234802,
      "learning_rate": 0.00019900893000273463,
      "loss": 0.0306,
      "step": 1551
    },
    {
      "epoch": 0.007300300102542875,
      "grad_norm": 3.4156172275543213,
      "learning_rate": 0.00019900798702462117,
      "loss": 0.0404,
      "step": 1552
    },
    {
      "epoch": 0.007305003904155338,
      "grad_norm": 4.2180633544921875,
      "learning_rate": 0.0001990070440465077,
      "loss": 0.7546,
      "step": 1553
    },
    {
      "epoch": 0.007309707705767801,
      "grad_norm": 3.632542848587036,
      "learning_rate": 0.0001990061010683942,
      "loss": 0.2721,
      "step": 1554
    },
    {
      "epoch": 0.007314411507380264,
      "grad_norm": 0.9155692458152771,
      "learning_rate": 0.00019900515809028073,
      "loss": 0.1148,
      "step": 1555
    },
    {
      "epoch": 0.007319115308992728,
      "grad_norm": 5.565780162811279,
      "learning_rate": 0.00019900421511216725,
      "loss": 0.7153,
      "step": 1556
    },
    {
      "epoch": 0.007323819110605191,
      "grad_norm": 0.49045392870903015,
      "learning_rate": 0.0001990032721340538,
      "loss": 0.0348,
      "step": 1557
    },
    {
      "epoch": 0.007328522912217654,
      "grad_norm": 0.49134618043899536,
      "learning_rate": 0.00019900232915594031,
      "loss": 0.0499,
      "step": 1558
    },
    {
      "epoch": 0.007333226713830117,
      "grad_norm": 1.9040557146072388,
      "learning_rate": 0.00019900138617782683,
      "loss": 0.2589,
      "step": 1559
    },
    {
      "epoch": 0.00733793051544258,
      "grad_norm": 2.281604766845703,
      "learning_rate": 0.00019900044319971335,
      "loss": 0.2648,
      "step": 1560
    },
    {
      "epoch": 0.0073426343170550435,
      "grad_norm": 11.491667747497559,
      "learning_rate": 0.00019899950022159987,
      "loss": 0.7595,
      "step": 1561
    },
    {
      "epoch": 0.007347338118667507,
      "grad_norm": 0.6182972192764282,
      "learning_rate": 0.0001989985572434864,
      "loss": 0.0699,
      "step": 1562
    },
    {
      "epoch": 0.00735204192027997,
      "grad_norm": 2.5922319889068604,
      "learning_rate": 0.0001989976142653729,
      "loss": 0.3437,
      "step": 1563
    },
    {
      "epoch": 0.0073567457218924335,
      "grad_norm": 5.138021945953369,
      "learning_rate": 0.00019899667128725943,
      "loss": 0.4237,
      "step": 1564
    },
    {
      "epoch": 0.0073614495235048965,
      "grad_norm": 4.2352824211120605,
      "learning_rate": 0.00019899572830914594,
      "loss": 0.3887,
      "step": 1565
    },
    {
      "epoch": 0.0073661533251173595,
      "grad_norm": 3.3455257415771484,
      "learning_rate": 0.0001989947853310325,
      "loss": 0.1876,
      "step": 1566
    },
    {
      "epoch": 0.0073708571267298234,
      "grad_norm": 1.4736741781234741,
      "learning_rate": 0.000198993842352919,
      "loss": 0.1583,
      "step": 1567
    },
    {
      "epoch": 0.0073755609283422865,
      "grad_norm": 3.180309295654297,
      "learning_rate": 0.00019899289937480553,
      "loss": 0.2636,
      "step": 1568
    },
    {
      "epoch": 0.0073802647299547495,
      "grad_norm": 1.2760204076766968,
      "learning_rate": 0.00019899195639669205,
      "loss": 0.111,
      "step": 1569
    },
    {
      "epoch": 0.0073849685315672126,
      "grad_norm": 1.256264328956604,
      "learning_rate": 0.00019899101341857856,
      "loss": 0.1044,
      "step": 1570
    },
    {
      "epoch": 0.007389672333179676,
      "grad_norm": 1.41753351688385,
      "learning_rate": 0.00019899007044046508,
      "loss": 0.1597,
      "step": 1571
    },
    {
      "epoch": 0.007394376134792139,
      "grad_norm": 3.686314582824707,
      "learning_rate": 0.0001989891274623516,
      "loss": 0.4671,
      "step": 1572
    },
    {
      "epoch": 0.0073990799364046025,
      "grad_norm": 3.383988380432129,
      "learning_rate": 0.00019898818448423812,
      "loss": 0.4775,
      "step": 1573
    },
    {
      "epoch": 0.007403783738017066,
      "grad_norm": 0.46275943517684937,
      "learning_rate": 0.00019898724150612464,
      "loss": 0.0526,
      "step": 1574
    },
    {
      "epoch": 0.007408487539629529,
      "grad_norm": 4.965036392211914,
      "learning_rate": 0.00019898629852801118,
      "loss": 0.8119,
      "step": 1575
    },
    {
      "epoch": 0.007413191341241992,
      "grad_norm": 2.9518661499023438,
      "learning_rate": 0.0001989853555498977,
      "loss": 0.244,
      "step": 1576
    },
    {
      "epoch": 0.007417895142854455,
      "grad_norm": 0.4855504333972931,
      "learning_rate": 0.00019898441257178422,
      "loss": 0.0397,
      "step": 1577
    },
    {
      "epoch": 0.007422598944466918,
      "grad_norm": 0.7756426930427551,
      "learning_rate": 0.00019898346959367074,
      "loss": 0.06,
      "step": 1578
    },
    {
      "epoch": 0.007427302746079382,
      "grad_norm": 0.4038020968437195,
      "learning_rate": 0.00019898252661555726,
      "loss": 0.0319,
      "step": 1579
    },
    {
      "epoch": 0.007432006547691845,
      "grad_norm": 0.7220945358276367,
      "learning_rate": 0.00019898158363744378,
      "loss": 0.0691,
      "step": 1580
    },
    {
      "epoch": 0.007436710349304308,
      "grad_norm": 2.4435341358184814,
      "learning_rate": 0.0001989806406593303,
      "loss": 0.281,
      "step": 1581
    },
    {
      "epoch": 0.007441414150916771,
      "grad_norm": 1.023919701576233,
      "learning_rate": 0.00019897969768121682,
      "loss": 0.0906,
      "step": 1582
    },
    {
      "epoch": 0.007446117952529234,
      "grad_norm": 2.085477590560913,
      "learning_rate": 0.00019897875470310333,
      "loss": 0.1973,
      "step": 1583
    },
    {
      "epoch": 0.007450821754141698,
      "grad_norm": 3.209566831588745,
      "learning_rate": 0.00019897781172498988,
      "loss": 0.4936,
      "step": 1584
    },
    {
      "epoch": 0.007455525555754161,
      "grad_norm": 2.4954171180725098,
      "learning_rate": 0.0001989768687468764,
      "loss": 0.2364,
      "step": 1585
    },
    {
      "epoch": 0.007460229357366624,
      "grad_norm": 0.36700478196144104,
      "learning_rate": 0.00019897592576876292,
      "loss": 0.0244,
      "step": 1586
    },
    {
      "epoch": 0.007464933158979087,
      "grad_norm": 0.356790691614151,
      "learning_rate": 0.00019897498279064944,
      "loss": 0.0318,
      "step": 1587
    },
    {
      "epoch": 0.00746963696059155,
      "grad_norm": 5.213973045349121,
      "learning_rate": 0.00019897403981253595,
      "loss": 0.6925,
      "step": 1588
    },
    {
      "epoch": 0.007474340762204013,
      "grad_norm": 0.7700883150100708,
      "learning_rate": 0.0001989730968344225,
      "loss": 0.0383,
      "step": 1589
    },
    {
      "epoch": 0.007479044563816477,
      "grad_norm": 3.2975306510925293,
      "learning_rate": 0.00019897215385630902,
      "loss": 0.2322,
      "step": 1590
    },
    {
      "epoch": 0.00748374836542894,
      "grad_norm": 3.026475191116333,
      "learning_rate": 0.00019897121087819554,
      "loss": 0.7011,
      "step": 1591
    },
    {
      "epoch": 0.007488452167041403,
      "grad_norm": 1.5428786277770996,
      "learning_rate": 0.00019897026790008203,
      "loss": 0.1208,
      "step": 1592
    },
    {
      "epoch": 0.007493155968653866,
      "grad_norm": 0.2287512868642807,
      "learning_rate": 0.00019896932492196857,
      "loss": 0.0177,
      "step": 1593
    },
    {
      "epoch": 0.007497859770266329,
      "grad_norm": 0.2510547637939453,
      "learning_rate": 0.0001989683819438551,
      "loss": 0.0136,
      "step": 1594
    },
    {
      "epoch": 0.007502563571878793,
      "grad_norm": 3.1322262287139893,
      "learning_rate": 0.0001989674389657416,
      "loss": 0.7879,
      "step": 1595
    },
    {
      "epoch": 0.007507267373491256,
      "grad_norm": 1.6190037727355957,
      "learning_rate": 0.00019896649598762813,
      "loss": 0.1125,
      "step": 1596
    },
    {
      "epoch": 0.007511971175103719,
      "grad_norm": 4.079925060272217,
      "learning_rate": 0.00019896555300951465,
      "loss": 0.342,
      "step": 1597
    },
    {
      "epoch": 0.007516674976716182,
      "grad_norm": 4.0910234451293945,
      "learning_rate": 0.0001989646100314012,
      "loss": 0.3805,
      "step": 1598
    },
    {
      "epoch": 0.007521378778328645,
      "grad_norm": 2.9319515228271484,
      "learning_rate": 0.00019896366705328771,
      "loss": 0.2639,
      "step": 1599
    },
    {
      "epoch": 0.007526082579941108,
      "grad_norm": 3.7148468494415283,
      "learning_rate": 0.00019896272407517423,
      "loss": 0.6517,
      "step": 1600
    },
    {
      "epoch": 0.007530786381553572,
      "grad_norm": 6.484891414642334,
      "learning_rate": 0.00019896178109706075,
      "loss": 0.3276,
      "step": 1601
    },
    {
      "epoch": 0.007535490183166035,
      "grad_norm": 0.9091915488243103,
      "learning_rate": 0.00019896083811894727,
      "loss": 0.0602,
      "step": 1602
    },
    {
      "epoch": 0.007540193984778498,
      "grad_norm": 6.144850254058838,
      "learning_rate": 0.0001989598951408338,
      "loss": 0.7151,
      "step": 1603
    },
    {
      "epoch": 0.007544897786390961,
      "grad_norm": 1.2686628103256226,
      "learning_rate": 0.0001989589521627203,
      "loss": 0.0988,
      "step": 1604
    },
    {
      "epoch": 0.007549601588003424,
      "grad_norm": 2.5975377559661865,
      "learning_rate": 0.00019895800918460683,
      "loss": 0.1934,
      "step": 1605
    },
    {
      "epoch": 0.007554305389615887,
      "grad_norm": 0.3707684874534607,
      "learning_rate": 0.00019895706620649334,
      "loss": 0.0421,
      "step": 1606
    },
    {
      "epoch": 0.007559009191228351,
      "grad_norm": 2.7485878467559814,
      "learning_rate": 0.0001989561232283799,
      "loss": 0.1658,
      "step": 1607
    },
    {
      "epoch": 0.007563712992840814,
      "grad_norm": 2.0539510250091553,
      "learning_rate": 0.0001989551802502664,
      "loss": 0.2,
      "step": 1608
    },
    {
      "epoch": 0.007568416794453277,
      "grad_norm": 5.670045852661133,
      "learning_rate": 0.00019895423727215293,
      "loss": 1.0534,
      "step": 1609
    },
    {
      "epoch": 0.00757312059606574,
      "grad_norm": 3.4583325386047363,
      "learning_rate": 0.00019895329429403945,
      "loss": 0.6441,
      "step": 1610
    },
    {
      "epoch": 0.007577824397678203,
      "grad_norm": 3.83882737159729,
      "learning_rate": 0.00019895235131592596,
      "loss": 0.6339,
      "step": 1611
    },
    {
      "epoch": 0.007582528199290667,
      "grad_norm": 0.4055306911468506,
      "learning_rate": 0.00019895140833781248,
      "loss": 0.0373,
      "step": 1612
    },
    {
      "epoch": 0.00758723200090313,
      "grad_norm": 5.343654155731201,
      "learning_rate": 0.000198950465359699,
      "loss": 0.8576,
      "step": 1613
    },
    {
      "epoch": 0.007591935802515593,
      "grad_norm": 2.480494976043701,
      "learning_rate": 0.00019894952238158552,
      "loss": 0.3965,
      "step": 1614
    },
    {
      "epoch": 0.007596639604128056,
      "grad_norm": 0.7539643049240112,
      "learning_rate": 0.00019894857940347204,
      "loss": 0.0496,
      "step": 1615
    },
    {
      "epoch": 0.007601343405740519,
      "grad_norm": 1.507466197013855,
      "learning_rate": 0.00019894763642535858,
      "loss": 0.1095,
      "step": 1616
    },
    {
      "epoch": 0.007606047207352982,
      "grad_norm": 0.6925960779190063,
      "learning_rate": 0.0001989466934472451,
      "loss": 0.0523,
      "step": 1617
    },
    {
      "epoch": 0.007610751008965446,
      "grad_norm": 0.04966451972723007,
      "learning_rate": 0.00019894575046913162,
      "loss": 0.0026,
      "step": 1618
    },
    {
      "epoch": 0.007615454810577909,
      "grad_norm": 3.504945993423462,
      "learning_rate": 0.00019894480749101814,
      "loss": 0.4126,
      "step": 1619
    },
    {
      "epoch": 0.007620158612190372,
      "grad_norm": 3.0644478797912598,
      "learning_rate": 0.00019894386451290466,
      "loss": 0.2618,
      "step": 1620
    },
    {
      "epoch": 0.007624862413802835,
      "grad_norm": 2.1780364513397217,
      "learning_rate": 0.0001989429215347912,
      "loss": 0.2506,
      "step": 1621
    },
    {
      "epoch": 0.007629566215415298,
      "grad_norm": 3.6390109062194824,
      "learning_rate": 0.00019894197855667772,
      "loss": 0.5778,
      "step": 1622
    },
    {
      "epoch": 0.0076342700170277614,
      "grad_norm": 2.33213472366333,
      "learning_rate": 0.00019894103557856422,
      "loss": 0.4107,
      "step": 1623
    },
    {
      "epoch": 0.007638973818640225,
      "grad_norm": 3.5405375957489014,
      "learning_rate": 0.00019894009260045073,
      "loss": 0.4024,
      "step": 1624
    },
    {
      "epoch": 0.007643677620252688,
      "grad_norm": 2.0179529190063477,
      "learning_rate": 0.00019893914962233728,
      "loss": 0.1398,
      "step": 1625
    },
    {
      "epoch": 0.007648381421865151,
      "grad_norm": 0.7846934795379639,
      "learning_rate": 0.0001989382066442238,
      "loss": 0.0571,
      "step": 1626
    },
    {
      "epoch": 0.0076530852234776145,
      "grad_norm": 2.125006675720215,
      "learning_rate": 0.00019893726366611032,
      "loss": 0.218,
      "step": 1627
    },
    {
      "epoch": 0.0076577890250900775,
      "grad_norm": 2.6115846633911133,
      "learning_rate": 0.00019893632068799684,
      "loss": 0.2592,
      "step": 1628
    },
    {
      "epoch": 0.007662492826702541,
      "grad_norm": 3.7855734825134277,
      "learning_rate": 0.00019893537770988335,
      "loss": 0.4821,
      "step": 1629
    },
    {
      "epoch": 0.0076671966283150044,
      "grad_norm": 0.678255021572113,
      "learning_rate": 0.0001989344347317699,
      "loss": 0.0421,
      "step": 1630
    },
    {
      "epoch": 0.0076719004299274675,
      "grad_norm": 6.648968696594238,
      "learning_rate": 0.00019893349175365642,
      "loss": 0.9579,
      "step": 1631
    },
    {
      "epoch": 0.0076766042315399305,
      "grad_norm": 0.771108865737915,
      "learning_rate": 0.00019893254877554294,
      "loss": 0.0626,
      "step": 1632
    },
    {
      "epoch": 0.0076813080331523936,
      "grad_norm": 0.732408344745636,
      "learning_rate": 0.00019893160579742946,
      "loss": 0.0492,
      "step": 1633
    },
    {
      "epoch": 0.007686011834764857,
      "grad_norm": 1.6897647380828857,
      "learning_rate": 0.00019893066281931597,
      "loss": 0.2315,
      "step": 1634
    },
    {
      "epoch": 0.0076907156363773205,
      "grad_norm": 1.9170438051223755,
      "learning_rate": 0.0001989297198412025,
      "loss": 0.3678,
      "step": 1635
    },
    {
      "epoch": 0.0076954194379897835,
      "grad_norm": 1.243452787399292,
      "learning_rate": 0.000198928776863089,
      "loss": 0.0804,
      "step": 1636
    },
    {
      "epoch": 0.007700123239602247,
      "grad_norm": 0.7616702914237976,
      "learning_rate": 0.00019892783388497553,
      "loss": 0.0624,
      "step": 1637
    },
    {
      "epoch": 0.00770482704121471,
      "grad_norm": 0.9347981810569763,
      "learning_rate": 0.00019892689090686205,
      "loss": 0.1247,
      "step": 1638
    },
    {
      "epoch": 0.007709530842827173,
      "grad_norm": 4.4129533767700195,
      "learning_rate": 0.0001989259479287486,
      "loss": 0.7318,
      "step": 1639
    },
    {
      "epoch": 0.007714234644439636,
      "grad_norm": 2.9866912364959717,
      "learning_rate": 0.00019892500495063511,
      "loss": 0.7071,
      "step": 1640
    },
    {
      "epoch": 0.0077189384460521,
      "grad_norm": 1.5822582244873047,
      "learning_rate": 0.00019892406197252163,
      "loss": 0.2728,
      "step": 1641
    },
    {
      "epoch": 0.007723642247664563,
      "grad_norm": 1.3037450313568115,
      "learning_rate": 0.00019892311899440815,
      "loss": 0.2277,
      "step": 1642
    },
    {
      "epoch": 0.007728346049277026,
      "grad_norm": 1.4067535400390625,
      "learning_rate": 0.00019892217601629467,
      "loss": 0.1149,
      "step": 1643
    },
    {
      "epoch": 0.007733049850889489,
      "grad_norm": 4.214066505432129,
      "learning_rate": 0.0001989212330381812,
      "loss": 0.4755,
      "step": 1644
    },
    {
      "epoch": 0.007737753652501952,
      "grad_norm": 3.6728708744049072,
      "learning_rate": 0.0001989202900600677,
      "loss": 0.2377,
      "step": 1645
    },
    {
      "epoch": 0.007742457454114416,
      "grad_norm": 2.3796355724334717,
      "learning_rate": 0.00019891934708195423,
      "loss": 0.423,
      "step": 1646
    },
    {
      "epoch": 0.007747161255726879,
      "grad_norm": 0.2395624816417694,
      "learning_rate": 0.00019891840410384074,
      "loss": 0.0141,
      "step": 1647
    },
    {
      "epoch": 0.007751865057339342,
      "grad_norm": 1.4714523553848267,
      "learning_rate": 0.0001989174611257273,
      "loss": 0.2453,
      "step": 1648
    },
    {
      "epoch": 0.007756568858951805,
      "grad_norm": 0.8269250392913818,
      "learning_rate": 0.0001989165181476138,
      "loss": 0.1137,
      "step": 1649
    },
    {
      "epoch": 0.007761272660564268,
      "grad_norm": 1.2023777961730957,
      "learning_rate": 0.00019891557516950033,
      "loss": 0.2054,
      "step": 1650
    },
    {
      "epoch": 0.007765976462176731,
      "grad_norm": 2.600313186645508,
      "learning_rate": 0.00019891463219138685,
      "loss": 0.4091,
      "step": 1651
    },
    {
      "epoch": 0.007770680263789195,
      "grad_norm": 2.513038158416748,
      "learning_rate": 0.0001989136892132734,
      "loss": 0.2294,
      "step": 1652
    },
    {
      "epoch": 0.007775384065401658,
      "grad_norm": 3.421490430831909,
      "learning_rate": 0.0001989127462351599,
      "loss": 0.3258,
      "step": 1653
    },
    {
      "epoch": 0.007780087867014121,
      "grad_norm": 0.863810122013092,
      "learning_rate": 0.0001989118032570464,
      "loss": 0.1154,
      "step": 1654
    },
    {
      "epoch": 0.007784791668626584,
      "grad_norm": 1.5918519496917725,
      "learning_rate": 0.00019891086027893292,
      "loss": 0.1617,
      "step": 1655
    },
    {
      "epoch": 0.007789495470239047,
      "grad_norm": 0.10716669261455536,
      "learning_rate": 0.00019890991730081944,
      "loss": 0.0053,
      "step": 1656
    },
    {
      "epoch": 0.00779419927185151,
      "grad_norm": 4.288917064666748,
      "learning_rate": 0.00019890897432270598,
      "loss": 0.8794,
      "step": 1657
    },
    {
      "epoch": 0.007798903073463974,
      "grad_norm": 1.0743263959884644,
      "learning_rate": 0.0001989080313445925,
      "loss": 0.153,
      "step": 1658
    },
    {
      "epoch": 0.007803606875076437,
      "grad_norm": 2.4675910472869873,
      "learning_rate": 0.00019890708836647902,
      "loss": 0.2249,
      "step": 1659
    },
    {
      "epoch": 0.0078083106766889,
      "grad_norm": 3.1599090099334717,
      "learning_rate": 0.00019890614538836554,
      "loss": 0.4823,
      "step": 1660
    },
    {
      "epoch": 0.007813014478301364,
      "grad_norm": 0.7109806537628174,
      "learning_rate": 0.0001989052024102521,
      "loss": 0.0633,
      "step": 1661
    },
    {
      "epoch": 0.007817718279913826,
      "grad_norm": 2.9848098754882812,
      "learning_rate": 0.0001989042594321386,
      "loss": 0.4197,
      "step": 1662
    },
    {
      "epoch": 0.00782242208152629,
      "grad_norm": 4.789748668670654,
      "learning_rate": 0.00019890331645402512,
      "loss": 0.7195,
      "step": 1663
    },
    {
      "epoch": 0.007827125883138752,
      "grad_norm": 1.5685973167419434,
      "learning_rate": 0.00019890237347591164,
      "loss": 0.2284,
      "step": 1664
    },
    {
      "epoch": 0.007831829684751216,
      "grad_norm": 3.1936209201812744,
      "learning_rate": 0.00019890143049779813,
      "loss": 0.5974,
      "step": 1665
    },
    {
      "epoch": 0.00783653348636368,
      "grad_norm": 2.839496374130249,
      "learning_rate": 0.00019890048751968468,
      "loss": 0.5918,
      "step": 1666
    },
    {
      "epoch": 0.007841237287976142,
      "grad_norm": 3.6090574264526367,
      "learning_rate": 0.0001988995445415712,
      "loss": 0.438,
      "step": 1667
    },
    {
      "epoch": 0.007845941089588606,
      "grad_norm": 1.4341397285461426,
      "learning_rate": 0.00019889860156345772,
      "loss": 0.158,
      "step": 1668
    },
    {
      "epoch": 0.007850644891201068,
      "grad_norm": 1.189231514930725,
      "learning_rate": 0.00019889765858534424,
      "loss": 0.1063,
      "step": 1669
    },
    {
      "epoch": 0.007855348692813532,
      "grad_norm": 3.1547510623931885,
      "learning_rate": 0.00019889671560723075,
      "loss": 0.3148,
      "step": 1670
    },
    {
      "epoch": 0.007860052494425994,
      "grad_norm": 1.2049331665039062,
      "learning_rate": 0.0001988957726291173,
      "loss": 0.1304,
      "step": 1671
    },
    {
      "epoch": 0.007864756296038458,
      "grad_norm": 2.099280595779419,
      "learning_rate": 0.00019889482965100382,
      "loss": 0.2932,
      "step": 1672
    },
    {
      "epoch": 0.007869460097650922,
      "grad_norm": 0.7474635243415833,
      "learning_rate": 0.00019889388667289034,
      "loss": 0.1172,
      "step": 1673
    },
    {
      "epoch": 0.007874163899263384,
      "grad_norm": 0.9308487772941589,
      "learning_rate": 0.00019889294369477686,
      "loss": 0.1557,
      "step": 1674
    },
    {
      "epoch": 0.007878867700875848,
      "grad_norm": 2.078902244567871,
      "learning_rate": 0.00019889200071666337,
      "loss": 0.3609,
      "step": 1675
    },
    {
      "epoch": 0.00788357150248831,
      "grad_norm": 2.3032777309417725,
      "learning_rate": 0.0001988910577385499,
      "loss": 0.4114,
      "step": 1676
    },
    {
      "epoch": 0.007888275304100774,
      "grad_norm": 2.156663656234741,
      "learning_rate": 0.0001988901147604364,
      "loss": 0.185,
      "step": 1677
    },
    {
      "epoch": 0.007892979105713238,
      "grad_norm": 3.2542736530303955,
      "learning_rate": 0.00019888917178232293,
      "loss": 0.4556,
      "step": 1678
    },
    {
      "epoch": 0.0078976829073257,
      "grad_norm": 0.9339601993560791,
      "learning_rate": 0.00019888822880420945,
      "loss": 0.0925,
      "step": 1679
    },
    {
      "epoch": 0.007902386708938164,
      "grad_norm": 3.3332226276397705,
      "learning_rate": 0.000198887285826096,
      "loss": 0.2175,
      "step": 1680
    },
    {
      "epoch": 0.007907090510550626,
      "grad_norm": 2.0969018936157227,
      "learning_rate": 0.00019888634284798251,
      "loss": 0.2834,
      "step": 1681
    },
    {
      "epoch": 0.00791179431216309,
      "grad_norm": 0.4259168803691864,
      "learning_rate": 0.00019888539986986903,
      "loss": 0.0431,
      "step": 1682
    },
    {
      "epoch": 0.007916498113775554,
      "grad_norm": 1.8309026956558228,
      "learning_rate": 0.00019888445689175555,
      "loss": 0.1828,
      "step": 1683
    },
    {
      "epoch": 0.007921201915388016,
      "grad_norm": 2.0512068271636963,
      "learning_rate": 0.0001988835139136421,
      "loss": 0.1809,
      "step": 1684
    },
    {
      "epoch": 0.00792590571700048,
      "grad_norm": 1.7498528957366943,
      "learning_rate": 0.0001988825709355286,
      "loss": 0.267,
      "step": 1685
    },
    {
      "epoch": 0.007930609518612942,
      "grad_norm": 0.5268799066543579,
      "learning_rate": 0.0001988816279574151,
      "loss": 0.0266,
      "step": 1686
    },
    {
      "epoch": 0.007935313320225406,
      "grad_norm": 7.294299602508545,
      "learning_rate": 0.00019888068497930163,
      "loss": 0.7067,
      "step": 1687
    },
    {
      "epoch": 0.007940017121837869,
      "grad_norm": 0.8969483971595764,
      "learning_rate": 0.00019887974200118814,
      "loss": 0.0637,
      "step": 1688
    },
    {
      "epoch": 0.007944720923450332,
      "grad_norm": 0.22193239629268646,
      "learning_rate": 0.0001988787990230747,
      "loss": 0.0109,
      "step": 1689
    },
    {
      "epoch": 0.007949424725062796,
      "grad_norm": 0.2120736539363861,
      "learning_rate": 0.0001988778560449612,
      "loss": 0.0086,
      "step": 1690
    },
    {
      "epoch": 0.007954128526675259,
      "grad_norm": 1.6925476789474487,
      "learning_rate": 0.00019887691306684773,
      "loss": 0.1156,
      "step": 1691
    },
    {
      "epoch": 0.007958832328287722,
      "grad_norm": 4.162353515625,
      "learning_rate": 0.00019887597008873425,
      "loss": 0.3222,
      "step": 1692
    },
    {
      "epoch": 0.007963536129900185,
      "grad_norm": 3.637390375137329,
      "learning_rate": 0.0001988750271106208,
      "loss": 0.6465,
      "step": 1693
    },
    {
      "epoch": 0.007968239931512648,
      "grad_norm": 2.20692777633667,
      "learning_rate": 0.0001988740841325073,
      "loss": 0.1445,
      "step": 1694
    },
    {
      "epoch": 0.007972943733125112,
      "grad_norm": 0.4566425681114197,
      "learning_rate": 0.00019887314115439383,
      "loss": 0.0142,
      "step": 1695
    },
    {
      "epoch": 0.007977647534737575,
      "grad_norm": 1.7467913627624512,
      "learning_rate": 0.00019887219817628032,
      "loss": 0.0957,
      "step": 1696
    },
    {
      "epoch": 0.007982351336350038,
      "grad_norm": 4.879810333251953,
      "learning_rate": 0.00019887125519816684,
      "loss": 0.1989,
      "step": 1697
    },
    {
      "epoch": 0.0079870551379625,
      "grad_norm": 12.817580223083496,
      "learning_rate": 0.00019887031222005338,
      "loss": 1.617,
      "step": 1698
    },
    {
      "epoch": 0.007991758939574965,
      "grad_norm": 7.328233242034912,
      "learning_rate": 0.0001988693692419399,
      "loss": 1.2878,
      "step": 1699
    },
    {
      "epoch": 0.007996462741187428,
      "grad_norm": 6.230271816253662,
      "learning_rate": 0.00019886842626382642,
      "loss": 0.2992,
      "step": 1700
    },
    {
      "epoch": 0.00800116654279989,
      "grad_norm": 4.32506799697876,
      "learning_rate": 0.00019886748328571294,
      "loss": 0.4707,
      "step": 1701
    },
    {
      "epoch": 0.008005870344412355,
      "grad_norm": 1.0705132484436035,
      "learning_rate": 0.0001988665403075995,
      "loss": 0.037,
      "step": 1702
    },
    {
      "epoch": 0.008010574146024817,
      "grad_norm": 1.6260892152786255,
      "learning_rate": 0.000198865597329486,
      "loss": 0.0728,
      "step": 1703
    },
    {
      "epoch": 0.00801527794763728,
      "grad_norm": 3.5518906116485596,
      "learning_rate": 0.00019886465435137252,
      "loss": 0.346,
      "step": 1704
    },
    {
      "epoch": 0.008019981749249743,
      "grad_norm": 13.115615844726562,
      "learning_rate": 0.00019886371137325904,
      "loss": 0.7335,
      "step": 1705
    },
    {
      "epoch": 0.008024685550862207,
      "grad_norm": 1.825149416923523,
      "learning_rate": 0.00019886276839514556,
      "loss": 0.0967,
      "step": 1706
    },
    {
      "epoch": 0.00802938935247467,
      "grad_norm": 3.1004045009613037,
      "learning_rate": 0.00019886182541703208,
      "loss": 1.0509,
      "step": 1707
    },
    {
      "epoch": 0.008034093154087133,
      "grad_norm": 3.6263041496276855,
      "learning_rate": 0.0001988608824389186,
      "loss": 0.5054,
      "step": 1708
    },
    {
      "epoch": 0.008038796955699597,
      "grad_norm": 2.4702374935150146,
      "learning_rate": 0.00019885993946080512,
      "loss": 0.3182,
      "step": 1709
    },
    {
      "epoch": 0.008043500757312059,
      "grad_norm": 1.8121405839920044,
      "learning_rate": 0.00019885899648269164,
      "loss": 0.1231,
      "step": 1710
    },
    {
      "epoch": 0.008048204558924523,
      "grad_norm": 0.5865593552589417,
      "learning_rate": 0.00019885805350457818,
      "loss": 0.0173,
      "step": 1711
    },
    {
      "epoch": 0.008052908360536987,
      "grad_norm": 4.211760520935059,
      "learning_rate": 0.0001988571105264647,
      "loss": 0.3743,
      "step": 1712
    },
    {
      "epoch": 0.008057612162149449,
      "grad_norm": 2.89089298248291,
      "learning_rate": 0.00019885616754835122,
      "loss": 0.1178,
      "step": 1713
    },
    {
      "epoch": 0.008062315963761913,
      "grad_norm": 2.590308904647827,
      "learning_rate": 0.00019885522457023774,
      "loss": 0.193,
      "step": 1714
    },
    {
      "epoch": 0.008067019765374375,
      "grad_norm": 3.5527424812316895,
      "learning_rate": 0.00019885428159212426,
      "loss": 0.4944,
      "step": 1715
    },
    {
      "epoch": 0.008071723566986839,
      "grad_norm": 5.170905113220215,
      "learning_rate": 0.00019885333861401077,
      "loss": 0.7539,
      "step": 1716
    },
    {
      "epoch": 0.008076427368599303,
      "grad_norm": 1.4687334299087524,
      "learning_rate": 0.0001988523956358973,
      "loss": 0.2122,
      "step": 1717
    },
    {
      "epoch": 0.008081131170211765,
      "grad_norm": 1.5147504806518555,
      "learning_rate": 0.0001988514526577838,
      "loss": 0.2141,
      "step": 1718
    },
    {
      "epoch": 0.008085834971824229,
      "grad_norm": 3.060941219329834,
      "learning_rate": 0.00019885050967967033,
      "loss": 0.3661,
      "step": 1719
    },
    {
      "epoch": 0.008090538773436691,
      "grad_norm": 5.323693752288818,
      "learning_rate": 0.00019884956670155685,
      "loss": 0.9233,
      "step": 1720
    },
    {
      "epoch": 0.008095242575049155,
      "grad_norm": 1.7966225147247314,
      "learning_rate": 0.0001988486237234434,
      "loss": 0.2104,
      "step": 1721
    },
    {
      "epoch": 0.008099946376661617,
      "grad_norm": 6.095632553100586,
      "learning_rate": 0.00019884768074532991,
      "loss": 0.5713,
      "step": 1722
    },
    {
      "epoch": 0.008104650178274081,
      "grad_norm": 1.972665786743164,
      "learning_rate": 0.00019884673776721643,
      "loss": 0.2674,
      "step": 1723
    },
    {
      "epoch": 0.008109353979886545,
      "grad_norm": 5.693216323852539,
      "learning_rate": 0.00019884579478910295,
      "loss": 1.0816,
      "step": 1724
    },
    {
      "epoch": 0.008114057781499007,
      "grad_norm": 1.3859658241271973,
      "learning_rate": 0.0001988448518109895,
      "loss": 0.3275,
      "step": 1725
    },
    {
      "epoch": 0.008118761583111471,
      "grad_norm": 1.2262957096099854,
      "learning_rate": 0.00019884390883287602,
      "loss": 0.0812,
      "step": 1726
    },
    {
      "epoch": 0.008123465384723933,
      "grad_norm": 1.9073294401168823,
      "learning_rate": 0.0001988429658547625,
      "loss": 0.1959,
      "step": 1727
    },
    {
      "epoch": 0.008128169186336397,
      "grad_norm": 1.0299091339111328,
      "learning_rate": 0.00019884202287664903,
      "loss": 0.1322,
      "step": 1728
    },
    {
      "epoch": 0.008132872987948861,
      "grad_norm": 0.5334911346435547,
      "learning_rate": 0.00019884107989853554,
      "loss": 0.0453,
      "step": 1729
    },
    {
      "epoch": 0.008137576789561323,
      "grad_norm": 0.6485097408294678,
      "learning_rate": 0.0001988401369204221,
      "loss": 0.0407,
      "step": 1730
    },
    {
      "epoch": 0.008142280591173787,
      "grad_norm": 2.0376908779144287,
      "learning_rate": 0.0001988391939423086,
      "loss": 0.1498,
      "step": 1731
    },
    {
      "epoch": 0.00814698439278625,
      "grad_norm": 1.7103208303451538,
      "learning_rate": 0.00019883825096419513,
      "loss": 0.1887,
      "step": 1732
    },
    {
      "epoch": 0.008151688194398713,
      "grad_norm": 2.309669256210327,
      "learning_rate": 0.00019883730798608165,
      "loss": 0.4539,
      "step": 1733
    },
    {
      "epoch": 0.008156391996011177,
      "grad_norm": 4.552093982696533,
      "learning_rate": 0.0001988363650079682,
      "loss": 0.832,
      "step": 1734
    },
    {
      "epoch": 0.00816109579762364,
      "grad_norm": 0.6083732843399048,
      "learning_rate": 0.0001988354220298547,
      "loss": 0.0761,
      "step": 1735
    },
    {
      "epoch": 0.008165799599236103,
      "grad_norm": 1.013610601425171,
      "learning_rate": 0.00019883447905174123,
      "loss": 0.0595,
      "step": 1736
    },
    {
      "epoch": 0.008170503400848565,
      "grad_norm": 0.6131076812744141,
      "learning_rate": 0.00019883353607362775,
      "loss": 0.0551,
      "step": 1737
    },
    {
      "epoch": 0.00817520720246103,
      "grad_norm": 1.440894603729248,
      "learning_rate": 0.00019883259309551424,
      "loss": 0.1237,
      "step": 1738
    },
    {
      "epoch": 0.008179911004073493,
      "grad_norm": 0.917576014995575,
      "learning_rate": 0.00019883165011740078,
      "loss": 0.0561,
      "step": 1739
    },
    {
      "epoch": 0.008184614805685955,
      "grad_norm": 2.5494139194488525,
      "learning_rate": 0.0001988307071392873,
      "loss": 0.4899,
      "step": 1740
    },
    {
      "epoch": 0.00818931860729842,
      "grad_norm": 0.3420131802558899,
      "learning_rate": 0.00019882976416117382,
      "loss": 0.0309,
      "step": 1741
    },
    {
      "epoch": 0.008194022408910881,
      "grad_norm": 3.015580177307129,
      "learning_rate": 0.00019882882118306034,
      "loss": 0.2784,
      "step": 1742
    },
    {
      "epoch": 0.008198726210523345,
      "grad_norm": 5.480629920959473,
      "learning_rate": 0.0001988278782049469,
      "loss": 0.7884,
      "step": 1743
    },
    {
      "epoch": 0.008203430012135807,
      "grad_norm": 1.2253788709640503,
      "learning_rate": 0.0001988269352268334,
      "loss": 0.0686,
      "step": 1744
    },
    {
      "epoch": 0.008208133813748271,
      "grad_norm": 5.944839000701904,
      "learning_rate": 0.00019882599224871992,
      "loss": 0.9709,
      "step": 1745
    },
    {
      "epoch": 0.008212837615360735,
      "grad_norm": 2.6527349948883057,
      "learning_rate": 0.00019882504927060644,
      "loss": 0.6035,
      "step": 1746
    },
    {
      "epoch": 0.008217541416973197,
      "grad_norm": 3.2975969314575195,
      "learning_rate": 0.00019882410629249296,
      "loss": 0.7424,
      "step": 1747
    },
    {
      "epoch": 0.008222245218585661,
      "grad_norm": 1.6520073413848877,
      "learning_rate": 0.00019882316331437948,
      "loss": 0.1186,
      "step": 1748
    },
    {
      "epoch": 0.008226949020198123,
      "grad_norm": 0.12562130391597748,
      "learning_rate": 0.000198822220336266,
      "loss": 0.0067,
      "step": 1749
    },
    {
      "epoch": 0.008231652821810587,
      "grad_norm": 4.259496212005615,
      "learning_rate": 0.00019882127735815252,
      "loss": 0.4526,
      "step": 1750
    },
    {
      "epoch": 0.008236356623423051,
      "grad_norm": 0.5274729132652283,
      "learning_rate": 0.00019882033438003904,
      "loss": 0.0558,
      "step": 1751
    },
    {
      "epoch": 0.008241060425035513,
      "grad_norm": 0.14568476378917694,
      "learning_rate": 0.00019881939140192558,
      "loss": 0.0081,
      "step": 1752
    },
    {
      "epoch": 0.008245764226647977,
      "grad_norm": 2.785494089126587,
      "learning_rate": 0.0001988184484238121,
      "loss": 0.3924,
      "step": 1753
    },
    {
      "epoch": 0.00825046802826044,
      "grad_norm": 2.2078826427459717,
      "learning_rate": 0.00019881750544569862,
      "loss": 0.3062,
      "step": 1754
    },
    {
      "epoch": 0.008255171829872903,
      "grad_norm": 5.594730377197266,
      "learning_rate": 0.00019881656246758514,
      "loss": 0.4712,
      "step": 1755
    },
    {
      "epoch": 0.008259875631485367,
      "grad_norm": 3.4765875339508057,
      "learning_rate": 0.00019881561948947166,
      "loss": 0.5057,
      "step": 1756
    },
    {
      "epoch": 0.00826457943309783,
      "grad_norm": 2.902120590209961,
      "learning_rate": 0.0001988146765113582,
      "loss": 0.125,
      "step": 1757
    },
    {
      "epoch": 0.008269283234710293,
      "grad_norm": 1.5012187957763672,
      "learning_rate": 0.0001988137335332447,
      "loss": 0.1104,
      "step": 1758
    },
    {
      "epoch": 0.008273987036322756,
      "grad_norm": 2.502786874771118,
      "learning_rate": 0.0001988127905551312,
      "loss": 0.3622,
      "step": 1759
    },
    {
      "epoch": 0.00827869083793522,
      "grad_norm": 2.233971118927002,
      "learning_rate": 0.00019881184757701773,
      "loss": 0.3395,
      "step": 1760
    },
    {
      "epoch": 0.008283394639547682,
      "grad_norm": 1.357305645942688,
      "learning_rate": 0.00019881090459890428,
      "loss": 0.1566,
      "step": 1761
    },
    {
      "epoch": 0.008288098441160146,
      "grad_norm": 2.2919273376464844,
      "learning_rate": 0.0001988099616207908,
      "loss": 0.2597,
      "step": 1762
    },
    {
      "epoch": 0.00829280224277261,
      "grad_norm": 1.3462700843811035,
      "learning_rate": 0.00019880901864267731,
      "loss": 0.0879,
      "step": 1763
    },
    {
      "epoch": 0.008297506044385072,
      "grad_norm": 3.0283260345458984,
      "learning_rate": 0.00019880807566456383,
      "loss": 0.4224,
      "step": 1764
    },
    {
      "epoch": 0.008302209845997536,
      "grad_norm": 3.2912442684173584,
      "learning_rate": 0.00019880713268645035,
      "loss": 0.2765,
      "step": 1765
    },
    {
      "epoch": 0.008306913647609998,
      "grad_norm": 2.70709490776062,
      "learning_rate": 0.0001988061897083369,
      "loss": 0.1863,
      "step": 1766
    },
    {
      "epoch": 0.008311617449222462,
      "grad_norm": 2.773153781890869,
      "learning_rate": 0.00019880524673022342,
      "loss": 0.2419,
      "step": 1767
    },
    {
      "epoch": 0.008316321250834926,
      "grad_norm": 3.441305637359619,
      "learning_rate": 0.00019880430375210993,
      "loss": 0.3819,
      "step": 1768
    },
    {
      "epoch": 0.008321025052447388,
      "grad_norm": 0.9709202647209167,
      "learning_rate": 0.00019880336077399643,
      "loss": 0.0947,
      "step": 1769
    },
    {
      "epoch": 0.008325728854059852,
      "grad_norm": 4.884029388427734,
      "learning_rate": 0.00019880241779588297,
      "loss": 0.5859,
      "step": 1770
    },
    {
      "epoch": 0.008330432655672314,
      "grad_norm": 1.8057152032852173,
      "learning_rate": 0.0001988014748177695,
      "loss": 0.0989,
      "step": 1771
    },
    {
      "epoch": 0.008335136457284778,
      "grad_norm": 2.7540230751037598,
      "learning_rate": 0.000198800531839656,
      "loss": 0.307,
      "step": 1772
    },
    {
      "epoch": 0.008339840258897242,
      "grad_norm": 0.6474680304527283,
      "learning_rate": 0.00019879958886154253,
      "loss": 0.0417,
      "step": 1773
    },
    {
      "epoch": 0.008344544060509704,
      "grad_norm": 2.6356563568115234,
      "learning_rate": 0.00019879864588342905,
      "loss": 0.2093,
      "step": 1774
    },
    {
      "epoch": 0.008349247862122168,
      "grad_norm": 7.9431562423706055,
      "learning_rate": 0.0001987977029053156,
      "loss": 1.4843,
      "step": 1775
    },
    {
      "epoch": 0.00835395166373463,
      "grad_norm": 5.493814468383789,
      "learning_rate": 0.0001987967599272021,
      "loss": 0.3712,
      "step": 1776
    },
    {
      "epoch": 0.008358655465347094,
      "grad_norm": 2.0980284214019775,
      "learning_rate": 0.00019879581694908863,
      "loss": 0.2392,
      "step": 1777
    },
    {
      "epoch": 0.008363359266959556,
      "grad_norm": 2.8935229778289795,
      "learning_rate": 0.00019879487397097515,
      "loss": 0.437,
      "step": 1778
    },
    {
      "epoch": 0.00836806306857202,
      "grad_norm": 0.3329949975013733,
      "learning_rate": 0.00019879393099286167,
      "loss": 0.0204,
      "step": 1779
    },
    {
      "epoch": 0.008372766870184484,
      "grad_norm": 2.8743011951446533,
      "learning_rate": 0.00019879298801474818,
      "loss": 0.4084,
      "step": 1780
    },
    {
      "epoch": 0.008377470671796946,
      "grad_norm": 2.479177236557007,
      "learning_rate": 0.0001987920450366347,
      "loss": 0.1539,
      "step": 1781
    },
    {
      "epoch": 0.00838217447340941,
      "grad_norm": 3.1404407024383545,
      "learning_rate": 0.00019879110205852122,
      "loss": 0.3276,
      "step": 1782
    },
    {
      "epoch": 0.008386878275021872,
      "grad_norm": 3.297215700149536,
      "learning_rate": 0.00019879015908040774,
      "loss": 0.5119,
      "step": 1783
    },
    {
      "epoch": 0.008391582076634336,
      "grad_norm": 2.5488293170928955,
      "learning_rate": 0.0001987892161022943,
      "loss": 0.2915,
      "step": 1784
    },
    {
      "epoch": 0.0083962858782468,
      "grad_norm": 3.910601854324341,
      "learning_rate": 0.0001987882731241808,
      "loss": 0.5845,
      "step": 1785
    },
    {
      "epoch": 0.008400989679859262,
      "grad_norm": 0.6222986578941345,
      "learning_rate": 0.00019878733014606732,
      "loss": 0.0512,
      "step": 1786
    },
    {
      "epoch": 0.008405693481471726,
      "grad_norm": 4.457002639770508,
      "learning_rate": 0.00019878638716795384,
      "loss": 0.8182,
      "step": 1787
    },
    {
      "epoch": 0.008410397283084188,
      "grad_norm": 2.4662699699401855,
      "learning_rate": 0.00019878544418984036,
      "loss": 0.2527,
      "step": 1788
    },
    {
      "epoch": 0.008415101084696652,
      "grad_norm": 1.4068259000778198,
      "learning_rate": 0.00019878450121172688,
      "loss": 0.1942,
      "step": 1789
    },
    {
      "epoch": 0.008419804886309116,
      "grad_norm": 1.9796483516693115,
      "learning_rate": 0.0001987835582336134,
      "loss": 0.186,
      "step": 1790
    },
    {
      "epoch": 0.008424508687921578,
      "grad_norm": 2.94681715965271,
      "learning_rate": 0.00019878261525549992,
      "loss": 0.4823,
      "step": 1791
    },
    {
      "epoch": 0.008429212489534042,
      "grad_norm": 1.4101861715316772,
      "learning_rate": 0.00019878167227738644,
      "loss": 0.4362,
      "step": 1792
    },
    {
      "epoch": 0.008433916291146504,
      "grad_norm": 4.285127639770508,
      "learning_rate": 0.00019878072929927298,
      "loss": 0.8415,
      "step": 1793
    },
    {
      "epoch": 0.008438620092758968,
      "grad_norm": 1.7421550750732422,
      "learning_rate": 0.0001987797863211595,
      "loss": 0.204,
      "step": 1794
    },
    {
      "epoch": 0.00844332389437143,
      "grad_norm": 2.1271445751190186,
      "learning_rate": 0.00019877884334304602,
      "loss": 0.2393,
      "step": 1795
    },
    {
      "epoch": 0.008448027695983894,
      "grad_norm": 1.2741549015045166,
      "learning_rate": 0.00019877790036493254,
      "loss": 0.2839,
      "step": 1796
    },
    {
      "epoch": 0.008452731497596358,
      "grad_norm": 2.3143069744110107,
      "learning_rate": 0.00019877695738681906,
      "loss": 0.1887,
      "step": 1797
    },
    {
      "epoch": 0.00845743529920882,
      "grad_norm": 0.31793898344039917,
      "learning_rate": 0.0001987760144087056,
      "loss": 0.023,
      "step": 1798
    },
    {
      "epoch": 0.008462139100821284,
      "grad_norm": 1.5100829601287842,
      "learning_rate": 0.00019877507143059212,
      "loss": 0.2382,
      "step": 1799
    },
    {
      "epoch": 0.008466842902433746,
      "grad_norm": 2.3873603343963623,
      "learning_rate": 0.0001987741284524786,
      "loss": 0.495,
      "step": 1800
    },
    {
      "epoch": 0.00847154670404621,
      "grad_norm": 3.8885533809661865,
      "learning_rate": 0.00019877318547436513,
      "loss": 0.6112,
      "step": 1801
    },
    {
      "epoch": 0.008476250505658674,
      "grad_norm": 1.6652178764343262,
      "learning_rate": 0.00019877224249625168,
      "loss": 0.2533,
      "step": 1802
    },
    {
      "epoch": 0.008480954307271136,
      "grad_norm": 1.679044246673584,
      "learning_rate": 0.0001987712995181382,
      "loss": 0.1536,
      "step": 1803
    },
    {
      "epoch": 0.0084856581088836,
      "grad_norm": 0.9853795170783997,
      "learning_rate": 0.00019877035654002471,
      "loss": 0.1173,
      "step": 1804
    },
    {
      "epoch": 0.008490361910496062,
      "grad_norm": 1.1754813194274902,
      "learning_rate": 0.00019876941356191123,
      "loss": 0.1441,
      "step": 1805
    },
    {
      "epoch": 0.008495065712108526,
      "grad_norm": 1.736059546470642,
      "learning_rate": 0.00019876847058379775,
      "loss": 0.1241,
      "step": 1806
    },
    {
      "epoch": 0.00849976951372099,
      "grad_norm": 3.7681376934051514,
      "learning_rate": 0.0001987675276056843,
      "loss": 0.5292,
      "step": 1807
    },
    {
      "epoch": 0.008504473315333452,
      "grad_norm": 3.8850905895233154,
      "learning_rate": 0.00019876658462757082,
      "loss": 0.2683,
      "step": 1808
    },
    {
      "epoch": 0.008509177116945916,
      "grad_norm": 2.7350704669952393,
      "learning_rate": 0.00019876564164945733,
      "loss": 0.366,
      "step": 1809
    },
    {
      "epoch": 0.008513880918558378,
      "grad_norm": 1.7076199054718018,
      "learning_rate": 0.00019876469867134385,
      "loss": 0.2156,
      "step": 1810
    },
    {
      "epoch": 0.008518584720170842,
      "grad_norm": 3.466487407684326,
      "learning_rate": 0.00019876375569323037,
      "loss": 0.4935,
      "step": 1811
    },
    {
      "epoch": 0.008523288521783304,
      "grad_norm": 0.4489178955554962,
      "learning_rate": 0.0001987628127151169,
      "loss": 0.0321,
      "step": 1812
    },
    {
      "epoch": 0.008527992323395768,
      "grad_norm": 4.997873306274414,
      "learning_rate": 0.0001987618697370034,
      "loss": 0.3385,
      "step": 1813
    },
    {
      "epoch": 0.008532696125008232,
      "grad_norm": 4.035560131072998,
      "learning_rate": 0.00019876092675888993,
      "loss": 1.2215,
      "step": 1814
    },
    {
      "epoch": 0.008537399926620694,
      "grad_norm": 4.69710111618042,
      "learning_rate": 0.00019875998378077645,
      "loss": 0.5332,
      "step": 1815
    },
    {
      "epoch": 0.008542103728233158,
      "grad_norm": 0.775684654712677,
      "learning_rate": 0.000198759040802663,
      "loss": 0.0664,
      "step": 1816
    },
    {
      "epoch": 0.00854680752984562,
      "grad_norm": 1.6455274820327759,
      "learning_rate": 0.0001987580978245495,
      "loss": 0.3061,
      "step": 1817
    },
    {
      "epoch": 0.008551511331458084,
      "grad_norm": 7.9589619636535645,
      "learning_rate": 0.00019875715484643603,
      "loss": 0.8355,
      "step": 1818
    },
    {
      "epoch": 0.008556215133070548,
      "grad_norm": 2.3311054706573486,
      "learning_rate": 0.00019875621186832255,
      "loss": 0.2529,
      "step": 1819
    },
    {
      "epoch": 0.00856091893468301,
      "grad_norm": 1.67664635181427,
      "learning_rate": 0.00019875526889020907,
      "loss": 0.1434,
      "step": 1820
    },
    {
      "epoch": 0.008565622736295474,
      "grad_norm": 0.9768384099006653,
      "learning_rate": 0.00019875432591209558,
      "loss": 0.07,
      "step": 1821
    },
    {
      "epoch": 0.008570326537907937,
      "grad_norm": 3.484919548034668,
      "learning_rate": 0.0001987533829339821,
      "loss": 0.4944,
      "step": 1822
    },
    {
      "epoch": 0.0085750303395204,
      "grad_norm": 2.474827527999878,
      "learning_rate": 0.00019875243995586862,
      "loss": 0.2585,
      "step": 1823
    },
    {
      "epoch": 0.008579734141132864,
      "grad_norm": 2.772810935974121,
      "learning_rate": 0.00019875149697775514,
      "loss": 0.2563,
      "step": 1824
    },
    {
      "epoch": 0.008584437942745327,
      "grad_norm": 1.2734748125076294,
      "learning_rate": 0.00019875055399964169,
      "loss": 0.0641,
      "step": 1825
    },
    {
      "epoch": 0.00858914174435779,
      "grad_norm": 4.447396278381348,
      "learning_rate": 0.0001987496110215282,
      "loss": 0.9493,
      "step": 1826
    },
    {
      "epoch": 0.008593845545970253,
      "grad_norm": 2.606912612915039,
      "learning_rate": 0.00019874866804341472,
      "loss": 0.2263,
      "step": 1827
    },
    {
      "epoch": 0.008598549347582717,
      "grad_norm": 2.780980110168457,
      "learning_rate": 0.00019874772506530124,
      "loss": 0.3228,
      "step": 1828
    },
    {
      "epoch": 0.008603253149195179,
      "grad_norm": 1.1897577047348022,
      "learning_rate": 0.00019874678208718776,
      "loss": 0.1741,
      "step": 1829
    },
    {
      "epoch": 0.008607956950807643,
      "grad_norm": 0.9192363619804382,
      "learning_rate": 0.0001987458391090743,
      "loss": 0.1184,
      "step": 1830
    },
    {
      "epoch": 0.008612660752420107,
      "grad_norm": 3.7028427124023438,
      "learning_rate": 0.0001987448961309608,
      "loss": 0.4026,
      "step": 1831
    },
    {
      "epoch": 0.008617364554032569,
      "grad_norm": 2.1218273639678955,
      "learning_rate": 0.00019874395315284732,
      "loss": 0.3264,
      "step": 1832
    },
    {
      "epoch": 0.008622068355645033,
      "grad_norm": 3.2539641857147217,
      "learning_rate": 0.00019874301017473384,
      "loss": 0.2958,
      "step": 1833
    },
    {
      "epoch": 0.008626772157257495,
      "grad_norm": 1.8512144088745117,
      "learning_rate": 0.00019874206719662038,
      "loss": 0.3518,
      "step": 1834
    },
    {
      "epoch": 0.008631475958869959,
      "grad_norm": 0.8585800528526306,
      "learning_rate": 0.0001987411242185069,
      "loss": 0.0609,
      "step": 1835
    },
    {
      "epoch": 0.008636179760482423,
      "grad_norm": 1.6091874837875366,
      "learning_rate": 0.00019874018124039342,
      "loss": 0.1885,
      "step": 1836
    },
    {
      "epoch": 0.008640883562094885,
      "grad_norm": 0.730455756187439,
      "learning_rate": 0.00019873923826227994,
      "loss": 0.07,
      "step": 1837
    },
    {
      "epoch": 0.008645587363707349,
      "grad_norm": 3.3033523559570312,
      "learning_rate": 0.00019873829528416646,
      "loss": 0.4987,
      "step": 1838
    },
    {
      "epoch": 0.00865029116531981,
      "grad_norm": 0.35030558705329895,
      "learning_rate": 0.000198737352306053,
      "loss": 0.0334,
      "step": 1839
    },
    {
      "epoch": 0.008654994966932275,
      "grad_norm": 0.8767347931861877,
      "learning_rate": 0.00019873640932793952,
      "loss": 0.0836,
      "step": 1840
    },
    {
      "epoch": 0.008659698768544739,
      "grad_norm": 5.04138708114624,
      "learning_rate": 0.00019873546634982604,
      "loss": 0.3467,
      "step": 1841
    },
    {
      "epoch": 0.0086644025701572,
      "grad_norm": 4.5780839920043945,
      "learning_rate": 0.00019873452337171256,
      "loss": 0.8299,
      "step": 1842
    },
    {
      "epoch": 0.008669106371769665,
      "grad_norm": 2.144926071166992,
      "learning_rate": 0.00019873358039359908,
      "loss": 0.1878,
      "step": 1843
    },
    {
      "epoch": 0.008673810173382127,
      "grad_norm": 0.630431056022644,
      "learning_rate": 0.0001987326374154856,
      "loss": 0.0384,
      "step": 1844
    },
    {
      "epoch": 0.00867851397499459,
      "grad_norm": 0.48944827914237976,
      "learning_rate": 0.0001987316944373721,
      "loss": 0.066,
      "step": 1845
    },
    {
      "epoch": 0.008683217776607053,
      "grad_norm": 2.3297955989837646,
      "learning_rate": 0.00019873075145925863,
      "loss": 0.4283,
      "step": 1846
    },
    {
      "epoch": 0.008687921578219517,
      "grad_norm": 3.566269636154175,
      "learning_rate": 0.00019872980848114515,
      "loss": 0.8877,
      "step": 1847
    },
    {
      "epoch": 0.00869262537983198,
      "grad_norm": 1.0753118991851807,
      "learning_rate": 0.0001987288655030317,
      "loss": 0.0821,
      "step": 1848
    },
    {
      "epoch": 0.008697329181444443,
      "grad_norm": 0.21582050621509552,
      "learning_rate": 0.00019872792252491822,
      "loss": 0.0123,
      "step": 1849
    },
    {
      "epoch": 0.008702032983056907,
      "grad_norm": 2.5685794353485107,
      "learning_rate": 0.00019872697954680473,
      "loss": 0.3016,
      "step": 1850
    },
    {
      "epoch": 0.008706736784669369,
      "grad_norm": 1.4145278930664062,
      "learning_rate": 0.00019872603656869125,
      "loss": 0.0949,
      "step": 1851
    },
    {
      "epoch": 0.008711440586281833,
      "grad_norm": 4.329471588134766,
      "learning_rate": 0.00019872509359057777,
      "loss": 0.39,
      "step": 1852
    },
    {
      "epoch": 0.008716144387894297,
      "grad_norm": 0.9755269885063171,
      "learning_rate": 0.0001987241506124643,
      "loss": 0.0655,
      "step": 1853
    },
    {
      "epoch": 0.008720848189506759,
      "grad_norm": 2.1531081199645996,
      "learning_rate": 0.0001987232076343508,
      "loss": 0.2572,
      "step": 1854
    },
    {
      "epoch": 0.008725551991119223,
      "grad_norm": 4.298008441925049,
      "learning_rate": 0.00019872226465623733,
      "loss": 0.3759,
      "step": 1855
    },
    {
      "epoch": 0.008730255792731685,
      "grad_norm": 2.6593596935272217,
      "learning_rate": 0.00019872132167812385,
      "loss": 0.3401,
      "step": 1856
    },
    {
      "epoch": 0.008734959594344149,
      "grad_norm": 5.736881732940674,
      "learning_rate": 0.0001987203787000104,
      "loss": 0.3256,
      "step": 1857
    },
    {
      "epoch": 0.008739663395956613,
      "grad_norm": 1.3134866952896118,
      "learning_rate": 0.0001987194357218969,
      "loss": 0.088,
      "step": 1858
    },
    {
      "epoch": 0.008744367197569075,
      "grad_norm": 3.1335909366607666,
      "learning_rate": 0.00019871849274378343,
      "loss": 0.1542,
      "step": 1859
    },
    {
      "epoch": 0.008749070999181539,
      "grad_norm": 2.9443211555480957,
      "learning_rate": 0.00019871754976566995,
      "loss": 0.2092,
      "step": 1860
    },
    {
      "epoch": 0.008753774800794001,
      "grad_norm": 1.8636113405227661,
      "learning_rate": 0.0001987166067875565,
      "loss": 0.1265,
      "step": 1861
    },
    {
      "epoch": 0.008758478602406465,
      "grad_norm": 0.3278704583644867,
      "learning_rate": 0.00019871566380944298,
      "loss": 0.0223,
      "step": 1862
    },
    {
      "epoch": 0.008763182404018927,
      "grad_norm": 1.1759040355682373,
      "learning_rate": 0.0001987147208313295,
      "loss": 0.0776,
      "step": 1863
    },
    {
      "epoch": 0.008767886205631391,
      "grad_norm": 4.13474702835083,
      "learning_rate": 0.00019871377785321602,
      "loss": 0.3523,
      "step": 1864
    },
    {
      "epoch": 0.008772590007243855,
      "grad_norm": 4.96852445602417,
      "learning_rate": 0.00019871283487510254,
      "loss": 0.7035,
      "step": 1865
    },
    {
      "epoch": 0.008777293808856317,
      "grad_norm": 0.8130399584770203,
      "learning_rate": 0.00019871189189698909,
      "loss": 0.0671,
      "step": 1866
    },
    {
      "epoch": 0.008781997610468781,
      "grad_norm": 6.868462562561035,
      "learning_rate": 0.0001987109489188756,
      "loss": 0.9575,
      "step": 1867
    },
    {
      "epoch": 0.008786701412081243,
      "grad_norm": 1.415086030960083,
      "learning_rate": 0.00019871000594076212,
      "loss": 0.1221,
      "step": 1868
    },
    {
      "epoch": 0.008791405213693707,
      "grad_norm": 3.7330708503723145,
      "learning_rate": 0.00019870906296264864,
      "loss": 0.5531,
      "step": 1869
    },
    {
      "epoch": 0.008796109015306171,
      "grad_norm": 4.896450042724609,
      "learning_rate": 0.0001987081199845352,
      "loss": 0.9149,
      "step": 1870
    },
    {
      "epoch": 0.008800812816918633,
      "grad_norm": 2.4116430282592773,
      "learning_rate": 0.0001987071770064217,
      "loss": 0.2323,
      "step": 1871
    },
    {
      "epoch": 0.008805516618531097,
      "grad_norm": 1.5911169052124023,
      "learning_rate": 0.00019870623402830823,
      "loss": 0.1025,
      "step": 1872
    },
    {
      "epoch": 0.00881022042014356,
      "grad_norm": 0.5551995038986206,
      "learning_rate": 0.00019870529105019474,
      "loss": 0.0313,
      "step": 1873
    },
    {
      "epoch": 0.008814924221756023,
      "grad_norm": 3.013516902923584,
      "learning_rate": 0.00019870434807208124,
      "loss": 0.1625,
      "step": 1874
    },
    {
      "epoch": 0.008819628023368487,
      "grad_norm": 1.2565016746520996,
      "learning_rate": 0.00019870340509396778,
      "loss": 0.0663,
      "step": 1875
    },
    {
      "epoch": 0.00882433182498095,
      "grad_norm": 1.408740520477295,
      "learning_rate": 0.0001987024621158543,
      "loss": 0.1114,
      "step": 1876
    },
    {
      "epoch": 0.008829035626593413,
      "grad_norm": 0.7909154295921326,
      "learning_rate": 0.00019870151913774082,
      "loss": 0.047,
      "step": 1877
    },
    {
      "epoch": 0.008833739428205875,
      "grad_norm": 0.10233913362026215,
      "learning_rate": 0.00019870057615962734,
      "loss": 0.0048,
      "step": 1878
    },
    {
      "epoch": 0.00883844322981834,
      "grad_norm": 1.3106118440628052,
      "learning_rate": 0.00019869963318151386,
      "loss": 0.0676,
      "step": 1879
    },
    {
      "epoch": 0.008843147031430802,
      "grad_norm": 3.2693209648132324,
      "learning_rate": 0.0001986986902034004,
      "loss": 0.6303,
      "step": 1880
    },
    {
      "epoch": 0.008847850833043265,
      "grad_norm": 2.4600324630737305,
      "learning_rate": 0.00019869774722528692,
      "loss": 0.2207,
      "step": 1881
    },
    {
      "epoch": 0.00885255463465573,
      "grad_norm": 2.638523817062378,
      "learning_rate": 0.00019869680424717344,
      "loss": 0.3915,
      "step": 1882
    },
    {
      "epoch": 0.008857258436268192,
      "grad_norm": 0.927291214466095,
      "learning_rate": 0.00019869586126905996,
      "loss": 0.0683,
      "step": 1883
    },
    {
      "epoch": 0.008861962237880655,
      "grad_norm": 4.647659778594971,
      "learning_rate": 0.00019869491829094648,
      "loss": 0.2251,
      "step": 1884
    },
    {
      "epoch": 0.008866666039493118,
      "grad_norm": 1.4481110572814941,
      "learning_rate": 0.000198693975312833,
      "loss": 0.081,
      "step": 1885
    },
    {
      "epoch": 0.008871369841105581,
      "grad_norm": 2.510619640350342,
      "learning_rate": 0.0001986930323347195,
      "loss": 0.1654,
      "step": 1886
    },
    {
      "epoch": 0.008876073642718045,
      "grad_norm": 0.910617470741272,
      "learning_rate": 0.00019869208935660603,
      "loss": 0.0357,
      "step": 1887
    },
    {
      "epoch": 0.008880777444330508,
      "grad_norm": 2.848397731781006,
      "learning_rate": 0.00019869114637849255,
      "loss": 0.4632,
      "step": 1888
    },
    {
      "epoch": 0.008885481245942971,
      "grad_norm": 1.0333857536315918,
      "learning_rate": 0.0001986902034003791,
      "loss": 0.07,
      "step": 1889
    },
    {
      "epoch": 0.008890185047555434,
      "grad_norm": 3.8487038612365723,
      "learning_rate": 0.00019868926042226562,
      "loss": 0.6178,
      "step": 1890
    },
    {
      "epoch": 0.008894888849167898,
      "grad_norm": 3.6419758796691895,
      "learning_rate": 0.00019868831744415213,
      "loss": 0.2936,
      "step": 1891
    },
    {
      "epoch": 0.008899592650780361,
      "grad_norm": 0.32086536288261414,
      "learning_rate": 0.00019868737446603865,
      "loss": 0.0172,
      "step": 1892
    },
    {
      "epoch": 0.008904296452392824,
      "grad_norm": 3.533799409866333,
      "learning_rate": 0.00019868643148792517,
      "loss": 0.53,
      "step": 1893
    },
    {
      "epoch": 0.008909000254005288,
      "grad_norm": 4.078021049499512,
      "learning_rate": 0.0001986854885098117,
      "loss": 0.4695,
      "step": 1894
    },
    {
      "epoch": 0.00891370405561775,
      "grad_norm": 1.6964675188064575,
      "learning_rate": 0.0001986845455316982,
      "loss": 0.089,
      "step": 1895
    },
    {
      "epoch": 0.008918407857230214,
      "grad_norm": 2.3005881309509277,
      "learning_rate": 0.00019868360255358473,
      "loss": 0.1033,
      "step": 1896
    },
    {
      "epoch": 0.008923111658842676,
      "grad_norm": 2.7952468395233154,
      "learning_rate": 0.00019868265957547125,
      "loss": 0.5413,
      "step": 1897
    },
    {
      "epoch": 0.00892781546045514,
      "grad_norm": 1.4163062572479248,
      "learning_rate": 0.0001986817165973578,
      "loss": 0.1479,
      "step": 1898
    },
    {
      "epoch": 0.008932519262067604,
      "grad_norm": 2.068108320236206,
      "learning_rate": 0.0001986807736192443,
      "loss": 0.1478,
      "step": 1899
    },
    {
      "epoch": 0.008937223063680066,
      "grad_norm": 1.4307875633239746,
      "learning_rate": 0.00019867983064113083,
      "loss": 0.2771,
      "step": 1900
    },
    {
      "epoch": 0.00894192686529253,
      "grad_norm": 9.141895294189453,
      "learning_rate": 0.00019867888766301735,
      "loss": 1.3442,
      "step": 1901
    },
    {
      "epoch": 0.008946630666904992,
      "grad_norm": 6.174325942993164,
      "learning_rate": 0.0001986779446849039,
      "loss": 0.8181,
      "step": 1902
    },
    {
      "epoch": 0.008951334468517456,
      "grad_norm": 4.6447954177856445,
      "learning_rate": 0.0001986770017067904,
      "loss": 0.7486,
      "step": 1903
    },
    {
      "epoch": 0.00895603827012992,
      "grad_norm": 4.01623010635376,
      "learning_rate": 0.00019867605872867693,
      "loss": 0.6122,
      "step": 1904
    },
    {
      "epoch": 0.008960742071742382,
      "grad_norm": 2.759044647216797,
      "learning_rate": 0.00019867511575056342,
      "loss": 0.2394,
      "step": 1905
    },
    {
      "epoch": 0.008965445873354846,
      "grad_norm": 3.5522241592407227,
      "learning_rate": 0.00019867417277244994,
      "loss": 0.2663,
      "step": 1906
    },
    {
      "epoch": 0.008970149674967308,
      "grad_norm": 4.6010637283325195,
      "learning_rate": 0.00019867322979433649,
      "loss": 0.4952,
      "step": 1907
    },
    {
      "epoch": 0.008974853476579772,
      "grad_norm": 2.460287094116211,
      "learning_rate": 0.000198672286816223,
      "loss": 0.3118,
      "step": 1908
    },
    {
      "epoch": 0.008979557278192236,
      "grad_norm": 1.108132004737854,
      "learning_rate": 0.00019867134383810952,
      "loss": 0.0851,
      "step": 1909
    },
    {
      "epoch": 0.008984261079804698,
      "grad_norm": 2.7144267559051514,
      "learning_rate": 0.00019867040085999604,
      "loss": 0.2913,
      "step": 1910
    },
    {
      "epoch": 0.008988964881417162,
      "grad_norm": 2.1003799438476562,
      "learning_rate": 0.0001986694578818826,
      "loss": 0.192,
      "step": 1911
    },
    {
      "epoch": 0.008993668683029624,
      "grad_norm": 0.6295852065086365,
      "learning_rate": 0.0001986685149037691,
      "loss": 0.0922,
      "step": 1912
    },
    {
      "epoch": 0.008998372484642088,
      "grad_norm": 2.01629376411438,
      "learning_rate": 0.00019866757192565563,
      "loss": 0.258,
      "step": 1913
    },
    {
      "epoch": 0.009003076286254552,
      "grad_norm": 1.797904133796692,
      "learning_rate": 0.00019866662894754214,
      "loss": 0.1929,
      "step": 1914
    },
    {
      "epoch": 0.009007780087867014,
      "grad_norm": 1.5813263654708862,
      "learning_rate": 0.00019866568596942866,
      "loss": 0.1608,
      "step": 1915
    },
    {
      "epoch": 0.009012483889479478,
      "grad_norm": 1.0932843685150146,
      "learning_rate": 0.00019866474299131518,
      "loss": 0.0895,
      "step": 1916
    },
    {
      "epoch": 0.00901718769109194,
      "grad_norm": 1.4304441213607788,
      "learning_rate": 0.0001986638000132017,
      "loss": 0.1401,
      "step": 1917
    },
    {
      "epoch": 0.009021891492704404,
      "grad_norm": 3.078279495239258,
      "learning_rate": 0.00019866285703508822,
      "loss": 0.1686,
      "step": 1918
    },
    {
      "epoch": 0.009026595294316866,
      "grad_norm": 1.0045597553253174,
      "learning_rate": 0.00019866191405697474,
      "loss": 0.0959,
      "step": 1919
    },
    {
      "epoch": 0.00903129909592933,
      "grad_norm": 2.5375614166259766,
      "learning_rate": 0.00019866097107886128,
      "loss": 0.2722,
      "step": 1920
    },
    {
      "epoch": 0.009036002897541794,
      "grad_norm": 0.3027409315109253,
      "learning_rate": 0.0001986600281007478,
      "loss": 0.0226,
      "step": 1921
    },
    {
      "epoch": 0.009040706699154256,
      "grad_norm": 0.7669804096221924,
      "learning_rate": 0.00019865908512263432,
      "loss": 0.0699,
      "step": 1922
    },
    {
      "epoch": 0.00904541050076672,
      "grad_norm": 0.7776923775672913,
      "learning_rate": 0.00019865814214452084,
      "loss": 0.0708,
      "step": 1923
    },
    {
      "epoch": 0.009050114302379182,
      "grad_norm": 1.500665307044983,
      "learning_rate": 0.00019865719916640736,
      "loss": 0.1285,
      "step": 1924
    },
    {
      "epoch": 0.009054818103991646,
      "grad_norm": 1.372710108757019,
      "learning_rate": 0.00019865625618829388,
      "loss": 0.123,
      "step": 1925
    },
    {
      "epoch": 0.00905952190560411,
      "grad_norm": 5.636141777038574,
      "learning_rate": 0.0001986553132101804,
      "loss": 0.7461,
      "step": 1926
    },
    {
      "epoch": 0.009064225707216572,
      "grad_norm": 0.40516746044158936,
      "learning_rate": 0.0001986543702320669,
      "loss": 0.0452,
      "step": 1927
    },
    {
      "epoch": 0.009068929508829036,
      "grad_norm": 4.7608208656311035,
      "learning_rate": 0.00019865342725395343,
      "loss": 1.2364,
      "step": 1928
    },
    {
      "epoch": 0.009073633310441498,
      "grad_norm": 0.2147858738899231,
      "learning_rate": 0.00019865248427583995,
      "loss": 0.0178,
      "step": 1929
    },
    {
      "epoch": 0.009078337112053962,
      "grad_norm": 1.8435783386230469,
      "learning_rate": 0.0001986515412977265,
      "loss": 0.1151,
      "step": 1930
    },
    {
      "epoch": 0.009083040913666426,
      "grad_norm": 2.747436761856079,
      "learning_rate": 0.00019865059831961302,
      "loss": 0.1036,
      "step": 1931
    },
    {
      "epoch": 0.009087744715278888,
      "grad_norm": 3.1500349044799805,
      "learning_rate": 0.00019864965534149953,
      "loss": 0.5141,
      "step": 1932
    },
    {
      "epoch": 0.009092448516891352,
      "grad_norm": 4.507219314575195,
      "learning_rate": 0.00019864871236338605,
      "loss": 0.5812,
      "step": 1933
    },
    {
      "epoch": 0.009097152318503814,
      "grad_norm": 1.37589430809021,
      "learning_rate": 0.0001986477693852726,
      "loss": 0.0669,
      "step": 1934
    },
    {
      "epoch": 0.009101856120116278,
      "grad_norm": 4.7711262702941895,
      "learning_rate": 0.00019864682640715912,
      "loss": 0.2695,
      "step": 1935
    },
    {
      "epoch": 0.00910655992172874,
      "grad_norm": 0.3131839334964752,
      "learning_rate": 0.0001986458834290456,
      "loss": 0.0278,
      "step": 1936
    },
    {
      "epoch": 0.009111263723341204,
      "grad_norm": 0.09391012042760849,
      "learning_rate": 0.00019864494045093213,
      "loss": 0.005,
      "step": 1937
    },
    {
      "epoch": 0.009115967524953668,
      "grad_norm": 4.808467388153076,
      "learning_rate": 0.00019864399747281865,
      "loss": 0.5547,
      "step": 1938
    },
    {
      "epoch": 0.00912067132656613,
      "grad_norm": 4.312599182128906,
      "learning_rate": 0.0001986430544947052,
      "loss": 0.3662,
      "step": 1939
    },
    {
      "epoch": 0.009125375128178594,
      "grad_norm": 4.4823784828186035,
      "learning_rate": 0.0001986421115165917,
      "loss": 0.713,
      "step": 1940
    },
    {
      "epoch": 0.009130078929791056,
      "grad_norm": 1.8575862646102905,
      "learning_rate": 0.00019864116853847823,
      "loss": 0.112,
      "step": 1941
    },
    {
      "epoch": 0.00913478273140352,
      "grad_norm": 5.692812919616699,
      "learning_rate": 0.00019864022556036475,
      "loss": 0.411,
      "step": 1942
    },
    {
      "epoch": 0.009139486533015984,
      "grad_norm": 1.2337207794189453,
      "learning_rate": 0.0001986392825822513,
      "loss": 0.0745,
      "step": 1943
    },
    {
      "epoch": 0.009144190334628446,
      "grad_norm": 0.7123286128044128,
      "learning_rate": 0.0001986383396041378,
      "loss": 0.0443,
      "step": 1944
    },
    {
      "epoch": 0.00914889413624091,
      "grad_norm": 3.8500988483428955,
      "learning_rate": 0.00019863739662602433,
      "loss": 0.1951,
      "step": 1945
    },
    {
      "epoch": 0.009153597937853373,
      "grad_norm": 4.221866607666016,
      "learning_rate": 0.00019863645364791085,
      "loss": 0.8137,
      "step": 1946
    },
    {
      "epoch": 0.009158301739465836,
      "grad_norm": 0.6207980513572693,
      "learning_rate": 0.00019863551066979734,
      "loss": 0.0554,
      "step": 1947
    },
    {
      "epoch": 0.0091630055410783,
      "grad_norm": 0.9040563106536865,
      "learning_rate": 0.00019863456769168389,
      "loss": 0.0709,
      "step": 1948
    },
    {
      "epoch": 0.009167709342690763,
      "grad_norm": 3.285335063934326,
      "learning_rate": 0.0001986336247135704,
      "loss": 0.2879,
      "step": 1949
    },
    {
      "epoch": 0.009172413144303226,
      "grad_norm": 4.161096572875977,
      "learning_rate": 0.00019863268173545692,
      "loss": 0.863,
      "step": 1950
    },
    {
      "epoch": 0.009177116945915689,
      "grad_norm": 4.307877540588379,
      "learning_rate": 0.00019863173875734344,
      "loss": 0.2442,
      "step": 1951
    },
    {
      "epoch": 0.009181820747528152,
      "grad_norm": 0.5454793572425842,
      "learning_rate": 0.00019863079577923,
      "loss": 0.0282,
      "step": 1952
    },
    {
      "epoch": 0.009186524549140615,
      "grad_norm": 0.8692917823791504,
      "learning_rate": 0.0001986298528011165,
      "loss": 0.0396,
      "step": 1953
    },
    {
      "epoch": 0.009191228350753079,
      "grad_norm": 2.16548228263855,
      "learning_rate": 0.00019862890982300303,
      "loss": 0.1647,
      "step": 1954
    },
    {
      "epoch": 0.009195932152365542,
      "grad_norm": 3.2223758697509766,
      "learning_rate": 0.00019862796684488954,
      "loss": 0.2841,
      "step": 1955
    },
    {
      "epoch": 0.009200635953978005,
      "grad_norm": 0.48356181383132935,
      "learning_rate": 0.00019862702386677606,
      "loss": 0.0224,
      "step": 1956
    },
    {
      "epoch": 0.009205339755590469,
      "grad_norm": 5.698901653289795,
      "learning_rate": 0.00019862608088866258,
      "loss": 0.5199,
      "step": 1957
    },
    {
      "epoch": 0.00921004355720293,
      "grad_norm": 0.4518698453903198,
      "learning_rate": 0.0001986251379105491,
      "loss": 0.0418,
      "step": 1958
    },
    {
      "epoch": 0.009214747358815395,
      "grad_norm": 0.6569637060165405,
      "learning_rate": 0.00019862419493243562,
      "loss": 0.0538,
      "step": 1959
    },
    {
      "epoch": 0.009219451160427859,
      "grad_norm": 6.812992095947266,
      "learning_rate": 0.00019862325195432214,
      "loss": 0.6741,
      "step": 1960
    },
    {
      "epoch": 0.00922415496204032,
      "grad_norm": 1.859641194343567,
      "learning_rate": 0.00019862230897620868,
      "loss": 0.0861,
      "step": 1961
    },
    {
      "epoch": 0.009228858763652785,
      "grad_norm": 2.9031126499176025,
      "learning_rate": 0.0001986213659980952,
      "loss": 0.1397,
      "step": 1962
    },
    {
      "epoch": 0.009233562565265247,
      "grad_norm": 4.704668998718262,
      "learning_rate": 0.00019862042301998172,
      "loss": 0.4033,
      "step": 1963
    },
    {
      "epoch": 0.00923826636687771,
      "grad_norm": 1.761831521987915,
      "learning_rate": 0.00019861948004186824,
      "loss": 0.1139,
      "step": 1964
    },
    {
      "epoch": 0.009242970168490175,
      "grad_norm": 4.59731912612915,
      "learning_rate": 0.00019861853706375476,
      "loss": 0.6271,
      "step": 1965
    },
    {
      "epoch": 0.009247673970102637,
      "grad_norm": 2.828322172164917,
      "learning_rate": 0.0001986175940856413,
      "loss": 0.1961,
      "step": 1966
    },
    {
      "epoch": 0.0092523777717151,
      "grad_norm": 1.7503265142440796,
      "learning_rate": 0.0001986166511075278,
      "loss": 0.0741,
      "step": 1967
    },
    {
      "epoch": 0.009257081573327563,
      "grad_norm": 0.05261896178126335,
      "learning_rate": 0.0001986157081294143,
      "loss": 0.0032,
      "step": 1968
    },
    {
      "epoch": 0.009261785374940027,
      "grad_norm": 2.8311848640441895,
      "learning_rate": 0.00019861476515130083,
      "loss": 0.1522,
      "step": 1969
    },
    {
      "epoch": 0.009266489176552489,
      "grad_norm": 0.2605423629283905,
      "learning_rate": 0.00019861382217318738,
      "loss": 0.0167,
      "step": 1970
    },
    {
      "epoch": 0.009271192978164953,
      "grad_norm": 4.243223190307617,
      "learning_rate": 0.0001986128791950739,
      "loss": 0.5661,
      "step": 1971
    },
    {
      "epoch": 0.009275896779777417,
      "grad_norm": 8.250027656555176,
      "learning_rate": 0.00019861193621696042,
      "loss": 0.1956,
      "step": 1972
    },
    {
      "epoch": 0.009280600581389879,
      "grad_norm": 0.31811708211898804,
      "learning_rate": 0.00019861099323884693,
      "loss": 0.0167,
      "step": 1973
    },
    {
      "epoch": 0.009285304383002343,
      "grad_norm": 3.8845229148864746,
      "learning_rate": 0.00019861005026073345,
      "loss": 0.914,
      "step": 1974
    },
    {
      "epoch": 0.009290008184614805,
      "grad_norm": 4.219247817993164,
      "learning_rate": 0.00019860910728262,
      "loss": 0.4198,
      "step": 1975
    },
    {
      "epoch": 0.009294711986227269,
      "grad_norm": 1.6821366548538208,
      "learning_rate": 0.00019860816430450652,
      "loss": 0.1619,
      "step": 1976
    },
    {
      "epoch": 0.009299415787839733,
      "grad_norm": 5.881168842315674,
      "learning_rate": 0.00019860722132639304,
      "loss": 1.0253,
      "step": 1977
    },
    {
      "epoch": 0.009304119589452195,
      "grad_norm": 4.689562797546387,
      "learning_rate": 0.00019860627834827953,
      "loss": 0.4535,
      "step": 1978
    },
    {
      "epoch": 0.009308823391064659,
      "grad_norm": 0.8992546200752258,
      "learning_rate": 0.00019860533537016607,
      "loss": 0.0536,
      "step": 1979
    },
    {
      "epoch": 0.009313527192677121,
      "grad_norm": 4.680046558380127,
      "learning_rate": 0.0001986043923920526,
      "loss": 0.2149,
      "step": 1980
    },
    {
      "epoch": 0.009318230994289585,
      "grad_norm": 0.45089876651763916,
      "learning_rate": 0.0001986034494139391,
      "loss": 0.0226,
      "step": 1981
    },
    {
      "epoch": 0.009322934795902049,
      "grad_norm": 4.172562122344971,
      "learning_rate": 0.00019860250643582563,
      "loss": 0.3549,
      "step": 1982
    },
    {
      "epoch": 0.009327638597514511,
      "grad_norm": 1.5220776796340942,
      "learning_rate": 0.00019860156345771215,
      "loss": 0.1406,
      "step": 1983
    },
    {
      "epoch": 0.009332342399126975,
      "grad_norm": 1.0390819311141968,
      "learning_rate": 0.0001986006204795987,
      "loss": 0.0745,
      "step": 1984
    },
    {
      "epoch": 0.009337046200739437,
      "grad_norm": 2.36112380027771,
      "learning_rate": 0.0001985996775014852,
      "loss": 0.1766,
      "step": 1985
    },
    {
      "epoch": 0.009341750002351901,
      "grad_norm": 1.8418055772781372,
      "learning_rate": 0.00019859873452337173,
      "loss": 0.1275,
      "step": 1986
    },
    {
      "epoch": 0.009346453803964363,
      "grad_norm": 0.7968916893005371,
      "learning_rate": 0.00019859779154525825,
      "loss": 0.0577,
      "step": 1987
    },
    {
      "epoch": 0.009351157605576827,
      "grad_norm": 2.2047131061553955,
      "learning_rate": 0.00019859684856714477,
      "loss": 0.2483,
      "step": 1988
    },
    {
      "epoch": 0.009355861407189291,
      "grad_norm": 0.27032703161239624,
      "learning_rate": 0.00019859590558903129,
      "loss": 0.0188,
      "step": 1989
    },
    {
      "epoch": 0.009360565208801753,
      "grad_norm": 4.282246112823486,
      "learning_rate": 0.0001985949626109178,
      "loss": 0.535,
      "step": 1990
    },
    {
      "epoch": 0.009365269010414217,
      "grad_norm": 1.6889479160308838,
      "learning_rate": 0.00019859401963280432,
      "loss": 0.0856,
      "step": 1991
    },
    {
      "epoch": 0.00936997281202668,
      "grad_norm": 0.4769444763660431,
      "learning_rate": 0.00019859307665469084,
      "loss": 0.0297,
      "step": 1992
    },
    {
      "epoch": 0.009374676613639143,
      "grad_norm": 0.08098479360342026,
      "learning_rate": 0.0001985921336765774,
      "loss": 0.0049,
      "step": 1993
    },
    {
      "epoch": 0.009379380415251607,
      "grad_norm": 0.8298177719116211,
      "learning_rate": 0.0001985911906984639,
      "loss": 0.0597,
      "step": 1994
    },
    {
      "epoch": 0.00938408421686407,
      "grad_norm": 3.7495486736297607,
      "learning_rate": 0.00019859024772035043,
      "loss": 0.1914,
      "step": 1995
    },
    {
      "epoch": 0.009388788018476533,
      "grad_norm": 2.3238260746002197,
      "learning_rate": 0.00019858930474223694,
      "loss": 0.2834,
      "step": 1996
    },
    {
      "epoch": 0.009393491820088995,
      "grad_norm": 2.7130086421966553,
      "learning_rate": 0.00019858836176412346,
      "loss": 0.2436,
      "step": 1997
    },
    {
      "epoch": 0.00939819562170146,
      "grad_norm": 0.06514612585306168,
      "learning_rate": 0.00019858741878600998,
      "loss": 0.0027,
      "step": 1998
    },
    {
      "epoch": 0.009402899423313923,
      "grad_norm": 3.8861749172210693,
      "learning_rate": 0.0001985864758078965,
      "loss": 0.6673,
      "step": 1999
    },
    {
      "epoch": 0.009407603224926385,
      "grad_norm": 2.6497976779937744,
      "learning_rate": 0.00019858553282978302,
      "loss": 0.4135,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 212594,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1938620207100723e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
