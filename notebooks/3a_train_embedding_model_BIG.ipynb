{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:48:44.909822Z",
     "start_time": "2024-12-13T23:48:43.060121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import unicodedata\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "2b49b999c30e0322",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:48:44.913695Z",
     "start_time": "2024-12-13T23:48:44.911701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StarRatingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        # Convert label to a one-hot vector: label in {1,...,5} -> one-hot of length 5\n",
    "        y_onehot = torch.zeros(5)\n",
    "        y_onehot[self.y[idx]-1] = 1.0\n",
    "        return x, y_onehot"
   ],
   "id": "34f3ee189af5ac9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T23:48:48.427708Z",
     "start_time": "2024-12-13T23:48:44.960904Z"
    }
   },
   "source": [
    "# train_dataset = torch.load('../data/2_training_ready/embedding00/train_dataset00.pth')\n",
    "# test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset00.pth')\n",
    "# val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset00.pth')\n",
    "test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset01.pth')\n",
    "val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset01.pth')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52882/3211474504.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset01.pth')\n",
      "/tmp/ipykernel_52882/3211474504.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset01.pth')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-13T23:48:59.302222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training1 = torch.load(\"../data/2_training_ready/embedding00/train_dataset01.pth\")\n",
    "training2 = torch.load(\"../data/2_training_ready/embedding00/train_dataset02.pth\")\n",
    "training3 = torch.load(\"../data/2_training_ready/embedding00/train_dataset03.pth\")\n",
    "training4 = torch.load(\"../data/2_training_ready/embedding00/train_dataset04.pth\")\n",
    "\n",
    "# # Combine the embeddings and labels\n",
    "# combined_embeddings = torch.cat([training1.X, training2.X, training3.X,training4.X], dim=0)\n",
    "# combined_labels = torch.cat([training1.y, training2.y, training3.y,training4.y], dim=0)\n",
    "# # \n",
    "# # # Create a new EmbeddingDataset with the combined data\n",
    "# train_dataset = StarRatingDataset(combined_embeddings.numpy(), combined_labels.numpy())"
   ],
   "id": "eecef8c74bc63b08",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52882/3878769508.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training1 = torch.load(\"../data/2_training_ready/embedding00/train_dataset01.pth\")\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Concatenate all X values\n",
    "combined_X = np.concatenate([\n",
    "    training1.X,\n",
    "    training2.X,\n",
    "    training3.X,\n",
    "    training4.X\n",
    "], axis=0)\n",
    "\n",
    "# Concatenate all y values\n",
    "combined_y = np.concatenate([\n",
    "    training1.y,\n",
    "    training2.y,\n",
    "    training3.y,\n",
    "    training4.y\n",
    "], axis=0)\n",
    "\n",
    "# Create a new dataset with the combined data\n",
    "train_dataset = StarRatingDataset(combined_X, combined_y)"
   ],
   "id": "92a7b5b064752530",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_X_, _y_ = train_dataset[0]  # Get the first sample\n",
    "print(f\"Shape of X: {_X_.shape}\")\n",
    "print(f\"Shape of y: {_y_.shape if isinstance(_y_, torch.Tensor) else type(_y_)}\")"
   ],
   "id": "3dd4d2ce0552e938",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PORT CODE FROM 4a AND REPURPOSE",
   "id": "8f1caca25c25a906"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset.y = torch.tensor(test_dataset.y).long()  # Convert to tensor and then to long\n",
    "train_dataset.y = torch.tensor(train_dataset.y).long()  # Convert to tensor and then to long\n",
    "val_dataset.y = torch.tensor(val_dataset.y).long()  # Convert to tensor and then to long"
   ],
   "id": "63a6adffa5462814",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim=1_024, hidden_dim_1=512,hidden_dim_2=256, output_dim=5, dropout=0.1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.LayerNorm(hidden_dim_1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.LayerNorm(hidden_dim_2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim_2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "6aba68779b11128c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim=1_024, hidden_dim=512, output_dim=5, dropout=0.1):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.LayerNorm(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n"
   ],
   "id": "5d1aa18acf26aa90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up your dataset and dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "38140386644a77b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Convert dataset labels to integers (class indices)\n",
    "# train_dataset.y = train_dataset.y.long()  # Keep 1-based indexing for star ratings\n",
    "# test_dataset.y = test_dataset.y.long()  # Keep 1-based indexing for star ratings\n",
    "# val_dataset.y = val_dataset.y.long()"
   ],
   "id": "8e1f26b12211cd44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model, loss function, and optimizer\n",
    "# input_dim = train_dataset.X.shape[1]  # Number of features in the embeddings\n",
    "# output_dim = len(torch.unique(train_dataset.y))  # Number of classes"
   ],
   "id": "5bae832df55cab1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = SimpleNN().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "74e9b7bb9e5ccb32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.631)"
   ],
   "id": "20ccd8e25fa0ebd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.126)"
   ],
   "id": "8ac68b113ec6616e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# optimizer = optim.AdamW(model.parameters(), lr=1e-4)",
   "id": "bd3f44bbc802bbdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training and evaluation loops\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_classes = torch.max(y, 1)[1]  # Convert one-hot to class indices\n",
    "            correct += (predicted == true_classes).sum().item()  # Adjust prediction for 1-based indexing\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ],
   "id": "f127cd06d4197b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# num_epochs = 10\n",
    "num_epochs = 3\n",
    "# log_interval = 1_500\n",
    "# log_interval = 10_000\n",
    "log_interval = 500"
   ],
   "id": "a94b8d4853335439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "eval_accuracies = []\n",
    "learning_rates = []  # New list to track learning rates\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append((epoch, current_lr))\n",
    "    # learning_rates.append(current_lr)\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X, y in pbar:\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        # loss = criterion(outputs, y - 1)  # Shift labels for 0-based indexing during training\n",
    "        loss = criterion(outputs, y)  # Shift labels for 0-based indexing during training\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log progress every log_interval steps\n",
    "        if step % log_interval == 0:\n",
    "            eval_accuracy = evaluate(model, test_loader)\n",
    "            eval_accuracies.append((step, eval_accuracy))\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "id": "cad515c320f8aeef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training loss and evaluation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "steps, accuracies = zip(*eval_accuracies)\n",
    "plt.plot(steps, accuracies, label=\"Evaluation Accuracy\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Evaluation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "acc9890d58a99706",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs, lrs = zip(*learning_rates)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, lrs, 'b-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "2963da2a5ca17f28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ],
   "id": "74792d9826e8717e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8450b23925efedb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce533723c4cc9372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "correct_values = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in tqdm(val_loader, desc=\"Hit Rate Eval\", unit=\"batch\"):\n",
    "        batch_y = torch.argmax(batch_y, dim=1)\n",
    "\n",
    "        batch_x = batch_x.to(\"cuda\")\n",
    "        batch_y = batch_y.to(\"cuda\")\n",
    "\n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # Compute hit rate\n",
    "        y_pred_classes = torch.argmax(torch.softmax(pred, dim=1), dim=1)\n",
    "        # y_true_classes = torch.argmax(batch_y, dim=1) # TK TK ADD BACK FOR EMD\n",
    "\n",
    "        for i in [i.item() for i in torch.argmax(torch.softmax(pred, dim=1), dim=1)]:\n",
    "            predictions.append(i)\n",
    "        for i in [i.item() for i in batch_y]: # torch.argmax(batch_y, dim=1):\n",
    "            correct_values.append(i)"
   ],
   "id": "10bb2cad90164ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pl.DataFrame({\"true\": correct_values, \"predicted\": predictions})\n",
    "df.write_csv(\"../output/predictions/embedding.csv\")"
   ],
   "id": "8a19449b98a79cdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "dedbca24b6a2d56b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a6a858cbf8ef80ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a0c84e0d65597fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bef9434e0f0db3bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87faa00af53d1beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cb9c81482174cc8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "31ac919726cdd2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ebb1cd07555429e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "72d98d549de715e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8780dabec242b9fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8aa5fab6a96d68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f44698ce6c89e3bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7979f44e8c372042",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4bf4cdc89c4555bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8d3e30f93a38b605",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d51b9a1156ea1f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5505f6a96ef5f1ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "610e0f7603d997d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c2240c1a10244924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OG EMBEDDING (SMALL)",
   "id": "bb894e7684695e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ReviewRatingPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, output_dim=5, dropout_rate=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Size of the input embeddings.\n",
    "            hidden_dim (int): Number of units in the hidden layers.\n",
    "            output_dim (int): Number of output classes (e.g., star ratings 1-5).\n",
    "            dropout_rate (float): Dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super(ReviewRatingPredictor, self).__init__()\n",
    "        \n",
    "        # Fully connected layers with dropout for regularization\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = ReviewRatingPredictor(input_dim = train_dataset[0][0].shape[0])\n",
    "model = model.to(\"cuda\")"
   ],
   "id": "c3b58fbd76184256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # model def \n",
    "# class SimpleRegressor(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=128):\n",
    "#         super(SimpleRegressor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_dim, 5)  # 5 outputs for probabilities\n",
    "#         \n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)  # logits\n",
    "#         return out\n",
    "# \n",
    "# input_dim = train_dataset[0][0].shape[0]\n",
    "# model = SimpleRegressor(input_dim)\n",
    "# model = model.to(\"cuda\")  # or \"cuda\" if available"
   ],
   "id": "ee02a40ac7f2156f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # loss\n",
    "# def emd_loss(y_pred, y_true):\n",
    "#     # y_pred: (batch, 5) logits\n",
    "#     # y_true: (batch, 5) one-hot vectors\n",
    "#     p = torch.softmax(y_pred, dim=1)\n",
    "#     P = torch.cumsum(p, dim=1)\n",
    "#     Q = torch.cumsum(y_true, dim=1)\n",
    "#     emd = torch.sum(torch.abs(P - Q), dim=1)  # sum over the five dimensions\n",
    "#     return torch.mean(emd)"
   ],
   "id": "5a3c7a257f948cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "a4482be656c391ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_every = 100\n",
    "eval_for = 10"
   ],
   "id": "31408b365e7dd06f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 1  \n",
    "\n",
    "lst_train_loss = []\n",
    "lst_eval_loss = []\n",
    "lst_hit_rates = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    ix = 0\n",
    "    # for batch_x, batch_y in train_loader:\n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TK REMOVE FOR EMD\n",
    "        batch_y = torch.argmax(batch_y, dim=1) \n",
    "        \n",
    "        batch_x = batch_x.to(\"cuda\")\n",
    "        batch_y = batch_y.to(\"cuda\")\n",
    "        \n",
    "        pred = model(batch_x)\n",
    "        # loss = emd_loss(pred, batch_y)\n",
    "        loss = criterion(pred, batch_y)     \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * len(batch_x)\n",
    "        lst_train_loss.append(train_loss)\n",
    "        \n",
    "        ix = ix + 1\n",
    "        if ix % eval_every == 0:\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            model.eval()\n",
    "            test_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            eval_ix = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in test_loader:\n",
    "                    if eval_ix >= eval_for:\n",
    "                        break  # Stop after eval_for steps\n",
    "                    \n",
    "                    # TK REMOVE FOR EMD    \n",
    "                    batch_y = torch.argmax(batch_y, dim=1) \n",
    "                    \n",
    "                    batch_x = batch_x.to(\"cuda\")\n",
    "                    batch_y = batch_y.to(\"cuda\")\n",
    "                \n",
    "                    pred = model(batch_x)\n",
    "                    # loss = emd_loss(pred, batch_y)\n",
    "                    loss = criterion(pred, batch_y)                   \n",
    "                    test_loss += loss.item() * len(batch_x)\n",
    "            \n",
    "                    # Compute hit rate\n",
    "                    # y_pred_classes: argmax of predictions\n",
    "                    # y_true_classes: argmax of one-hot labels\n",
    "                    y_pred_classes = torch.argmax(torch.softmax(pred, dim=1), dim=1)\n",
    "                    # y_true_classes = torch.argmax(batch_y, dim=1) # TK TK ADD BACK FOR EMD\n",
    "            \n",
    "                    correct += (y_pred_classes == batch_y).sum().item()\n",
    "                    total += len(batch_x)\n",
    "                    \n",
    "                    eval_ix = eval_ix + 1\n",
    "                lst_eval_loss.append(test_loss)\n",
    "                hit_rate = correct / total\n",
    "                lst_hit_rates.append(hit_rate)"
   ],
   "id": "8233d1067ff3c162",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(lst_train_loss)",
   "id": "334029d757041785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(lst_eval_loss)",
   "id": "cc7520d357e59915",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(lst_hit_rates)",
   "id": "b0833eba5ebe333a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correct, total = 0, 0\n",
    "lst_hit_rates = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in tqdm(val_loader, desc=\"Hit Rate Eval\", unit=\"batch\"):\n",
    "        \n",
    "        batch_y = torch.argmax(batch_y, dim=1) \n",
    "        \n",
    "        batch_x = batch_x.to(\"cuda\")\n",
    "        batch_y = batch_y.to(\"cuda\")\n",
    "        \n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # Compute hit rate\n",
    "        y_pred_classes = torch.argmax(torch.softmax(pred, dim=1), dim=1)\n",
    "        # y_true_classes = torch.argmax(batch_y, dim=1) # TK TK ADD BACK FOR EMD\n",
    "\n",
    "        correct += (y_pred_classes == batch_y).sum().item()\n",
    "        total += len(batch_x)\n",
    "        \n",
    "    hit_rate = correct / total\n",
    "    lst_hit_rates.append(hit_rate)"
   ],
   "id": "149ba93e8f993386",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(lst_hit_rates[0])",
   "id": "28875f86050148b9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
