{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:51:58.673808Z",
     "start_time": "2024-12-14T00:51:56.861353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import unicodedata\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "2b49b999c30e0322",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:51:58.677746Z",
     "start_time": "2024-12-14T00:51:58.675908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class StarRatingDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        # Convert label to a one-hot vector: label in {1,...,5} -> one-hot of length 5\n",
    "        y_onehot = torch.zeros(5)\n",
    "        y_onehot[self.y[idx]-1] = 1.0\n",
    "        return x, y_onehot"
   ],
   "id": "34f3ee189af5ac9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:02.211100Z",
     "start_time": "2024-12-14T00:51:58.723526Z"
    }
   },
   "source": [
    "# train_dataset = torch.load('../data/2_training_ready/embedding00/train_dataset00.pth')\n",
    "# test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset00.pth')\n",
    "# val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset00.pth')\n",
    "test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset01.pth')\n",
    "val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset01.pth')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15150/3211474504.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dataset = torch.load('../data/2_training_ready/embedding00/test_dataset01.pth')\n",
      "/tmp/ipykernel_15150/3211474504.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  val_dataset = torch.load('../data/2_training_ready/embedding00/val_dataset01.pth')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:22.311842Z",
     "start_time": "2024-12-14T00:52:02.215793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training1 = torch.load(\"../data/2_training_ready/embedding00/train_dataset01.pth\")\n",
    "training2 = torch.load(\"../data/2_training_ready/embedding00/train_dataset02.pth\")\n",
    "training3 = torch.load(\"../data/2_training_ready/embedding00/train_dataset03.pth\")\n",
    "training4 = torch.load(\"../data/2_training_ready/embedding00/train_dataset04.pth\")\n",
    "\n",
    "# # Combine the embeddings and labels\n",
    "# combined_embeddings = torch.cat([training1.X, training2.X, training3.X,training4.X], dim=0)\n",
    "# combined_labels = torch.cat([training1.y, training2.y, training3.y,training4.y], dim=0)\n",
    "# # \n",
    "# # # Create a new EmbeddingDataset with the combined data\n",
    "# train_dataset = StarRatingDataset(combined_embeddings.numpy(), combined_labels.numpy())"
   ],
   "id": "eecef8c74bc63b08",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15150/3878769508.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training1 = torch.load(\"../data/2_training_ready/embedding00/train_dataset01.pth\")\n",
      "/tmp/ipykernel_15150/3878769508.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training2 = torch.load(\"../data/2_training_ready/embedding00/train_dataset02.pth\")\n",
      "/tmp/ipykernel_15150/3878769508.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training3 = torch.load(\"../data/2_training_ready/embedding00/train_dataset03.pth\")\n",
      "/tmp/ipykernel_15150/3878769508.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  training4 = torch.load(\"../data/2_training_ready/embedding00/train_dataset04.pth\")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:23.408539Z",
     "start_time": "2024-12-14T00:52:22.315534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concatenate all X values\n",
    "combined_X = np.concatenate([\n",
    "    training1.X,\n",
    "    training2.X,\n",
    "    training3.X,\n",
    "    training4.X\n",
    "], axis=0)\n",
    "\n",
    "# Concatenate all y values\n",
    "combined_y = np.concatenate([\n",
    "    training1.y,\n",
    "    training2.y,\n",
    "    training3.y,\n",
    "    training4.y\n",
    "], axis=0)\n",
    "\n",
    "# Create a new dataset with the combined data\n",
    "train_dataset = StarRatingDataset(combined_X, combined_y)"
   ],
   "id": "92a7b5b064752530",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:23.414658Z",
     "start_time": "2024-12-14T00:52:23.412198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_X_, _y_ = train_dataset[0]  # Get the first sample\n",
    "print(f\"Shape of X: {_X_.shape}\")\n",
    "print(f\"Shape of y: {_y_.shape if isinstance(_y_, torch.Tensor) else type(_y_)}\")"
   ],
   "id": "3dd4d2ce0552e938",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([1024])\n",
      "Shape of y: torch.Size([5])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PORT CODE FROM 4a AND REPURPOSE",
   "id": "8f1caca25c25a906"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:23.466292Z",
     "start_time": "2024-12-14T00:52:23.455277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_dataset.y = torch.tensor(test_dataset.y).long()  # Convert to tensor and then to long\n",
    "train_dataset.y = torch.tensor(train_dataset.y).long()  # Convert to tensor and then to long\n",
    "val_dataset.y = torch.tensor(val_dataset.y).long()  # Convert to tensor and then to long"
   ],
   "id": "63a6adffa5462814",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:23.518006Z",
     "start_time": "2024-12-14T00:52:23.516029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim=1_024, hidden_dim_1=512,hidden_dim_2=256, output_dim=5, dropout=0.1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim_1),\n",
    "            nn.LayerNorm(hidden_dim_1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.LayerNorm(hidden_dim_2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim_2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "6aba68779b11128c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:23.746702Z",
     "start_time": "2024-12-14T00:52:23.570661Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ba2518a53ae96930",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m total_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(p\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal number of parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_params\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim=1_024, hidden_dim=512, output_dim=5, dropout=0.1):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.LayerNorm(hidden_dim),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n"
   ],
   "id": "5d1aa18acf26aa90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:29.818888Z",
     "start_time": "2024-12-14T00:52:29.817139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up your dataset and dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "38140386644a77b9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Convert dataset labels to integers (class indices)\n",
    "# train_dataset.y = train_dataset.y.long()  # Keep 1-based indexing for star ratings\n",
    "# test_dataset.y = test_dataset.y.long()  # Keep 1-based indexing for star ratings\n",
    "# val_dataset.y = val_dataset.y.long()"
   ],
   "id": "8e1f26b12211cd44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model, loss function, and optimizer\n",
    "# input_dim = train_dataset.X.shape[1]  # Number of features in the embeddings\n",
    "# output_dim = len(torch.unique(train_dataset.y))  # Number of classes"
   ],
   "id": "5bae832df55cab1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:31.108565Z",
     "start_time": "2024-12-14T00:52:31.002883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = SimpleNN().to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "74e9b7bb9e5ccb32",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T00:52:33.307501Z",
     "start_time": "2024-12-14T00:52:33.305695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ],
   "id": "d89c8e98c0890188",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 658949\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.631)"
   ],
   "id": "20ccd8e25fa0ebd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.126)"
   ],
   "id": "8ac68b113ec6616e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# optimizer = optim.AdamW(model.parameters(), lr=1e-4)",
   "id": "bd3f44bbc802bbdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training and evaluation loops\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_classes = torch.max(y, 1)[1]  # Convert one-hot to class indices\n",
    "            correct += (predicted == true_classes).sum().item()  # Adjust prediction for 1-based indexing\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ],
   "id": "f127cd06d4197b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# num_epochs = 10\n",
    "num_epochs = 3\n",
    "# log_interval = 1_500\n",
    "# log_interval = 10_000\n",
    "log_interval = 500"
   ],
   "id": "a94b8d4853335439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "eval_accuracies = []\n",
    "learning_rates = []  # New list to track learning rates\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append((epoch, current_lr))\n",
    "    # learning_rates.append(current_lr)\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X, y in pbar:\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        # loss = criterion(outputs, y - 1)  # Shift labels for 0-based indexing during training\n",
    "        loss = criterion(outputs, y)  # Shift labels for 0-based indexing during training\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log progress every log_interval steps\n",
    "        if step % log_interval == 0:\n",
    "            eval_accuracy = evaluate(model, test_loader)\n",
    "            eval_accuracies.append((step, eval_accuracy))\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "id": "cad515c320f8aeef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training loss and evaluation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "steps, accuracies = zip(*eval_accuracies)\n",
    "plt.plot(steps, accuracies, label=\"Evaluation Accuracy\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Evaluation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "acc9890d58a99706",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs, lrs = zip(*learning_rates)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, lrs, 'b-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "2963da2a5ca17f28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8450b23925efedb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce533723c4cc9372",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "correct_values = []\n",
    "model.eval()\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in tqdm(val_loader, desc=\"Hit Rate Eval\", unit=\"batch\"):\n",
    "        batch_y = torch.argmax(batch_y, dim=1)\n",
    "\n",
    "        batch_x = batch_x.to(\"cuda\")\n",
    "        batch_y = batch_y.to(\"cuda\")\n",
    "\n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # Compute hit rate\n",
    "        y_pred_classes = torch.argmax(torch.softmax(pred, dim=1), dim=1)\n",
    "        # y_true_classes = torch.argmax(batch_y, dim=1) # TK TK ADD BACK FOR EMD\n",
    "\n",
    "        for i in [i.item() for i in torch.argmax(torch.softmax(pred, dim=1), dim=1)]:\n",
    "            predictions.append(i)\n",
    "        for i in [i.item() for i in batch_y]: # torch.argmax(batch_y, dim=1):\n",
    "            correct_values.append(i)"
   ],
   "id": "10bb2cad90164ab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fb88ba02dd360eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pl.DataFrame({\"true\": correct_values, \"predicted\": predictions})\n",
    "df.write_csv(\"../output/predictions/embedding_BIG.csv\")"
   ],
   "id": "8a19449b98a79cdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a6a858cbf8ef80ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a0c84e0d65597fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bef9434e0f0db3bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "87faa00af53d1beb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cb9c81482174cc8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "31ac919726cdd2ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ebb1cd07555429e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "72d98d549de715e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8780dabec242b9fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8aa5fab6a96d68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f44698ce6c89e3bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7979f44e8c372042",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4bf4cdc89c4555bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8d3e30f93a38b605",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
