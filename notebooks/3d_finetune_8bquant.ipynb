{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T13:55:15.336428Z",
     "start_time": "2024-12-11T13:55:13.232454Z"
    }
   },
   "source": [
    "# standard python imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "# huggingface libraries\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM # , setup_chat_format\n",
    "from trl.commands.cli import train\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import wasserstein_distance"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:22.189972Z",
     "start_time": "2024-12-09T20:05:21.274208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"optim00\",  # Change this to your project name\n",
    "    name=\"local_run_02\",\n",
    "    config={\n",
    "        \"model_name\": \"quant_for_local\",\n",
    "        \"task\": \"response_only\",\n",
    "        \"timestamp\": \"2024.11.18.18_02\"\n",
    "    }\n",
    ")"
   ],
   "id": "156269a09afdf314",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrichard-archer\u001B[0m (\u001B[33myale-som\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/richardarcher/Dropbox/main/2_courses/0_Optimization/final_project/notebooks/wandb/run-20241209_150521-lt8ead4c</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yale-som/optim00/runs/lt8ead4c' target=\"_blank\">local_run_02</a></strong> to <a href='https://wandb.ai/yale-som/optim00' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/yale-som/optim00' target=\"_blank\">https://wandb.ai/yale-som/optim00</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/yale-som/optim00/runs/lt8ead4c' target=\"_blank\">https://wandb.ai/yale-som/optim00/runs/lt8ead4c</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/yale-som/optim00/runs/lt8ead4c?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x743630873590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:22.233493Z",
     "start_time": "2024-12-09T20:05:22.231991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = \"/home/richardarcher/Dropbox/Sci24_LLM_Polarization/project_/weights_local/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\"\n",
    "new_model = \"../weights/sft/run00\"\n",
    "\n",
    "PATH_data_to_train_on = \"../data/1_train_test_split/df_train.csv\"\n",
    "PATH_data_to_test_on = \"../data/1_train_test_split/df_test.csv\""
   ],
   "id": "469706ea86e642d5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.103677Z",
     "start_time": "2024-12-09T20:05:22.278538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Match input dtype\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, quantization_config=nf4_config, device_map=\"auto\")"
   ],
   "id": "312adf3726bd82c3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "019d9e1bab7c48e6b2665cb549302363"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.306293Z",
     "start_time": "2024-12-09T20:05:27.108034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    tokenizer_file=os.path.join(base_model, 'tokenizer.json'),\n",
    "    tokenizer_config_file=os.path.join(base_model, 'tokenizer_config.json'),\n",
    "    special_tokens_map_file=os.path.join(base_model, 'special_tokens_map.json'),\n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "id": "e17b8080f21f9683",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.319120Z",
     "start_time": "2024-12-09T20:05:27.317655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.pad_token_id = 128004 # tokenizer.convert_tokens_to_ids(\"<|finetune_right_pad_id|>\")\n",
    "model.config.pad_token_id =128004 # tokenizer.convert_tokens_to_ids(\"<|finetune_right_pad_id|>\")"
   ],
   "id": "4ca6f41751ca076a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.390519Z",
     "start_time": "2024-12-09T20:05:27.358381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ],
   "id": "2c6bbbfacf2c5b52",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.406866Z",
     "start_time": "2024-12-09T20:05:27.402844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_trainable_params(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_percentage = 100 * trainable_params / total_params\n",
    "    print(f\"{trainable_percentage:.2f}% of parameters are trainable\")\n",
    "\n",
    "# Call the function after applying PEFT\n",
    "print_trainable_params(model)"
   ],
   "id": "ae2afcd12eca8ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04% of parameters are trainable\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### HANDLE DATA",
   "id": "1d54a2eaec3de099"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:27.502818Z",
     "start_time": "2024-12-09T20:05:27.501007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_prompt(review):\n",
    "    system_prompt = f\"You read Yelp reviews and return a number (1, 2, 3, 4, or 5) that represents your besst guess of the number of star ratings that were given by that reviewer. Return just the number 1, 2, 3, 4, or 5, with no context, explanation, or special symbols.\"\n",
    "    prompt = f\"Here is the review to evaluate: [[[{review}]]]. Remember, you read Yelp reviews and return a number (1, 2, 3, 4, or 5) that represents your besst guess of the number of star ratings that were given by that reviewer. Return just the number 1, 2, 3, 4, or 5, with no context, explanation, or special symbols.\"\n",
    "        \n",
    "    return system_prompt, prompt"
   ],
   "id": "d6d7f4bca77bbaa5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.060544Z",
     "start_time": "2024-12-09T20:05:27.544076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pl.read_csv(PATH_data_to_train_on)\n",
    "df_test = pl.read_csv(PATH_data_to_test_on)"
   ],
   "id": "8b26f1d61feb9f19",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TEMP: JUST TO SEE HOW IT GOES",
   "id": "5efccc6c6b495d78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.297772Z",
     "start_time": "2024-12-09T20:05:28.074902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = df_train.sample(n=100_000, seed=0)\n",
    "df_test = df_test.sample(n=10_000, seed=0)"
   ],
   "id": "802806e223adc313",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.467633Z",
     "start_time": "2024-12-09T20:05:28.311657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lst_system_prompt, lst_prompt = [], []\n",
    "for row in df_train.iter_rows(named=True):\n",
    "    system_prompt, prompt = create_prompt(row[\"text\"])\n",
    "    lst_system_prompt.append(system_prompt)\n",
    "    lst_prompt.append(prompt)\n",
    "df_train = df_train.with_columns(pl.Series(lst_system_prompt).alias(\"instruction\"), pl.Series(lst_prompt).alias(\"input\"))\n",
    "output = [int(i) for i in df_train[\"stars\"].to_list()]\n",
    "df_train = df_train.with_columns(pl.Series(output).alias(\"output\"))"
   ],
   "id": "c391c78b71897f83",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.511641Z",
     "start_time": "2024-12-09T20:05:28.486210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lst_system_prompt, lst_prompt = [], []\n",
    "for row in df_test.iter_rows(named=True):\n",
    "    system_prompt, prompt = create_prompt(row[\"text\"])\n",
    "    lst_system_prompt.append(system_prompt)\n",
    "    lst_prompt.append(prompt)\n",
    "df_test = df_test.with_columns(pl.Series(lst_system_prompt).alias(\"instruction\"), pl.Series(lst_prompt).alias(\"input\"))\n",
    "output = [int(i) for i in df_test[\"stars\"].to_list()]\n",
    "df_test = df_test.with_columns(pl.Series(output).alias(\"output\"))"
   ],
   "id": "66f427d7d91811a6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.864495Z",
     "start_time": "2024-12-09T20:05:28.527445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = Dataset.from_polars(df_train)\n",
    "test_dataset = Dataset.from_polars(df_test)"
   ],
   "id": "da436c05db659ac5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.880967Z",
     "start_time": "2024-12-09T20:05:28.879471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_dataset.shape[0]\n",
    "# test_dataset.shape[0]"
   ],
   "id": "8a644509c18391eb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.922507Z",
     "start_time": "2024-12-09T20:05:28.920527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def TO_GET_LEN(tokenizer):\n",
    "#     def TO_GET_LEN_INNER(row):\n",
    "#         row_json = [{\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "#                     {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "#                     {\"role\": \"assistant\", \"content\": row[\"output\"]}]\n",
    "# \n",
    "#         row[\"list_of_tokens\"] = tokenizer.apply_chat_template(row_json, tokenize=True)\n",
    "#         return row\n",
    "# \n",
    "#     return TO_GET_LEN_INNER\n",
    "# \n",
    "# train_dataset = train_dataset.map(\n",
    "#     TO_GET_LEN(tokenizer),\n",
    "# )\n",
    "# \n",
    "# test_dataset = test_dataset.map(\n",
    "#     TO_GET_LEN(tokenizer),\n",
    "# )\n",
    "# \n",
    "# \n",
    "# max_seq_length_needed1 = max(train_dataset.map(lambda x: {\"length\": len(x[\"list_of_tokens\"])})[\"length\"])+1\n",
    "# max_seq_length_needed2 = max(test_dataset.map(lambda x: {\"length\": len(x[\"list_of_tokens\"])})[\"length\"])+1\n",
    "# max_seq_length_needed=max(max_seq_length_needed1, max_seq_length_needed2)"
   ],
   "id": "454cf1376e36b4b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:28.964838Z",
     "start_time": "2024-12-09T20:05:28.963083Z"
    }
   },
   "cell_type": "code",
   "source": "# print(f\"{max_seq_length_needed=}\")",
   "id": "d331db457d3a3314",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:29.006637Z",
     "start_time": "2024-12-09T20:05:29.005024Z"
    }
   },
   "cell_type": "code",
   "source": "max_seq_length_needed = 1_638",
   "id": "aafe58c099feb52e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### max seq length needed for the train set: \n",
    "\n",
    "don't run the above again, takes too long "
   ],
   "id": "d7bd3f38e09ad74b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:29.049651Z",
     "start_time": "2024-12-09T20:05:29.046995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_but_not_tokenize(example):\n",
    "    test = example[\"instruction\"]\n",
    "    # assert isinstance(test, list), \"Input 'example' must be a list, this is probably because formatting function needs >1 eg\"\n",
    "    # assert not isinstance(test, str), \"Input 'example' must be a list, not a string\"\n",
    "    \n",
    "    output_texts = []\n",
    "    \n",
    "    if isinstance(test, list):\n",
    "        K_range = len(test)\n",
    "        \n",
    "        for i in range(K_range):\n",
    "            row_json = [{\"role\": \"system\", \"content\": example['instruction'][i]},\n",
    "                {\"role\": \"user\", \"content\": example['input'][i]},\n",
    "                {\"role\": \"assistant\", \"content\": example['output'][i]}]\n",
    "            text = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=False)\n",
    "            \n",
    "            output_texts.append(text)\n",
    "    \n",
    "    elif isinstance(test, str):\n",
    "        # K_range = 1\n",
    "        row_json = [{\"role\": \"system\", \"content\": example['instruction']},\n",
    "            {\"role\": \"user\", \"content\": example['input']},\n",
    "            {\"role\": \"assistant\", \"content\": example['output']}]\n",
    "        text = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=False)\n",
    "        \n",
    "        output_texts.append(text)\n",
    "    else:\n",
    "        assert False, \"ERROR: WHAT IS GOING INTO FORMAT_BUT_NOT_TOKENIZE???\" \n",
    "    \n",
    "    return output_texts\n",
    "\n",
    "# tests"
   ],
   "id": "b91d7b468d92a64a",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:29.092995Z",
     "start_time": "2024-12-09T20:05:29.090251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ],
   "id": "3a1366839d5254d4",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:29.136588Z",
     "start_time": "2024-12-09T20:05:29.133350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False  # Disable KV cache during training"
   ],
   "id": "f855b7330541850e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:29.192739Z",
     "start_time": "2024-12-09T20:05:29.177401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = SFTConfig(\n",
    "    max_seq_length=max_seq_length_needed,\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4, # 4\n",
    "    # optim=\"adamw_torch\",\n",
    "    # optim=\"paged_adamw_32bit\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    # TK TK OPTIM1\n",
    "    bf16=True, # was false\n",
    "    group_by_length=True,\n",
    "    # TK TK OPTIM2\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"local000\"\n",
    ")"
   ],
   "id": "c8f1949d6d84a50d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T20:05:40.133866Z",
     "start_time": "2024-12-09T20:05:29.222311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=format_but_not_tokenize,\n",
    "    data_collator=collator,\n",
    "    # ADDED THE BELOW IF IT BREAKS REMOVE IT OR FIX\n",
    "    # compute_metrics=custom_evals,  # Add this line\n",
    ")"
   ],
   "id": "abf9f4f861f3bc10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17a90059786a4a3aa85528e1f38f4b97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99b8ee05c8bc47d0a8b077458d888737"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:19:29.041990Z",
     "start_time": "2024-12-09T20:05:40.148902Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "be2cefe4682cb13b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1955' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1955/12500 1:13:29 < 6:36:47, 0.44 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py:2123\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2121\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2124\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py:2481\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2475\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2476\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2477\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2478\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2479\u001B[0m )\n\u001B[1;32m   2480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2481\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2483\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2484\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2485\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2486\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2487\u001B[0m ):\n\u001B[1;32m   2488\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2489\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py:3612\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m   3610\u001B[0m         scaled_loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m   3611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3612\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maccelerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3613\u001B[0m     \u001B[38;5;66;03m# Finally we need to normalize the loss for reporting\u001B[39;00m\n\u001B[1;32m   3614\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_items_in_batch \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/accelerate/accelerator.py:2241\u001B[0m, in \u001B[0;36mAccelerator.backward\u001B[0;34m(self, loss, **kwargs)\u001B[0m\n\u001B[1;32m   2239\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlomo_backward(loss, learning_rate)\n\u001B[1;32m   2240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2241\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    580\u001B[0m     )\n\u001B[0;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/gofaster00/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e87a82490a1f617b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "83e627f2ef8e8ccf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b8e55c8fbcf6e04b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T13:55:36.498096Z",
     "start_time": "2024-12-11T13:55:36.248902Z"
    }
   },
   "cell_type": "code",
   "source": "!ls ../weights/sft/run00/checkpoint-1000",
   "id": "91697e6288bfbf99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adapter_config.json\t   rng_state.pth\t    tokenizer.json\r\n",
      "adapter_model.safetensors  scheduler.pt\t\t    trainer_state.json\r\n",
      "optimizer.pt\t\t   special_tokens_map.json  training_args.bin\r\n",
      "README.md\t\t   tokenizer_config.json\r\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
