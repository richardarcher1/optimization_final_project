{
 "cells": [
  {
   "cell_type": "code",
   "id": "435963e08022eb6f",
   "metadata": {},
   "source": [
    "# standard python imports\n",
    "import os\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# huggingface libraries\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    # HfArgumentParser,\n",
    "    # TrainingArguments,\n",
    "    pipeline,\n",
    "    # logging,\n",
    "    LlamaForCausalLM\n",
    ")\n",
    "from peft import (\n",
    "#     LoraConfig,\n",
    "    PeftModel,\n",
    "#     prepare_model_for_kbit_training,\n",
    "#     get_peft_model,\n",
    ")\n",
    "# from datasets import load_dataset, Dataset\n",
    "# from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "# import wandb\n",
    "\n",
    "import polars as pl\n",
    "# import pandas as pd\n",
    "\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T15:02:11.042185Z",
     "start_time": "2024-12-10T15:02:11.040096Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 26,
   "source": "",
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:50.133688Z",
     "start_time": "2024-12-10T14:50:50.132213Z"
    }
   },
   "cell_type": "code",
   "source": "from torch.utils.data import Dataset",
   "id": "9d10465b3296410f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:50.251865Z",
     "start_time": "2024-12-10T14:50:50.250182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_prompt(review):\n",
    "    system_prompt = f\"You read Yelp reviews and return a number (1, 2, 3, 4, or 5) that represents your besst guess of the number of star ratings that were given by that reviewer. Return just the number 1, 2, 3, 4, or 5, with no context, explanation, or special symbols.\"\n",
    "    prompt = f\"Here is the review to evaluate: [[[{review}]]]. Remember, you read Yelp reviews and return a number (1, 2, 3, 4, or 5) that represents your besst guess of the number of star ratings that were given by that reviewer. Return just the number 1, 2, 3, 4, or 5, with no context, explanation, or special symbols.\"\n",
    "        \n",
    "    return system_prompt, prompt"
   ],
   "id": "1a69d01046c63b8f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:51.502537Z",
     "start_time": "2024-12-10T14:50:50.995634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = pl.read_csv(\"../data/1_train_test_split/df_train.csv\")\n",
    "df_test = pl.read_csv(\"../data/1_train_test_split/df_test.csv\")\n",
    "df_val = pl.read_csv(\"../data/1_train_test_split/df_validation.csv\")"
   ],
   "id": "dd217abf75548e67",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:53.222742Z",
     "start_time": "2024-12-10T14:50:53.220807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_prompts_to_df(df):\n",
    "    lst_system_prompt, lst_prompt = [], []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        system_prompt, prompt = create_prompt(row[\"text\"])\n",
    "        lst_system_prompt.append(system_prompt)\n",
    "        lst_prompt.append(prompt)\n",
    "    df = df.with_columns(pl.Series(lst_system_prompt).alias(\"system_prompt\"), pl.Series(lst_prompt).alias(\"prompt\"))\n",
    "    return df"
   ],
   "id": "4569f4f51c609b23",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:54.990570Z",
     "start_time": "2024-12-10T14:50:53.721882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train = add_prompts_to_df(df_train)\n",
    "df_test = add_prompts_to_df(df_test)\n",
    "df_val = add_prompts_to_df(df_val)"
   ],
   "id": "58e55d8b46dc33ac",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:55.946087Z",
     "start_time": "2024-12-10T14:50:55.944587Z"
    }
   },
   "cell_type": "code",
   "source": "base_model = \"/home/richardarcher/Dropbox/Sci24_LLM_Polarization/project_/weights_local/models--meta-llama--Meta-Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\"",
   "id": "d0637a7cfd7d8f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:50:56.636905Z",
     "start_time": "2024-12-10T14:50:56.463817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    tokenizer_file=os.path.join(base_model, 'tokenizer.json'),\n",
    "    tokenizer_config_file=os.path.join(base_model, 'tokenizer_config.json'),\n",
    "    special_tokens_map_file=os.path.join(base_model, 'special_tokens_map.json'),\n",
    "    trust_remote_code=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = 'left'"
   ],
   "id": "5b80609607e8f1f1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:51:02.182039Z",
     "start_time": "2024-12-10T14:50:57.603534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    # load_in_8bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Match input dtype\n",
    "\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(base_model, quantization_config=nf4_config)"
   ],
   "id": "b9ecd21975edface",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d95e756d56e43219ff117ffa9b7100d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:51:02.185239Z",
     "start_time": "2024-12-10T14:51:02.183781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not tokenizer.pad_token_id:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = model.config.eos_token_id"
   ],
   "id": "1a274ceb09ddb14f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rosetta stone! \n",
    "# with torch.no_grad():\n",
    "#     for row in df_val.iter_rows(named=True):\n",
    "#         \n",
    "#         message = [\n",
    "#             {\"role\": \"system\", \"content\": row[\"system_prompt\"]},\n",
    "#             {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "#         ]\n",
    "# \n",
    "#         inputs_message = tokenizer.apply_chat_template(message, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "# \n",
    "#         # outputs = model(inputs_message)\n",
    "#         outputs = model(\n",
    "#             # **inputs_message,\n",
    "#             inputs_message,\n",
    "#             output_hidden_states=True,\n",
    "#             return_dict=True\n",
    "#         )\n",
    "#         \n",
    "#         logits = outputs.logits          # shape: [batch_size, seq_len, vocab_size]\n",
    "#         next_token_logits = logits[0, -1, :]\n",
    "#         \n",
    "#         hidden_states = outputs.hidden_states # len: 33\n",
    "#         second_to_last_layer = hidden_states[-2]  # shape: batch_size, seq_len, 4096\n",
    "#         \n",
    "#         break"
   ],
   "id": "7bd605678dfead0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:51:15.575452Z",
     "start_time": "2024-12-10T14:51:15.573560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        # embeddings: list of numpy arrays or torch tensors\n",
    "        # labels: list of scalars\n",
    "        self.X = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float16)  # or long, depending on your task\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "629db8bb6067b468",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # RUNS BUT TOO SLOW \n",
    "# # ALSO ROSETTA STONE \n",
    "# embeddings = []\n",
    "# labels = []\n",
    "# \n",
    "# model.eval() # should be duplicative but just in case\n",
    "# with torch.no_grad():\n",
    "#     # for row in df_val.iter_rows(named=True):\n",
    "#     for row in tqdm(df_test.iter_rows(named=True), total=len(df_test)):\n",
    "#         message = [\n",
    "#             {\"role\": \"system\", \"content\": row[\"system_prompt\"]},\n",
    "#             {\"role\": \"user\", \"content\": row[\"prompt\"]},\n",
    "#         ]\n",
    "# \n",
    "#         inputs_message = tokenizer.apply_chat_template(\n",
    "#             message, \n",
    "#             add_generation_prompt=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         ).to(\"cuda\")\n",
    "# \n",
    "#         outputs = model(\n",
    "#             inputs_message,\n",
    "#             output_hidden_states=True,\n",
    "#             return_dict=True\n",
    "#         )\n",
    "#         \n",
    "#         # logits = outputs.logits          # shape: [batch_size, seq_len, vocab_size]\n",
    "#         # next_token_logits = logits[0, -1, :]\n",
    "# \n",
    "#         # Extract second-to-last layer hidden states\n",
    "#         # `hidden_states` is a tuple of length num_layers\n",
    "#         hidden_states = outputs.hidden_states \n",
    "#         # Typically: hidden_states[-2] shape: [batch_size, seq_len, hidden_dim]\n",
    "#         # If you only have a single example per batch (batch_size=1), you can do:\n",
    "#         embedding_vec = hidden_states[-2][0, -1, :].cpu().numpy()\n",
    "#         \n",
    "#         embeddings.append(embedding_vec)\n",
    "#         labels.append(row[\"stars\"])\n",
    "#         \n",
    "# # Convert to a Dataset\n",
    "# dataset = EmbeddingDataset(embeddings, labels)"
   ],
   "id": "4e8f2eabde697dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "28a368d8a27c976d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:59:28.259697Z",
     "start_time": "2024-12-10T14:59:28.257126Z"
    }
   },
   "cell_type": "code",
   "source": "df_test.shape",
   "id": "758ca690eb20a77e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148609, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:59:31.342409Z",
     "start_time": "2024-12-10T14:59:31.340220Z"
    }
   },
   "cell_type": "code",
   "source": "df_train.shape",
   "id": "540e0b60f27971e2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850373, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:59:37.371146Z",
     "start_time": "2024-12-10T14:59:37.369190Z"
    }
   },
   "cell_type": "code",
   "source": "df_val.shape",
   "id": "15fd1a5f40c946c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# this is the switch for which dataset is getting embedded",
   "id": "7e2fbf66b670b8ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:00:08.669978Z",
     "start_time": "2024-12-10T15:00:08.668524Z"
    }
   },
   "cell_type": "code",
   "source": "# df = df_val",
   "id": "bf1af97071387e4f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:32:50.365271Z",
     "start_time": "2024-12-10T15:32:50.363257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(0, 1_000, 4):\n",
    "    # print(i)\n",
    "    if i%(4*20)==0:\n",
    "        print(i)"
   ],
   "id": "e3f7f0b6d1c50a73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "80\n",
      "160\n",
      "240\n",
      "320\n",
      "400\n",
      "480\n",
      "560\n",
      "640\n",
      "720\n",
      "800\n",
      "880\n",
      "960\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:01:08.159248Z",
     "start_time": "2024-12-10T15:00:10.332258Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab806bf5d13c41bdb0ae6298fc675679"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17,
   "source": [
    "def df_to_dataset(df, batch_size = 4):\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    rows = df.to_dicts()  # returns a list of row dictionaries\n",
    "    with torch.no_grad():\n",
    "        # USE TQDM LOCAL OR THE IX ON THE CLUSTER\n",
    "        # for i in tqdm(range(0, len(df), batch_size)):\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            if i%(batch_size*20)==0:\n",
    "                print(f\"CURRENTLY OPERATING ON IX={i}/{len(df)}\")\n",
    "            batch_rows = rows[i : i + batch_size]\n",
    "        \n",
    "            # Prepare batched input\n",
    "            batch_messages = [\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": r[\"system_prompt\"]},\n",
    "                    {\"role\": \"user\", \"content\": r[\"prompt\"]}\n",
    "                ]\n",
    "                for r in batch_rows\n",
    "            ]\n",
    "        \n",
    "            # Tokenize the entire batch at once\n",
    "            inputs_message = tokenizer.apply_chat_template(\n",
    "                batch_messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(\"cuda\")\n",
    "        \n",
    "            # Single forward pass for the entire batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    inputs_message,\n",
    "                    output_hidden_states=True,\n",
    "                    return_dict=True\n",
    "                )\n",
    "        \n",
    "            # Extract embeddings for the entire batch at once\n",
    "            hidden_states = outputs.hidden_states\n",
    "            # hidden_states[-2].shape: [batch_size, seq_len, hidden_dim]\n",
    "            # We want the last token in seq_len dimension:\n",
    "            embeddings_batch = hidden_states[-2][:, -1, :].cpu().numpy()\n",
    "        \n",
    "            # Add them to a growing list\n",
    "            for j, r in enumerate(batch_rows):\n",
    "                embeddings.append(embeddings_batch[j])\n",
    "                labels.append(r[\"stars\"])\n",
    "        \n",
    "        # Convert to a Dataset\n",
    "        dataset = EmbeddingDataset(np.array(embeddings), labels)\n",
    "    return dataset\n"
   ],
   "id": "22a3c90dd955fb3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:02:36.566353Z",
     "start_time": "2024-12-10T15:02:36.562963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"NOW OPERATING ON VAL\")\n",
    "dataset_val = df_to_dataset(df_val, 4)\n",
    "print(\"NOW SAVING VAL\")\n",
    "torch.save(dataset_val,\"../data/2_training_ready/mymethod/take00/training.pt\")\n",
    "print(\"VAL SAVED\")\n",
    "dataset_test = df_to_dataset(df_test, 4)\n",
    "print(\"NOW SAVING TEST\")\n",
    "torch.save(dataset_test,\"../data/2_training_ready/mymethod/take00/val.pt\")\n",
    "print(\"TEST SAVED\")\n",
    "dataset_train = df_to_dataset(df_train, 4)\n",
    "print(\"NOW SAVING TRAIN\")\n",
    "torch.save(dataset_train,\"../data/2_training_ready/mymethod/take00/testing.pt\")\n",
    "print(\"TRAIN SAVED\")"
   ],
   "id": "b9505586220bfe28",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T15:04:50.968279Z",
     "start_time": "2024-12-10T15:04:50.954541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \n",
    "# \n"
   ],
   "id": "5e41403672817c0",
   "outputs": [],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
