{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import unicodedata\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "321c3b599e99dd96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        # embeddings: list of numpy arrays or torch tensors\n",
    "        # labels: list of scalars\n",
    "        self.X = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float16)  # or long, depending on your task\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "f30b5142bb7a35b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "val = torch.load(\"../data/2_training_ready/mymethod/take00/val.pt\")",
   "id": "c19e0b0656f086a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "testing = torch.load(\"../data/2_training_ready/mymethod/take00/testing.pt\")\n",
   "id": "a0c9404482ec7ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training1 = torch.load(\"../data/2_training_ready/mymethod/take00/training1.pt\")\n",
    "training2 = torch.load(\"../data/2_training_ready/mymethod/take00/training2.pt\")\n",
    "training3 = torch.load(\"../data/2_training_ready/mymethod/take00/training3.pt\")\n",
    "# training4 = torch.load(\"../data/2_training_ready/mymethod/take00/training4.pt\")"
   ],
   "id": "cc21c2155c3a0939",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Combine the embeddings and labels\n",
    "# combined_embeddings = torch.cat([training1.X, training2.X, training3.X,training4.X], dim=0)\n",
    "# combined_labels = torch.cat([training1.y, training2.y, training3.y,training4.y], dim=0)\n",
    "# \n",
    "# # Create a new EmbeddingDataset with the combined data\n",
    "# training = EmbeddingDataset(combined_embeddings.numpy(), combined_labels.numpy())"
   ],
   "id": "3fb7774148641387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine the embeddings and labels\n",
    "combined_embeddings = torch.cat([training1.X, training2.X, training3.X], dim=0)\n",
    "combined_labels = torch.cat([training1.y, training2.y, training3.y], dim=0)\n",
    "\n",
    "# Create a new EmbeddingDataset with the combined data\n",
    "training = EmbeddingDataset(combined_embeddings.numpy(), combined_labels.numpy())"
   ],
   "id": "3a23c2f701b5ea98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# OG \n",
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             # nn.ReLU(),\n",
    "#             # nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ],
   "id": "ee9a8b8aa4dd9fb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# class SimpleNN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(SimpleNN, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.1),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ],
   "id": "6bb0528c6ee70002",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim=4096, hidden_dim=2048, output_dim=5, dropout=0.1):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ],
   "id": "13b8aaea6dc402c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "e627965027b979bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up your dataset and dataloaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testing, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "53ae71a83b4f5d6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert dataset labels to integers (class indices)\n",
    "training.y = training.y.long()  # Keep 1-based indexing for star ratings\n",
    "testing.y = testing.y.long()  # Keep 1-based indexing for star ratings\n",
    "val.y = val.y.long()"
   ],
   "id": "c5e445345b66270",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define model, loss function, and optimizer\n",
    "input_dim = training.X.shape[1]  # Number of features in the embeddings\n",
    "hidden_dim = 2048\n",
    "output_dim = len(torch.unique(training.y))  # Number of classes\n",
    "\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim).to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.631)"
   ],
   "id": "f7ba23f94693ff07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training and evaluation loops\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            X = X.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted + 1 == y).sum().item()  # Adjust prediction for 1-based indexing\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n"
   ],
   "id": "da543e2e70d4c22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 10\n",
    "log_interval = 1_000"
   ],
   "id": "d6f1844af62c2d21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "step = 0\n",
    "\n",
    "train_losses = []\n",
    "eval_accuracies = []\n",
    "learning_rates = []  # New list to track learning rates\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    learning_rates.append((epoch, current_lr))\n",
    "    # learning_rates.append(current_lr)\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X, y in pbar:\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y - 1)  # Shift labels for 0-based indexing during training\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "        # Log progress every log_interval steps\n",
    "        if step % log_interval == 0:\n",
    "            eval_accuracy = evaluate(model, test_loader)\n",
    "            eval_accuracies.append((step, eval_accuracy))\n",
    "            print(f\"Step {step}, Loss: {loss.item():.4f}, Eval Accuracy: {eval_accuracy:.4f}\")\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    scheduler.step()\n",
    "\n"
   ],
   "id": "af019d7a8ce3fc35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot training loss and evaluation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "steps, accuracies = zip(*eval_accuracies)\n",
    "plt.plot(steps, accuracies, label=\"Evaluation Accuracy\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Evaluation Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "326b72659e895102",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs, lrs = zip(*learning_rates)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, lrs, 'b-')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "b2270458b47fdd03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n",
    "val_accuracy = evaluate(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")"
   ],
   "id": "6a8573cc9c04ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cf97d72c6af20ec8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6301dfee3aef6d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aae4c83faa5238c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0cf261934014d5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Gather all embeddings from the testing dataset into a single tensor\n",
    "# all_embeddings = []\n",
    "# for i in range(len(testing)):\n",
    "#     X, y = testing[i]  # X is embeddings, y is target\n",
    "#     # Ensure X is a tensor of shape [embedding_dim]\n",
    "#     # If not, you might need to reshape or extract the embedding part\n",
    "#     all_embeddings.append(X)\n",
    "# \n",
    "# test_embeddings = torch.stack(all_embeddings, dim=0)  # Shape: (N, D)\n",
    "# \n",
    "# # Compute per-dimension means and variances\n",
    "# dimension_means = test_embeddings.mean(dim=0)\n",
    "# dimension_vars = test_embeddings.var(dim=0, unbiased=False)\n",
    "# \n",
    "# print(\"Mean of means:\", dimension_means.mean().item())\n",
    "# print(\"Mean of variances:\", dimension_vars.mean().item())\n",
    "# \n",
    "# plt.figure(figsize=(10,4))\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.hist(dimension_means.cpu().numpy(), bins=50, color='skyblue', edgecolor='black')\n",
    "# plt.title(\"Distribution of Per-Dimension Means\")\n",
    "# plt.xlabel(\"Mean Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# \n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(dimension_vars.cpu().numpy(), bins=50, color='lightgreen', edgecolor='black')\n",
    "# plt.title(\"Distribution of Per-Dimension Variances\")\n",
    "# plt.xlabel(\"Variance Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ],
   "id": "22ec118080e0060f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
