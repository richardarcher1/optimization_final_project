Lmod Warning: Unknown Option: "silent"




Modules based on Lua: Version 8.7.37   2024-04-09 08:31 +00:00
    by Robert McLay mclay@tacc.utexas.edu

module [options] sub-command [args ...]

Help sub-commands:
------------------
  help                              prints this message
  help                module [...]  print help message from module(s)

Loading/Unloading sub-commands:
-------------------------------
  load | add          module [...]  load module(s)
  try-load | try-add  module [...]  Add module(s), do not complain if not
                                    found
  del | unload        module [...]  Remove module(s), do not complain if not
                                    found
  swap | sw | switch  m1 m2         unload m1 and load m2
  purge                             unload all modules
  refresh                           reload aliases from current list of
                                    modules.
  update                            reload all currently loaded modules.

Listing / Searching sub-commands:
---------------------------------
  list                              List loaded modules
  list                s1 s2 ...     List loaded modules that match the
                                    pattern
  avail | av                        List available modules
  avail | av          string        List available modules that contain
                                    "string".
  category | cat                    List all categories
  category | cat      s1 s2 ...     List all categories that match the
                                    pattern and display their modules
  overview | ov                     List all available modules by short
                                    names with number of versions
  overview | ov       string        List available modules by short names
                                    with number of versions that contain
                                    "string"
  spider                            List all possible modules
  spider              module        List all possible version of that module
                                    file
  spider              string        List all module that contain the
                                    "string".
  spider              name/version  Detailed information about that version
                                    of the module.
  whatis              module        Print whatis information about module
  keyword | key       string        Search all name and whatis that contain
                                    "string".

Searching with Lmod:
--------------------
  All searching (spider, list, avail, keyword) support regular expressions:
  

  -r spider           '^p'          Finds all the modules that start with
                                    `p' or `P'
  -r spider           mpi           Finds all modules that have "mpi" in
                                    their name.
  -r spider           'mpi$         Finds all modules that end with "mpi" in
                                    their name.

Handling a collection of modules:
--------------------------------
  save | s                          Save the current list of modules to a
                                    user defined "default" collection.
  save | s            name          Save the current list of modules to
                                    "name" collection.
  reset                             The same as "restore system"
  restore | r                       Restore modules from the user's
                                    "default" or system default.
  restore | r         name          Restore modules from "name" collection.
  restore             system        Restore module state to system defaults.
  savelist                          List of saved collections.
  describe | mcc      name          Describe the contents of a module
                                    collection.
  disable             name          Disable (i.e. remove) a collection.

Deprecated commands:
--------------------
  getdefault          [name]        load name collection of modules or
                                    user's "default" if no name given.
                                    ===> Use "restore" instead <====
  setdefault          [name]        Save current list of modules to name if
                                    given, otherwise save as the default
                                    list for you the user.
                                    ===> Use "save" instead. <====

Miscellaneous sub-commands:
---------------------------
  is-loaded           modulefile    return a true status if module is loaded
  is-avail            modulefile    return a true status if module can be
                                    loaded
  show                modulefile    show the commands in the module file.
  use [-a]            path          Prepend or Append path to MODULEPATH.
  unuse               path          remove path from MODULEPATH.
  tablelist                         output list of active modules as a lua
                                    table.

Important Environment Variables:
--------------------------------
  LMOD_COLORIZE                     If defined to be "YES" then Lmod prints
                                    properties and warning in color.

    --------------------------------------------------------------------------

Lmod Web Sites

  Documentation:    https://lmod.readthedocs.org
  GitHub:           https://github.com/TACC/Lmod
  SourceForge:      https://lmod.sf.net
  TACC Homepage:    https://www.tacc.utexas.edu/research-development/tacc-projects/lmod

  To report a bug please read https://lmod.readthedocs.io/en/latest/075_bug_reporting.html
    --------------------------------------------------------------------------


GPU CHECK
Wed Dec 11 11:03:21 2024       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Quadro RTX 8000                On  |   00000000:25:00.0 Off |                    0 |
| N/A   28C    P8             14W /  250W |       0MiB /  46080MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Quadro RTX 8000                On  |   00000000:81:00.0 Off |                    0 |
| N/A   24C    P8             13W /  250W |       0MiB /  46080MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Quadro RTX 8000                On  |   00000000:E2:00.0 Off |                    0 |
| N/A   25C    P8             15W /  250W |       0MiB /  46080MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
===============================================================================================================
PYTHON SCRIPT:
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: richard-archer (yale-som). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.6
wandb: Run data is saved locally in /gpfs/home/rka28/optimization_final_project/wandb/run-20241211_110329-lwfes4m4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cluster_run_00
wandb: ⭐️ View project at https://wandb.ai/yale-som/optim00
wandb: 🚀 View run at https://wandb.ai/yale-som/optim00/runs/lwfes4m4
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  2.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]
0.04% of parameters are trainable
1703936many parameters are trainable
Map:   0%|          | 0/100000 [00:00<?, ? examples/s]Map:   1%|          | 1000/100000 [00:00<00:33, 2946.52 examples/s]Map:   2%|▏         | 2000/100000 [00:00<00:24, 3949.58 examples/s]Map:   3%|▎         | 3000/100000 [00:00<00:21, 4426.83 examples/s]Map:   4%|▍         | 4000/100000 [00:00<00:20, 4750.72 examples/s]Map:   5%|▌         | 5000/100000 [00:01<00:19, 4971.52 examples/s]Map:   6%|▌         | 6000/100000 [00:01<00:18, 5098.17 examples/s]Map:   7%|▋         | 7000/100000 [00:01<00:17, 5205.50 examples/s]Map:   8%|▊         | 8000/100000 [00:01<00:17, 5311.79 examples/s]Map:   9%|▉         | 9000/100000 [00:01<00:16, 5409.08 examples/s]Map:  10%|█         | 10000/100000 [00:02<00:16, 5401.77 examples/s]Map:  11%|█         | 11000/100000 [00:02<00:16, 5429.45 examples/s]Map:  12%|█▏        | 12000/100000 [00:02<00:16, 5453.15 examples/s]Map:  13%|█▎        | 13000/100000 [00:02<00:15, 5464.74 examples/s]Map:  14%|█▍        | 14000/100000 [00:02<00:15, 5489.51 examples/s]Map:  15%|█▌        | 15000/100000 [00:02<00:15, 5509.84 examples/s]Map:  16%|█▌        | 16000/100000 [00:03<00:15, 5520.62 examples/s]Map:  17%|█▋        | 17000/100000 [00:03<00:14, 5538.65 examples/s]Map:  18%|█▊        | 18000/100000 [00:03<00:14, 5543.86 examples/s]Map:  19%|█▉        | 19000/100000 [00:03<00:14, 5494.55 examples/s]Map:  20%|██        | 20000/100000 [00:03<00:14, 5477.78 examples/s]Map:  21%|██        | 21000/100000 [00:04<00:14, 5414.74 examples/s]Map:  22%|██▏       | 22000/100000 [00:04<00:14, 5452.17 examples/s]Map:  23%|██▎       | 23000/100000 [00:04<00:14, 5489.56 examples/s]Map:  24%|██▍       | 24000/100000 [00:04<00:13, 5518.40 examples/s]Map:  25%|██▌       | 25000/100000 [00:04<00:13, 5531.11 examples/s]Map:  26%|██▌       | 26000/100000 [00:04<00:13, 5546.67 examples/s]Map:  27%|██▋       | 27000/100000 [00:05<00:13, 5545.77 examples/s]Map:  28%|██▊       | 28000/100000 [00:05<00:13, 5524.11 examples/s]Map:  29%|██▉       | 29000/100000 [00:05<00:12, 5519.50 examples/s]Map:  30%|███       | 30000/100000 [00:05<00:12, 5530.68 examples/s]Map:  31%|███       | 31000/100000 [00:05<00:12, 5511.96 examples/s]Map:  32%|███▏      | 32000/100000 [00:06<00:12, 5419.28 examples/s]Map:  33%|███▎      | 33000/100000 [00:06<00:12, 5457.36 examples/s]Map:  34%|███▍      | 34000/100000 [00:06<00:12, 5494.07 examples/s]Map:  35%|███▌      | 35000/100000 [00:06<00:11, 5520.61 examples/s]Map:  36%|███▌      | 36000/100000 [00:06<00:11, 5506.12 examples/s]Map:  37%|███▋      | 37000/100000 [00:07<00:13, 4529.44 examples/s]Map:  38%|███▊      | 38000/100000 [00:07<00:13, 4763.36 examples/s]Map:  39%|███▉      | 39000/100000 [00:07<00:12, 4967.42 examples/s]Map:  40%|████      | 40000/100000 [00:07<00:11, 5113.01 examples/s]Map:  41%|████      | 41000/100000 [00:07<00:11, 5218.20 examples/s]Map:  42%|████▏     | 42000/100000 [00:07<00:11, 5248.62 examples/s]Map:  43%|████▎     | 43000/100000 [00:08<00:10, 5318.23 examples/s]Map:  44%|████▍     | 44000/100000 [00:08<00:10, 5385.79 examples/s]Map:  45%|████▌     | 45000/100000 [00:08<00:10, 5434.10 examples/s]Map:  46%|████▌     | 46000/100000 [00:08<00:09, 5460.19 examples/s]Map:  47%|████▋     | 47000/100000 [00:08<00:09, 5486.89 examples/s]Map:  48%|████▊     | 48000/100000 [00:09<00:09, 5500.62 examples/s]Map:  49%|████▉     | 49000/100000 [00:09<00:09, 5488.63 examples/s]Map:  50%|█████     | 50000/100000 [00:09<00:09, 5500.60 examples/s]Map:  51%|█████     | 51000/100000 [00:09<00:08, 5484.71 examples/s]Map:  52%|█████▏    | 52000/100000 [00:09<00:08, 5484.08 examples/s]Map:  53%|█████▎    | 53000/100000 [00:09<00:08, 5453.47 examples/s]Map:  54%|█████▍    | 54000/100000 [00:10<00:08, 5474.77 examples/s]Map:  55%|█████▌    | 55000/100000 [00:10<00:08, 5476.50 examples/s]Map:  56%|█████▌    | 56000/100000 [00:10<00:08, 5453.00 examples/s]Map:  57%|█████▋    | 57000/100000 [00:10<00:07, 5472.02 examples/s]Map:  58%|█████▊    | 58000/100000 [00:10<00:07, 5493.81 examples/s]Map:  59%|█████▉    | 59000/100000 [00:11<00:07, 5482.06 examples/s]Map:  60%|██████    | 60000/100000 [00:11<00:07, 5508.49 examples/s]Map:  61%|██████    | 61000/100000 [00:11<00:07, 5441.99 examples/s]Map:  62%|██████▏   | 62000/100000 [00:11<00:06, 5446.37 examples/s]Map:  63%|██████▎   | 63000/100000 [00:11<00:06, 5477.26 examples/s]Map:  64%|██████▍   | 64000/100000 [00:11<00:06, 5471.01 examples/s]Map:  65%|██████▌   | 65000/100000 [00:12<00:06, 5488.77 examples/s]Map:  66%|██████▌   | 66000/100000 [00:12<00:06, 5487.79 examples/s]Map:  67%|██████▋   | 67000/100000 [00:12<00:06, 5476.87 examples/s]Map:  68%|██████▊   | 68000/100000 [00:12<00:05, 5488.93 examples/s]Map:  69%|██████▉   | 69000/100000 [00:12<00:05, 5474.84 examples/s]Map:  70%|███████   | 70000/100000 [00:13<00:05, 5449.55 examples/s]Map:  71%|███████   | 71000/100000 [00:13<00:05, 5469.86 examples/s]Map:  72%|███████▏  | 72000/100000 [00:13<00:05, 5502.09 examples/s]Map:  73%|███████▎  | 73000/100000 [00:13<00:04, 5489.16 examples/s]Map:  74%|███████▍  | 74000/100000 [00:13<00:04, 5487.58 examples/s]Map:  75%|███████▌  | 75000/100000 [00:13<00:04, 5494.10 examples/s]Map:  76%|███████▌  | 76000/100000 [00:14<00:05, 4347.61 examples/s]Map:  77%|███████▋  | 77000/100000 [00:14<00:04, 4638.31 examples/s]Map:  78%|███████▊  | 78000/100000 [00:14<00:04, 4851.83 examples/s]Map:  79%|███████▉  | 79000/100000 [00:14<00:04, 4979.49 examples/s]Map:  80%|████████  | 80000/100000 [00:15<00:03, 5129.96 examples/s]Map:  81%|████████  | 81000/100000 [00:15<00:03, 5235.09 examples/s]Map:  82%|████████▏ | 82000/100000 [00:15<00:03, 5301.26 examples/s]Map:  83%|████████▎ | 83000/100000 [00:15<00:03, 5178.42 examples/s]Map:  84%|████████▍ | 84000/100000 [00:15<00:03, 5259.86 examples/s]Map:  85%|████████▌ | 85000/100000 [00:15<00:02, 5308.20 examples/s]Map:  86%|████████▌ | 86000/100000 [00:16<00:02, 5383.79 examples/s]Map:  87%|████████▋ | 87000/100000 [00:16<00:02, 5397.63 examples/s]Map:  88%|████████▊ | 88000/100000 [00:16<00:02, 5383.13 examples/s]Map:  89%|████████▉ | 89000/100000 [00:16<00:02, 5435.16 examples/s]Map:  90%|█████████ | 90000/100000 [00:16<00:01, 5482.85 examples/s]Map:  91%|█████████ | 91000/100000 [00:17<00:01, 5448.81 examples/s]Map:  92%|█████████▏| 92000/100000 [00:17<00:01, 5458.31 examples/s]Map:  93%|█████████▎| 93000/100000 [00:17<00:01, 5477.35 examples/s]Map:  94%|█████████▍| 94000/100000 [00:17<00:01, 5489.85 examples/s]Map:  95%|█████████▌| 95000/100000 [00:17<00:00, 5486.61 examples/s]Map:  96%|█████████▌| 96000/100000 [00:17<00:00, 5511.26 examples/s]Map:  97%|█████████▋| 97000/100000 [00:18<00:00, 5491.96 examples/s]Map:  98%|█████████▊| 98000/100000 [00:18<00:00, 5507.09 examples/s]Map:  99%|█████████▉| 99000/100000 [00:18<00:00, 5478.49 examples/s]Map: 100%|██████████| 100000/100000 [00:18<00:00, 5484.02 examples/s]Map: 100%|██████████| 100000/100000 [00:18<00:00, 5338.94 examples/s]
Map:   0%|          | 0/10000 [00:00<?, ? examples/s]Map:  10%|█         | 1000/10000 [00:00<00:01, 5499.38 examples/s]Map:  20%|██        | 2000/10000 [00:00<00:01, 5504.75 examples/s]Map:  30%|███       | 3000/10000 [00:00<00:01, 5458.65 examples/s]Map:  40%|████      | 4000/10000 [00:00<00:01, 5468.88 examples/s]Map:  50%|█████     | 5000/10000 [00:00<00:00, 5465.48 examples/s]Map:  60%|██████    | 6000/10000 [00:01<00:00, 5456.19 examples/s]Map:  70%|███████   | 7000/10000 [00:01<00:00, 5476.59 examples/s]Map:  80%|████████  | 8000/10000 [00:01<00:00, 5457.86 examples/s]Map:  90%|█████████ | 9000/10000 [00:01<00:00, 5498.86 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 4420.14 examples/s]Map: 100%|██████████| 10000/10000 [00:01<00:00, 5067.46 examples/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[2024-12-11 11:04:11,251] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
  0%|          | 0/12500 [00:00<?, ?it/s]/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 8000 does not support bfloat16 compilation natively, skipping
  warnings.warn(
/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: Quadro RTX 8000 does not support bfloat16 compilation natively, skipping
  warnings.warn(
Traceback (most recent call last):
  File "/gpfs/home/rka28/optimization_final_project/code/3_llama_finetune.py", line 217, in <module>
    main()
  File "/gpfs/home/rka28/optimization_final_project/code/3_llama_finetune.py", line 212, in main
    trainer.train()
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 465, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/peft/peft_model.py", line 1642, in forward
    with self._enable_peft_forward_hooks(**kwargs):
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/peft/peft_model.py", line 1644, in torch_dynamo_resume_in_forward_at_1642
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1135, in forward
    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in torch_dynamo_resume_in_forward_at_1190
    loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **loss_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1269, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 1064, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 526, in __call__
    return _compile(
           ^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 924, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 666, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_utils_internal.py", line 87, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 699, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py", line 1322, in transform_code_object
    transformations(instructions, code_options)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 219, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py", line 634, in transform
    tracer.run()
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2796, in run
    super().run()
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():
          ^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 582, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 1680, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 830, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py", line 324, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/functions.py", line 111, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 836, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3011, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 3139, in inline_call_
    tracer.run()
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():
          ^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 2220, in BINARY_OP
    return _binary_op_lookup[inst.arg](self, inst)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py", line 301, in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/builtin.py", line 967, in call_function
    return handler(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/builtin.py", line 943, in _handle_insert_op_in_graph
    return wrap_fx_proxy(tx, proxy)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py", line 2037, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/variables/builder.py", line 2124, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2082, in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2017, in get_fake_value
    ret_val = wrap_fake_exception(
              ^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 1574, in wrap_fake_exception
    return fn()
           ^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2018, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2150, in run_node
    raise RuntimeError(make_error_message(e)).with_traceback(
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_dynamo/utils.py", line 2132, in run_node
    return node.target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/utils/_stats.py", line 21, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1238, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1692, in dispatch
    return self._cached_dispatch_impl(func, types, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 1339, in _cached_dispatch_impl
    output = self._dispatch_impl(func, types, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2021, in _dispatch_impl
    self.wrap_meta_outputs_with_default_device_logic(
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2143, in wrap_meta_outputs_with_default_device_logic
    return tree_map(wrap, r)
           ^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/utils/_pytree.py", line 964, in tree_map
    return treespec.unflatten(map(func, *flat_args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/utils/_pytree.py", line 803, in unflatten
    leaves = list(leaves)
             ^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 2121, in wrap
    ) = FakeTensor._find_common_device(func, flat_args)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 872, in _find_common_device
    merge_devices(arg)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/torch/_subclasses/fake_tensor.py", line 867, in merge_devices
    raise RuntimeError(
torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in function truediv>(*(FakeTensor(..., device='cuda:2', size=(), grad_fn=<NllLossBackward0>), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)), **{}):
Unhandled FakeTensor Device Propagation for aten.div.Tensor, found two different devices cuda:2, cuda:0

from user code:
   File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 46, in ForCausalLMLoss
    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/transformers/loss/loss_utils.py", line 28, in fixed_cross_entropy
    loss = loss / num_items_in_batch

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

[1;34mwandb[0m: 🚀 View run [33mcluster_run_00[0m at: [34mhttps://wandb.ai/yale-som/optim00/runs/lwfes4m4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20241211_110329-lwfes4m4/logs[0m
Traceback (most recent call last):
  File "/gpfs/home/rka28/.conda/envs/gofaster00/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1168, in launch_command
    simple_launcher(args)
  File "/gpfs/home/rka28/.conda/envs/gofaster00/lib/python3.12/site-packages/accelerate/commands/launch.py", line 763, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/gpfs/home/rka28/.conda/envs/gofaster00/bin/python3.12', 'code/3_llama_finetune.py']' returned non-zero exit status 1.
python path executed
SBATCH FINISHED
